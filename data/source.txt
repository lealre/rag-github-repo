Directory structure:
└── lvgalvao-data-engineering-roadmap/
    ├── README.md
    ├── 00-automacao-data-qualiy-excel-etl/
    │   ├── README.md
    │   ├── mkdocs.yml
    │   ├── pyproject.toml
    │   ├── requirements.txt
    │   ├── .python-version
    │   ├── data/
    │   ├── docs/
    │   │   ├── app.md
    │   │   ├── backend.md
    │   │   ├── frontend.md
    │   │   ├── index.md
    │   │   └── pics/
    │   ├── pics/
    │   ├── src/
    │   │   ├── app.py
    │   │   ├── backend.py
    │   │   ├── contrato.py
    │   │   └── frontend.py
    │   └── tests/
    │       └── test_app.py
    ├── 01-como-estruturar-projetos-e-processos-de-dados-do-zero/
    │   ├── README.md
    │   ├── exemplo.env
    │   ├── meus_logs.txt
    │   ├── mkdocs.yml
    │   ├── pyproject.toml
    │   ├── requirements.txt
    │   ├── .python-version
    │   ├── data/
    │   │   ├── correto.xlsx
    │   │   ├── correto_v2.xlsx
    │   │   ├── errado.xlsx
    │   │   └── multiplos_erros.xlsx
    │   ├── docs/
    │   │   └── index.md
    │   ├── sql/
    │   │   ├── create.sql
    │   │   └── insert.sql
    │   ├── src/
    │   │   ├── app.py
    │   │   ├── backend.py
    │   │   ├── contrato.py
    │   │   └── frontend.py
    │   ├── tests/
    │   │   ├── __init__.py
    │   │   ├── test_app.py
    │   │   ├── test_integration.py
    │   │   └── test_unit.py
    │   ├── .github/
    │   │   └── workflows/
    │   │       └── ci.yml
    │   └── .vscode/
    ├── 02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/
    │   ├── README.md
    │   ├── inferred_schema.xls
    │   ├── main.py
    │   ├── mkdocs.yml
    │   ├── my_duckdb.db
    │   ├── pyproject.toml
    │   ├── schema_crm.py
    │   ├── .python-version
    │   ├── app/
    │   │   ├── __init__.py
    │   │   ├── etl.py
    │   │   ├── etl_infer_schema.py
    │   │   ├── ler_duckdb.py
    │   │   ├── schema.py
    │   │   └── schema_email.py
    │   ├── docs/
    │   │   └── index.md
    │   ├── exemplo/
    │   │   ├── exemplo00.py
    │   │   ├── exemplo01.py
    │   │   └── exemplo02.py
    │   ├── pic/
    │   ├── sql/
    │   │   ├── create_table_produtos_bronze.sql
    │   │   ├── create_table_produtos_bronze_email.sql
    │   │   ├── insert_into_tabela_bronze.sql
    │   │   ├── insert_into_tabela_bronze_email.sql
    │   │   └── insert_wrong_values_into_tabela_bronze.sql
    │   ├── tests/
    │   │   ├── __init__.py
    │   │   └── test_func_etl.py
    │   └── .github/
    │       └── workflows/
    │           └── CI.yml
    ├── 03-deploy-de-apps-dados-com-docker/
    │   ├── README.md
    │   ├── README copy.md
    │   ├── pyproject.toml
    │   ├── .python-version
    │   ├── pics/
    │   └── src/
    │       ├── collector/
    │       │   ├── Dockerfile
    │       │   └── requirements.txt
    │       └── dashboard/
    │           ├── Dockerfile
    │           ├── app.py
    │           └── requirements.txt
    ├── 04-workflow-orchestration-deploy-airflow/
    │   ├── README.md
    │   ├── exemplo_00.py
    │   ├── exemplo_01.py
    │   ├── exemplo_02/
    │   │   ├── exemplo_02.py
    │   │   └── requirements.txt
    │   ├── exemplo_03/
    │   │   ├── exemplo_03.py
    │   │   ├── requirements.txt
    │   │   ├── pics/
    │   │   └── pipeline/
    │   │       └── pipeline.py
    │   ├── exemplo_04/
    │   │   ├── primeira_dag_com_python_operator.py
    │   │   ├── quarta_dag_com_python_operator.py
    │   │   ├── quinta_dag_com_python_operator.py
    │   │   ├── segunda_dag_com_python_operator.py
    │   │   └── terceira_dag_com_python_operator.py
    │   ├── exemplo_05/
    │   │   ├── pegar_um_personagem.py
    │   │   └── pegar_um_personagem_airflow.py
    │   ├── exemplo_06/
    │   │   ├── controller.py
    │   │   ├── db.py
    │   │   ├── main.py
    │   │   ├── models.py
    │   │   ├── schema.py
    │   │   └── final/
    │   │       ├── capturar_pokemon.py
    │   │       ├── controller.py
    │   │       ├── db.py
    │   │       ├── models.py
    │   │       └── schema.py
    │   ├── exemplo_07/
    │   │   ├── README.md
    │   │   ├── Dockerfile
    │   │   ├── packages.txt
    │   │   ├── requirements.txt
    │   │   ├── .env_example
    │   │   ├── dags/
    │   │   │   └── duckdb_custom_operator_example.py
    │   │   └── include/
    │   │       └── custom_operators/
    │   │           └── postgres_to_duckdb_operator.py
    │   └── pic/
    ├── 05-redis-mongodb-esse-tal-de-nosql/
    │   ├── README.md
    │   ├── pyproject.toml
    │   ├── .env-example
    │   ├── docker/
    │   │   └── docker-compose.yml
    │   └── src/
    │       ├── start.py
    │       ├── browser/
    │       │   ├── b_ml_simple.py
    │       │   ├── generic_crawler.py
    │       │   ├── crawlers/
    │       │   │   └── default_crawler.py
    │       │   └── provider/
    │       │       ├── __init__.py
    │       │       ├── generic_b_crawler.py
    │       │       └── actions/
    │       │           ├── __init__.py
    │       │           └── dict.py
    │       ├── mitm/
    │       │   ├── __init__,py
    │       │   ├── start.py
    │       │   ├── addon/
    │       │   │   ├── __init__.py
    │       │   │   └── proxy_controller.py
    │       │   └── test/
    │       │       ├── download.py
    │       │       ├── ip.py
    │       │       └── mitmproxy-ca-cert.pem
    │       ├── request/
    │       │   ├── generic_crawler.py
    │       │   ├── r_ml_simple.py
    │       │   └── crawlers/
    │       │       ├── __init__.py
    │       │       └── default_crawler.py
    │       └── tools/
    │           ├── __init__.py
    │           ├── mongodb.py
    │           └── redis.py
    ├── 06-restAPI-fastAPI-deploy/
    │   ├── primeiro-dia/
    │   │   ├── README.md
    │   │   ├── exemplo_00.py
    │   │   ├── exemplo_01_a.py
    │   │   ├── exemplo_01_b.py
    │   │   ├── exemplo_02_pid_e_porta.py
    │   │   ├── exemplo_03_a_usando_api.py
    │   │   ├── exemplo_03_b_usando_cliente.py
    │   │   ├── exemplo_04_decorador_a.py
    │   │   ├── exemplo_04_decorador_b.py
    │   │   ├── exemplo_05_diferentes_tipos_de_respostas.py
    │   │   ├── api_modelo_ml/
    │   │   │   ├── exemplo_00_func.py
    │   │   │   ├── exemplo_01_api.py
    │   │   │   ├── exemplo_02_api_request_response.py
    │   │   │   ├── exemplo_03_banco.py
    │   │   │   ├── modelo.py
    │   │   │   ├── modelo_casas.pkl
    │   │   │   ├── requirements.txt
    │   │   │   └── src/
    │   │   │       ├── Dockerfile-backend
    │   │   │       ├── Dockerfile-frontend
    │   │   │       ├── docker-compose.yml
    │   │   │       ├── requirements-backend.txt
    │   │   │       ├── requirements-frontend.txt
    │   │   │       └── src/
    │   │   │           ├── app.py
    │   │   │           ├── frontend.py
    │   │   │           ├── modelo.py
    │   │   │           ├── modelo_casas.pkl
    │   │   │           └── __pycache__/
    │   │   ├── api_openapi/
    │   │   │   ├── README.md
    │   │   │   ├── exemplo_01.py
    │   │   │   ├── exemplo_02.py
    │   │   │   ├── exemplo_03.py
    │   │   │   └── exemplo_04.py
    │   │   ├── async/
    │   │   │   ├── main_asy.py
    │   │   │   └── main_sync.py
    │   │   ├── response_json_html_xml/
    │   │   │   ├── readme.md
    │   │   │   └── multiple.py
    │   │   └── rinha_backend/
    │   │       ├── README.md
    │   │       ├── Dockerfile
    │   │       ├── arquivo.txt
    │   │       ├── docker-compose.yml
    │   │       ├── nginx.conf
    │   │       ├── pyproject.toml
    │   │       ├── requirements.txt
    │   │       ├── script.sql
    │   │       ├── .python-version
    │   │       ├── app/
    │   │       │   ├── database.py
    │   │       │   ├── main.py
    │   │       │   ├── models.py
    │   │       │   ├── schemas.py
    │   │       │   └── __pycache__/
    │   │       └── tests/
    │   │           └── __init__.py
    │   └── segundo/
    │       ├── fabio-api-crud/
    │       │   ├── README.md
    │       │   ├── docker-compose.yml
    │       │   ├── pyproject.toml
    │       │   └── src/
    │       │       ├── __init__.py
    │       │       ├── __pycache__/
    │       │       ├── controller/
    │       │       │   ├── user.py
    │       │       │   └── __pycache__/
    │       │       ├── db/
    │       │       │   ├── connection.py
    │       │       │   └── __pycache__/
    │       │       ├── models/
    │       │       │   ├── user.py
    │       │       │   └── __pycache__/
    │       │       └── routes/
    │       │           ├── __init__.py
    │       │           ├── user.py
    │       │           └── __pycache__/
    │       ├── kaio-auth-fastapi-docker/
    │       │   ├── README.md
    │       │   ├── Dockerfile
    │       │   ├── Makefile
    │       │   ├── docker-compose.yml
    │       │   ├── mkdocs.yml
    │       │   ├── pyproject.toml
    │       │   ├── .dockerignore
    │       │   ├── .env.example
    │       │   ├── .flake8
    │       │   ├── .pre-commit-config.yaml
    │       │   ├── .python-version
    │       │   ├── database/
    │       │   │   └── .gitkeep
    │       │   ├── docs/
    │       │   │   └── index.md
    │       │   ├── src/
    │       │   │   ├── __init__.py
    │       │   │   ├── main.py
    │       │   │   └── core/
    │       │   │       ├── __init__.py
    │       │   │       ├── app_log.py
    │       │   │       ├── config.py
    │       │   │       ├── crud.py
    │       │   │       ├── db.py
    │       │   │       ├── deps.py
    │       │   │       ├── models.py
    │       │   │       ├── schemas.py
    │       │   │       └── security.py
    │       │   └── tests/
    │       │       └── __init__.py
    │       └── luciano-starter/
    │           ├── README.md
    │           ├── main.py
    │           ├── old.py
    │           ├── programa_a.py
    │           ├── programa_b.py
    │           ├── programa_c.py
    │           ├── programa_d.py
    │           ├── requirements.txt
    │           └── recursos/
    │               └── arquivo.txt
    ├── 07-amazon-sns-sqs/
    │   └── README.md
    ├── 08-kafka-pubsub-streaming/
    │   ├── README.md
    │   ├── docker-compose.yml
    │   ├── pyproject.toml
    │   ├── .python-version
    │   ├── kafka-brokers/
    │   │   └── README.md
    │   ├── kafka-consumers/
    │   │   ├── README.md
    │   │   ├── kafka_consumers.py
    │   │   └── kafka_consumers_02.py
    │   ├── kafka-demo/
    │   │   └── README.md
    │   ├── kafka-ecosystem/
    │   │   ├── README.md
    │   │   ├── kafka_eco.webp
    │   │   ├── kafka-connect/
    │   │   │   ├── README.md
    │   │   │   └── kafka_consumer_connect.py
    │   │   ├── ksqlDB/
    │   │   │   └── README.md
    │   │   └── schema_registry/
    │   │       └── README.md
    │   ├── kafka-marketing-project/
    │   │   ├── README.md
    │   │   ├── docker-compose.yml
    │   │   ├── consumer/
    │   │   │   ├── Dockerfile
    │   │   │   ├── consumer.py
    │   │   │   └── requirements.txt
    │   │   ├── dashboard/
    │   │   │   ├── Dockerfile
    │   │   │   └── dashboard.py
    │   │   └── producer/
    │   │       ├── Dockerfile
    │   │       ├── producer.py
    │   │       └── requirements.txt
    │   ├── kafka-producers/
    │   │   ├── README-producers.md
    │   │   └── kafka_producers.py
    │   ├── kafka-topics-partitions/
    │   │   └── README.md
    │   └── python-client/
    │       ├── Dockerfile
    │       ├── consume.py
    │       ├── produce.py
    │       ├── requirements.txt
    │       └── run.sh
    ├── 09-streamlit-dashboard-realtime/
    │   ├── README.md
    │   ├── app.py
    │   ├── pyproject.toml
    │   ├── .python-version
    │   ├── exemplo/
    │   │   ├── main.py
    │   │   └── main_exercicio.py
    │   ├── exemplo_erros_ruff/
    │   │   └── erros_ruff.py
    │   ├── exemplo_libs_grafico/
    │   │   └── exemplo.py
    │   ├── pic/
    │   ├── projeto_coleta/
    │   │   ├── coleta.py
    │   │   ├── coleta_postgres_psycopg2.py
    │   │   └── coleta_postgres_sqlalchemy.py
    │   ├── projeto_dash/
    │   │   ├── dash.py
    │   │   ├── dash_postgres.py
    │   │   └── dash_postgres_cache.py
    │   └── survey/
    │       ├── README.md
    │       ├── dash.py
    │       ├── dash_postgres.py
    │       ├── dash_postgres_cache.py
    │       └── exemplo.env
    ├── 10-infra-as-a-code-terraform/
    │   └── README.md
    ├── Bootcamp - Cloud para dados/
    │   ├── README.md
    │   ├── Aula_01/
    │   │   ├── README.md
    │   │   ├── app.py
    │   │   ├── index.html
    │   │   └── uber-raw-data-sep14.csv.gz
    │   ├── Aula_02/
    │   │   ├── README.md
    │   │   ├── pics/
    │   │   └── projeto/
    │   │       ├── README.md
    │   │       ├── main.py
    │   │       ├── main_v2.py
    │   │       └── .env
    │   ├── Aula_03/
    │   │   └── README.md
    │   ├── Aula_04/
    │   │   └── README.md
    │   ├── Aula_05/
    │   │   └── README.md
    │   ├── Aula_06/
    │   │   └── README.md
    │   ├── Aula_07/
    │   │   └── README.md
    │   ├── Aula_08/
    │   │   ├── README.md
    │   │   ├── Dockerfile
    │   │   ├── requirements.txt
    │   │   └── src/
    │   │       ├── fetch.py
    │   │       ├── fetch_1.py
    │   │       ├── fetch_2.py
    │   │       ├── fetch_3.py
    │   │       └── fetch_4.py
    │   ├── Aula_09/
    │   │   └── README.md
    │   ├── Aula_10/
    │   │   └── README.md
    │   ├── Aula_11/
    │   │   ├── README.md
    │   │   ├── exemplo.py
    │   │   └── exemplo_02.py
    │   ├── Aula_12/
    │   │   └── README.md
    │   ├── Aula_13/
    │   │   ├── README.md
    │   │   ├── enviando.py
    │   │   ├── recebendo.py
    │   │   └── pics/
    │   ├── Aula_14/
    │   │   └── README.md
    │   ├── Aula_15/
    │   │   ├── README.md
    │   │   ├── Policy.txt
    │   │   ├── index.html
    │   │   └── index.zip
    │   ├── Aula_16_17/
    │   │   ├── README.md
    │   │   ├── Dockerfile
    │   │   ├── README-resourcegroup.md
    │   │   ├── app.py
    │   │   ├── projeto_01.py
    │   │   ├── projeto_02.py
    │   │   ├── projeto_03.py
    │   │   ├── projeto_05.py
    │   │   ├── requirements.txt
    │   │   └── data/
    │   └── Aula_18_19/
    │       ├── README.md
    │       ├── Dockerfile
    │       ├── README-resourcegroup.md
    │       ├── app.py
    │       ├── projeto_01.py
    │       ├── projeto_02.py
    │       ├── projeto_03.py
    │       ├── projeto_05.py
    │       ├── requirements.txt
    │       └── data/
    ├── Bootcamp - Python com Laennder/
    │   ├── README.md
    │   ├── Aula_02/
    │   │   ├── readme.md
    │   │   ├── csv_to_parquet.py
    │   │   ├── parquet_to_csv.py
    │   │   ├── requirements.txt
    │   │   └── exercicios/
    │   │       ├── readme.md
    │   │       └── exercicio01.py
    │   └── Aula_05/
    │       └── README.md
    ├── Bootcamp - Python para dados/
    │   ├── README.md
    │   ├── aula01/
    │   │   ├── README.md
    │   │   ├── kpi.py
    │   │   └── aovivo/
    │   │       ├── desafio.py
    │   │       ├── exemplo_01.py
    │   │       └── exemplo_02.py
    │   ├── aula02/
    │   │   ├── README.md
    │   │   ├── desafio.py
    │   │   ├── desafio_resolvido.py
    │   │   ├── exercicios.py
    │   │   ├── aovivo/
    │   │   │   ├── desafio.py
    │   │   │   ├── exercicios.py
    │   │   │   └── main.py
    │   │   └── pics/
    │   ├── aula03/
    │   │   ├── README.md
    │   │   ├── desafio.py
    │   │   ├── exercicios.py
    │   │   ├── main.py
    │   │   ├── aovivo/
    │   │   │   ├── README.md
    │   │   │   ├── arquivos.py
    │   │   │   ├── aula_algoritmo.py
    │   │   │   ├── aws.py
    │   │   │   ├── data_science.py
    │   │   │   ├── db.py
    │   │   │   ├── desafio.py
    │   │   │   ├── exemplo_doc.py
    │   │   │   ├── exemplo_lista.py
    │   │   │   ├── exercicios.py
    │   │   │   ├── gustavo.py
    │   │   │   ├── main.py
    │   │   │   └── utils.py
    │   │   └── pics/
    │   │       └── 6.webp
    │   ├── aula04/
    │   │   ├── README.md
    │   │   ├── exercicios.py
    │   │   ├── ordem.py
    │   │   └── pic/
    │   ├── aula05/
    │   │   ├── README.md
    │   │   └── pic/
    │   ├── aula06/
    │   │   └── aovivo/
    │   │       ├── README.md
    │   │       ├── Dockerfile
    │   │       ├── front.js
    │   │       ├── main.py
    │   │       ├── main_02.py
    │   │       ├── pyproject.toml
    │   │       ├── script.sql
    │   │       ├── .flake8
    │   │       ├── .pre-commit-config.yaml
    │   │       ├── .python-version
    │   │       ├── create_table/
    │   │       │   ├── crm.sql
    │   │       │   └── vendas.sql
    │   │       └── helper/
    │   │           ├── aws.py
    │   │           └── azure.py
    │   ├── aula07/
    │   │   ├── README.md
    │   │   ├── desafio.py
    │   │   ├── desafio_Lucas_Andrade.py
    │   │   ├── desafio_com_pydantic.py
    │   │   ├── desafio_com_type_hint.py
    │   │   ├── aovivo/
    │   │   │   ├── README.md
    │   │   │   ├── __init__.py
    │   │   │   ├── cliente.py
    │   │   │   ├── desafio.py
    │   │   │   ├── etl.py
    │   │   │   ├── main.py
    │   │   │   └── pyproject.toml
    │   │   └── pic/
    │   ├── aula08/
    │   │   ├── README.md
    │   │   ├── __init__.py
    │   │   ├── etl.py
    │   │   ├── pipeline.py
    │   │   ├── schema.py
    │   │   ├── __pycache__/
    │   │   ├── aovivo/
    │   │   │   ├── README.md
    │   │   │   ├── __init__.py
    │   │   │   ├── etl.py
    │   │   │   ├── pipeline.py
    │   │   │   ├── pyproject.toml
    │   │   │   ├── .python-version
    │   │   │   └── data/
    │   │   ├── data/
    │   │   └── pic/
    │   ├── aula09/
    │   │   ├── README.md
    │   │   ├── __init__.py
    │   │   ├── etl.py
    │   │   ├── exemplo_00.py
    │   │   ├── hello.py
    │   │   ├── log.py
    │   │   ├── main.py
    │   │   ├── pipeline.py
    │   │   ├── singleton_decorator.py
    │   │   ├── tenacity_decorator.py
    │   │   ├── timer.py
    │   │   ├── __pycache__/
    │   │   ├── data/
    │   │   └── pic/
    │   ├── aula11-15/
    │   │   ├── README.md
    │   │   ├── OOP.md
    │   │   ├── desafio.py
    │   │   ├── exercicios.py
    │   │   ├── main.py
    │   │   ├── 01-basico/
    │   │   │   ├── class.py
    │   │   │   └── functional.py
    │   │   ├── 02-encapsulamento/
    │   │   │   ├── conexao.py
    │   │   │   ├── encaps.py
    │   │   │   ├── postgre.py
    │   │   │   └── sqlite.py
    │   │   ├── 03-heranca/
    │   │   │   └── etl.py
    │   │   ├── 04-polimorfismo/
    │   │   │   ├── overload.py
    │   │   │   └── override.py
    │   │   ├── 05-GettereSetter/
    │   │   │   ├── basico.py
    │   │   │   └── decorator.py
    │   │   ├── Aula02/
    │   │   │   ├── csv-test.py
    │   │   │   └── src/
    │   │   │       ├── usar.py
    │   │   │       └── interface/
    │   │   │           ├── __init__.py
    │   │   │           └── classes/
    │   │   │               ├── __init__.py
    │   │   │               └── csv_class.py
    │   │   ├── Aula03/
    │   │   │   ├── README.md
    │   │   │   ├── data/
    │   │   │   │   └── txt_files/
    │   │   │   │       └── test.txt
    │   │   │   ├── json_files/
    │   │   │   └── src/
    │   │   │       ├── __main__.py
    │   │   │       └── lib/
    │   │   │           ├── classes/
    │   │   │           │   ├── AbstractDataSource.py
    │   │   │           │   ├── CsvSource.py
    │   │   │           │   ├── FilesSources.py
    │   │   │           │   ├── JsonSource.py
    │   │   │           │   ├── TxtSource.py
    │   │   │           │   ├── __init__.py
    │   │   │           │   └── aws/
    │   │   │           │       └── s3.py
    │   │   │           └── contract/
    │   │   │               └── __init__.py
    │   │   ├── Aula04/
    │   │   │   ├── Collector/
    │   │   │   │   ├── main.py
    │   │   │   │   ├── contracts/
    │   │   │   │   │   ├── __init__.py
    │   │   │   │   │   └── schema.py
    │   │   │   │   ├── datasource/
    │   │   │   │   │   └── api.py
    │   │   │   │   └── tools/
    │   │   │   │       ├── retry.py
    │   │   │   │       └── aws/
    │   │   │   │           ├── __init__.py
    │   │   │   │           └── client.py
    │   │   │   └── FakeApi/
    │   │   │       ├── __init__.py
    │   │   │       └── start.py
    │   │   └── basics/
    │   │       ├── func_csv.py
    │   │       ├── pessoa-class.py
    │   │       └── pessoa.py
    │   ├── aula16/
    │   │   ├── README.md
    │   │   ├── create.py
    │   │   ├── database.db
    │   │   ├── desafio.py
    │   │   ├── main.py
    │   │   ├── pyproject.toml
    │   │   ├── read.py
    │   │   ├── read_ops.py
    │   │   ├── read_ops_2.py
    │   │   ├── read_sql.sql
    │   │   ├── ebook/
    │   │   └── planejado/
    │   │       ├── exemplo_00_dict.py
    │   │       ├── exemplo_01_classe.py
    │   │       ├── exemplo_02_dataclasse.py
    │   │       ├── exemplo_03_classe_com_validador.py
    │   │       ├── exemplo_04_dataclasse_com_validador.py
    │   │       ├── exemplo_05_pydantic.py
    │   │       ├── exemplo_06_orm.py
    │   │       ├── exemplo_07_sql_model.py
    │   │       ├── exemplo_classe_validador.py
    │   │       ├── exemplo_dataclasse_validador.py
    │   │       └── exemplo_pydantic.py
    │   ├── aula17/
    │   │   ├── README.md
    │   │   ├── desafio.db
    │   │   ├── desafio.py
    │   │   ├── desafio_query.py
    │   │   ├── desafio_query_sql.sql
    │   │   ├── exercicio_01.py
    │   │   ├── exercicio_02.py
    │   │   ├── exercicio_03.py
    │   │   ├── exercicio_04.py
    │   │   ├── meubanco.db
    │   │   ├── pyproject.toml
    │   │   └── pics/
    │   ├── aula18/
    │   │   ├── README.md
    │   │   ├── exemplo_00.py
    │   │   ├── exemplo_01.py
    │   │   ├── exemplo_02_json.py
    │   │   ├── json_exemplo.py
    │   │   ├── pokemon.db
    │   │   ├── pyproject.toml
    │   │   ├── pics/
    │   │   └── src/
    │   │       ├── controller.py
    │   │       ├── db.py
    │   │       ├── main.py
    │   │       ├── models.py
    │   │       ├── schema.py
    │   │       └── __pycache__/
    │   ├── aula19/
    │   │   ├── README.md
    │   │   ├── Dockerfile
    │   │   ├── pics/
    │   │   └── src/
    │   │       ├── database.py
    │   │       ├── main.py
    │   │       ├── models.py
    │   │       ├── schema.py
    │   │       └── __pycache__/
    │   └── aula20/
    │       ├── README.md
    │       ├── docker-compose.yml
    │       ├── pyproject.toml
    │       ├── .dockerignore
    │       ├── .python-version
    │       ├── assets/
    │       ├── backend/
    │       │   ├── Dockerfile
    │       │   ├── crud.py
    │       │   ├── database.py
    │       │   ├── main.py
    │       │   ├── models.py
    │       │   ├── requirements.txt
    │       │   ├── router.py
    │       │   ├── schemas.py
    │       │   └── .dockerignore
    │       └── frontend/
    │           ├── Dockerfile
    │           ├── app.py
    │           ├── requirements.txt
    │           ├── .dockerignore
    │           └── .streamlit/
    │               └── config.toml
    ├── Bootcamp - SQL e Analytics/
    │   ├── README.md
    │   ├── Aula-01/
    │   │   ├── README.md
    │   │   └── create_table.sql
    │   ├── Aula-02/
    │   │   ├── README.md
    │   │   └── desafio.sql
    │   ├── Aula-03/
    │   │   ├── README.md
    │   │   └── desafio.sql
    │   ├── Aula-04/
    │   │   ├── README.md
    │   │   └── desafio.sql
    │   ├── Aula-05/
    │   │   └── README.md
    │   ├── Aula-06/
    │   │   ├── README.md
    │   │   └── table.sql
    │   ├── Aula-07/
    │   │   ├── README.md
    │   │   ├── create_table.sql
    │   │   ├── python.py
    │   │   ├── store_procedure.sql
    │   │   └── store_procedure_com_saldo_transacao.sql
    │   ├── Aula-08/
    │   │   ├── README.md
    │   │   └── table.sql
    │   ├── Aula-09/
    │   │   └── README.md
    │   ├── Aula-10/
    │   │   └── README.md
    │   ├── Aula-11/
    │   │   └── README.md
    │   ├── Aula-12/
    │   │   ├── README.md
    │   │   └── main.py
    │   ├── Aula-13/
    │   │   └── README.md
    │   └── pics/
    ├── WSL/
    │   └── readme.md
    ├── Workshop - Git e Github/
    │   ├── Aula_01/
    │   │   └── README.md
    │   └── Aula_02/
    │       └── README.md
    ├── Youtube/
    │   └── Live_10/
    │       └── README.md
    └── pics/
        ├── bootcamps/
        ├── convidados/
        ├── workshops/
        └── youtube/

================================================
File: /README.md
================================================
# Roadmap de Engenharia de Dados 

<p align="center">
  <a href="https://suajornadadedados.com.br/"><img src="pics/logo.png" alt="Jornada de Dados"></a>
</p>
<p align="center">
    <em>Nossa missão é fornecer o melhor ensino em engenharia de dados</em>
</p>

Bem-vindo ao **Roadmap de Engenharia de Dados** da **Jornada de Dados**! Este repositório foi construído colaborativamente e tem como objetivo capacitá-lo para a carreira de engenharia de dados, ajudando profissionais como você a atingir novos patamares em sua trajetória profissional.

Aqui você encontrará:

- **Roadmap de Estudo:** Links, referências e materiais complementares para auxiliar no seu aprendizado. Se você quer ter um plano de estudo, encontrará uma série de projetos e recursos que o guiarão passo a passo no desenvolvimento das habilidades essenciais em engenharia de dados.
- **Códigos das Aulas:** Todos os códigos e materiais utilizados durante as aulas estão disponíveis neste repositório para que você possa praticar e aprofundar seu conhecimento.
- **Calendário dos Próximos Encontros:** Fique por dentro das datas e temas dos próximos workshops e bootcamps da Jornada de Dados.

Se você gostou do conteúdo e quer se inscrever em nosso programa profissional, pode fazer isso aqui:

---

## Próximos Eventos

| Nome/Github                                      | Onde     | Data   | Mês       |
|-------------------------------------------|----------|--------|-----------|
| [Pipeline de dados no Azure](https://github.com/lvgalvao/workshop-azure-jornadadedados)                | Jornada  | 21/12  | Hoje  |
| ETL 10 bilhões de linhas           | Youtube  | Terça dia 14/01 ás 19h30  | Janeiro   |
| Databricks e delta table (OLAP + OLTP)           | Jornada  | 22/01  | Janeiro   |
| Bootcamp Multiengine Databricks, Spark e DuckDB           | Jornada  | 27/01 até 31/01  | Janeiro   |
| Convidado surpresa           | Jornada  | 04/02  | Janeiro   |
| Kubernetes e Airflow no Google Cloud                     | Jornada  | 22/02  | Fevereiro     |
| Bootcamp de AI Agents                     | Jornada  | 27/03  | Março     |

![roadmap](./pics/roadmap_2025.png)

Tem sugestões de Tópicos? [clique aqui](https://github.com/lvgalvao/data-engineering-roadmap/issues)

![imagem](./pics/issue.png)

Abra uma Issue aqui

![imagem](./pics/issue.png)

Participe e interaja

![imagem](./pics/issue.png)

___

## Youtube - Workshops ao vivo - Lives abertas

| Nome                                      | Link                                                    | Duração  | Stack/Github |
|-------------------------------------------|---------------------------------------------------------|----------|------ |
| Extração de API do zero | [Assistir no YouTube](https://youtube.com/live/xvCwZ73muV8) | 2h  | [GitHub, Python, API, SQL, Azure, Streamlit](https://github.com/lvgalvao/ETLProjectAPIExtract) |
| Pipeline ETL - Web Scraping com Requests e Beautiful Soup 4 | [Assistir no YouTube](https://youtube.com/live/z1EOlFV8g7g) | 2h  | GitHub, Python, Requests e Beautiful Soup 4 |
| Pipeline de Dados com GA4 e Typeform | [Assistir no YouTube](https://youtube.com/live/kt72obCvo0k) | 6h  | Python, SQL e Cursor AI |
| Pipeline Gen AI - ETL com API e CRM de vendas | [Assistir no YouTube](https://youtube.com/live/I-4noY9hGTQ) | 6h  | Python, SQL, OpenAI, Langchain e Git  |
| WORKSHOP ABERTO #1 - DO ZERO AO DEPLOY COM LUCIANO | [Assistir no YouTube](https://www.youtube.com/watch?v=HxY2UhHkFWA) | 2h  | GitHub, VirtualEnv, TDD, taskipy, pytest, Streamlit, Selenium, Pydantic, MkDocs |
| Criando ETL Com Python e DUCKDB DO ZERO AO DEPLOY ft. [@mehd-io](https://github.com/mehd-io) | [Assistir no YouTube](https://www.youtube.com/watch?v=4w6UQNn_6X0) | 1h47min  | DuckDB, SQL, S3, CSV, JSON, MotherDuck, Streamlit, Docker, Render |
| CRIANDO ETL COM PYTHON E DUCKDB DO ZERO AO DEPLOY | [Assistir no YouTube](https://www.youtube.com/watch?v=eXXImkz-vMs) | 2h32min  | DuckDB, SQLAlchemy, Google Drive, SQL, Postgres, Python, CSV, Parquet, JSON, Streamlit, Docker, Render |
| Modern Data Stack com SQL - Parte 1 de 3 ft. [@MarcLamberti](https://github.com/marclamberti) | [Assistir no YouTube](https://www.youtube.com/watch?v=lhMIMrEj_4Q) | 2h  | AirFlow, Astro-cli, Docker, Airbyte Cloud , Render, Postgres|
| Modern Data Stack com SQL - Parte 2 de 3 | [Assistir no YouTube](https://www.youtube.com/watch?v=WG96Z7uGTHg) | 1h53min  | SQL, dbt, Render |
| Modern Data Stack com SQL - Parte 3 de 3 | [Assistir no YouTube](https://www.youtube.com/watch?v=brfl7hdC060) | 1h53min  | Python, AirFlow, SQL, dbt, Render, Docker |
___

# Workshops - Quem sabe faz ao vivo

| Nome                                      | Link                                                    | Profissional  | Stack |
|-------------------------------------------|---------------------------------------------------------|----------|------ |
| Construindo um Pipeline ETL em Tempo Real | [Assistir no YouTube](https://youtube.com/live/daUC8kMzeLw) | Caio Machado  | Kafka, PostgreSQL e Streamlit |
| Como sair do ZERO com SQL na AWS | [Assistir no YouTube](https://youtube.com/live/ko3D76GP5d4) | Ghabriel Fiorotti  | ETL Parquet S3 Athena e Glue |
| Plataforma com Big Query do Zero| [Assistir no YouTube](https://youtube.com/live/NP08fHker5U) | Alan Lanceloty | Python, Airflow, dbt, soda e docker |
| Qualidade de dados e Contrato de Dados  | [Assistir no YouTube](https://youtube.com/live/IQtuWsNmB4o) | Renan Heckert  | Pandera e Pydantic |

# Workshops - Especialistas

| Nome                                      | Link                                                    | Profissional  | Stack |
|-------------------------------------------|---------------------------------------------------------|----------|------ |
| Como sair do ZERO em Observabilidade com Logfire| [Assistir no YouTube](https://youtube.com/live/bxtsTP0a0mU) | Marcelo Trylesinski  | Logfire |
| dbt no Airflow - Como melhorar o desempenho do seu deploy de forma correta | [Assistir no YouTube](https://youtube.com/live/xvCwZ73muV8) | Tatiana Martins | Airflow e dbt-core |
| Construa Data Apps Completos com Briefer | [Assistir no YouTube](https://youtube.com/live/6KyxRpX6oY4) | Lucas Costa  | Briefer, SQL e Python |
| Como sair do ZERO no AIRBYTE| [Assistir no YouTube](https://youtube.com/live/4hQroajva0s) | Alan Lanceloty  | Airbyte |
| Como criar do ZERO um Lakehouse | [Assistir no YouTube](https://youtube.com/live/O9q5owTOpMw) | Nilton Ueda  | Conceitual |

___


## Youtube - Vídeos tutoriais
| Nome                                      | Link                                                    | Duração  | Stack |
|-------------------------------------------|---------------------------------------------------------|----------|------ |
| Top 5 Projetos de Engenharia de Dados Aprenda ETL, Python e SQL Gratuitamente! | [Assistir no YouTube](https://www.youtube.com/watch?v=ldjbV_0mqXI) | 14min | Python, DuckDB, Spark, GitHub, Docker |
| O que é CLOUD? Explicação COMPLETA para DADOS (Deploy Python e SQL na AWS, Azure, GCP com Terraform) | [Assistir no YouTube](https://www.youtube.com/watch?v=Iff6Nr3sK4U) | 15min | Python, GitHub, AWS, Azur, GCP, SQL, Docker, Terraform |
| Como fazer o Deploy de Airflow na EC2 AWS | [Assistir no YouTube](https://www.youtube.com/watch?v=aYLmKbxXcls) | 14min | AWS, EC2, Airflow | 
| Amazon SQS e Rabbit MQ eu preciso mesmo disso? | [Assistir no YouTube](https://www.youtube.com/watch?v=sSBFCffBSac) | 5min | SQS, Rabbit MQ | 
| Segredos para economizar com a AWS | [Assistir no YouTube](https://www.youtube.com/watch?v=aKvCjSQHb_w) | 5min | AWS | 
| CRUD, qual a vantagem de usar um ORM? | [Assistir no YouTube](https://www.youtube.com/watch?v=hl5YjfvqkB0) | 6min | SQL Alchemy, Python, ORM, SQL | 
| Como instalar Python em 2024 + Pyenv, PIP, VENV, PIPX e Poetry | [Assistir no YouTube](https://www.youtube.com/watch?v=9LYqtLuD7z4) | 33min | Pyenv, PIP, VENV, PIPX, Poetry, Python | 
| Como instalar Python em 2024 + VSCode, Git e GitHub do Zero | [Assistir no YouTube](https://www.youtube.com/watch?v=-M4pMd2yQOM) | 33min | Python, VSCode, Git, GitHub | 
| O que é o arquivo __init__.py em Python? Explicado com 4 exemplos | [Assistir no YouTube](https://www.youtube.com/watch?v=H7rINLV6e0I) | 14min | Python | 


---

## Especialização Jornada de dados

| Nome do Treinamento                     | Link GitHub | Status       | Descrição                                                                              |
|-----------------------------------------|-------------|--------------|---------------------------------------------------------------------------------------|
| Python para Dados                       | [Link](#)   | Concluído    | Fundamentos de Python para engenharia de dados, com foco em bibliotecas como Pandas.  |
| SQL para Analytics Engineer             | [Link](#)   | Concluído    | Domine SQL avançado para análise de dados e otimização de consultas complexas.        |
| dbt-core                                | [Link](#)   | Concluído    | Utilize dbt-core para transformação de dados no data warehouse com práticas modernas. |
| Web Scraping Avançado                   | [Link](#)   | Concluído    | Técnicas avançadas de extração de dados de sites e manipulação de APIs.               |
| Cloud para Dados                        | [Link](#)   | Em andamento | Serviços de nuvem aplicados à engenharia de dados com AWS, Azure e GCP.               |

---

Caso queira sugerir temas para próximos workshops, [abra uma issue](https://GitHub.com/lvgalvao/data-engineering-roadmap/issues).

================================================
File: /00-automacao-data-qualiy-excel-etl/README.md
================================================
# Automação Data Quality Excel Etl

Seja bem vindo ao Projeto Automação Data Quality Excel Etl

Não perca [nosso workshop aberto](https://www.jornadadedados2024.com.br/) no dia 24/01

Vamos fazer todo o código desse projeto ao vivo

## Conteúdo

Esse repositório faz parte da Jornada de Dados 2024 

Para saber mais [acesse aqui](../README.md)

- [Automação Data Quality Excel Etl](#automação-data-quality-excel-etl)
  - [Conteúdo](#conteúdo)
  - [Você sabe o que faz um engenheiro de dados?](#você-sabe-o-que-faz-um-engenheiro-de-dados)
  - [Ele ainda trabalha com todas essas tecnologias](#ele-ainda-trabalha-com-todas-essas-tecnologias)
  - [Isso explica melhor](#isso-explica-melhor)
  - [Te convido a criar uma solução](#te-convido-a-criar-uma-solução)
  - [Mais 500 alunos em 2023 aprovaram com 92% de satisfação](#mais-500-alunos-em-2023-aprovaram-com-92-de-satisfação)
  - [Não perca](#não-perca)
  - [Objetivo](#objetivo)
  - [Rodando o projeto](#rodando-o-projeto)
  - [Possui dúvidas?](#possui-dúvidas)
  - [Agora para o projeto?](#agora-para-o-projeto)

## Você sabe o que faz um engenheiro de dados?

![Figura01](./pics/radar-600x333.png)

É o responsável por criar plataformas e pipeline de dados com qualidade

## Ele ainda trabalha com todas essas tecnologias

![Figura01](./pics/lista_de_tecnologias.png)

Mas acho que…

## Isso explica melhor

![Figura01](./pics/meme_barbie.png)

## Te convido a criar uma solução

De engenharia de dados ao vivo comigo dia 24/01 às 20h: “Do Zero ao Deploy”

![Figura01](./pics/arquitetura.png)

Vamos iniciar o seu portfólio?

## Mais 500 alunos em 2023 aprovaram com 92% de satisfação

![Figura01](./pics/workshop.png)

## Não perca 

[Nosso workshop aberto](https://www.jornadadedados2024.com.br/) no dia 24/01

![Figura01](./pics/cadastro.png)

## Objetivo

* Testes com Pytest e Selenium
* Documentando com Mkdcos
* O resto é codando em Python e tomando Coca-Cola

## Rodando o projeto

```bash
git clone
cd 
```

## Possui dúvidas? 

- Fale comigo [Link do Linkedin](https://www.linkedin.com/in/lucianovasconcelosf/)

## Agora para o projeto?

- Toda a documentação do projeto é feita usando Mkdocs




================================================
File: /00-automacao-data-qualiy-excel-etl/mkdocs.yml
================================================
site_name: "My Library"

theme:
  name: "material"

plugins:
- search
- mkdocstrings

================================================
File: /00-automacao-data-qualiy-excel-etl/pyproject.toml
================================================
[tool.poetry]
name = "00-automacao-data-qualiy-excel-etl"
version = "0.1.0"
description = ""
authors = ["lvgalvaofilho"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
pydantic = {extras = ["email"], version = "^2.8.2"}
streamlit = "^1.37.0"
openpyxl = "^3.1.5"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.taskipy.tasks]
docs = "lsof -ti :8000 | xargs kill -9 | mkdocs serve"
run = "lsof -ti :8501 | xargs kill -9 | streamlit run src/app.py"
test = "lsof -ti :8501 | xargs kill -9 | pytest tests"

================================================
File: /00-automacao-data-qualiy-excel-etl/requirements.txt
================================================
mkdocs==1.5.3
streamlit==1.30.0
taskipy==1.12.2
pytest==7.4.4
selenium==4.16.0
openpyxl==3.1.2
pydantic[email]==2.5.3

================================================
File: /00-automacao-data-qualiy-excel-etl/.python-version
================================================
3.12


================================================
File: /00-automacao-data-qualiy-excel-etl/docs/app.md
================================================
## Módulo `app.py`

**Função `main`**

::: src.app.main
    options:
      show_root_heading: false

================================================
File: /00-automacao-data-qualiy-excel-etl/docs/backend.md
================================================
## Módulo `backend.py`

**Função `process_excel`**

::: src.backend.process_excel

================================================
File: /00-automacao-data-qualiy-excel-etl/docs/frontend.md
================================================
## Módulo frontend.py

::: src.frontend.ExcelValidatorUI
    handler: python
    options:
      members:
        - set_page_config
      show_root_heading: false
      show_source: false


================================================
File: /00-automacao-data-qualiy-excel-etl/docs/index.md
================================================
# Fluxo do Workshop

Fique calmo, tudo vai dar certo.

![Imagem](./pics/config_01.png)

Tem tudo em detalhes aqui no [Data Project Starter Kit](https://github.com/lvgalvao/DataProjectStarterKit)

## 1) Configuração inicial

#### 1) Vamos criar um novo projeto no Git e Github
  
- Acessar site Github e criar nossa pasta
- Fazendo um teste simples para ver se tudo está bem
- Criando um README

```bash
touch README.md
```

- Salvando ele

```bash
git add .
git commit -m "docs: adicionando arquivo README"
git push --set-upstream origin main
```
- Deletando ele
```bash
git add .
git commit -m "docs: remover arquivo README"
git push
```

- Recuperando ele

```bash
git log
git checkout 3eab9123874b4ec51b0ab6d103a9483f2250c23f -- README.md
git add .
git push
```
  
#### 2) Vamos definir nossa versão do Python usando o Pyenv

```bash
python --versions
pyenv versions
pyenv local 3.11.5
```

#### 3) Vamos criar nosso ambiente virtual
Para criar o ambiente virtual, abra o terminal dentro da pasta criada e faça:

```bash
python -m venv nome_do_ambiente_virtual 
# O padrao é utilizar .venv
source nome_do_ambiente_virtual/bin/activate
# Usuários Linux e mac
nome_do_ambiente_virtual\Scripts\Activate
# Usuários Windows
```

#### 4) Instalando uma biblioteca

```bash
pip install selenium
```

#### 4) Replicando ambientes

Agora, se quisermos rodar o nosso projeto em outra máquina, não será necessário baixar as dependências uma a uma, basta fazer:

```bash
pip freeze > requirements.txt
pip install -r requirements.txt  
```

#### 5) Desativando o ambiente virtual

E por fim, para desativar o ambiente virtual:
```bash
deactive
```

#### 6) Criando o .gitignore

```bash
touch .gitignore
```

[Site com exemplo de arquivo](https://www.toptal.com/developers/gitignore/api/python)

#### 7) Melhorando nosso README

```md

    ### Instalação e Configuração

    1. Clone o repositório:

    ```bash
    git clone https://github.com/lvgalvao/dataprojectstarterkit.git
    cd dataprojectstarterkit
    ```

    2. Configure a versão correta do Python com `pyenv`:

    ```bash
    pyenv install 3.11.5
    pyenv local 3.11.5
    ```

    3. Instale as dependências do projeto:

    ```bash
    python -m venv .venv
    # O padrao é utilizar .venv
    source .venv/bin/activate
    # Usuários Linux e mac
    .venv\Scripts\Activate
    # Usuários Windows
    pip install -r requirements.txt  
    ```
```

## 2) Precisamos falar de testes

Diferença entre fases de teste, tipos de teste e formas de execução. Hoje em dia há muita confusão quando se fala em fases de teste, tipos de teste e formas de execução. 

Se você, assim como eu, já ouviu as frases a seguir várias vezes, então esse [artigo é para você!](https://www.zup.com.br/blog/tipos-de-teste)

- “Fulano sabe teste funcional e não automatizado”;
- “desenvolva o teste unitário antes dos funcionais”; 
- “precisamos que os testes sejam 100% automatizados”; 
- “cadê a massa de dados para os testes de contrato?”

### Pirâmide de teste
Uma maneira mais visual de exemplificar um pouco sobre as fases de teste e os tipos de teste que cada fase contempla, é a pirâmide de automação de teste.

![Imagem](./pics/testes.png)

#### 1) Criando nosso primeiro teste

```bash
pip install pytest
pip install selenium
```

[Como instalar o webdriver](https://medium.com/@wmonteiro/executando-o-selenium-com-o-python-em-windows-c876bc60bf99)

![Imagem](./pics/TDD_1.jpg)

Vamos criar nosso arquivo de teste

```bash
mkdir tests
cd tests
touch test_app.py
```

arquivo `test_app.py`
```python
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from time import sleep

# Precismaos definir qual driver vamos utilizar
driver = webdriver.Firefox()

# Define um timeout implícito
driver.set_page_load_timeout(5)  # 5 segundos

# Vamos fazer uma tratativa de try-except de entrar na nossa página
try:
    driver.get("http://localhost:8501")
    sleep(5)
    print("Acessou a página com sucesso")
except TimeoutException:
    print("Tempo de carregamento da página excedeu o limite.")
finally:
    driver.quit()
```

Agora que já temos nosso teste vamos desenvolver nosso primeiro código

Para isso vamos trabalhar com o *streamlit*

![streamlit](./pics/streamlit.png)

Instalando o streamlit

```bash
pip install streamlit
```

Vamos fazer o nosso Hello World

```bash
mkdir src
cd src
touch app.py
```

arquivo `app.py`
```python
import streamlit as st

# Título do App
st.title('Nosso Primeiro App com Streamlit')

# Escrevendo um Hello World com markdown
st.markdown('**Hello world!** 🌍')

# Escrevendo texto
st.write('Esta é uma demonstração de algumas funcionalidades do Streamlit.')

# Input de texto do usuário
input_texto = st.text_input('Digite algo aqui:')

# Mostrando o texto digitado
st.write(f'Você digitou: {input_texto}')

# Slider para números
numero = st.slider('Escolha um número', 0, 100, 50)

# Exibir o número escolhido
st.write(f'Você escolheu o número: {numero}')

# Gráfico de barras simples
import pandas as pd
import numpy as np

# Criando dados aleatórios
dados = pd.DataFrame({
  'colunas': ['A', 'B', 'C', 'D', 'E'],
  'valores': np.random.randn(5)
})
```

### Temos nosso frontend /o/

### Agora vamos para uma tangente

Temos um problema com nosso processo que muda de porta

Sempre que subimos uma nova aplicação ele está usando uma outra porta

Precisamos "matar" esse processo e reutilizar a porta 8501

Usamos o comando lsof (List Open Files) para verificar os processos que estão conectados nessa porta

```bash
lsof -i :8501
```

Depois usamos o comando kill para matar esse processo

```bash
kill -9 [PID]
```

Podemos simplificar usando somente uma linha

```bash
lsof -ti :8501 | xargs kill -9
```

No Windows, o comando `lsof` (List Open Files), que é comum em sistemas baseados em Unix como Linux e macOS, não está disponível. No entanto, você pode realizar uma tarefa similar para verificar quais processos estão usando uma porta específica (por exemplo, a porta 8501) usando o Resource Monitor ou comandos no Prompt de Comando. Aqui estão duas maneiras de fazer isso:

### 1. Usando o Resource Monitor

1. Pressione `Ctrl + Shift + Esc` para abrir o Gerenciador de Tarefas.
2. Vá para a aba "Desempenho" e clique em "Monitor de Recursos" na parte inferior.
3. No Resource Monitor, vá para a aba "Rede".
4. Olhe na seção "Portas de Escuta" para encontrar a porta 8505 e veja quais processos estão associados a ela.

### 2. Usando o Prompt de Comando

1. Abra o Prompt de Comando como administrador (isso é necessário para executar comandos que acessam informações de rede).
    
2. Digite o seguinte comando:
    
    ```cmd
    netstat -ano | findstr :8501
    ```
    
    Esse comando lista todas as conexões e portas de escuta (`netstat -ano`) e filtra os resultados para mostrar apenas as entradas relacionadas à porta 8505 (`findstr :8501`).
    
3. Você verá uma lista de entradas, se houver alguma, mostrando o protocolo, endereço local, endereço estrangeiro, estado, e o PID (ID do Processo) associado à porta 8505.
    
4. Se você quiser saber qual aplicativo está associado a um PID específico, você pode encontrar este PID na aba "Detalhes" do Gerenciador de Tarefas.
Para finalizar um processo em uma linha de comando no Windows, combinando a busca do processo pela porta e o encerramento do processo, você pode usar o PowerShell. O PowerShell é mais poderoso e flexível do que o Prompt de Comando tradicional para este tipo de operação. Aqui está como você pode fazer isso:

Abra o PowerShell como administrador e execute o seguinte comando:

```powershell
Get-NetTCPConnection -LocalPort 8501 | Select-Object -ExpandProperty OwningProcess | ForEach-Object {Stop-Process -Id $_ -Force}
```

Este comando faz o seguinte:

1. `Get-NetTCPConnection -LocalPort 8501`: Obtém todas as conexões TCP que estão escutando na porta 8501.
    
2. `Select-Object -ExpandProperty OwningProcess`: Seleciona os IDs dos processos (PID) que estão escutando naquela porta.
    
3. `ForEach-Object {Stop-Process -Id $_ -Force}`: Para cada PID encontrado, usa o `Stop-Process` para encerrar o processo. A opção `-Force` é usada para garantir que o processo seja encerrado.

### Taskipy - Para não ficar toda essa quantidade de código, vamos usar o Taskipy

![Taskipy](./pics/taskipy.png)

Basicamente o Taskipy é um short de comandos

Vamos instalar ele com o comando

```bash
pip install taskipy
```

Criar um arquivo de configuração

```bash
touch pyproject.toml
```

E dentro desse arquivo `pyproject.toml` incluir os comandos que queremos

```toml
[tool.taskipy.tasks]
run = "lsof -ti :8501 | xargs kill -9 | streamlit run src/app.py"
```

Agora conseguimos simplificar e tornar nosso processo de rodar nossa aplicação mais rápido

### Saindo da tangente

```bash
python tests/test_app.py
```

E temos o nosso primeiro teste passando!

Agora temos duas opções.

### Escrever um novo teste ou refatorar.

## 3) Nossa primeira refatoração

Vamos melhorar os nossos testes usando o pytest

1) Vamos criar uma função que inicia o nosso driver

2) Vamos criar uma função que testa se o site está online

Para nossa função vamos usar o módulo fixture do pytest

```python
import pytest
import subprocess
from selenium import webdriver

@pytest.fixture
def driver():
    # Iniciar o Streamlit em background
    process = subprocess.Popen(["streamlit", "run", "src/app.py"])

    # Iniciar o WebDriver usando GeckoDriver
    driver = webdriver.Firefox()
    driver.set_page_load_timeout(5)
    yield driver

    # Fechar o WebDriver e o Streamlit após o teste
    driver.quit()
    process.kill()

def test_app_opens(driver):
    # Verificar se a página abre
    driver.get("http://localhost:8501")
```

Além disso,
Podemos incluir um comando novo no task

```pyproject.toml
test = "lsof -ti :8501 | xargs kill -9 | pytest tests -v"
```

#### Nosso segundo teste


Vamos escrever um teste que cheque se o title ta página é `validador de schema excel`

Para isso vamos criar mais um teste

```python
def test_check_title_is(driver):
    # Verificar se a página abre
    driver.get("http://localhost:8501")
    # Verifica se o titulo de página é
    sleep(5)
    # Capturar o título da página
    page_title = driver.title

    # Verificar se o título da página é o esperado
    expected_title = "Validador de schema excel"  # Substitua com o título real esperado
    assert page_title == expected_title, f"O título da página era '{page_title}', mas esperava-se '{expected_title}'"
```   

Vamos revisitar nossa aplicação também

```python
import streamlit as st

# Título do App
st.title('Validador de schema excel')
```

Nosso teste não passa =(

O motivo? 

**O streamlit e o selenium chamam coisas diferentes com o mesmo nome!**

```python
import streamlit as st

st.set_page_config(
    page_title="Validador de schema excel"
)
```

### 3) Terceira Feature

#### Adicionar um texto no h1

test_app.py
```python
from selenium.webdriver.common.by import By

def test_check_streamlit_h1(driver):
    # Acessar a página do Streamlit
    driver.get("http://localhost:8501")

    # Aguardar para garantir que a página foi carregada
    sleep(5)  # Espera 5 segundos

    # Capturar o primeiro elemento <h1> da página
    h1_element = driver.find_element(By.TAG_NAME, "h1")

    # Verificar se o texto do elemento <h1> é o esperado
    expected_text = "Insira o seu excel para validação"
    assert h1_element.text == expected_text

```

app.py
```python
st.title("Insira o seu excel para validação")
```

### 4) Agora vamos criar um teste que o usuário pode inserir um excel, e vai aparecer uma mensagem de sucesso

Vamos criar nossa nova função


test_app.py
```python
def test_check_usuario_pode_inserir_um_excel_e_receber_uma_mensagem(driver):
    # Acessar a página do Streamlit
    driver.get("http://localhost:8501")

    # Aguardar para garantir que a página foi carregada
    sleep(5)  # Espera 5 segundos

    # Realizar o upload do arquivo de sucesso
    success_file_path = os.path.abspath("data/arquivo_excel.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(success_file_path)

    # Aguardar a mensagem de sucesso
    sleep(5)
    assert "O schema do arquivo Excel está correto!" in driver.page_source
```

app.py
```python
arquivo = st.file_uploader("Carregue seu arquivo Excel aqui", type=["xlsx"])

if arquivo:
    st.success("O schema do arquivo Excel está correto!")
```

Agora vamos parar com nosso frontend e ir para a parte do backend

## Pydantic

Um exemplo de KPI

Vamos instalar o Pydantic

```bash
pip install "pydantic[email]" openpyxl
```

Criar um arquivo de estes unitários `test_unit.py`

```python
import pytest
from datetime import datetime
from src.contrato import Vendas, CategoriaEnum
from pydantic import ValidationError

# Testes com dados válidos
def test_vendas_com_dados_validos():
    dados_validos = {
        "email": "comprador@example.com",
        "data": datetime.now(),
        "valor": 100.50,
        "produto": "Produto X",
        "quantidade": 3,
        "categoria": "categoria3",
    }
    
    # A sintaxe **dados_validos é uma forma de desempacotamento de dicionários em Python. 
    # O que isso faz é passar os pares chave-valor no dicionário dados_validos como argumentos nomeados para o construtor da classe Vendas.

    venda = Vendas(**dados_validos)
    
    assert venda.email == dados_validos["email"]
    assert venda.data == dados_validos["data"]
    assert venda.valor == dados_validos["valor"]
    assert venda.produto == dados_validos["produto"]
    assert venda.quantidade == dados_validos["quantidade"]
    assert venda.categoria == dados_validos["categoria"]

# Testes com dados inválidos
def test_vendas_com_dados_invalidos():
    dados_invalidos = {
        "email": "comprador",
        "data": "não é uma data",
        "valor": -100,
        "produto": "",
        "quantidade": -1,
        "categoria": "categoria3"
    }

    with pytest.raises(ValidationError):
        Vendas(**dados_invalidos)

# Teste de validação de categoria
def test_validacao_categoria():
    dados = {
        "email": "comprador@example.com",
        "data": datetime.now(),
        "valor": 100.50,
        "produto": "Produto Y",
        "quantidade": 1,
        "categoria": "categoria inexistente",
    }

    with pytest.raises(ValidationError):
        Vendas(**dados)
```

Criar nosso arquivo de contrato `contrato.py`

Porque temos contrato de software"

Pydantic é um serializador de ORM + Json também

```python
primeira_venda = {
    "email": "lvgalvaofilho",
    "valor": 50.50,
}

def validador_email(venda):
    if not isinstance(venda, str) or '@' not in venda:
        raise ValueError(f"Email invalido: {venda}")
    
def validador_de_valor(venda):
    if not isinstance(venda, float) and venda > 0:
        raise ValueError(f"Valor não é valido: {venda}")
    
try:
    validador_email(primeira_venda["email"])
    validador_de_valor(primeira_venda["valor"])
    print("Todos os dados são validos.")
except ValueError as e:
    print(f"Erro na validação {e}")
```

```python
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any

class CategoriaEnum(Enum):
    ELETRONICOS = 'eletronicos'
    ALIMENTOS = 'alimentos'
    VESTUARIO = 'vestuario'

def validar_email(valor: Any) -> None:
    if not isinstance(valor, str) or "@" not in valor:
        raise ValueError(f"Email inválido: {valor}")

def validar_positive_float(valor: Any) -> None:
    if not isinstance(valor, float) or valor <= 0:
        raise ValueError(f"Valor inválido (deve ser um float positivo): {valor}")

def validar_positive_int(valor: Any) -> None:
    if not isinstance(valor, int) or valor <= 0:
        raise ValueError(f"Quantidade inválida (deve ser um int positivo): {valor}")

@dataclass
class Vendas:
    email: str = field(metadata={"validate": validar_email})
    data: datetime
    valor: float = field(metadata={"validate": validar_positive_float})
    quantidade: int = field(metadata={"validate": validar_positive_int})
    produto: str
    categoria: CategoriaEnum

    def __post_init__(self):
        for field_name, field_def in self.__dataclass_fields__.items():
            if 'validate' in field_def.metadata:
                validator = field_def.metadata['validate']
                valor = getattr(self, field_name)
                validator(valor)
                
```

```python
from pydantic import BaseModel, EmailStr, PositiveFloat, PositiveInt, validator
from datetime import datetime
from enum import Enum

class CategoriaEnum(str, Enum):
    categoria1 = "categoria1"
    categoria2 = "categoria2"
    categoria3 = "categoria3"


class Vendas(BaseModel):

    """
    Modelo de dados para as vendas.

    Args:
        email (str): email do comprador
        data (datetime): data da compra
        valor (int): valor da compra
        produto (str): nome do produto
        quantidade (int): quantidade de produtos
        categoria (str): categoria do produto

    """
    email: EmailStr
    data: datetime
    valor: PositiveFloat
    quantidade: PositiveInt
    categoria: CategoriaEnum

    @validator('categoria')
    def categoria_deve_estar_no_enum(cls, error):
        return errore
```

# Nossos testes já passam /o/

# Vamos refatorar nossa aplicação

Vamos segregar a lógica do frontend (streamlit)

Do app.py

Vamos sair disso

```python
import streamlit as st

st.set_page_config(
    page_title="Validador de schema excel"
)

st.title("Insira o seu excel para validação")

arquivo = st.file_uploader("Carregue seu arquivo Excel aqui", type=["xlsx"])

if arquivo:
    st.success("O schema do arquivo Excel está correto!")
```
para isso

```python
from frontend import ExcelValidadorUI
from backend import process_excel

def main():
    ui = ExcelValidadorUI()
    ui.display_header()

    upload_file = ui.upload_file()

    if upload_file:
        result, error = process_excel(upload_file)
        ui.display_results(result, error)

if __name__ == "__main__":
    main()
```
# Vamos criar nosso backend

```python
import pandas as pd
from contrato import Vendas

def process_excel(uploaded_file):
    try:
        df = pd.read_excel(uploaded_file)

        # Verificar se há colunas extras no DataFrame
        extra_cols = set(df.columns) - set(Vendas.model_fields.keys())
        if extra_cols:
            return False, f"Colunas extras detectadas no Excel: {', '.join(extra_cols)}"

        # Validar cada linha com o schema escolhido
        for index, row in df.iterrows():
            try:
                _ = Vendas(**row.to_dict())
            except Exception as e:
                raise ValueError(f"Erro na linha {index + 2}: {e}")

        return True, None

    except ValueError as ve:
        return False, str(ve)
    except Exception as e:
        return False, f"Erro inesperado: {str(e)}"
```

# Vamos para o noss último teste!

Arquivo `test_app.py`
```python
def test_failed_upload(driver):
    driver.get("http://localhost:8501")

    # Aguardar um tempo para a aplicação carregar
    sleep(5)

    # Realizar o upload do arquivo de falha
    failure_file_path = os.path.abspath("data/failure.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(failure_file_path)

    # Aguardar a mensagem de erro
    sleep(5)
    assert "Erro na validação" in driver.page_source
```

Vamos precisar mudar o nosso frontend

```
    def display_results(self):
        return st.success("O schema do arquivo Excel está correto!")        
```

```python
def display_results(self, result, error):
    if error:
        st.error(f"Erro na validação: {error}")
    else:
        st.success("O schema do arquivo Excel está correto!")
```

# Nossa documentação

```
bash
pip install mkdocs mkdocstrings
``` 

Vamos inserir nossa documentação

Vamos revisitar nosso código e inserir as docstrings


backend.py
```python
"""
Processa um arquivo Excel, validando-o contra um esquema específico.
Args:
    uploaded_file: Um arquivo Excel carregado pelo usuário.
Returns:
    Uma tupla (resultado, erro), onde 'resultado' é um booleano indicando se a validação
    foi bem-sucedida e 'erro' é uma mensagem de erro se a validação falhar.
""" 
```

frontend.py
```python
"""
Classe responsável por gerar a interface de usuário para o validador de arquivos Excel.
"""
```

contrato.py
```python
"""
Modelo de dados para as vendas.
Args:
    email (str): email do comprador
    data (datetime): data da compra
    valor (int): valor da compra
    produto (str): nome do produto
    quantidade (int): quantidade de produtos
    categoria (str): categoria do produto
"""
```

```bash
pip install mkdocs "mkdocstrings[python]" mkdocs-material
```

```bash
mkdocs new
```

mkdocs.yml
```
site_name: "My Library"

theme:
  name: "material"

plugins:
- search
- mkdocstrings
```

```
mkdocs gh-deploy
```


# Aula 01

O que é CI/CD?

Começando com nossa primeira pipeline

```yml
name: ci
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - name: Primeiro passo
        run: echo hello world!

      - name: Segundo Passo
        run: echo $(ls)

      - name: Terceiro passo
        run: echo $(pwd)
```

Legal né?

Repara que no ls, não temos nada!

Agora vamos "puxar nosso código"

Mostrar github actions marketplace

```yml
    steps:
      - name: Copia os arquivos do repositório
        uses: actions/checkout@v4
```

Agora vamos instalar Python

Mostrar github actions marketplace

```yml
    steps:
      - name: Copia os arquivos do repositório
        uses: actions/checkout@v4
```

Instalando o nosso Python

```yml
      - name: Instalar o Python  
        uses: actions/setup-python@v5
        with:
          python-version: '3.11.5' 
```

E se eu tenho mais de um Python?

Exemplo, O pandas ele roda do 3.9 até o 3.12

Como fazer isso?

```yml
jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.6, 3.7, 3.8, 3.9, 3.10, 3.11.5]

    - name: Configurar Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
```

Tornando nosso build mais rápido

```yml
      - name: Instalar Dependencias somente de testes
        run: pip install -r requirements-test.txt
```

Rodando os nossos testes

```yml
      - name: Instalar Poetry
        run: pytest tests/test_unit.py
```

Posso definir branchs específicas

```yml
on:
  push:
    branches: [ main ]
  
  pull_request:
    branches: [ main ]
```

## 2) Removendo o push de main

# Temos 1 bug

# Precisamos visualizar mais de 2 erros

Vamos fazer nosso test

```
def test_check_mais_de_uma_mensagem_de_erro(driver):
    # Acessar a página do Streamlit
    driver.get("http://localhost:8501")

    # Aguardar para garantir que a página foi carregada
    sleep(3)  # Espera 5 segundos

    # Realizar o upload do arquivo de sucesso
    success_file_path = os.path.abspath("data/multiplos_erros.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(success_file_path)

    # Aguardar a mensagem de sucesso
    sleep(3)
    # Localizar todas as ocorrências da mensagem de erro
    error_messages = driver.find_elements(By.XPATH, "//*[contains(text(), 'Erro na validação')]")
```

mudando no UI
```
    def display_results(self, result, errors):
        if errors:
            for error in errors:
                st.error(error)  # Exibe cada erro em uma linha separada
        else:
            st.success("O schema do arquivo Excel está correto!")
```

```
        # Validar cada linha com o schema escolhido
        for index, row in df.iterrows():
            try:
                _ = Vendas(**row.to_dict())
            except Exception as e:
                erros.append(f"Erro na linha {index + 2}: {e}")

        return True, erros
```

```python
```
Para isso precisamos guardar os erros em uma lista

# Salvando no banco

- O que não vamos falar agora.
- Docker e ORM

-> Usar SQL direto e vou usar PANDAS para salvar no SQL

- Quais testes quero fazer?

Vamos falar de um teste de integração

É um teste que vai validar se algo fora do nosso sistema está funcionando corretamente

alguns pontos

`dotenv`

`.env`

`pandas.read_sql()`

`test_integration.py`

```python
import pandas as pd
import os
from dotenv import load_dotenv

load_dotenv(".env")

# Lê as variáveis de ambiente
POSTGRES_USER = os.getenv('POSTGRES_USER')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')
POSTGRES_HOST = os.getenv('POSTGRES_HOST')
POSTGRES_PORT = os.getenv('POSTGRES_PORT')
POSTGRES_DB = os.getenv('POSTGRES_DB')

# Cria a URL de conexão com o banco de dados
DATABASE_URL = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"

def test_read_data_and_check_schema():
    df = pd.read_sql('SELECT * FROM vendas', con=DATABASE_URL)

    # Verificar se o DataFrame não está vazio
    assert not df.empty, "O DataFrame está vazio."

    # Verificar o schema (colunas e tipos de dados)
    expected_dtype = {
        'id': 'int64',
        'email': 'object',  # object em Pandas corresponde a string em SQL
        'data': 'datetime64[ns]',
        'valor': 'float64',
        'produto': 'object',
        'quantidade': 'int64',
        'categoria': 'object'
    }

    assert df.dtypes.to_dict() == expected_dtype, "O schema do DataFrame não corresponde ao esperado."

```

um teste_funcional

`test_app.py`

```python
def test_check_usuario_insere_um_excel_valido_e_aparece_um_botao(driver):
    # Acessar a página do Streamlit
    driver.get("http://localhost:8501")

    # Aguardar para garantir que a página foi carregada
    sleep(3)  # Espera 3 segundos

    # Realizar o upload do arquivo de sucesso
    success_file_path = os.path.abspath("data/correto.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(success_file_path)

    # Aguardar a mensagem de sucesso
    sleep(3)
    assert "O schema do arquivo Excel está correto!" in driver.page_source
    # Verificar se o botão "Salvar no Banco de Dados" está presente
    buttons = driver.find_elements(By.XPATH, "//button")
    save_button = None
    for button in buttons:
        if button.text == "Salvar no Banco de Dados":
            save_button = button
            break

    assert save_button is not None and save_button.is_displayed()
```

## Vamos criar um banco de dados

Será um postgres.

Vamos usar o render

Vamos usar o dbeaver

create.sql
```sql
CREATE TABLE vendas (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) NOT NULL,
    data TIMESTAMP NOT NULL,
    valor NUMERIC(10, 2) NOT NULL CHECK (valor >= 0),
    quantidade INTEGER NOT NULL CHECK (quantidade >= 0),
    produto VARCHAR(255) NOT NULL,
    categoria VARCHAR(50) NOT NULL
);
```

delete.sql
```sql
-- Exemplo com dados valido

INSERT INTO vendas (email, data, valor, produto, quantidade, categoria)
VALUES (
    'comprador@example.com', 
    '2023-09-15 12:00:00',  -- Substitua pela data atual formatada como string
    100.50, 
    'Produto X', 
    3, 
    'categoria3'
);

-- Exemplo com dado invalido

INSERT INTO vendas (email, data, valor, produto, quantidade, categoria)
VALUES (
    'comprador', 
    '2023-09-15 12:00:00',  -- Substitua pela data atual formatada como string
    100.50, 
    'Produto X', 
    3, 
    'categoria3'
);
```

`app.py`

Vamos ter que criar um novo fluxo -> Desenhar na tela

```python
    if uploaded_file:
        df, result, errors = process_excel(uploaded_file)
        ui.display_results(result, errors)

        if errors:
            ui.display_wrong_message()
        elif ui.display_save_button():
            # Se não houver erros e o botão for exibido, exibir o botão e fazer o log
            save_dataframe_to_sql(df)
            ui.display_success_message()
```

backend.py
```python
import pandas as pd
from contrato import Vendas
from dotenv import load_dotenv
import os

load_dotenv(".env")

# Lê as variáveis de ambiente
POSTGRES_USER = os.getenv('POSTGRES_USER')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')
POSTGRES_HOST = os.getenv('POSTGRES_HOST')
POSTGRES_PORT = os.getenv('POSTGRES_PORT')
POSTGRES_DB = os.getenv('POSTGRES_DB')

# Cria a URL de conexão com o banco de dados
DATABASE_URL = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"

# Carrega as variáveis de ambiente
load_dotenv()

def process_excel(uploaded_file):
    try:
        df = pd.read_excel(uploaded_file)
        erros = []
        # Verificar se há colunas extras no DataFrame
        extra_cols = set(df.columns) - set(Vendas.model_fields.keys())
        if extra_cols:
            return False, f"Colunas extras detectadas no Excel: {', '.join(extra_cols)}"

        # Validar cada linha com o schema escolhido
        for index, row in df.iterrows():
            try:
                _ = Vendas(**row.to_dict())
            except Exception as e:
                erros.append(f"Erro na linha {index + 2}: {e}")

        # Retorna tanto o resultado da validação, os erros, quanto o DataFrame
        return df, True, erros

    except Exception as e:
        # Se houver exceção, retorna o erro e um DataFrame vazio
        return pd.DataFrame(), f"Erro inesperado: {str(e)}"
    
def save_dataframe_to_sql(df):
    # Salva o DataFrame no banco de dados
    df.to_sql('vendas', con=DATABASE_URL, if_exists='replace', index=False)
```

`frontend.py`
```python
import streamlit as st

class ExcelValidadorUI:

    def __init__(self):
        self.set_page_config()

    def set_page_config(self):
        st.set_page_config(
            page_title="Validador de schema excel"
        )

    def display_header(self):
        st.title("Insira o seu excel para validação")

    def upload_file(self):
        return st.file_uploader("Carregue seu arquivo Excel aqui", type=["xlsx"])

    def display_results(self, result, errors):
        if errors:
            for error in errors:
                st.error(f"Erro na validação: {error}")
        else:
            st.success("O schema do arquivo Excel está correto!")

    def display_save_button(self):
        return st.button("Salvar no Banco de Dados")

    def display_wrong_message(self):
        return st.error("Necessário corrigir a planilha!")
    
    def display_success_message(self):
        return st.success("Dados salvos com sucesso no banco de dados!")
```

Como criar uma PR?

```
Descrição
Objetivo da PR: Descreva qual é o objetivo principal desta PR. O que ela pretende alcançar? Isso pode incluir a resolução de um problema específico, a implementação de uma nova funcionalidade ou qualquer outra coisa relevante.

Contexto: Explique o contexto por trás desta PR. Isso pode incluir informações adicionais sobre o problema que está sendo resolvido ou a motivação por trás da nova funcionalidade.

Cliente:

Ticket:

Testes Realizados: Descreva os testes que foram realizados para garantir que as alterações funcionam conforme o esperado. Certifique-se de mencionar se foram adicionados novos testes unitários ou se os testes existentes foram atualizados.

Documentação:

```

from logging import debug, info, error, warning, critical

```Log
from frontend import ExcelValidadorUI
from backend import process_excel, save_dataframe_to_sql
from logging import warning
from datetime import datetime

def main():
    ui = ExcelValidadorUI()
    ui.display_header()

    uploaded_file = ui.upload_file()

    if uploaded_file:
        df, result, errors = process_excel(uploaded_file)
        ui.display_results(result, errors)

        if errors:
            ui.display_wrong_message()
        elif ui.display_save_button():
            # Se não houver erros e o botão for exibido, exibir o botão e fazer o log
            save_dataframe_to_sql(df)
            ui.display_success_message()

if __name__ == "__main__":
    main()

```

from logging import basicConfig

basicConfig

```
from frontend import ExcelValidadorUI
from backend import process_excel, save_dataframe_to_sql
from logging import warning, info
from datetime import datetime

from logging import basicConfig

basicConfig (
    filename='meus_logs.txt',
    filemode='a',
    encoding='utf-8',
    format='%(levelname)s:%(asctime)s:%message)s'
)

def main():
    ui = ExcelValidadorUI()
    ui.display_header()

    uploaded_file = ui.upload_file()

    if uploaded_file:
        df, result, errors = process_excel(uploaded_file)
        ui.display_results(result, errors)

        if errors:
            ui.display_wrong_message()
            warning("erro ao subir excel")
        elif ui.display_save_button():
            # Se não houver erros e o botão for exibido, exibir o botão e fazer o log
            save_dataframe_to_sql(df)
            ui.display_success_message()
            info("excel subiu para o banco ás 14h")

if __name__ == "__main__":
    main()
    
```

Tem como colocar no terminal e no arquivo?

Para isso vamos usar o Loguru

```
pip install loguru
```

================================================
File: /00-automacao-data-qualiy-excel-etl/src/app.py
================================================
from frontend import ExcelValidatorUI
from backend import process_excel
from contrato import Vendas

def main():

    ui = ExcelValidatorUI()
    ui.display_header()

    model_choice = Vendas
    uploaded_file = ui.upload_file()

    if uploaded_file is not None:
        result, error = process_excel(uploaded_file, 
                                      model_choice)
        ui.display_results(result, error)

if __name__ == "__main__":
    main()

================================================
File: /00-automacao-data-qualiy-excel-etl/src/backend.py
================================================
import pandas as pd
from contrato import Vendas

def process_excel(uploaded_file, model_name):
    """
    Processa um arquivo Excel, validando-o contra um esquema específico.

    Args:
        uploaded_file: Um arquivo Excel carregado pelo usuário.
        model_name: Nome do modelo de dados a ser usado para validação.

    Returns:
        Uma tupla (resultado, erro), onde 'resultado' é um booleano indicando se a validação
        foi bem-sucedida e 'erro' é uma mensagem de erro se a validação falhar.
    """    
    try:
        df = pd.read_csv(uploaded_file)

        # Verificar se há colunas extras no DataFrame
        extra_cols = set(df.columns) - set(Vendas.model_fields.keys())
        if extra_cols:
            return False, f"Colunas extras detectadas no Excel: {', '.join(extra_cols)}"

        # Validar cada linha com o schema escolhido
        for index, row in df.iterrows():
            try:
                _ = Vendas(**row.to_dict())
            except Exception as e:
                raise ValueError(f"Erro na linha {index + 2}: {e}")

        return True, None

    except ValueError as ve:
        return False, str(ve)
    except Exception as e:
        return False, f"Erro inesperado: {str(e)}"

================================================
File: /00-automacao-data-qualiy-excel-etl/src/contrato.py
================================================
from pydantic import BaseModel, EmailStr, PositiveFloat, PositiveInt, validator
from datetime import datetime
from enum import Enum

class CategoriaEnum(str, Enum):
    categoria1 = "categoria1"
    categoria2 = "categoria2"
    categoria3 = "categoria3"


class Vendas(BaseModel):

    """
    Modelo de dados para as vendas.

    Args:
        email (str): email do comprador
        data (datetime): data da compra
        valor (int): valor da compra
        produto (str): nome do produto
        quantidade (int): quantidade de produtos
        categoria (str): categoria do produto

    """
    email: EmailStr
    data: datetime
    valor: PositiveFloat
    quantidade: PositiveInt
    categoria: CategoriaEnum

    @validator('categoria')
    def categoria_deve_estar_no_enum(cls, error):
        return error

================================================
File: /00-automacao-data-qualiy-excel-etl/src/frontend.py
================================================
import streamlit as st

class ExcelValidatorUI:
    """
    Classe responsável por gerar a interface de usuário para o validador de arquivos Excel.
    """
    def __init__(self):
        self.set_page_config()

    def set_page_config(self):
        st.set_page_config(page_title="Validador de Schema de Excel", layout="wide")

    def display_header(self):
        st.title("Validador de Schema de Excel")

    def upload_file(self):
        return st.file_uploader("Carregue seu arquivo Excel aqui", type=["csv"])

    def display_results(self, result, error):
        if error:
            st.error(f"Erro na validação: {error}")
        else:
            st.success("O schema do arquivo Excel está correto!")


================================================
File: /00-automacao-data-qualiy-excel-etl/tests/test_app.py
================================================
import os
import pytest
import subprocess
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException  # Importando TimeoutException
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys


#

@pytest.fixture(scope="module")
def driver():
    # Iniciar o Streamlit em background
    process = subprocess.Popen(["streamlit", "run", "src/app.py"])

    # Iniciar o WebDriver usando GeckoDriver
    driver = webdriver.Firefox()
    yield driver

    # Fechar o WebDriver e o Streamlit após o teste
    driver.quit()
    process.kill()

def test_app_opens(driver):
    driver.get("http://localhost:8501")

    # Aguardar um tempo para a aplicação carregar
    time.sleep(5)

    # Verificar se o título da página é o esperado

# def test_successful_upload(driver):
#     driver.get("http://localhost:8501")

#     # Aguardar um tempo para a aplicação carregar
#     time.sleep(5)

    # Realizar o upload do arquivo de sucesso
    success_file_path = os.path.abspath("data/success.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(success_file_path)

    # Aguardar a mensagem de sucesso
    time.sleep(5)
    assert "O schema do arquivo Excel está correto!" in driver.page_source

# def test_failed_upload(driver):
#     driver.get("http://localhost:8501")

#     # Aguardar um tempo para a aplicação carregar
#     time.sleep(5)

#     # Realizar o upload do arquivo de falha
#     failure_file_path = os.path.abspath("data/failure.xlsx")
#     driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(failure_file_path)

#     # Aguardar a mensagem de erro
#     time.sleep(5)
#     assert "Erro na validação" in driver.page_source

def test_successful_upload_using_select(driver):
    driver.get("http://localhost:8501")
    time.sleep(5)  # Aguarda a aplicação carregar

    try:
        select_box = driver.find_element(By.CLASS_NAME, "stSelectbox")
        ActionChains(driver).move_to_element(select_box).click().perform()
        time.sleep(2)  # Aguarda o selectbox abrir

        ActionChains(driver).send_keys("Usuario").send_keys(Keys.ENTER).perform()
        time.sleep(2)  # Aguarda a seleção ser feita

    except Exception as e:
        print(f"Erro durante a interação com o selectbox: {e}")
        return

    # Realizar o upload do arquivo de sucesso
    success_file_path = os.path.abspath("data/success.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(success_file_path)
    time.sleep(5)  # Aguarda o upload do arquivo

    # Aguardar a mensagem de sucesso
    success_message = "O schema do arquivo Excel está correto!"
    assert success_message in driver.page_source

def test_failed_upload_using_select(driver):
    driver.get("http://localhost:8501")
    time.sleep(5)  # Aguarda a aplicação carregar

    try:
        select_box = driver.find_element(By.CLASS_NAME, "stSelectbox")
        ActionChains(driver).move_to_element(select_box).click().perform()
        time.sleep(2)  # Aguarda o selectbox abrir

        ActionChains(driver).send_keys("Usuario").send_keys(Keys.ENTER).perform()
        time.sleep(2)  # Aguarda a seleção ser feita

    except Exception as e:
        print(f"Erro durante a interação com o selectbox: {e}")
        return

    # Realizar o upload do arquivo de falha
    failure_file_path = os.path.abspath("data/failure.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(failure_file_path)
    time.sleep(5)  # Aguarda o upload do arquivo

    # Aguardar a mensagem de sucesso
    failure_message = "Erro na validação"
    assert failure_message in driver.page_source

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/README.md
================================================
### Instalação e Configuração

Nosso projeto:

https://lvgalvao-workshop-aberto-aovivo-srcapp-508jxg.streamlit.app/

1. Clone o repositório:
```bash
git clone https://github.com/lvgalvao/Workshop-aberto-aovivo
cd Workshop-aberto-aovivo
```
2. Configure a versão correta do Python com `pyenv`:
```bash
pyenv install 3.11.5
pyenv local 3.11.5
```
3. Instale as dependências do projeto:
```bash
python -m venv .venv
# O padrao é utilizar .venv
source .venv/bin/activate
# Usuários Linux e mac
.venv\Scripts\Activate
# Usuários Windows
pip install -r requirements.txt  
```

4. Rode o projeto
```bash
task run
```

5. Rode os testes
```bash
task test
```

6. Rode a documentação
```bash
task docs
```

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/exemplo.env
================================================
POSTGRES_USER=seu_usuario
POSTGRES_PASSWORD=sua_senha
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=nome_do_seu_banco_de_dados
SENTRY_DNS=seu_dns

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/mkdocs.yml
================================================
site_name: "My Library"

theme:
  name: "material"

plugins:
- search
- mkdocstrings

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/pyproject.toml
================================================
[tool.taskipy.tasks]
run = "lsof -ti :8501 | xargs kill -9 | streamlit run src/app.py"
test = "lsof -ti :8501 | xargs kill -9 | pytest tests -v"


================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/requirements.txt
================================================
altair==5.2.0
annotated-types==0.6.0
attrs==23.2.0
Babel==2.14.0
blinker==1.7.0
cachetools==5.3.2
certifi==2023.11.17
charset-normalizer==3.3.2
click==8.1.7
colorama==0.4.6
dnspython==2.5.0
email-validator==2.1.0.post1
et-xmlfile==1.1.0
ghp-import==2.1.0
gitdb==4.0.11
GitPython==3.1.41
griffe==0.39.1
h11==0.14.0
idna==3.6
importlib-metadata==7.0.1
iniconfig==2.0.0
Jinja2==3.1.3
jsonschema==4.21.1
jsonschema-specifications==2023.12.1
Markdown==3.5.2
markdown-it-py==3.0.0
MarkupSafe==2.1.4
mdurl==0.1.2
mergedeep==1.3.4
mkdocs==1.5.3
mkdocs-autorefs==0.5.0
mkdocs-material==9.5.5
mkdocs-material-extensions==1.3.1
mkdocstrings==0.24.0
mkdocstrings-python==1.8.0
numpy==1.26.3
openpyxl==3.1.2
outcome==1.3.0.post0
packaging==23.2
paginate==0.5.6
pandas==2.2.0
pathspec==0.12.1
pillow==10.2.0
platformdirs==4.1.0
pluggy==1.4.0
protobuf==4.25.2
psutil==5.9.8
psycopg2-binary==2.9.9
pyarrow==15.0.0
pydantic==2.5.3
pydantic_core==2.14.6
pydeck==0.8.1b0
Pygments==2.17.2
pymdown-extensions==10.7
PySocks==1.7.1
pytest==7.4.4
python-dateutil==2.8.2
python-dotenv==1.0.1
pytz==2023.3.post1
PyYAML==6.0.1
pyyaml_env_tag==0.1
referencing==0.32.1
regex==2023.12.25
requests==2.31.0
rich==13.7.0
rpds-py==0.17.1
selenium==4.17.2
sentry-sdk==1.39.2
six==1.16.0
smmap==5.0.1
sniffio==1.3.0
sortedcontainers==2.4.0
SQLAlchemy==2.0.25
streamlit==1.30.0
taskipy==1.12.2
tenacity==8.2.3
toml==0.10.2
tomli==2.0.1
toolz==0.12.1
tornado==6.4
trio==0.24.0
trio-websocket==0.11.1
typing_extensions==4.9.0
tzdata==2023.4
tzlocal==5.2
urllib3==2.1.0
validators==0.22.0
watchdog==3.0.0
wsproto==1.2.0
zipp==3.17.0


================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/.python-version
================================================
3.11.5


================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/docs/index.md
================================================
# Welcome to MkDocs

For full documentation visit [mkdocs.org](https://www.mkdocs.org).

## Contrato

::: src.contrato.Vendas

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/sql/create.sql
================================================
CREATE TABLE vendas (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) NOT NULL,
    data TIMESTAMP NOT NULL,
    valor NUMERIC(10, 2) NOT NULL CHECK (valor >= 0),
    quantidade INTEGER NOT NULL CHECK (quantidade >= 0),
    produto VARCHAR(255) NOT NULL,
    categoria VARCHAR(50) NOT NULL
);

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/sql/insert.sql
================================================
-- Exemplo com dados valido

INSERT INTO vendas (email, data, valor, produto, quantidade, categoria)
VALUES (
    'comprador@example.com', 
    '2023-09-15 12:00:00',  -- Substitua pela data atual formatada como string
    100.50, 
    'Produto X', 
    3, 
    'categoria3'
);

-- Exemplo com dado invalido

INSERT INTO vendas (email, data, valor, produto, quantidade, categoria)
VALUES (
    'comprador', 
    '2023-09-15 12:00:00',  -- Substitua pela data atual formatada como string
    100.50, 
    'Produto X', 
    3, 
    'categoria3'
);

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/src/app.py
================================================
from frontend import ExcelValidadorUI
from backend import process_excel, save_dataframe_to_sql
from dotenv import load_dotenv
import sentry_sdk
import os
import logging

load_dotenv(".env")

sentry_sdk.init(
    dsn=os.getenv('SENTRY_DNS'),
    traces_sample_rate=1.0,
    profiles_sample_rate=1.0,
)

def main():
    ui = ExcelValidadorUI()
    ui.display_header()

    uploaded_file = ui.upload_file()

    if uploaded_file:
        df, result, errors = process_excel(uploaded_file)
        ui.display_results(result, errors)

        if errors:
            ui.display_wrong_message()
            sentry_sdk.capture_message("Erro ao subir excel")
            logging.error("Test")
        elif ui.display_save_button():
            # Se não houver erros e o botão for exibido, exibir o botão e fazer o log
            save_dataframe_to_sql(df)
            ui.display_success_message()
            sentry_sdk.capture_message("Banco de dados foi atualizado")
            logging.error("Test")

if __name__ == "__main__":
    main()

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/src/backend.py
================================================
import pandas as pd
from contrato import Vendas
from dotenv import load_dotenv
import os

load_dotenv(".env")

# Lê as variáveis de ambiente
POSTGRES_USER = os.getenv('POSTGRES_USER')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')
POSTGRES_HOST = os.getenv('POSTGRES_HOST')
POSTGRES_PORT = os.getenv('POSTGRES_PORT')
POSTGRES_DB = os.getenv('POSTGRES_DB')

# Cria a URL de conexão com o banco de dados
DATABASE_URL = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"

# Carrega as variáveis de ambiente
load_dotenv()

def process_excel(uploaded_file):
    try:
        df = pd.read_excel(uploaded_file)
        erros = []
        # Verificar se há colunas extras no DataFrame
        extra_cols = set(df.columns) - set(Vendas.model_fields.keys())
        if extra_cols:
            return False, f"Colunas extras detectadas no Excel: {', '.join(extra_cols)}"

        # Validar cada linha com o schema escolhido
        for index, row in df.iterrows():
            try:
                _ = Vendas(**row.to_dict())
            except Exception as e:
                erros.append(f"Erro na linha {index + 2}: {e}")

        # Retorna tanto o resultado da validação, os erros, quanto o DataFrame
        return df, True, erros

    except Exception as e:
        # Se houver exceção, retorna o erro e um DataFrame vazio
        return pd.DataFrame(), f"Erro inesperado: {str(e)}"
    
def save_dataframe_to_sql(df):
    # Salva o DataFrame no banco de dados
    df.to_sql('vendas', con=DATABASE_URL, if_exists='replace', index=False)

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/src/contrato.py
================================================
from pydantic import BaseModel, EmailStr, PositiveFloat, PositiveInt, field_validator
from datetime import datetime
from enum import Enum

class CategoriaEnum(str, Enum):
    categoria1 = "categoria1"
    categoria2 = "categoria2"
    categoria3 = "categoria3"

class Vendas(BaseModel):

    email: EmailStr
    data: datetime
    valor: PositiveFloat
    quantidade: PositiveInt
    produto: str
    categoria: CategoriaEnum

    @field_validator('categoria')
    def categoria_deve_estar_no_enum(cls, error):
        return error

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/src/frontend.py
================================================
import streamlit as st

class ExcelValidadorUI:

    def __init__(self):
        self.set_page_config()

    def set_page_config(self):
        st.set_page_config(
            page_title="Validador de schema excel"
        )

    def display_header(self):
        st.title("Validador de schema excel")

    def upload_file(self):
        return st.file_uploader("Carregue seu arquivo Excel aqui", type=["xlsx"])

    def display_results(self, result, errors):
        if errors:
            for error in errors:
                st.error(f"Erro na validação: {error}")
        else:
            st.success("O schema do arquivo Excel está correto!")

    def display_save_button(self):
        return st.button("Salvar no Banco de Dados")
    
    def display_wrong_message(self):
        return st.error("Necessário corrigir a planilha!")
    
    def display_success_message(self):
        return st.success("Dados salvos com sucesso no banco de dados!")

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/tests/test_app.py
================================================
from selenium import webdriver
from time import sleep
import pytest
import subprocess
from selenium.webdriver.common.by import By
import os
from selenium.webdriver.firefox.options import Options

@pytest.fixture
def driver():
    # Iniciar o Streamlit em background
    process = subprocess.Popen(["streamlit", "run", "src/app.py"])
    options = Options()
    options.headless = True  # Executar em modo headless
    driver = webdriver.Firefox(options=options)
    # Iniciar o WebDriver usando GeckoDriver
    driver.set_page_load_timeout(5)
    yield driver

    # Fechar o WebDriver e o Streamlit após o teste
    driver.quit()
    process.kill()

def test_app_opens(driver):
    # Verificar se a página abre
    driver.get("http://localhost:8501")
    sleep(2)

def test_check_title_is(driver):
    # Verificar se a página abre
    driver.get("http://localhost:8501")
    # Verifica se o titulo de página é
    sleep(2)
    # Capturar o título da página
    page_title = driver.title

    # Verificar se o título da página é o esperado
    expected_title = "Validador de schema excel"  # Substitua com o título real esperado
    assert page_title == expected_title

def test_check_streamlit_h1(driver):
    # Acessar a página do Streamlit
    driver.get("http://localhost:8501")

    # Aguardar para garantir que a página foi carregada
    sleep(2)  # Espera 5 segundos

    # Capturar o primeiro elemento <h1> da página
    h1_element = driver.find_element(By.TAG_NAME, "h1")

    # Verificar se o texto do elemento <h1> é o esperado
    expected_text = "Insira o seu excel para validação"
    assert h1_element.text == expected_text

def test_check_usuario_pode_inserir_um_excel_e_receber_uma_mensagem(driver):
    # Acessar a página do Streamlit
    driver.get("http://localhost:8501")

    # Aguardar para garantir que a página foi carregada
    sleep(3)  # Espera 5 segundos

    # Realizar o upload do arquivo de sucesso
    success_file_path = os.path.abspath("data/correto.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(success_file_path)

    # Aguardar a mensagem de sucesso
    sleep(3)
    assert "O schema do arquivo Excel está correto!" in driver.page_source

def test_check_mais_de_uma_mensagem_de_erro(driver):
    # Acessar a página do Streamlit
    driver.get("http://localhost:8501")

    # Aguardar para garantir que a página foi carregada
    sleep(3)  # Espera 5 segundos

    # Realizar o upload do arquivo de sucesso
    multiple_erros_file_path = os.path.abspath("data/multiplos_erros.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(multiple_erros_file_path)

    # Aguardar a mensagem de sucesso
    sleep(3)
    # Localizar todas as ocorrências da mensagem de erro
    error_messages = driver.find_elements(By.XPATH, "//*[contains(text(), 'Erro na validação')]")

    # Verificar se existem pelo menos duas mensagens de erro
    assert len(error_messages) == 2, f"Quantidade de mensagens de erro encontradas: {len(error_messages)}"

def test_check_usuario_insere_um_excel_valido_e_aparece_um_botao(driver):
    # Acessar a página do Streamlit
    driver.get("http://localhost:8501")

    # Aguardar para garantir que a página foi carregada
    sleep(3)  # Espera 3 segundos

    # Realizar o upload do arquivo de sucesso
    success_file_path = os.path.abspath("data/correto.xlsx")
    driver.find_element(By.CSS_SELECTOR, 'input[type="file"]').send_keys(success_file_path)

    # Aguardar a mensagem de sucesso
    sleep(3)
    assert "O schema do arquivo Excel está correto!" in driver.page_source
    # Verificar se o botão "Salvar no Banco de Dados" está presente
    buttons = driver.find_elements(By.XPATH, "//button")
    save_button = None
    for button in buttons:
        if button.text == "Salvar no Banco de Dados":
            save_button = button
            break

    assert save_button is not None and save_button.is_displayed()

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/tests/test_integration.py
================================================
import pandas as pd
import os
from dotenv import load_dotenv

load_dotenv(".env")

# Lê as variáveis de ambiente
POSTGRES_USER = os.getenv('POSTGRES_USER')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')
POSTGRES_HOST = os.getenv('POSTGRES_HOST')
POSTGRES_PORT = os.getenv('POSTGRES_PORT')
POSTGRES_DB = os.getenv('POSTGRES_DB')

# Cria a URL de conexão com o banco de dados
DATABASE_URL = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"

def test_read_data_and_check_schema():
    df = pd.read_sql('SELECT * FROM vendas', con=DATABASE_URL)

    # Verificar se o DataFrame não está vazio
    assert not df.empty, "O DataFrame está vazio."

    # Verificar o schema (colunas e tipos de dados)
    expected_dtype = {
        'id': 'int64',
        'email': 'object',  # object em Pandas corresponde a string em SQL
        'data': 'datetime64[ns]',
        'valor': 'float64',
        'produto': 'object',
        'quantidade': 'int64',
        'categoria': 'object'
    }

    assert df.dtypes.to_dict() == expected_dtype, "O schema do DataFrame não corresponde ao esperado."



================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/tests/test_unit.py
================================================
import pytest
from datetime import datetime
from src.contrato import Vendas
from pydantic import ValidationError

# Testes com dados válidos
def test_vendas_com_dados_validos():
    dados_validos = {
        "email": "comprador@example.com",
        "data": datetime.now(),
        "valor": 100.50,
        "produto": "Produto X",
        "quantidade": 3,
        "categoria": "categoria3",
    }

    # A sintaxe **dados_validos é uma forma de desempacotamento de dicionários em Python. 
    # O que isso faz é passar os pares chave-valor no dicionário dados_validos como argumentos nomeados para o construtor da classe Vendas.

    venda = Vendas(**dados_validos)

    assert venda.email == dados_validos["email"]
    assert venda.data == dados_validos["data"]
    assert venda.valor == dados_validos["valor"]
    assert venda.produto == dados_validos["produto"]
    assert venda.quantidade == dados_validos["quantidade"]
    assert venda.categoria == dados_validos["categoria"]

# Testes com dados inválidos
def test_vendas_com_dados_invalidos():
    dados_invalidos = {
        "email": "comprador",
        "data": "não é uma data",
        "valor": -100,
        "produto": "",
        "quantidade": -1,
        "categoria": "categoria3"
    }

    with pytest.raises(ValidationError):
        Vendas(**dados_invalidos)

# Teste de validação de categoria
def test_validacao_categoria():
    dados = {
        "email": "comprador@example.com",
        "data": datetime.now(),
        "valor": 100.50,
        "produto": "Produto Y",
        "quantidade": 1,
        "categoria": "categoria inexistente",
    }

    with pytest.raises(ValidationError):
        Vendas(**dados)

================================================
File: /01-como-estruturar-projetos-e-processos-de-dados-do-zero/.github/workflows/ci.yml
================================================
name: ci
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9","3.10","3.11","3.12"]

    steps:
      - name: Copia os arquivos do repositório
        uses: actions/checkout@v4

      - name: Instalar o Python  
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version}} 

      - name: Instalar Poetry
        run: pip install -r requirements-test.txt

      - name: Rodar os testes
        run: pytest tests/test_unit.py

      - name: Rodar os testes
        run: echo hello world!        

      - name: Segundo Passo
        run: echo $(ls)

      - name: Terceiro passo
        run: echo $(pwd)

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/README.md
================================================
# workshop_02_aovivo

Visite minha documentacao

[![image](/pic/print.png)](https://lvgalvao.github.io/workshop_02_aovivo/)


1. Clone o repositório:

```bash
git clone https://github.com/lvgalvao/workshop_02_aovivo.git
cd workshop_02_aovivo
```

2. Configure a versão correta do Python com `pyenv`:

```bash
pyenv install 3.11.5
pyenv local 3.11.5
```

3. Configurar poetry para Python version 3.11.5 e ative o ambiente virtual:

```bash
poetry env use 3.11.5
poetry shell
```

4. Instale as dependencias do projeto:

```bash
poetry install
```

5. Execute os testes para garantir que tudo está funcionando como esperado:

```bash
poetry run task test
```

6. Execute o comando para ver a documentação do projeto:

```bash
poetry run task test
```

7. Execute o comando de execucão da pipeline para realizar a ETL:

```bash
poetry run python app/etl.py
```

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/mkdocs.yml
================================================
site_name: My Docs

theme:
  name: material

plugins:
  - search
  - mermaid2
  - mkdocstrings

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/pyproject.toml
================================================
[tool.poetry]
name = "workshop-02-aovivo"
version = "0.1.0"
description = ""
authors = ["lvgalvaofilho <lvgalvaofilho@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
mkdocs = "^1.5.3"
mkdocs-mermaid2-plugin = "^1.1.1"
mkdocs-material = "^9.5.11"
mkdocstrings = {extras = ["python"], version = "^0.24.0"}
taskipy = "^1.12.2"
isort = "^5.13.2"
black = "^24.2.0"
pytest = "^8.0.1"
pandas = "^2.2.1"
tqdm = "^4.66.2"
duckdb = "^0.10.0"
pydantic = "^2.6.2"
pandera = {extras = ["io"], version = "^0.18.0"}
sqlalchemy = "^2.0.27"
python-dotenv = "^1.0.1"
psycopg2 = "^2.9.9"
psycopg2-binary = "^2.9.9"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.taskipy.tasks]
format = """
isort .
black .
"""
kill = "kill -9 $(lsof -t -i :8000)"
test = "pytest -v"
run = """
python3 app/main.py
"""
doc = "mkdocs serve"

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/schema_crm.py
================================================
from pandera import Check, Column, DataFrameSchema, Index, MultiIndex

schema = DataFrameSchema(
    columns={
        "id_produto": Column(
            dtype="int64",
            checks=[
                Check.greater_than_or_equal_to(min_value=1.0),
                Check.less_than_or_equal_to(max_value=10.0),
            ],
            nullable=False,
            unique=False,
            coerce=False,
            required=True,
            regex=False,
            description=None,
            title=None,
        ),
        "nome": Column(
            dtype="object",
            checks=None,
            nullable=False,
            unique=False,
            coerce=False,
            required=True,
            regex=False,
            description=None,
            title=None,
        ),
        "quantidade": Column(
            dtype="int64",
            checks=[
                Check.greater_than_or_equal_to(min_value=20.0),
                Check.less_than_or_equal_to(max_value=200.0),
            ],
            nullable=False,
            unique=False,
            coerce=False,
            required=True,
            regex=False,
            description=None,
            title=None,
        ),
        "preco": Column(
            dtype="float64",
            checks=[
                Check.greater_than_or_equal_to(min_value=5.0),
                Check.less_than_or_equal_to(max_value=120.0),
            ],
            nullable=False,
            unique=False,
            coerce=False,
            required=True,
            regex=False,
            description=None,
            title=None,
        ),
        "categoria": Column(
            dtype="object",
            checks=None,
            nullable=False,
            unique=False,
            coerce=False,
            required=True,
            regex=False,
            description=None,
            title=None,
        ),
    },
    checks=None,
    index=Index(
        dtype="int64",
        checks=[
            Check.greater_than_or_equal_to(min_value=0.0),
            Check.less_than_or_equal_to(max_value=9.0),
        ],
        nullable=False,
        coerce=False,
        name=None,
        description=None,
        title=None,
    ),
    dtype=None,
    coerce=True,
    strict=False,
    name=None,
    ordered=False,
    unique=None,
    report_duplicates="all",
    unique_column_names=False,
    add_missing_columns=False,
    title=None,
    description=None,
)


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/.python-version
================================================
3.11.5


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/etl.py
================================================
import os
from pathlib import Path

import pandas as pd
import pandera as pa
from dotenv import load_dotenv
from sqlalchemy import create_engine

from schema import ProdutoSchema, ProductSchemaKPI

def load_settings():
    """Carrega as configurações a partir de variáveis de ambiente."""
    dotenv_path = Path.cwd() / '.env'
    load_dotenv(dotenv_path=dotenv_path)

    settings = {
        "db_host": os.getenv("POSTGRES_HOST"),
        "db_user": os.getenv("POSTGRES_USER"),
        "db_pass": os.getenv("POSTGRES_PASSWORD"),
        "db_name": os.getenv("POSTGRES_DB"),
        "db_port": os.getenv("POSTGRES_PORT"),
    }
    return settings

@pa.check_output(ProdutoSchema, lazy=True)
def extrair_do_sql(query: str) -> pd.DataFrame:
    """
    Extrai dados do banco de dados SQL usando a consulta fornecida.

    Args:
        query: A consulta SQL para extrair dados.

    Returns:
        Um DataFrame do Pandas contendo os dados extraídos.
    """
    settings = load_settings()

    # Criar a string de conexão com base nas configurações
    connection_string = f"postgresql://{settings['db_user']}:{settings['db_pass']}@{settings['db_host']}:{settings['db_port']}/{settings['db_name']}"

    # Criar engine de conexão
    engine = create_engine(connection_string)

    with engine.connect() as conn, conn.begin():
            df_crm = pd.read_sql(query, conn)

    return df_crm

@pa.check_input(ProdutoSchema, lazy=True)
@pa.check_output(ProductSchemaKPI, lazy=True)
def transformar(df: pd.DataFrame) -> pd.DataFrame:
    """
    Transforma os dados do DataFrame aplicando cálculos e normalizações.

    Args:
        df: DataFrame do Pandas contendo os dados originais.

    Returns:
        DataFrame do Pandas após a aplicação das transformações.
    """
    # Calcular valor_total_estoque
    df['valor_total_estoque'] = df['quantidade'] * df['preco']
    
    # Normalizar categoria para maiúsculas
    df['categoria_normalizada'] = df['categoria'].str.lower()
    
    # Determinar disponibilidade (True se quantidade > 0)
    df['disponibilidade'] = df['quantidade'] > 0
    
    return df

import duckdb
import pandas as pd

@pa.check_input(ProductSchemaKPI, lazy=True)
def load_to_duckdb(df: pd.DataFrame, table_name: str, db_file: str = 'my_duckdb.db'):
    """
    Carrega o DataFrame no DuckDB, criando ou substituindo a tabela especificada.

    Args:
        df: DataFrame do Pandas para ser carregado no DuckDB.
        table_name: Nome da tabela no DuckDB onde os dados serão inseridos.
        db_file: Caminho para o arquivo DuckDB. Se não existir, será criado.
    """
    # Conectar ao DuckDB. Se o arquivo não existir, ele será criado.
    con = duckdb.connect(database=db_file, read_only=False)
    
    # Registrar o DataFrame como uma tabela temporária
    con.register('df_temp', df)
    
    # Utilizar SQL para inserir os dados da tabela temporária em uma tabela permanente
    # Se a tabela já existir, substitui.
    con.execute(f"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df_temp")
    
    # Fechar a conexão
    con.close()


if __name__ == "__main__":
    
    query = "SELECT * FROM produtos_bronze_email"
    df_crm = extrair_do_sql(query=query)
    df_crm_kpi = transformar(df_crm)

    with open("inferred_schema.json", "r") as file:
         file.write(df_crm_kpi.to_json())

    load_to_duckdb(df=df_crm_kpi, table_name="tabela_kpi")

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/etl_infer_schema.py
================================================
import os
from pathlib import Path

import pandas as pd
import pandera as pa
from dotenv import load_dotenv
from sqlalchemy import create_engine


def load_settings():
    """Carrega as configurações a partir de variáveis de ambiente."""
    dotenv_path = Path.cwd() / '.env'
    load_dotenv(dotenv_path=dotenv_path)

    settings = {
        "db_host": os.getenv("POSTGRES_HOST"),
        "db_user": os.getenv("POSTGRES_USER"),
        "db_pass": os.getenv("POSTGRES_PASSWORD"),
        "db_name": os.getenv("POSTGRES_DB"),
        "db_port": os.getenv("POSTGRES_PORT"),
    }
    return settings

def extrair_do_sql(query: str) -> pd.DataFrame:

    settings = load_settings()

    # Criar a string de conexão com base nas configurações
    connection_string = f"postgresql://{settings['db_user']}:{settings['db_pass']}@{settings['db_host']}:{settings['db_port']}/{settings['db_name']}"

    # Criar engine de conexão
    engine = create_engine(connection_string)

    with engine.connect() as conn, conn.begin():
            df_crm = pd.read_sql(query, conn)

    return df_crm

if __name__ == "__main__":
    
    query = "SELECT * FROM produtos_bronze"
    df_crm = extrair_do_sql(query=query)
    schema_crm = pa.infer_schema(df_crm)

    with open("schema_crm.py", "w", encoding="utf-8") as arquivo:
         arquivo.write(schema_crm.to_script())

    print(schema_crm)

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/ler_duckdb.py
================================================
import duckdb

def read_from_duckdb_and_print(table_name: str, db_file: str = 'my_duckdb.db'):
    """
    Lê dados de uma tabela DuckDB e imprime os resultados.

    Parâmetros:
    - table_name: Nome da tabela de onde os dados serão lidos.
    - db_file: Caminho para o arquivo DuckDB.
    """
    # Conectar ao DuckDB
    con = duckdb.connect(database=db_file)

    # Executar consulta SQL
    query = f"SELECT * FROM {table_name}"
    result = con.execute(query).fetchall()

    # Fechar a conexão
    con.close()

    # Imprimir os resultados
    for row in result:
        print(row)

if __name__ == "__main__":
    # Nome da tabela para consulta
    table_name = "tabela_kpi"
    
    # Ler dados da tabela e imprimir os resultados
    read_from_duckdb_and_print(table_name)

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/schema.py
================================================
import pandera as pa
from pandera.typing import DataFrame, Series

email_regex = r"[^@]+@[^@]+\.[^@]+"

class ProdutoSchema(pa.SchemaModel):
    """
    Define o esquema para a validação de dados de produtos com Pandera.
    
    Este esquema inclui campos básicos para produtos, incluindo um campo de e-mail
    validado por uma expressão regular.

    Attributes:
        id_produto (Series[int]): Identificador do produto, deve estar entre 1 e 20.
        nome (Series[str]): Nome do produto.
        quantidade (Series[int]): Quantidade disponível do produto, deve estar entre 20 e 200.
        preco (Series[float]): Preço do produto, deve estar entre 5.0 e 120.0.
        categoria (Series[str]): Categoria do produto.
        email (Series[str]): E-mail associado ao produto, deve seguir o formato padrão de e-mails.
    """
    id_produto: Series[int]
    nome: Series[str]
    quantidade: Series[int] = pa.Field(ge=20, le=200)
    preco: Series[float] = pa.Field(ge=05.0, le=120.0)
    categoria: Series[str]
    email: Series[str] = pa.Field(regex=email_regex)

    class Config:
        coerce = True
        strict = True

class ProductSchemaKPI(ProdutoSchema):

    valor_total_estoque: Series[float] = pa.Field(ge=0)  # O valor total em estoque deve ser >= 0
    categoria_normalizada: Series[str]  # Assume-se que a categoria será uma string, não precisa de check específico além de ser uma string
    disponibilidade: Series[bool]  # Disponibilidade é um booleano, então não precisa de check específico


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/schema_email.py
================================================
import pandera as pa
from pandera.typing import Series

email_regex = r"[^@]+@[^@]+\.[^@]+"

class ProdutoSchemaEmail(pa.SchemaModel):
    id_produto: Series[int] = pa.Field(ge=1, le=10)
    nome: Series[str]
    quantidade: Series[int] = pa.Field(ge=20, le=200)
    preco: Series[float] = pa.Field(ge=5.0, le=120.0)
    categoria: Series[str]
    email: Series[str] = pa.Field(regex=email_regex)

    class Config:
        coerce = True
        strict = True

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/docs/index.md
================================================
# Workshop 02 - Data Quality

Para desenvolver o desafio de negocio, vamos montar a seguinte ETL

## Fluxo

```mermaid
graph TD;
    A[Configura Variáveis] --> B[Ler o Banco SQL];
    B --> V[Validação do Schema de Entrada];
    V -->|Falha| X[Alerta de Erro];
    V -->|Sucesso| C[Transformar os KPIs];
    C --> Y[Validação do Schema de Saída];
    Y -->|Falha| Z[Alerta de Erro];
    Y -->|Sucesso| D[Salvar no DuckDB];
```

# Contrato de dados

::: app.schema.ProdutoSchema

# Transformacoes

## Configura Variáveis

::: app.etl.load_settings

## Ler o Banco SQL
::: app.etl.extrair_do_sql

## Transformar os KPIs

::: app.etl.transformar

## Salvar no DuckDB
S
::: app.etl.load_to_duckdb


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/exemplo/exemplo00.py
================================================
from pydantic import BaseModel, PositiveFloat, PositiveInt

dados = {
    "id_produto": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    "nome": ["Produto A", "Produto B", "Produto C", "Produto D", "Produto E", 
             "Produto F", "Produto G", "Produto H", "Produto I", "Produto J"],
    "quantidade": [100, 150, 200, 50, 120, 80, 60, 30, 90, 20],
    "preco": [10.0, 20.0, 15.0, 5.0, 22.0, 45.0, 120.0, 85.0, 55.0, 100.0],
    "categoria": ["eletronicos", "mobilia", "informatica", "decoracao", "eletronicos", 
                  "mobilia", "informatica", "decoracao", "eletronicos", "mobilia"]
}

class SchemaDados(BaseModel):
    id_produto: int
    nome: str
    quantidade: PositiveInt
    preco: PositiveFloat
    categoria: str

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/exemplo/exemplo01.py
================================================
from datetime import datetime
from typing import Tuple

from pydantic import BaseModel, PositiveInt, validate_call


class NumeroPositivo(BaseModel):
    numero: PositiveInt

@validate_call()
def calculadora(x: NumeroPositivo, y: NumeroPositivo) -> NumeroPositivo:
    return x + y

print(calculadora(4,-5))
print(calculadora(6,7))

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/exemplo/exemplo02.py
================================================
import pandas as pd
import pandera as pa
from pandera import Check, Column, DataFrameSchema

df = pd.DataFrame({
    "column1": [5, 10, 20],
    "column2": ["a", "b", "c"],
    "column3": pd.to_datetime(["2010", "2011", "2012"]),
})
schema = pa.infer_schema(df)

with open("inferred_schema.py", "w") as file:
         file.write(schema.to_script())


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/create_table_produtos_bronze.sql
================================================
CREATE TABLE produtos_bronze (
    id_produto SERIAL PRIMARY KEY,
    nome VARCHAR(255) NOT NULL,
    quantidade INT NOT NULL,
    preco FLOAT NOT NULL,
    categoria VARCHAR(255) NOT NULL
);


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/create_table_produtos_bronze_email.sql
================================================
CREATE TABLE produtos_bronze_email (
    id_produto SERIAL PRIMARY KEY,
    nome VARCHAR(255) NOT NULL,
    quantidade INT NOT NULL,
    preco FLOAT NOT NULL,
    categoria VARCHAR(255) NOT NULL,
    email VARCHAR(255) NOT NULL
);


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/insert_into_tabela_bronze.sql
================================================
INSERT INTO produtos_bronze (nome, quantidade, preco, categoria) VALUES
('Produto A', 100, 10.0, 'eletronicos'),
('Produto B', 150, 20.0, 'mobilia'),
('Produto C', 200, 15.0, 'informatica'),
('Produto D', 50, 5.0, 'decoracao'),
('Produto E', 120, 22.0, 'eletronicos'),
('Produto F', 80, 45.0, 'mobilia'),
('Produto G', 60, 120.0, 'informatica'),
('Produto H', 30, 85.0, 'decoracao'),
('Produto I', 90, 55.0, 'eletronicos'),
('Produto J', 20, 100.0, 'mobilia');


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/insert_into_tabela_bronze_email.sql
================================================
INSERT INTO produtos_bronze (nome, quantidade, preco, categoria, email) VALUES
('Produto A', 100, 10.0, 'eletronicos','produtoA@example.com' ),
('Produto B', 150, 20.0, 'mobilia', 'produtoA@example.com'),
('Produto C', 200, 15.0, 'informatica', 'produtoA@example.com'),
('Produto D', 50, 5.0, 'decoracao', 'produtoA@example.com'),
('Produto E', 120, 22.0, 'eletronicos', 'produtoA@example.com'),
('Produto F', 80, 45.0, 'mobilia', 'produtoA@example.com'),
('Produto G', 60, 120.0, 'informatica', 'produtoA@example.com'),
('Produto H', 30, 85.0, 'decoracao', 'produtoA@example.com'),
('Produto I', 90, 55.0, 'eletronicos', 'produtoA@example.com'),
('Produto J', 20, 100.0, 'mobilia', 'produtoA@example.com');


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/insert_wrong_values_into_tabela_bronze.sql
================================================
INSERT INTO produtos_bronze (nome, quantidade, preco, categoria) VALUES
('Produto A', 100, 10.0, 'eletronicos'),
('Produto B', -150, 20.0, 'mobilia'),
('Produto C', 200, 15.0, 'informatica'),
('Produto D', 50, 5.0, 'decoracao'),
('Produto E', 120, 22.0, 'eletronicos'),
('Produto F', 80, 45.0, 'mobilia'),
('Produto G', 60, 120.0, 'informatica'),
('Produto H', 30, 85.0, 'decoracao'),
('Produto I', 90, 55.0, 'eletronicos'),
('Produto J', 20, 100.0, 'mobilia');


================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/tests/test_func_etl.py
================================================
import pandas as pd

from app.etl import transformar

def test_calculo_valor_total_estoque():
    # Preparação
    df = pd.DataFrame({
        'quantidade': [10, 5],
        'preco': [20.0, 100.0],
        'categoria': ['brinquedos', 'eletrônicos']
    })
    expected = pd.Series([200.0, 500.0], name='valor_total_estoque')

    # Ação
    result = transformar(df)

    # Verificação
    pd.testing.assert_series_equal(result['valor_total_estoque'], expected)

def test_normalizacao_categoria():
    # Preparação
    df = pd.DataFrame({
        'quantidade': [1, 2],
        'preco': [10.0, 20.0],
        'categoria': ['brinquedos', 'eletrônicos']
    })
    expected = pd.Series(['BRINQUEDOS', 'ELETRÔNICOS'], name='categoria_normalizada')

    # Ação
    result = transformar(df)

    # Verificação
    pd.testing.assert_series_equal(result['categoria_normalizada'], expected)

def test_determinacao_disponibilidade():
    # Preparação
    df = pd.DataFrame({
        'quantidade': [0, 2],
        'preco': [10.0, 20.0],
        'categoria': ['brinquedos', 'eletrônicos']
    })
    expected = pd.Series([False, True], name='disponibilidade')

    # Ação
    result = transformar(df)

    # Verificação
    pd.testing.assert_series_equal(result['disponibilidade'], expected)

# Para rodar os testes, execute `pytest nome_do_arquivo.py` no terminal.

================================================
File: /02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/.github/workflows/CI.yml
================================================
name: ci

on: push
jobs:
    build-and-test:
        runs-on: ubuntu-latest
        steps:
            - name: Baixar o repositório
              uses: actions/checkout@v4

            - name: Instalar o Python
              uses: actions/setup-python@v5
              with:
                python-version: 3.11.5

            - name: Instalar o Poetry via pip
              run: pip install poetry

            - name: Instalar dependências com o Poetry
              run: poetry install

            - name: Rodar minha rotina de testes com o Poetry
              run: poetry run pytest tests -v

================================================
File: /03-deploy-de-apps-dados-com-docker/README copy.md
================================================
# Deploy de Aplicações de Dados Utilizando Docker

## Objetivo do Workshop

Este workshop de 4 horas visa fornecer aos analistas de dados e engenheiros de dados uma introdução prática ao uso do Docker para deploy de aplicações de dados. Os participantes aprenderão a empacotar e implantar eficientemente aplicações de dados, incluindo uma ETL em Python, um banco de dados PostgreSQL e um dashboard interativo usando Streamlit, tudo dentro de containers Docker.

![Solução](./pics/arquitetura.png)

## Cinco Motivos para Aprender Docker em Nosso Workshop

1. **Ensino do Zero**: Independentemente do seu nível de experiência prévia, vamos começar com os conceitos básicos de Docker, assegurando que todos os participantes tenham uma compreensão sólida dos fundamentos para construir sobre eles.

2. **Facilidade para Subir o Deploy**: Demonstraremos como Docker simplifica o processo de deploy de aplicações, permitindo que você foque na construção e no aprimoramento de suas aplicações, em vez de gastar tempo com configurações complexas de ambiente.

3. **Solução Versátil**: Docker é uma ferramenta poderosa que resolve uma variedade de problemas de desenvolvimento e operações, facilitando a colaboração entre equipes e melhorando a eficiência no ciclo de vida de desenvolvimento de software.

4. **Vantagens de Utilizar Docker na Cloud**: Explore como Docker se integra perfeitamente com serviços de cloud, potencializando a escalabilidade, a portabilidade e a eficiência dos recursos em ambientes de cloud computing.

5. **Solução Moderna**: Aprenda sobre as práticas atuais de desenvolvimento e operações que estão moldando o futuro da tecnologia. Docker é uma habilidade essencial em muitas áreas de TI, incluindo engenharia de dados, e dominá-la abrirá novas oportunidades profissionais.

## Agenda

### Parte 1: Introdução ao Docker (9:00 - 10:30)

- **9:00 - 9:15**: Boas-vindas e Introdução
- **9:15 - 9:45**: Docker e o Ecossistema de Cloud
- **9:45 - 10:00**: Introdução ao Heroku
- **10:00 - 10:30**: Prática: Deploy de uma Aplicação Simples no Docker

### Intervalo (10:30 - 11:00)

### Parte 2: Aplicações de Dados Avançadas com Docker (11:00 - 13:00)

- **11:00 - 11:30**: Deploy de um Banco de Dados PostgreSQL com Docker
- **11:30 - 12:15**: Construção de uma Solução de Dashboard com Streamlit e DuckDB
- **12:15 - 12:55**: Projeto Integrado: Dashboard com Dados do PostgreSQL
- **12:55 - 13:00**: Conclusão e Encerramento

## Pré-requisitos

- Assistir ao vídeo de Python + Vscode + Git
- Assistir ao vídeo de Poetry

## Estrutura do Projeto

Este repositório contém os diretórios e arquivos necessários para acompanhar as atividades práticas do workshop:

- `dashboard/`: Contém os arquivos para o deploy do dashboard Streamlit.
- `etl/`: Contém os scripts de ETL para processamento de dados.
- `postgres/`: Contém os arquivos necessários para configurar o banco de dados PostgreSQL em um container Docker.

## Como Usar

Cada diretório no repositório contém um `README.md` com instruções específicas sobre como construir e executar as aplicações correspondentes usando Docker.

================================================
File: /03-deploy-de-apps-dados-com-docker/pyproject.toml
================================================
[tool.poetry]
name = "deploy-de-apps-dados-no-docker"
version = "0.1.0"
description = ""
authors = ["Luciano Filho <lvgalvaofilho@gmail.com>"]
readme = "README.md"
packages = [{include = "deploy_de_apps_dados_no_docker"}]

[tool.poetry.dependencies]
python = "^3.11"
streamlit = "^1.31.1"
duckdb = "^0.10.0"
python-dotenv = "^1.0.1"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /03-deploy-de-apps-dados-com-docker/.python-version
================================================
3.12


================================================
File: /03-deploy-de-apps-dados-com-docker/src/collector/requirements.txt
================================================
streamlit
duckdb
sqlalchemy

================================================
File: /03-deploy-de-apps-dados-com-docker/src/dashboard/Dockerfile
================================================
# dashboard/Dockerfile
FROM python:3.8-slim
WORKDIR /app
COPY . /app
RUN pip install -r requirements.txt
CMD ["streamlit", "run", "app.py"]

================================================
File: /03-deploy-de-apps-dados-com-docker/src/dashboard/app.py
================================================
# dashboard/app.py
import streamlit as st
import duckdb
from dotenv import load_dotenv
import os

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém as variáveis de ambiente
DB_HOST = os.getenv('DB_HOST')
DB_PORT = os.getenv('DB_PORT')
DB_NAME = os.getenv('DB_NAME')
DB_USER = os.getenv('DB_USER')
DB_PASSWORD = os.getenv('DB_PASSWORD')

st.title('Dashboard de Dados')

# Cria a conexão com DuckDB em memória
con = duckdb.connect(database=':memory:', read_only=False)

# Instala e carrega a extensão Postgres no DuckDB
con.execute("INSTALL 'postgres';")
con.execute("LOAD 'postgres';")

# Conexão com o banco de dados PostgreSQL utilizando as variáveis de ambiente
pg_connection_string = f"host={DB_HOST} port={DB_PORT} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD}"

# Anexa o banco de dados PostgreSQL ao DuckDB
con.execute(f"ATTACH '{pg_connection_string}' AS pg_db (TYPE 'postgres');")

# Agora você pode realizar consultas SQL como se as tabelas do PostgreSQL fossem tabelas do DuckDB
result = con.execute("SELECT * FROM pg_db.public.my_table;").fetchall()

# Exibe o resultado no Streamlit
st.write(result)


================================================
File: /03-deploy-de-apps-dados-com-docker/src/dashboard/requirements.txt
================================================
streamlit
duckdb
sqlalchemy

================================================
File: /04-workflow-orchestration-deploy-airflow/README.md
================================================
# Orquestração de ETLs

## Eu realmente preciso do Airflow?

https://link.excalidraw.com/l/8pvW6zbNUnD/4OrmI6gFNlj

Vamos pensar em uma pipeline inicial, onde possui 3 atividades e uma pipeline que encadeia ela.

![Exemplo_00](./pic/exemplo_00.png)

exemplo_00.py
```python
from time import sleep

def primeira_atividade():
    print("Primeira atividade iniciada")
    sleep(1)
    print("Primeira atividade finalizada")

def segunda_atividade():
    print("Segunda atividade iniciada")
    sleep(1)
    print("Segunda atividade finalizada")

def terceira_atividade():
    print("Terceira atividade iniciada")
    sleep(1)
    print("Terceira atividade finalizada")

def pipeline():
    primeira_atividade()
    segunda_atividade()
    terceira_atividade()
    print("Pipeline finalizada")

if __name__ == "__main__":
    pipeline()
```

Qual o problema de realizar uma ETL desse jeito?

![Exemplo_01](./pic/exemplo_01.png)

Como programar ela a cada 10 segundos?

exemplo_01.py
```python
from time import sleep

def primeira_atividade():
    print("Primeira atividade iniciada")
    sleep(1)
    print("Primeira atividade finalizada")

def segunda_atividade():
    print("Segunda atividade iniciada")
    sleep(1)
    print("Segunda atividade finalizada")

def terceira_atividade():
    print("Terceira atividade iniciada")
    sleep(1)
    print("Terceira atividade finalizada")

def pipeline():
    primeira_atividade()
    segunda_atividade()
    terceira_atividade()
    print("Pipeline finalizada")

if __name__ == "__main__":
    while True:
        pipeline()
        sleep(10)
```

Qual o problema de realizar uma ETL desse jeito?

![Exemplo_02](./pic/exemplo_02.png)

Como ter o log a auditoria do que foi feito?

```python
from time import sleep

from loguru import logger

logger.add("execution_logs.log", format="{time} - {message}", level="INFO", rotation="1 day")

def primeira_atividade():
    logger.info("Primeira atividade iniciada")
    sleep(1)
    logger.info("Primeira atividade finalizada")

def segunda_atividade():
    logger.info("Segunda atividade iniciada")
    sleep(1)
    logger.info("Segunda atividade finalizada")

def terceira_atividade():
    logger.info("Terceira atividade iniciada")
    sleep(1)
    logger.info("Terceira atividade finalizada")

def pipeline():
    primeira_atividade()
    segunda_atividade()
    terceira_atividade()
    logger.info("Pipeline finalizada")

if __name__ == "__main__":
    while True:
        pipeline()
        sleep(10)
```

Qual o problema de realizar uma ETL desse jeito?

![Exemplo_03](./pic/exemplo_03.png)

Como ter um webserver (dashboard) para ver o que foi feito?

(...)

## Vocês entenderam onde eu quero chegar

Gastei 10% do tempo gerando valor e 90% do tempo reinventando a roda

## Airflow Overview

![Exemplo_03](./pic/airflow_overview.png)

O Apache Airflow é uma plataforma projetada para criar, agendar e monitorar fluxos de trabalho de forma programática.

Quando os fluxos de trabalho são definidos como código, eles se tornam mais fáceis de manter, versionar, testar e colaborar.

Utilize o Airflow para compor fluxos de trabalho como grafos acíclicos dirigidos (DAGs) de tarefas. O agendador do Airflow executa suas tarefas em uma série de workers respeitando as dependências definidas. Ferramentas de linha de comando abrangentes facilitam a realização de operações complexas nos DAGs. A interface de usuário intuitiva permite visualizar facilmente os pipelines em execução, monitorar o progresso e resolver problemas quando necessário.

O Airflow é especialmente útil em contextos de engenharia de dados e ciência de dados, pois permite a automação e a orquestração de processos complexos de tratamento de dados, treinamento de modelos de machine learning, execução de ETLs e muito mais. Tudo isso contribui para uma gestão mais eficiente do ciclo de vida dos dados e dos modelos preditivos.

## User Interface

- **DAGs**: Visão deral do seu ambiente.

  ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/dags.png)

- **Grid**: Visão de todas as execuções de um DAG.

  ![Grid](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/grid.png)

- **Graph**: Visão de todas as tarefas de um DAG e suas dependências.

  ![Graph](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/graph.png)

- **Task Duration**: Tempo de execução de cada tarefa de um DAG.

  ![Task Duration](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/duration.png)

- **Gantt**: Duração de cada execução de um DAG.

  ![Gantt](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/gantt.png)

- **Code**: Código de cada DAG.

  ![Code](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/code.png)

## Quick Start

Vamos fazer o nosso primeiro projeto com Airflow, vamos refatorar o nosso código acima.

Porém, precisamos criar toda nossa infraestrutura do Airflow.

[![Exemplo_04](./pic/airflow_tech.png)](https://docs.astronomer.io/learn/airflow-components)

Como subir toda essa infra?

Para isso vamos conhecer o Astro CLI

## Criando um projeto Astro

Em um diretório vazio, rode o seguinte comando:

```bash
astro dev init
```

Esse comando vai gerar os seguintes arquivos no seu diretório:

![astro_init](./pic/astro_init.png)

```graphql
.
├── .env                        # Variáveis de ambiente locais
├── dags                        # Onde suas DAGs ficam
│   ├── example-dag-basic.py    # DAG de exemplo que mostra uma simples pipeline de dados ETL
│   └── example-dag-advanced.py # DAG de exemplo que mostra recursos mais avançados do Airflow, como a API TaskFlow
├── Dockerfile                  # Para a imagem Docker do Astro Runtime, variáveis de ambiente e sobrescritas
├── include                     # Para quaisquer outros arquivos que você gostaria de incluir
├── plugins                     # Para quaisquer plugins personalizados ou da comunidade do Airflow
│   └── example-plugin.py
├── tests                       # Para quaisquer arquivos de teste de unidade das DAGs a serem executados com pytest
│   └── test_dag_example.py     # Teste que verifica erros básicos em suas DAGs
├── airflow_settings.yaml       # Para suas conexões do Airflow, variáveis e pools (localmente)
├── packages.txt                # Para pacotes a nível do sistema operacional
└── requirements.txt            # Para pacotes Python
```

### DAGs

[Documentação](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)


Uma DAG (Directed Acyclic Graph ou Grafo Acíclico Direcionado) é o conceito central do Airflow, reunindo Tarefas, organizadas com dependências e relações para dizer como elas devem ser executadas.

![dag](./pic/dag.png)

Ela define quatro Tarefas - A, B, C e D - e dita a ordem na qual devem ser executadas, e quais tarefas dependem de quais outras. Também determina com que frequência a DAG deve ser executada - talvez "a cada 5 minutos a partir de amanhã", ou "todos os dias desde 1º de janeiro de 2020".

A própria DAG não se preocupa com o que está acontecendo dentro das tarefas; ela está meramente preocupada em como executá-las - a ordem de execução, quantas vezes tentar novamente em caso de falha, se elas têm tempos de espera, e assim por diante.

Declarando uma DAG Existem três maneiras de declarar uma DAG - você pode usar um gerenciador de contexto, que adicionará a DAG a qualquer coisa dentro dele implicitamente:

```python
import datetime

from airflow import DAG
from airflow.operators.empty import EmptyOperator

with DAG(
    dag_id="meu_nome_de_dag",
    start_date=datetime.datetime(2021, 1, 1),
    schedule="@daily",
    catchup=False
):
    EmptyOperator(task_id="tarefa")
```

Ou, você pode usar um construtor padrão, passando a DAG para qualquer operador que você usar:

```python
import datetime

from airflow import DAG
from airflow.operators.empty import EmptyOperator

minha_dag = DAG(
    dag_id="meu_nome_de_dag",
    start_date=datetime.datetime(2024, 3, 23),
    schedule="@daily",
    catchup=False,  # Adiciona esta linha
)
EmptyOperator(task_id="tarefa", dag=minha_dag)
```

Ou, você pode usar o decorador `@dag` para transformar uma função em um gerador de DAG:

```python
import datetime

from airflow.decorators import dag
from airflow.operators.empty import EmptyOperator

@dag(start_date=datetime.datetime(2024, 3, 23), schedule="@daily", catchup=False)
def gerar_dag():
    EmptyOperator(task_id="tarefa")

gerar_dag()
```

DAGs não são nada sem Tasks para rodar, e essas sempre vão vir em forma de Operadores, Sensores ou Taskflows.

### Dependências de Tasks

Uma Task/Operador normalmente não vive sozinha, ela depende de outras tasks (aquelas que suas que são seu upstream), e outras tasks dependem dela (aquelas que são seu downstream). Declaram essas dependencias entre elas é o que cria uma estrutura de DAG (as edges/linhas dos directed acyclic graph)

Existem duas maneiras principais de declarar suas dependências, os operadores `>>` e `<<`

```python
first_task >> [second_task, third_task]
third_task << fourth_task
```

Você também pode declarar de uma maneira mais explicita com os métodos `set_upstream` e `set_downstream`

```python
first_task.set_downstream([second_task, third_task])
third_task.set_upstream(fourth_task)
```

Também existem alguns atalhos para declarar DAGs mais compelxas. Se você quer que duas lista de duas TASKS dependam todas de uma da outra, você pode usar o `cross_downstream`:

```python
from airflow.models.baseoperator import cross_downstream

# Replaces
# [op1, op2] >> op3
# [op1, op2] >> op4
cross_downstream([op1, op2], [op3, op4])
```

E se você quer que uma série de tasks tenham depedências entre sí, você pode usar uma `chain`:

```python
from airflow.models.baseoperator import chain

# Replaces op1 >> op2 >> op3 >> op4
chain(op1, op2, op3, op4)

# Replaces
# op1 >> op2 >> op4 >> op6
# op1 >> op3 >> op5 >> op6
chain(op1, [op2, op3], [op4, op5], op6)
```

### Carregando DAGs

Airflow carrega as DAGs dos seus arquivos Python, declarados dentro do seu `DAG_FOLDER`. Que irá pegar cada arquivo, executar, e carregar dentro de um objeto do tipo DAG.

### Rodando as DAGs

As DAGs vão rodar sempre de duas maneiras:

- Quando você são `triggered` seja de forma manual ou via uma API

- Definida via `schedule`, que é parte integrante de uma DAG

Você pode declarar seu schedule via argumento, exatamente assim:

O Airflow utiliza uma notação semelhante à dos cron jobs para definir os intervalos de agendamento das DAGs. Aqui estão alguns exemplos dos valores possíveis para o parâmetro schedule:

@once - Executa a DAG apenas uma vez, no momento da sua criação.
"* * * * *" - Executa a DAG a cada minuto
@hourly - Executa a DAG a cada hora.
@daily - Executa a DAG uma vez por dia.
@weekly - Executa a DAG uma vez por semana.
@monthly - Executa a DAG uma vez por mês.
@yearly - Executa a DAG uma vez por ano.
None - Se você não quer que a DAG seja agendada, mas apenas acionada manualmente, você pode usar None.
Notação cron (e.g., "0 0 * * *" para meia-noite diária, "*/10 * * * *" para a cada 10 minutos) - Permite especificar intervalos de agendamento personalizados utilizando a sintaxe cron.

### Nossa primeira ETL com Python

### Fazendo o Nosso Deploy

### Refatorando um Projeto seguindo MVC



================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_00.py
================================================
from time import sleep

def primeira_atividade():
    print("Primeira atividade iniciada")
    sleep(1)
    print("Primeira atividade finalizada")

def segunda_atividade():
    print("Segunda atividade iniciada")
    sleep(1)
    print("Segunda atividade finalizada")

def terceira_atividade():
    print("Terceira atividade iniciada")
    sleep(1)
    print("Terceira atividade finalizada")

def pipeline():
    primeira_atividade()
    segunda_atividade()
    terceira_atividade()
    print("Pipeline finalizada")

if __name__ == "__main__":
    pipeline()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_01.py
================================================
from time import sleep

def primeira_atividade():
    print("Primeira atividade iniciada")
    sleep(1)
    print("Primeira atividade finalizada")

def segunda_atividade():
    print("Segunda atividade iniciada")
    sleep(1)
    print("Segunda atividade finalizada")

def terceira_atividade():
    print("Terceira atividade iniciada")
    sleep(1)
    print("Terceira atividade finalizada")

def pipeline():
    primeira_atividade()
    segunda_atividade()
    terceira_atividade()
    print("Pipeline finalizada")

if __name__ == "__main__":
    while True:
        pipeline()
        sleep(10)


================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_02/exemplo_02.py
================================================
from time import sleep

from loguru import logger

logger.add("execution_logs.log", format="{time} - {message}", level="INFO", rotation="1 day")

def primeira_atividade():
    logger.info("Primeira atividade iniciada")
    sleep(1)
    logger.info("Primeira atividade finalizada")

def segunda_atividade():
    logger.info("Segunda atividade iniciada")
    sleep(1)
    logger.info("Segunda atividade finalizada")

def terceira_atividade():
    logger.info("Terceira atividade iniciada")
    sleep(1)
    logger.info("Terceira atividade finalizada")

def pipeline():
    primeira_atividade()
    segunda_atividade()
    terceira_atividade()
    logger.info("Pipeline finalizada")

if __name__ == "__main__":
    while True:
        pipeline()
        sleep(10)

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_02/requirements.txt
================================================
loguru

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_03/exemplo_03.py
================================================
# poetry add streamlit pandas

import streamlit as st
import pandas as pd
import subprocess

# Função para carregar os dados do arquivo CSV
def load_data():
    df = pd.read_csv("execution_logs.log")
    return df

# Função para executar o script Python
def run_python_script():
    subprocess.run("poetry run python pipeline/pipeline.py")


# Layout do aplicativo Streamlit
def main():
    st.title("Visualização de Logs e Execução de Scripts")
    st.image("pics/AirflowLogo.png")

    # Carregar os dados do arquivo CSV
    df = load_data()

    # Exibir os dados na interface do Streamlit
    st.write("Logs de Execução:", df)

    # Botão para atualizar os dados
    if st.button("Atualizar Dados"):
        df = load_data()
        st.write("Dados Atualizados com Sucesso!")

    # Botão para executar o script Python
    if st.button("Executar Script Python"):
        run_python_script()
        st.write("Script Python executado com sucesso!")

if __name__ == "__main__":
    main()


================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_03/requirements.txt
================================================
loguru
pandas
streamlit

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_03/pipeline/pipeline.py
================================================
from time import sleep

from loguru import logger

logger.add("execution_logs.log", format="{time} - {message}", level="INFO", rotation="1 day")

def primeira_atividade():
    logger.info("Primeira atividade iniciada")
    sleep(1)
    logger.info("Primeira atividade finalizada")

def segunda_atividade():
    logger.info("Segunda atividade iniciada")
    sleep(1)
    logger.info("Segunda atividade finalizada")

def terceira_atividade():
    logger.info("Terceira atividade iniciada")
    sleep(1)
    logger.info("Terceira atividade finalizada")

def pipeline():
    primeira_atividade()
    segunda_atividade()
    terceira_atividade()
    logger.info("Pipeline finalizada")

if __name__ == "__main__":
    while True:
        pipeline()
        sleep(10)

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_04/primeira_dag_com_python_operator.py
================================================
from datetime import datetime

from airflow.decorators import dag, task

from time import sleep

@dag(start_date=datetime(2024, 3, 23), 
     schedule="@daily", 
     catchup=False)
def primeira_dag_com_python_operator():
    """
    minha primeira Pipipeline
    """
    @task
    def primeira_atividade():
        print("Primeira atividade iniciada")
        sleep(1)
        print("Primeira atividade finalizada")

    @task
    def segunda_atividade():
        print("Segunda atividade iniciada")
        sleep(1)
        print("Segunda atividade finalizada")

    @task
    def terceira_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    t1 = primeira_atividade()
    t2 = segunda_atividade()
    t3 = terceira_atividade()

    t1 >> t2 >> t3

primeira_dag_com_python_operator()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_04/quarta_dag_com_python_operator.py
================================================
from datetime import datetime

from airflow.decorators import dag, task
from airflow.models.baseoperator import cross_downstream

from time import sleep

@dag(start_date=datetime(2024, 3, 23), 
     schedule="@daily", 
     catchup=False)
def quarta_dag_com_python_operator():
    """
    minha primeira Pipipeline
    """
    @task
    def primeira_atividade():
        print("Primeira atividade iniciada")
        sleep(1)
        print("Primeira atividade finalizada")

    @task
    def segunda_atividade():
        print("Segunda atividade iniciada")
        sleep(1)
        print("Segunda atividade finalizada")

    @task
    def terceira_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    @task
    def quarta_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    t1 = primeira_atividade()
    t2 = segunda_atividade()
    t3 = terceira_atividade()
    t4 = quarta_atividade()

    cross_downstream([t1,t2],[t3,t4])

quarta_dag_com_python_operator()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_04/quinta_dag_com_python_operator.py
================================================
from datetime import datetime

from airflow.decorators import dag, task
from airflow.models.baseoperator import chain

from time import sleep

@dag(start_date=datetime(2024, 3, 23), 
     schedule="@daily", 
     catchup=False)
def quinta_dag_com_python_operator():
    """
    minha primeira Pipipeline
    """
    @task(owner="luciano", retries=3)
    def primeira_atividade():
        """
        essa e minha primeira atividade
        """
        print("Primeira atividade iniciada")
        sleep(1)
        print("Primeira atividade finalizada")

    @task
    def segunda_atividade():
        print("Segunda atividade iniciada")
        sleep(1)
        print("Segunda atividade finalizada")

    @task
    def terceira_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    @task
    def quarta_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    @task
    def quinta_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")
    
    @task
    def sexta_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    t1 = primeira_atividade()
    t2 = segunda_atividade()
    t3 = terceira_atividade()
    t4 = quarta_atividade()
    t5 = quinta_atividade()
    t6 = sexta_atividade()

    chain(t1,[t2,t3],[t4,t5], t6)

quinta_dag_com_python_operator()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_04/segunda_dag_com_python_operator.py
================================================
from datetime import datetime

from airflow.decorators import dag, task

from time import sleep

@dag(start_date=datetime(2024, 3, 23), 
     schedule="@daily", 
     catchup=False)
def segunda_dag_com_python_operator():
    """
    minha primeira Pipipeline
    """
    @task
    def primeira_atividade():
        print("Primeira atividade iniciada")
        sleep(1)
        print("Primeira atividade finalizada")

    @task
    def segunda_atividade():
        print("Segunda atividade iniciada")
        sleep(1)
        print("Segunda atividade finalizada")

    @task
    def terceira_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    @task
    def quarta_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    t1 = primeira_atividade()
    t2 = segunda_atividade()
    t3 = terceira_atividade()
    t4 = quarta_atividade()

    t1 >> [t2,t3]
    t3 << t4

segunda_dag_com_python_operator()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_04/terceira_dag_com_python_operator.py
================================================
from datetime import datetime

from airflow.decorators import dag, task

from time import sleep

@dag(start_date=datetime(2024, 3, 23), 
     schedule="@daily", 
     catchup=False)
def terceira_dag_com_python_operator():
    """
    minha primeira Pipipeline
    """
    @task
    def primeira_atividade():
        print("Primeira atividade iniciada")
        sleep(1)
        print("Primeira atividade finalizada")

    @task
    def segunda_atividade():
        print("Segunda atividade iniciada")
        sleep(1)
        print("Segunda atividade finalizada")

    @task
    def terceira_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    @task
    def quarta_atividade():
        print("Terceira atividade iniciada")
        sleep(1)
        print("Terceira atividade finalizada")

    t1 = primeira_atividade()
    t2 = segunda_atividade()
    t3 = terceira_atividade()
    t4 = quarta_atividade()

    t1.set_downstream([t2,t3])
    t3.set_upstream(t4)

terceira_dag_com_python_operator()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_05/pegar_um_personagem.py
================================================
import requests

API = "https://www.boredapi.com/api/activity"

def descobrir_atividade():

    def pegar_atividade():
        r = requests.get(API, timeout=10)
        return r.json()
    
    def salvar_atividade_em_um_arquivo(response):
        filepath = "./activity.txt"	
        with open(filepath, "w") as f:
            f.write(f"Hoje eu vou: {response['activity']}")
        return filepath

    def ler_atividade_do_arquivo(filepath):
        with open(filepath, "r") as f:
            print(f.read())

    real_response = pegar_atividade()
    real_caminho = salvar_atividade_em_um_arquivo(real_response)
    ler_atividade_do_arquivo(real_caminho)

descobrir_atividade()


================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_05/pegar_um_personagem_airflow.py
================================================
from airflow.decorators import dag, task

from airflow.models import Variable

from datetime import datetime

import requests

API = "https://www.boredapi.com/api/activity"

@dag(dag_id="pegar_um_personagem_airflow",
     description="pipeline que pega um personagem, salva em um arquivo temp e le",
     schedule="* * * * *",
     start_date=datetime(2024,3,23),
     catchup=False,
     tags=["tutorial"])
def descobrir_atividade():

    @task
    def pegar_atividade():
        r = requests.get(API, timeout=10)
        return r.json()
    
    @task
    def salvar_atividade_em_um_arquivo(response):
        filepath = Variable.get("activity_file")
        with open(filepath, "a") as f:
            f.write(f"Hoje eu vou: {response['activity']}")
        return filepath

    @task
    def ler_atividade_do_arquivo(filepath):
        with open(filepath, "r") as f:
            print(f.read())

    t1 = pegar_atividade()
    t2 = salvar_atividade_em_um_arquivo(t1)
    t3 = ler_atividade_do_arquivo(t2)

    t1 >> t2 >> t3

descobrir_atividade()


================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/controller.py
================================================
import requests
from db import SessionLocal, engine, Base
from models import Pokemon
from schema import PokemonSchema
from random import randint

Base.metadata.create_all(bind=engine)

def gerar_numero_aleatorio():
    return randint(1, 350)

def fetch_pokemon_data(pokemon_id: int):
    response = requests.get(f"https://pokeapi.co/api/v2/pokemon/{pokemon_id}")
    if response.status_code == 200:
        data = response.json()
        types = ', '.join(type['type']['name'] for type in data['types'])
        return PokemonSchema(name=data['name'], type=types)
    else:
        return None

def add_pokemon_to_db(pokemon_schema: PokemonSchema) -> Pokemon:
    with SessionLocal() as db:
        db_pokemon = Pokemon(name=pokemon_schema.name, type=pokemon_schema.type)
        db.add(db_pokemon)
        db.commit()
        db.refresh(db_pokemon)
    return db_pokemon

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/db.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

SQLALCHEMY_DATABASE_URL = "postgresql://dbuser:ldxrJfmAE68oYsMKqKhcQwSeXh4Kevgc@dpg-cnv1g4ljm4es73dpa26g-a.oregon-postgres.render.com/dbname_3s0u"
engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/main.py
================================================
import time
import random
from controller import fetch_pokemon_data, add_pokemon_to_db, gerar_numero_aleatorio

def main():
    while True:
        pokemon_id = gerar_numero_aleatorio()  # Gera um ID aleatório entre 1 e 350
        pokemon_schema = fetch_pokemon_data(pokemon_id)
        if pokemon_schema:
            print(f"Adicionando {pokemon_schema.name} ao banco de dados.")
            add_pokemon_to_db(pokemon_schema)
        else:
            print(f"Não foi possível obter dados para o Pokémon com ID {pokemon_id}.")
        time.sleep(10)

if __name__ == "__main__":
    main()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/models.py
================================================
from sqlalchemy import Column, Integer, String, DateTime
from sqlalchemy.sql import func
from db import Base

class Pokemon(Base):
    __tablename__ = 'pokemons'
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String)
    type = Column(String)
    created_at = Column(DateTime, default=func.now())  # Campo adicionado

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/schema.py
================================================
from pydantic import BaseModel

class PokemonSchema(BaseModel):
    name: str
    type: str

    class Config:
        from_attributes = True


================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/final/capturar_pokemon.py
================================================
from airflow.decorators import task, dag

from include.controller import fetch_pokemon_data, add_pokemon_to_db, gerar_numero_aleatorio

from datetime import datetime

@dag(dag_id="capturar_pokemon",
     description="pipeline_para_capturar_pokemon",
     start_date=datetime(2024,3,23),
     schedule="* * * * *",
     catchup=False)
def capturar_pokemon():

    @task(task_id='gerar_numero_aleatorio')
    def task_gerar_numero_aleatorio():
        return gerar_numero_aleatorio()

    @task(task_id='fetch_pokemon_data')
    def task_fetch_pokemon_data(numero_aleatorio):
        return fetch_pokemon_data(numero_aleatorio)
    
    @task(task_id='add_pokemon_to_db')
    def task_add_pokemon_to_db(pokemon_data):
        add_pokemon_to_db(pokemon_data)
    
    t1 = task_gerar_numero_aleatorio()
    t2 = task_fetch_pokemon_data(t1)
    t3 = task_add_pokemon_to_db(t2)

    t1 >> t2 >> t3

capturar_pokemon()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/final/controller.py
================================================
import requests
from .db import SessionLocal, engine, Base
from .models import Pokemon
from .schema import PokemonSchema
from random import randint

Base.metadata.create_all(bind=engine)

def gerar_numero_aleatorio():
    return randint(1, 350)

def fetch_pokemon_data(pokemon_id: int):
    response = requests.get(f"https://pokeapi.co/api/v2/pokemon/{pokemon_id}")
    if response.status_code == 200:
        data = response.json()
        types = ', '.join(type['type']['name'] for type in data['types'])
        return PokemonSchema(name=data['name'], type=types)
    else:
        return None

def add_pokemon_to_db(pokemon_schema: PokemonSchema) -> Pokemon:
    with SessionLocal() as db:
        db_pokemon = Pokemon(name=pokemon_schema.name, type=pokemon_schema.type)
        db.add(db_pokemon)
        db.commit()
        db.refresh(db_pokemon)
    return db_pokemon

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/final/db.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

SQLALCHEMY_DATABASE_URL = "postgresql://dbuser:ldxrJfmAE68oYsMKqKhcQwSeXh4Kevgc@dpg-cnv1g4ljm4es73dpa26g-a.oregon-postgres.render.com/dbname_3s0u"
engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/final/models.py
================================================
from sqlalchemy import Column, Integer, String, DateTime
from sqlalchemy.sql import func
from .db import Base

class Pokemon(Base):
    __tablename__ = 'pokemons'
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String)
    type = Column(String)
    created_at = Column(DateTime, default=func.now())  # Campo adicionado

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_06/final/schema.py
================================================
from pydantic import BaseModel

class PokemonSchema(BaseModel):
    name: str
    type: str

    class Config:
        from_attributes = True


================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_07/README.md
================================================
Ao utilizar super().__init__(*args, **kwargs) dentro do método __init__ da sua classe derivada (neste caso, PostgresToDuckDBOperator), você está efetivamente chamando o construtor da classe pai (BaseOperator) e passando a ele quaisquer argumentos adicionais ou argumentos de palavra-chave que foram recebidos. Isso garante que a inicialização definida na classe pai seja executada, permitindo que sua classe derivada se beneficie de toda a lógica de inicialização e configuração já implementada na classe pai.

Esse mecanismo é particularmente útil em frameworks e bibliotecas onde a herança é um meio comum de estender funcionalidades. No Airflow, por exemplo, ao criar um operador customizado, geralmente você quer que ele tenha todos os comportamentos de um operador padrão do Airflow, incluindo a capacidade de receber argumentos padrão como task_id, retries, dag, entre outros, que são definidos e tratados na classe BaseOperator. Ao chamar super().__init__(*args, **kwargs), você assegura que seu operador customizado não apenas tem seus próprios atributos e métodos, mas também herda e se comporta conforme esperado dentro do ecossistema Airflow.

Essa prática não só apoia o princípio DRY (Don't Repeat Yourself) ao evitar a duplicação de código, como também facilita a manutenção e a atualização do código ao longo do tempo, já que as mudanças na lógica comum da classe pai automaticamente se aplicam a todas as suas subclasses, a menos que explicitamente sobrescritas.


================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_07/Dockerfile
================================================
FROM --platform=linux/amd64 quay.io/astronomer/astro-runtime:8.6.0

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_07/packages.txt
================================================
build-essential

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_07/requirements.txt
================================================
# Astro Runtime includes the following pre-installed providers packages: https://docs.astronomer.io/astro/runtime-image-architecture#provider-packages
duckdb==0.9.2
airflow-provider-duckdb==0.2.0
astro-sdk-python[duckdb]==1.6.1
apache-airflow-providers-postgres

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_07/.env_example
================================================
MOTHERDUCK_TOKEN='your motherduck token'

================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_07/dags/duckdb_custom_operator_example.py
================================================
from airflow.decorators import dag
from pendulum import datetime
from include.custom_operators.postgres_to_duckdb_operator import PostgresToDuckDBOperator

CONNECTION_DUCKDB = "my_motherduck_conn"  # minha connection ID da MotherDuck connection
CONNECTION_POSTGRESDB = "my_postgresdb_conn" # minha connection ID do PostgreSQL connection

@dag(start_date=datetime(2024, 3, 23), schedule="*/5 * * * *", catchup=False)
def pipeline_de_migracao_postgres_to_duckdb():
    PostgresToDuckDBOperator(
        task_id="postgres_to_duckdb",
        postgres_schema="public",
        postgres_table_name="pokemons",
        duckdb_conn_id=CONNECTION_DUCKDB,
        postgres_conn_id=CONNECTION_POSTGRESDB
    )

pipeline_de_migracao_postgres_to_duckdb()


================================================
File: /04-workflow-orchestration-deploy-airflow/exemplo_07/include/custom_operators/postgres_to_duckdb_operator.py
================================================
from airflow.models.baseoperator import BaseOperator
from duckdb_provider.hooks.duckdb_hook import DuckDBHook
from airflow.hooks.base_hook import BaseHook
import os

class PostgresToDuckDBOperator(BaseOperator):

    """
    https://github.com/apache/airflow/blob/main/airflow/models/baseoperator.py

    Essa classe define o operador customizado. Herda de BaseOperator, 
    o que significa que adota todas as funcionalidades de um operador 
    do Airflow, mas com lógica customizada definida no método execute.

    Operador que carrega um Postgres dentro de uma tabela no Duckdb.

    :param postgres_schema: Nome do schema de origem no PostgreSQL.
    :param postgres_table_name: Nome da tabela de origem no PostgreSQL e destino no DuckDB.
    :param duckdb_conn_id: ID da conexão Airflow para o DuckDB.
    :param postgres_conn_id: ID da conexão Airflow para o PostgreSQL.
    """

    def __init__(
        self,
        postgres_schema,
        postgres_table_name,
        duckdb_conn_id,
        postgres_conn_id,
        *args, **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.postgres_schema = postgres_schema
        self.postgres_table_name = postgres_table_name
        self.duckdb_conn_id = duckdb_conn_id
        self.postgresdb_conn_id = postgres_conn_id

    def execute(self, context):
        """
        Este método contém a lógica principal do operador, 
        que é executada quando a task correspondente ao operador 
        é acionada em um DAG do Airflow. Aqui está o que acontece 
        neste método:
        """
        ts = context["ts"]
        duckdb_hook = DuckDBHook(duckdb_conn_id=self.duckdb_conn_id)
        postgresql_conn = BaseHook.get_connection(self.postgresdb_conn_id)
        duckdb_conn = duckdb_hook.get_conn()
        duckdb_conn.execute("INSTALL postgres;")
        duckdb_conn.execute("LOAD postgres;")
        duckdb_conn.execute(f"""
            CREATE TABLE IF NOT EXISTS {self.postgres_table_name} 
            AS SELECT * FROM postgres_scan('
                host={postgresql_conn.host} 
                user={postgresql_conn.login} 
                port=5432 
                dbname={postgresql_conn.schema} 
                password={postgresql_conn.password}', 
                '{self.postgres_schema}', 
                '{self.postgres_table_name}');""")
        query = f"""
            INSERT INTO {self.postgres_table_name} 
            SELECT * FROM postgres_scan(
                'host={postgresql_conn.host} user={postgresql_conn.login} port=5432 dbname={postgresql_conn.schema} password={postgresql_conn.password}',
                '{self.postgres_schema}',
                '{self.postgres_table_name}')
                WHERE created_at > (SELECT MAX(created_at) FROM {self.postgres_table_name});
            """
        duckdb_conn.execute(query)
        self.log.info(f"Inserted new rows into {self.postgres_table_name}")

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/README.md
================================================
# Workshop Crawler

# Projeto de Web Scraping com Redis e MongoDB

## Visão Geral

Este projeto é focado em técnicas de Web Scraping usando Python. Utilizamos Redis e MongoDB como nossos bancos de dados para armazenamento e gerenciamento de dados coletados durante o processo de raspagem de dados.

### 1. Redis

Redis é um armazenamento de estrutura de dados em memória, usado como banco de dados, cache e message broker. Ele suporta estruturas de dados como strings, hashes, listas, sets, sorted sets com consultas de intervalo, bitmaps, hyperloglogs, geospatial indexes e streams. Redis tem um desempenho excepcional por manter os dados em memória.

### 2. MongoDB

MongoDB é um banco de dados NoSQL orientado a documentos. É escalável e flexível, permitindo que você armazene dados em um formato semelhante ao JSON com esquemas dinâmicos. Isso torna a integração de dados em certos tipos de aplicações mais fácil e rápida.

### 3. Web Scraping

Web Scraping é a técnica de extrair dados de websites. Este processo envolve fazer requisições HTTP para o servidor do site desejado, coletar dados da página e, em seguida, analisar esses dados para extrair informações úteis. Utilizamos Python e suas bibliotecas como BeautifulSoup e Scrapy para realizar estas tarefas.



================================================
File: /05-redis-mongodb-esse-tal-de-nosql/pyproject.toml
================================================
[tool.poetry]
name = "poetry new workshop-crawler"
version = "0.1.0"
description = "Projeto para o Workshop de Crawler"
authors = ["Fabio Cantarim <fabio.cantarim@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.12"
requests = "^2.31.0"
selenium = "^4.19.0"
beautifulsoup4 = "^4.12.3"
pandas = "^2.2.2"
redis = "^5.0.3"
pymongo = "^4.6.3"
mitmproxy = "^10.2.4"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /05-redis-mongodb-esse-tal-de-nosql/.env-example
================================================
REDIS_HOST=""
REDIS_PORT=""

USE_PROXY=False
PROXY_USER=""
PROXY_PASSWORD=""
PROXY_URL=""
PROXY_PORT=""

MONGO_HOST=""
MONGO_PORT=""
MONGO_DATABASE=""
MONGO_COLLECTION=""

HEADLESS="False"

DEFAULT_PROXY_USERNAME=""
DEFAULT_PROXY_PASSWORD=""

ARG1=""
ARG2=""

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/docker/docker-compose.yml
================================================
version: '3.8'

services:
  redis:
    image: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  mongodb:
    image: mongo
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db

volumes:
  redis_data:
  mongodb_data:


================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/start.py
================================================
from browser.generic_crawler import GenericBrowserCrawler
from request.generic_crawler import GenericRequestCrawler


ml = GenericBrowserCrawler("Ml").crawl('Nintendo Switch')
az = GenericBrowserCrawler("Amazon").crawl('Playstation')

ml2 = GenericRequestCrawler("Ml").crawl('Xbox')
az2 = GenericRequestCrawler("Amazon").crawl('Sega')

print(az2)

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/browser/b_ml_simple.py
================================================
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd
import time

class MercadoLivreCrawler:
    def __init__(self):
        # Configurando as opções do Chrome para rodar headless
        self.chrome_options = Options()
        # self.chrome_options.add_argument("--headless")  # Rodar o Chrome em modo headless
        self.chrome_options.add_argument("--no-sandbox")  # Evitar problemas de sandbox
        self.chrome_options.add_argument("--disable-gpu")
        self.chrome_options.add_argument("--disable-setuid-sandbox")
        self.chrome_options.add_argument("--disable-web-security")
        self.chrome_options.add_argument("--disable-dev-shm-usage")
        self.chrome_options.add_argument("--memory-pressure-off")
        self.chrome_options.add_argument("--ignore-certificate-errors")
        self.chrome_options.add_argument("--disable-features=site-per-process")

        # Inicializando o driver do Chrome
        self.driver = webdriver.Chrome(options=self.chrome_options)

    def execute_command(self, query):
        # Navegando para a página de pesquisa do Mercado Livre
        self.driver.get(f"https://lista.mercadolivre.com.br/{query.replace(' ', '-')}")
        
        # Aguardando um momento para a página carregar completamente
        time.sleep(5)

        # Obtendo o HTML da página
        html = self.driver.page_source

        # Analisando o HTML com BeautifulSoup
        soup = BeautifulSoup(html, "html.parser")

        # Extraindo os dados que você deseja
        results = soup.find_all("div", class_="ui-search-result")

        data = []
        for result in results:
            # Aqui você pode extrair informações específicas de cada resultado, como título, preço, etc.
            link = None
            title = result.find("h2", class_="ui-search-item__title").text.strip()
            price = result.find("span", class_="andes-money-amount__fraction").text.strip()
            link_tag =  result.find("a", class_="ui-search-link")
            if link_tag:
                link = link_tag.get("href")
            data.append({"Produto": title, "Preço": price, "URL": link})

        # Fechando o navegador
        self.driver.quit()

        return data

    def send_dataframe(self, query):
        data = self.execute_command(query)
        df = pd.DataFrame(data)
        return df

# Exemplo de utilização
crawler = MercadoLivreCrawler()
dataframe = crawler.send_dataframe("fire emblem warriros")
print(dataframe)


================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/browser/generic_crawler.py
================================================
##https://json-ld.org/learn.html
import time
import json
import pandas as pd
from datetime import datetime
from bs4 import BeautifulSoup
from browser.crawlers.default_crawler import AbstractCrawler
from browser.provider.actions.dict import action_dict

class GenericBrowserCrawler(AbstractCrawler):
    def __init__(self, type):
        super().__init__()
        self.type = type
        self.steps = json.loads(self.get_steps(self.type))
        if self.steps is None:
            raise("Crawler Não configurado!")

    def crawl(self, query):
        self.query = query
        self.execute_before()
        df = self.execute_main()
        self.execute_after()
        self.mongo.save_dataframe(df)
        print("Wait")
        
    def execute_main(self):
        self.browser.get(f"{self.steps["link"]["path"]}{self.query.replace(' ', self.steps["link"]["connector"])}")
        time.sleep(5)
        self.content = self.extraction()
        self.browser.quit()
        return self.transform_to_df_and_improve(self.content)    
    
    def execute_before(self):
        before = self.steps["script"]["before"]
        if before:
            for action in before:
                if action_dict[action] is None:
                    raise("Script não definido")
                action_dict[action](self.browser, before[action])
            return

    def execute_after(self):
        after = self.steps["script"]["after"]
        if after:
            for action in after:
                if action_dict[action] is None:
                    raise("Script não definido")
                action_dict[action](self.browser, after[action])
            return

    def extraction(self):
        self.html = self.browser.page_source
        
        soup = BeautifulSoup(self.html, "html.parser")
        
        if self.steps["search"]["custom"]:
            results = soup.find_all(self.steps["search"]["tag"], self.steps["search"]["custom"])
        else:
            results = soup.find_all(self.steps["search"]["tag"], class_=self.steps["search"]["class"])

        data = []
        for result in results:
            product = {}
            for step in self.steps["product"]:
                value = self.steps["product"][step]
                try:
                    content = eval(value)
                except:
                    content = None
                product[step] = content
            data.append(product)
        return data

    def transform_to_df_and_improve(self, data):
        df = pd.DataFrame(data)
        df = df.assign(keyword=self.query)
        df = df.assign(ecommerce = self.type)
        df = df.assign(dateTimeReference=datetime.now().isoformat())
        df = df.assign(crawlerType = "Browser")
        return df



================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/browser/crawlers/default_crawler.py
================================================
from abc import ABC, abstractmethod

from browser.provider.generic_b_crawler import GenericBrowserCrawler
from tools.redis import RedisClient
from tools.mongodb import MongoConnection


class AbstractCrawler(ABC):
    def __init__(self):
        self.browser = GenericBrowserCrawler().get_browser()
        self.redis = RedisClient.get()
        self.mongo = MongoConnection()

    @abstractmethod
    def crawl(self):
        pass
    
    @abstractmethod
    def execute_main(self):
        pass
    
    @abstractmethod
    def execute_before(self):
        pass
    
    @abstractmethod
    def execute_before(self):
        pass

    def get_steps(self, site):
        return self.redis.get(site)
    
    def save_data(self, data):
        try:
            self.mongo.save_dataframe(data)
        except:
            raise("Não foi possível salvar os dados no Mongo")

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/browser/provider/generic_b_crawler.py
================================================
import os
from selenium import webdriver

class GenericBrowserCrawler:

    browser: None
    options = webdriver.ChromeOptions()

    default_options =  [
        "--no-sandbox",
        "--disable-gpu",
        "--disable-setuid-sandbox",
        "--disable-web-security",
        "--disable-dev-shm-usage",
        "--memory-pressure-off",
        "--ignore-certificate-errors",
        "--disable-features=site-per-process",
        "--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"]

    def get_browser(self, args: list[str] = None):
        new_args = args
        if args is None:
            new_args = self.default_options
        self.set_options(new_args)
        return webdriver.Chrome(options=self.options)
    
    def is_headless(self):
        headless = os.getenv('HEADLESS')
        if headless is None:
            self.options.add_argument("--headless")

    
    def set_options(self, args: list[str] | None):
        self.is_headless()
        self.set_proxy()
        if args:
            for arg in args:
                self.options.add_argument(arg)

    def set_proxy(self):
        if os.getenv("USE_PROXY"):
            #Proxy url possibilities: IP or Protocol://User:Password@IP:Port
            user  = os.getenv("PROXY_USER")
            password = os.getenv("PROXY_PASSWORD")
            url = os.getenv("PROXY_URL")
            port = os.getenv("PROXY_PORT")
            proxy_provider = f'http://{user}:{password}@{url}:{port}'
            self.options.add_argument(f'--proxy-server={proxy_provider}')


================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/browser/provider/actions/dict.py
================================================

def goto(browser, url):
    return browser.get(url)



action_dict = {}


action_dict['goto'] = goto

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/mitm/start.py
================================================
from mitmproxy.tools.main import mitmdump
import os

username = os.getenv("DEFAULT_PROXY_USERNAME")
password = os.getenv("DEFAULT_PROXY_PASSWORD")
server = os.getenv("DEFAULT_PROXY_SERVER", 'brd.superproxy.io')
port = os.getenv("DEFAULT_PROXY_PORT", 22225)


mitmdump(args=[
    "-s", "./mitm/addon/proxy_controller.py",
    "--mode", f"upstream:http://{server}:{port}",
    "--upstream-auth", f"{username}:{password}",
    "--set stream_large_bodies=5g",
    "--set connection_strategy=lazy"
    "--set upstream_cert=false",
    "--ssl-insecure"
])

"""
--set connection_strategy=lazy

Determine when server connections should be established. 
When set to lazy, mitmproxy tries to defer establishing an upstream connection as long as possible. 
This makes it possible to use server replay while being offline. 
When set to eager, mitmproxy can detect protocols with server-side greetings, as well as accurately mirror TLS ALPN negotiation.
Default: eager
Choices: eager, lazy
"""

"""
--set stream_large_bodies=5g

Stream data to the client if response body exceeds the given threshold. 
If streamed, the body will not be stored in any way, and such responses cannot be modified. 
Understands k/m/g suffixes, i.e. 3m for 3 megabytes.
Default: None
"""

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/mitm/addon/proxy_controller.py
================================================
import logging
import base64

from os import environ as env
from uuid import uuid4

from mitmproxy import ctx, http
from mitmproxy.connection import Server
from mitmproxy.http import HTTPFlow
from mitmproxy.utils import strutils
from mitmproxy.net.server_spec import ServerSpec
# from mitmproxy.proxy.layers.http import HTTPMode

LOGGING_LEVEL = env.get("LOGGING_LEVEL", "INFO").upper()
logger = logging.getLogger(__name__)
logger.setLevel(LOGGING_LEVEL)

class ProxyControllerAddon:
    def responseheaders(self, flow: HTTPFlow):
        """
        Enables streaming for all responses.
        This method sets `stream` attribute to True in the response, 
        allowing streaming large responses.

        Equivalent to passing `--set stream_large_bodies=1` to mitmproxy.
        """
        flow.response.stream = True

    def request(self, flow: HTTPFlow):
        self.set_proxy_configs(flow)
    
    
    def response(self, flow: HTTPFlow):
        pass

    def set_proxy_configs(self, flow: HTTPFlow):
        if self.has_new_proxy_auth(flow):
                flow.request.headers["Proxy-Authorization"] = self.has_new_proxy_auth(flow)
        if self.has_new_proxy_server(flow):
            address = self.has_new_proxy_server(flow)
            logger.info(f"UPSTREAM_PROXY_ENDPOINT: {address}")
            is_proxy_change = address != flow.server_conn.address
            if is_proxy_change:
                flow.server_conn = Server(address=address)
                if flow.request.headers["x-New-Proxy-Server"] != "No-Proxy":
                    flow.server_conn.via = ServerSpec(("http", address))            

    def has_new_proxy_auth(self, flow: HTTPFlow):
        """
        Checks if the request contains the x-New-Proxy-Auth header.
        If found, retrieves the parameter and encodes it in base64, 
        then concatenates the string "Basic" with the encoded value.
        The parameter should be in the format {username}:{password}.
        """
        if "x-New-Proxy-Auth" in flow.request.headers:
            user, password = flow.request.headers["x-New-Proxy-Auth"].split(":")
            session = 15054
            new_auth = f'{user}-session-{session}-c_tag-{session}:{password}'
            return "Basic " + base64.b64encode(strutils.always_bytes(new_auth)).decode("utf-8")
        return None

    def has_new_proxy_server(self, flow: HTTPFlow):
        """
        Checks if the request contains the x-New-Proxy-Server header.
        If found, retrieves the parameter in the format {host}:{port}.
        The parameter could be an IP address or hostname.
        Returns a tuple containing the host (string) and port (integer).
        """
        if "x-New-Proxy-Server" in flow.request.headers:
            if flow.request.headers["x-New-Proxy-Server"] == "No-Proxy":
                return {"localhost", 80}
            else:
                proxy_server = flow.request.headers["x-New-Proxy-Server"]
                host,port = proxy_server.split(":")
                return (host, int(port))
        return 
  
addons = [ProxyControllerAddon()]

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/mitm/test/download.py
================================================
import requests
from os import environ as env

# Configuração do proxy para o MITM Proxy
proxy = {
    'http': 'http://localhost:8080',
    'https': 'http://localhost:8080',
}

cert_path = './mitmproxy-ca-cert.pem'


# URL do arquivo que você deseja baixar
url = 'http://releases.ubuntu.com/22.04.4/ubuntu-22.04.4-desktop-amd64.iso'

headers = {
    # 'Proxy-Authorization': env.get("ARG1"),
    'Proxy-Authentication': env.get("ARG2"),
    # 'Proxy-Server' : '20.33.5.27:8888'
}

# Realiza a solicitação HTTP através do MITM Proxy
with requests.get(url, stream=True, proxies=proxy, verify=cert_path, headers=headers) as r:
# with requests.get(url, stream=True, proxies=proxy) as r:    
# Verifica o tamanho total do arquivo, se disponível
    total_size = r.headers.get('content-length')
    if total_size is None:
        print("O tamanho total do arquivo é desconhecido.")
    else:
        total_size = int(total_size)

    print(f'Tamanho do arquivo é {total_size}')
    # Inicializa a quantidade de bytes recebidos
    downloaded_size = 0

    # Trata a resposta conforme necessário
    if r.status_code == 200:
        with open('arquivo_baixado.iso', 'wb') as f:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk: # Verifica se há dados no chunk
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    if total_size:
                        progress = downloaded_size / total_size * 100
                        print(f"Progresso: {progress:.2f}%")
        print("Download concluído.")
    else:
        print(f"Erro: {r.status_code}")


================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/mitm/test/ip.py
================================================
import requests
from os import environ as env

url = "http://lumtest.com/myip.json"
# url = "http://www.globo.com"

proxy = {
    'http': 'http://localhost:8080',
    'https': 'http://localhost:8080',
}

cert_path = './mitmproxy-ca-cert.pem'

headers = {
    'Proxy-Authorization': env.get("ARG1"),
    # 'x-New-Proxy-Auth': env.get("ARG2")
    # 'x-New-Proxy-Server' : '38.145.211.246:8899'
    # 'x-New-Proxy-Server' : 'No-Proxy'

}

# Fazendo a requisição GET
response = requests.get(url, proxies=proxy, verify=cert_path, headers=headers)
# Verifica se a requisição foi bem-sucedida (código de status 200)
if response.status_code == 200:
    # Imprime o conteúdo da resposta (o endereço IP retornado pelo site)
    print("Endereço IP retornado pelo site:", response.json()["ip"])
    print("Cidade", response.json()["geo"] )
    
else:
    # Se a requisição não foi bem-sucedida, imprime o código de status
    print("Falha na requisição. Código de status:", response.status_code)

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/mitm/test/mitmproxy-ca-cert.pem
================================================
-----BEGIN CERTIFICATE-----
MIIDNTCCAh2gAwIBAgIUdyrzXy9rHnJXw+rpyMcUopNCl50wDQYJKoZIhvcNAQEL
BQAwKDESMBAGA1UEAwwJbWl0bXByb3h5MRIwEAYDVQQKDAltaXRtcHJveHkwHhcN
MjQwMzEyMTcyNzQ0WhcNMzQwMzEyMTcyNzQ0WjAoMRIwEAYDVQQDDAltaXRtcHJv
eHkxEjAQBgNVBAoMCW1pdG1wcm94eTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC
AQoCggEBALvbpilEsx8WYdV+SuA2JWYVN/7idrc3Qz3uCXg9gAuHAAyrMAYZMQfD
IVsc1iAxVqPev3PsJZP5Y8xp3cXrNsOHNnCbnXL0i7uOFeFadYGMLe7a9VaAuDv0
iviUcse1hTKOF2r3Lb2Kkk572x7UcBErujlqOrw6+6RNKa5B03ud+pkPhTv7tT8Z
wbJ2VAA70+64E13zv88Gnk8Ln9Jzq8Nw3yrzszhp1GuEjPrDQGRZ+0nz38XVfhv3
6Yukfm8GZb0NTWott1/swXWzLJUI4bWeC+y9o77etBC1aREuHal8mRhmVF5z2iqN
fO4y5TZusAGs1ILceXaQa66zb1K8VOcCAwEAAaNXMFUwDwYDVR0TAQH/BAUwAwEB
/zATBgNVHSUEDDAKBggrBgEFBQcDATAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYE
FPK5t017L37CqHbfwXksKewGAJwmMA0GCSqGSIb3DQEBCwUAA4IBAQBELjafhk2B
zdzKksZVcTNYzCSIvcUE6NakjdcTk0ntt826XX37us8E3p9x8v76q7Wi9jR1TVd5
9DuMQzzpXEsQRV5b+j1rCMjMuqNvDW1mNzHskOro//qEnBveFPAV2JYhZTrDMTn7
HKRbMktsrFD36hOmtagzJ+NTv8dXRBGV9ZFOUhA7bdesLRJLkXtpXoJwPVslhAwq
s42trpO09altAu+7bZ7T18KeO4lSMPZKKZVZdPDCBFOzNvhwneB3+6GYt7aaq7hA
bqciGRRWao2ua3d2hw+r2q1ulUw7QBH/UP/2HVg0g5tTPqvX2349t0MB8gl1ZdYo
BpX7V9gmpdKx
-----END CERTIFICATE-----


================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/request/generic_crawler.py
================================================
from datetime import datetime
import json

from bs4 import BeautifulSoup
import pandas as pd
import requests
from request.crawlers.default_crawler import AbstractCrawler


class GenericRequestCrawler(AbstractCrawler):
    def __init__(self, type):
        super().__init__()
        self.type = type

    def crawl(self,query):
        self.query = query
        self.configs = json.loads(self.get_steps(self.type))
        if self.configs is None:
            raise("Crawler Não configurado!")
        self.get_data()
        self.extraction()
        self.transform_to_df_and_improve()
        self.save_data(self.df)


    def get_data(self):
        url = f"{self.configs["link"]["path"]}{self.query.replace(' ', self.configs["link"]["connector"])}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }

        response = requests.get(url, headers=headers)

        if response.status_code > 400:
            raise("Status Code != 200")    
        
        self.data = response.text
        

    def extraction(self):
        soup = BeautifulSoup(self.data, "html.parser")
        
        if self.configs["search"]["custom"]:
            results = soup.find_all(self.configs["search"]["tag"], self.configs["search"]["custom"])
        else:
            results = soup.find_all(self.configs["search"]["tag"], class_=self.configs["search"]["class"])

        data = []
        for result in results:
            product = {}
            for step in self.configs["product"]:
                value = self.configs["product"][step]
                try:
                    content = eval(value)
                except:
                    content = None
                product[step] = content
            data.append(product)
        self.df = data

    def transform_to_df_and_improve(self):
        df = pd.DataFrame(self.df)
        df = df.assign(keyword=self.query)
        df = df.assign(ecommerce = self.type)
        df = df.assign(dateTimeReference=datetime.now().isoformat())
        df = df.assign(crawlerType = "Request")
        self.df = df


================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/request/r_ml_simple.py
================================================
import requests
from bs4 import BeautifulSoup
import pandas as pd

class MercadoLivreCrawler:
    def execute_command(self, query):
        url = f"https://lista.mercadolivre.com.br/{query.replace(' ', '-')}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }

        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            results = soup.find_all("div", class_="ui-search-result")

            data = []
            for result in results:
                link = None
                title = result.find("h2", class_="ui-search-item__title").text.strip()
                price = result.find("span", class_="andes-money-amount__fraction").text.strip()
                link_tag =  result.find("a", class_="ui-search-link")
                if link_tag:
                    link = link_tag.get("href")
                data.append({"Produto": title, "Preço": price, "URL": link})

            return data
        else:
            print("Falha ao fazer a solicitação HTTP.")
            return None

    def send_dataframe(self, query):
        data = self.execute_command(query)
        if data:
            df = pd.DataFrame(data)
            return df
        else:
            return None

# Exemplo de utilização
crawler = MercadoLivreCrawler()
dataframe = crawler.send_dataframe("iphone 12")
if dataframe is not None:
    print(dataframe)
else:
    print("Erro ao obter os dados.")


================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/request/crawlers/default_crawler.py
================================================
from abc import ABC, abstractmethod

from browser.provider.generic_b_crawler import GenericBrowserCrawler
from tools.redis import RedisClient
from tools.mongodb import MongoConnection


class AbstractCrawler(ABC):
    def __init__(self):
        self.redis = RedisClient.get()
        self.mongo = MongoConnection()

    @abstractmethod
    def crawl(self):
        pass

    def get_steps(self, site):
        return self.redis.get(site)
    
    def save_data(self, data):
        try:
            self.mongo.save_dataframe(data)
        except:
            raise("Não foi possível salvar os dados no Mongo")

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/tools/mongodb.py
================================================
import pandas as pd
import os
from pymongo import MongoClient

class MongoConnection:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super().__new__(cls)
            cls._instance._client = None
            cls._instance._db = None
            cls._instance._collection = None
        return cls._instance

    def __init__(self, host='localhost', port=27017, database='test', collection='data'):
        if self._client is None:
            self.host = os.getenv("MONGO_HOST", host)
            self.port = int(os.getenv("MONGO_PORT",port))
            self.database_name = os.getenv("MONGO_DATABASE",database)
            self.collection_name = os.getenv("MONGO_COLLECTION",collection)
            self._connect()

    def _connect(self):
        self._client = MongoClient(self.host, self.port)
        self._db = self._client[self.database_name]
        self._collection = self._db[self.collection_name]

    def save_dataframe(self, df):
        # Convertendo o DataFrame para formato JSON
        data = df.to_dict(orient='records')
        # Inserindo os dados na coleção MongoDB
        self._collection.insert_many(data)
        print("DataFrame salvo no MongoDB com sucesso.")

    def close_connection(self):
        if self._client:
            self._client.close()
            print("Conexão fechada com sucesso.")

================================================
File: /05-redis-mongodb-esse-tal-de-nosql/src/tools/redis.py
================================================
import os
import redis


class RedisClient:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(RedisClient, cls).__new__(cls, *args, **kwargs)
            cls._instance._redis_client = cls._instance._connect_to_redis()
        return cls._instance

    @staticmethod
    def _load_config():
        return {
            "host": os.getenv("REDIS_HOST","localhost"),
            "port": os.getenv("REDIS_PORT",6379),
            "decode_responses": True
        }

    @classmethod
    def _connect_to_redis(cls):
        config = cls._load_config()
        redis_client = redis.StrictRedis(**config)
        return redis_client

    @classmethod
    def get(cls):
        return cls()._redis_client

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/README.md
================================================
## Objetivo

## README em revisão

Excalidraw: https://link.excalidraw.com/l/8pvW6zbNUnD/7APGEyy8COJ

Este workshop visa introduzir os conceitos fundamentais (Redes, protocolo, HTTP, comumicação entre processos etc) e as práticas necessárias para criar uma API usando a framework FastAPI em Python.

Os participantes aprenderão desde a configuração do ambiente de desenvolvimento até a criação e documentação de APIs.

## Curl (Cliente por linha de comando)

### Introdução ao Curl

**Curl** (Client URL) é uma ferramenta de software e uma biblioteca de linha de comando usada para transferir dados com URLs. Ela suporta uma diversidade de protocolos, como HTTP, HTTPS, FTP, FTPS, SCP, SFTP, entre outros. Amplamente utilizada por desenvolvedores e administradores de sistema, a ferramenta permite a interação com servidores web e outros tipos de servidores de internet para baixar ou enviar dados.

### Por Que Usar Curl?

**1. Versatilidade:** Curl pode ser usado para testar conectividade de API, desenvolver e depurar serviços web, automatizar tarefas de upload e download de arquivos e muito mais.

**2. Suporte Amplo de Protocolos:** Suportando uma grande variedade de protocolos de comunicação de dados, curl é extremamente flexível para qualquer necessidade de rede.

**3. Automação:** Curl é ideal para scripts automatizados devido à sua natureza de linha de comando. Ele pode ser integrado em scripts bash ou shell para automação de tarefas de rede.

**4. Disponibilidade:** Disponível em quase todas as plataformas Unix-like, incluindo Linux e macOS, e também em Windows, curl é uma ferramenta universal.

**5. Comunidade e Suporte:** Sendo um projeto de código aberto, curl é bem documentado e tem uma comunidade ativa que contribui para sua melhoria contínua.

### O Que é o Protocolo HTTP?

**HTTP (HyperText Transfer Protocol)** é o protocolo de comunicação utilizado para transmitir informações na web. É a base para qualquer troca de dados na internet, permitindo a comunicação entre clientes web (navegadores) e servidores web. O protocolo define métodos de requisição que indicam a ação desejada para um determinado recurso, como `GET` para solicitar dados de um recurso, ou `POST` para submeter dados para serem processados a um recurso.

### Características do HTTP:

**1. Simples e Extensível:** Projetado para ser simples e fácil de implementar, enquanto permite extensões para aumentar sua funcionalidade.

**2. Stateless:** HTTP é um protocolo sem estado, o que significa que cada requisição é independente das outras e não deve afetar o comportamento das outras requisições. Contudo, sessões e cookies podem ser usados para adicionar estado em comunicações HTTP.

**3. Flexível:** HTTP permite a transferência de qualquer tipo de dados, desde que ambas as partes (cliente e servidor) possam interpretar esses dados.

Aqui estão os oito exercícios propostos para praticar o uso do `curl`, acompanhados das respostas esperadas para cada um:

### Exercício 1: Fazendo Requisições Básicas

**Objetivo**: Familiarizar-se com requisições GET e a saída do `curl`.

* **Comando**:
    
    ```bash
    curl -v http://httpbin.org/get
    ```
    
* **Resposta Esperada**: Você verá detalhes completos da requisição e da resposta, incluindo cabeçalhos HTTP enviados e recebidos. Isso inclui informações sobre o método usado (GET), o host acessado, e cabeçalhos como `User-Agent` e `Accept`.

### Exercício 2: Trabalhando com Parâmetros de Query

**Objetivo**: Aprender a enviar parâmetros de query em URLs.

* **Comando**:
    
    ```bash
    curl http://httpbin.org/get?name=John&age=30
    ```
    
* **Resposta Esperada**: A resposta de httpbin.org refletirá os parâmetros que você enviou. No JSON de resposta, você verá um objeto "args" contendo `"name": "John", "age": "30"`.

### Exercício 3: Postando Dados JSON

**Objetivo**: Praticar o envio de dados JSON em uma requisição POST.

* **Comando**:
    
    ```bash
    curl -X POST http://httpbin.org/post -H "Content-Type: application/json" -d '{"username":"john", "password":"12345"}'
    ```

Importância do -X:
O uso do -X é particularmente importante quando você precisa realizar operações específicas que requerem diferentes tipos de métodos HTTP. Por exemplo:

GET: Usado para solicitar dados de um recurso específico.
POST: Usado para enviar dados para serem processados para um recurso específico. Normalmente resulta em uma mudança de estado ou efeitos colaterais no servidor.
PUT: Usado para enviar dados para atualizar um recurso existente.
DELETE: Usado para deletar um recurso específico.
    
* **Resposta Esperada**: Httpbin irá ecoar de volta os dados que você enviou em um objeto JSON. A seção `json` da resposta incluirá os dados `{"username": "john", "password": "12345"}`.

### Exercício 4: Usando Diferentes Métodos HTTP

**Objetivo**: Experimentar com diferentes métodos HTTP, como POST, DELETE, PUT.

* **Comando PUT**:
    
    ```bash
    curl -X PUT http://httpbin.org/put -d "data=example"
    ```
    
* **Comando DELETE**:
    
    ```bash
    curl -X DELETE http://httpbin.org/delete
    ```
    
* **Resposta Esperada**: Para PUT, httpbin mostrará os dados que você enviou no corpo da requisição, enquanto para DELETE, você receberá uma confirmação de que a requisição DELETE foi recebida, geralmente sem corpo de dados.

### Exercício 5: Manipulando Headers

**Objetivo**: Aprender a enviar cabeçalhos customizados.

* **Comando**:
    
    ```bash
    curl http://httpbin.org/headers -H "X-My-Custom-Header: 12345"
    ```
    
* **Resposta Esperada**: A resposta incluirá um objeto `headers` que mostra todos os cabeçalhos recebidos, incluindo seu cabeçalho personalizado `X-My-Custom-Header` com o valor `12345`.

### Exercício 6: Trabalhando com Cookies

**Objetivo**: Entender como enviar e receber cookies.

* **Comando**:
    
    ```bash
    curl http://httpbin.org/cookies/set?name=value
    curl http://httpbin.org/cookies
    ```
    
* **Resposta Esperada**: Após definir o cookie, a segunda requisição mostrará um objeto `cookies` com o par `{"name": "value"}`.

### Exercício 7: Baixando e Salvando Arquivos

**Objetivo**: Praticar o download de arquivos usando `curl`.

* **Comando**:
    
    ```bash
    curl https://via.placeholder.com/150 -o example.jpg
    ```
    
* **Resposta Esperada**: O arquivo de imagem será baixado e salvo localmente com o nome `example.jpg`. Você não verá saída no terminal, exceto mensagens relacionadas ao progresso do download.

### Exercício 8: Explorando APIs Restritas

**Objetivo**: Aprender a lidar com autenticação.

* **Comando**:
    
    ```bash
    curl -u user:passwd https://httpbin.org/basic-auth/user/passwd
    ```
    
* **Resposta Esperada**: Se a autenticação for bem-sucedida, httpbin retornará um status de sucesso e confirmará que você foi autenticado. 

## Programação

### 1. Introdução a APIs e Web Servers

* **O que é uma API?**
    * Explicação de API (Interface de Programação de Aplicações) como um conjunto de regras e especificações que softwares podem seguir para se comunicar.

* **Diferenças entre API e Web Server**
    * Clarificação de que um Web Server lida com requisições HTTP para servir conteúdo web, enquanto uma API fornece uma interface para realizar operações específicas através de um servidor.

### 2. Introdução ao FastAPI

* **Visão Geral do FastAPI**
    * Discussão sobre as características do FastAPI, como alta performance, fácil aprendizado e recursos como a geração automática de documentação.
* **Comparação com Outras Frameworks**
    * Breve comparação do FastAPI com outras frameworks populares como Flask e Django, destacando a facilidade de uso e eficiência em operações assíncronas.

### 3. Configuração do Ambiente de Desenvolvimento

* **Instalação do Python e Setup do Ambiente**
    * Passo a passo para configurar o ambiente Python, incluindo a instalação do FastAPI e do servidor Uvicorn usando pip.

### 4. Criando sua Primeira API com FastAPI

* **Hello World API**
    * Tutorial para criar um endpoint básico que retorna uma mensagem de "Hello, World!" usando FastAPI.

```python
from fastapi import FastAPI

app = FastAPI()

# Decorator -> É aqui que faz a mágica de transformar nossa função.
@app.get("/")
# Function é função padrão do Python
def root(): # O nome não importa
    return {"message": "Hello world!"} # Essa será a data que vamos retornar ao usuário
```

```bash
uvicorn main:app
```

```bash
uvicorn main:app --reload
```

HTTP metodos

#### Curl

Para usar `curl` para fazer uma requisição à sua API que está rodando com FastAPI, você pode enviar dados como JSON através de uma requisição POST. Suponhamos que você tenha um endpoint em sua API que espera receber os dados de uma casa e então retorna uma previsão de preço baseada nessas informações. Aqui está como você pode fazer isso com `curl`.

### Exemplo de Requisição com Curl

Suponha que seu endpoint para prever o preço da casa esteja configurado como `http://127.0.0.1:8000/prever/` e aceite um JSON com dois campos: `tamanho` e `quartos`. Aqui está como você pode enviar uma requisição:

```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/prever/' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "tamanho": 120,
  "quartos": 3
}'
```

### Explicação do Comando Curl

* `-X 'POST'`: Especifica o método HTTP para a requisição, que é POST neste caso.
* `http://127.0.0.1:8000/prever/`: URL do endpoint da API.
* `-H 'accept: application/json'`: Define o cabeçalho HTTP para indicar que a resposta esperada deve ser em JSON.
* `-H 'Content-Type: application/json'`: Define o cabeçalho HTTP para indicar que o corpo da requisição está em formato JSON.
* `-d '{...}'`: Os dados sendo enviados à API. Substitua os valores de `tamanho` e `quartos` conforme necessário para os dados específicos da casa que você quer avaliar.

### Testando a Requisição

1. Certifique-se de que sua API FastAPI esteja rodando e acessível em `http://127.0.0.1:8000`.
2. Abra um terminal e execute o comando `curl` fornecido.
3. Observe a resposta da API, que deve incluir a previsão do preço da casa baseada nos dados fornecidos.

Usar os cabeçalhos Accept e Content-Type nas suas requisições HTTP é uma forma de comunicar claramente ao servidor tanto o formato dos dados que você está enviando quanto o formato que você espera receber em resposta:

Content-Type: application/json: Este cabeçalho informa ao servidor que o corpo da requisição que você está enviando está em formato JSON. É uma maneira de dizer, "Ei, os dados que estou enviando estão em JSON; por favor, interprete-os dessa forma."
Accept: application/json: Este cabeçalho diz ao servidor que você deseja que a resposta seja em JSON. Isso é particularmente útil em APIs que podem retornar dados em diferentes formatos. Ao especificar application/json, você está solicitando que a API responda com dados nesse formato específico.

#### Postman

Também temos a opção usar o Postman (uma aplicação)


### 5. Trabalhando com Dados

* **Uso de Modelos Pydantic**
    * Introdução aos modelos Pydantic para validação de dados e como integrá-los com FastAPI.
* **Endpoints GET e POST**
    * Criação de exemplos práticos de endpoints que lidam com métodos GET e POST para enviar e receber dados.

### 6. Uvicorn: O Servidor ASGI

* **Por que Uvicorn?**
    * Explicação sobre o papel do Uvicorn como um servidor ASGI, necessário para executar aplicações FastAPI de forma eficiente e assíncrona.

### 7. Documentação Automática

* **Swagger UI**
    * Demonstração de como acessar e utilizar a documentação automática gerada pelo FastAPI.

### 8. Exercícios Práticos

* **Hands-on Coding**
    * Série de exercícios práticos para aplicar o conhecimento adquirido na criação de APIs mais complexas.
* **Desafio com Banco de Dados**
    * Desafio para criar uma API que interage com um banco de dados usando operações básicas de CRUD.

### 9. Sessão de Perguntas e Respostas (Q&A)

* **Discussão Aberta**
    * Espaço para perguntas, troca de ideias e esclarecimento de dúvidas.

### 10. Encerramento

* **Recursos para Aprendizado Contínuo**
    * Compartilhamento de recursos adicionais, como documentação online, tutoriais e fóruns para aprofundamento nos temas abordados.

## Materiais Necessários

* Slides das apresentações.
* Códigos de exemplo e templates para exercícios.
* Acesso à Internet para documentação e pesquisa.

## Pré-requisitos

* Conhecimento básico de programação em Python.
* Ambiente de desenvolvimento Python configurado com acesso à internet.

## Recursos Adicionais

* Documentação Oficial do FastAPI
* Pydantic Documentation
* [Uvicorn Documentation](https://www.uvicorn.org/)


## Ideias

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_00.py
================================================
import random
import time

def numero_aleatorio():
    """ Gera um numero aleatorio. """
    return random.randint(1, 10)

def dobra_um_numero(num):
    """ Calcula o quadrado do dobro de um número. """
    return num ** 2

if __name__ == "__main__":
    while True:
        num = random.randint(1, 10)  # Gera um número aleatório entre 1 e 10
        resultado = dobra_um_numero(num)
        print(f"O quadrado do dobro de {num} é {resultado}")
        time.sleep(1)  # Pausa a execução por 1 segundo


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_01_a.py
================================================
import random
import time

def gerar_numero_aleatorio():
    num = random.randint(1, 10)
    print(num)
    with open('numeros.txt', 'a') as file:
        file.write(f"{num}\n")

if __name__ == "__main__":
    while True:
        gerar_numero_aleatorio()
        time.sleep(1)


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_01_b.py
================================================
import time
def dobrar_numero(num):
    return num * 2

def ler_ultimo_numero():
    try:
        with open('numeros.txt', 'r') as file:
            lines = file.readlines()
            if lines:
                return int(lines[-1].strip())
            else:
                print("Arquivo está vazio.")
                return None
    except FileNotFoundError:
        print("Arquivo não encontrado.")
        return None

if __name__ == "__main__":
    while True:
        num = ler_ultimo_numero()
        if num is not None:
            resultado = dobrar_numero(num)
            print(f"O quadrado do dobro do último número ({num}) é {resultado}")
            time.sleep(1)

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_02_pid_e_porta.py
================================================
# aplicacao simples so para mostrar o que é PID vs PORTA

import streamlit as st
import numpy as np
import pandas as pd

dataframe = pd.DataFrame(
    np.random.randn(10, 20),
    columns=('col %d' % i for i in range(20)))
st.table(dataframe)

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_03_a_usando_api.py
================================================
from fastapi import FastAPI

import random
import time

app = FastAPI()

@app.get("/gerar_numero_aleatorio")
def gerar_numero_aleatorio():
    num = random.randint(1, 10)
    return {"data": num} # num

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_03_b_usando_cliente.py
================================================
import time
import requests

def dobrar_numero(num):
    return num * 2

def ler_ultimo_numero():
    url = "http://127.0.0.1:8000/gerar_numero_aleatorio"
    response = requests.get(url)
    data = response.json() # response.test
    return data["data"] # data


if __name__ == "__main__":
    while True:
        num = ler_ultimo_numero()
        if num is not None:
            resultado = dobrar_numero(num)
            print(f"O quadrado do dobro do último número {num} é {resultado}")
            time.sleep(1)

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_04_decorador_a.py
================================================
import time
from functools import wraps

def alou_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        while True:
            print("alou")
            time.sleep(1)
            result = func(*args, **kwargs)
            return result
    return wrapper

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_04_decorador_b.py
================================================
import random
import time
from programa_04_decorador_exemplo import alou_decorator


@alou_decorator
def gerar_numero_aleatorio():
    num = random.randint(1, 10)
    print(num)
    with open('numeros.txt', 'a') as file:
        file.write(f"{num}\n")

if __name__ == "__main__":
    while True:
        gerar_numero_aleatorio()
        time.sleep(1)


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_05_diferentes_tipos_de_respostas.py
================================================
from fastapi import FastAPI
from fastapi.responses import StreamingResponse, JSONResponse, HTMLResponse

import csv
from io import StringIO

app = FastAPI()

@app.get("/listar-csv")
def listar_csv():
    data = [["Name", "Age"], ["Alice", 30], ["Bob", 25]] # Dados
    stream = StringIO() # Transforma um objeto em um arquivo
    csv.writer(stream).writerows(data) # Dessa forma escrevemos os "data" no formarto csv no stream
    return stream.getvalue()

@app.get("/download-csv/")
def download_csv():
    data = [["Name", "Age"], ["Alice", 30], ["Bob", 25]] # Dados
    stream = StringIO() # Transforma um objeto em um arquivo
    csv.writer(stream).writerows(data) # Dessa forma escrevemos os "data" no formarto csv no stream
    return StreamingResponse(iter([stream.read()]), media_type="text/csv", headers={"Content-Disposition": "attachment; filename=report.csv"})

@app.get("/portal")
async def get_portal(teleport: bool = False):
    if teleport:
        return HTMLResponse(content="""
                            <html>
                                <head>
                                    <title>Some HTML in here</title>
                                </head>
                                <body>
                                    <h1>Look ma! HTML!</h1>
                                </body>
                            </html>""")
    return JSONResponse(content={"message": "Here's your interdimensional portal."})

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/exemplo_00_func.py
================================================
import joblib

modelo = joblib.load("modelo_casas.pkl")

def prever_preco(tamanho: int,
                 quartos: int,
                 n_vagas: int):
    
    dados_entrada = [[tamanho, quartos, n_vagas]]
    preco_estimado = modelo.predict(dados_entrada)[0]
    return round(preco_estimado,2)

print(prever_preco(200,3,3))

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/exemplo_01_api.py
================================================
from fastapi import FastAPI
import joblib
from pydantic import BaseModel

app = FastAPI()

modelo = joblib.load("modelo_casas.pkl")

class EspecificacoesCasa(BaseModel):
    tamanho: int
    quartos: int
    n_vagas: int

@app.post("/prever/")
def prever_preco(especificacoes_cada: EspecificacoesCasa):
    
    dados_entrada = [[especificacoes_cada.tamanho, especificacoes_cada.quartos, especificacoes_cada.n_vagas]]
    preco_estimado = modelo.predict(dados_entrada)[0]
    return {
        "preco_estimado": round(preco_estimado,2)
    }

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/exemplo_02_api_request_response.py
================================================
from fastapi import FastAPI
import joblib
from pydantic import BaseModel, EmailStr, PositiveInt, PositiveFloat
from typing import Optional

app = FastAPI()

modelo = joblib.load("modelo_casas.pkl")

class EspecificacoesCasa(BaseModel):
    tamanho: PositiveInt
    quartos: PositiveInt
    n_vagas: PositiveInt

class EspecificacoesCasaRequest(EspecificacoesCasa):
    email: Optional[EmailStr] = None

class EspecificacoesCasaResponse(BaseModel):
    preco_estimado: PositiveFloat
    dados: EspecificacoesCasa

@app.post("/prever/", response_model=EspecificacoesCasaResponse)
def prever_preco(especificacoes_cada: EspecificacoesCasa):
    
    dados_entrada = [[especificacoes_cada.tamanho, especificacoes_cada.quartos, especificacoes_cada.n_vagas]]
    preco_estimado = modelo.predict(dados_entrada)[0]
    return EspecificacoesCasaResponse(preco_estimado=preco_estimado, dados=especificacoes_cada)

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/exemplo_03_banco.py
================================================
from fastapi import FastAPI
import joblib
from pydantic import BaseModel, EmailStr, PositiveInt, PositiveFloat
from typing import Optional

app = FastAPI()

modelo = joblib.load("modelo_casas.pkl")

class EspecificacoesCasa(BaseModel):
    tamanho: PositiveInt
    quartos: PositiveInt
    n_vagas: PositiveInt

class EspecificacoesCasaRequest(EspecificacoesCasa):
    email: Optional[EmailStr] = None

class EspecificacoesCasaResponse(BaseModel):
    preco_estimado: PositiveFloat
    dados: EspecificacoesCasa

CRM = []

@app.post("/prever/", response_model=EspecificacoesCasaResponse)
def prever_preco(especificacoes_cada: EspecificacoesCasa):
    
    dados_entrada = [[especificacoes_cada.tamanho, especificacoes_cada.quartos, especificacoes_cada.n_vagas]]
    preco_estimado = modelo.predict(dados_entrada)[0]
    response = EspecificacoesCasaResponse(preco_estimado=preco_estimado, dados=especificacoes_cada)
    CRM.append(response.model_dump())
    return response

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/modelo.py
================================================
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import joblib

# Exemplo de dados: [tamanho, quartos, vagas], preço
X = np.array([
    [100, 3, 2],
    [150, 4, 3],
    [120, 2, 1],
    [300, 5, 4],
    [200, 3, 2],
    [250, 4, 3],
    [180, 3, 2],
    [140, 2, 1],
    [320, 5, 4],
    [210, 3, 0]  # Exemplo de uma casa sem vaga de garagem
])
y = np.array([200000, 300000, 180000, 500000, 400000, 450000, 360000, 220000, 520000, 410000])

# Dividindo os dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando o modelo de regressão linear
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Avaliando o modelo com o conjunto de teste
y_pred = modelo.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"MSE: {mse}")

# Salvando o modelo treinado
joblib.dump(modelo, 'modelo_casas.pkl')


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/requirements.txt
================================================
scikit-learn
pydantic[email]

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/Dockerfile-backend
================================================
FROM python:3.11

# Set the working directory
WORKDIR /app

# Copy the requirements file and install dependencies
COPY ./requirements-backend.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements-backend.txt

# Copy the application code
COPY ./src .

# Command to run the application
CMD ["uvicorn", "src.app:app", "--reload", "--host", "0.0.0.0", "--port", "8000"]


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/Dockerfile-frontend
================================================
FROM python:3.11

# Set the working directory
WORKDIR /app

# Copy the requirements file and install dependencies
COPY ./requirements-frontend.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements-frontend.txt

# Copy the application code
COPY ./src .

# Command to run the application
CMD ["streamlit", "run", "src/frontend.py", "--server.port=5000"]

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/docker-compose.yml
================================================
version: "3.9"

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile-backend
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    networks:
      - app-network

  frontend:
    build:
      context: .
      dockerfile: Dockerfile-frontend
    ports:
      - "5000:5000"  # Assuming your frontend runs on port 5000
    volumes:
      - .:/app
    networks:
      - app-network

  mongodb:
    image: mongo:latest
    hostname: mongodb
    volumes:
      - mongodb_data:/data/db
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
    networks:
      - app-network

  mongo-express:
    image: mongo-express:latest
    hostname: mongo-express
    ports:
      - "8081:8081"
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: example
      ME_CONFIG_MONGODB_SERVER: mongodb
    depends_on:
      - mongodb
    networks:
      - app-network
    

volumes:
  mongodb_data:
networks:
  app-network:
    driver: bridge

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/requirements-backend.txt
================================================
email_validator
fastapi
pydantic
pymongo
python-dotenv
uvicorn
joblib
scikit-learn

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/requirements-frontend.txt
================================================
streamlit
requests

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/src/app.py
================================================
from pymongo import MongoClient
from fastapi import FastAPI
import joblib
from pydantic import BaseModel, EmailStr, PositiveInt, PositiveFloat
from typing import Optional, List

app = FastAPI()

modelo = joblib.load("./src/modelo_casas.pkl")

class EspecificacoesCasa(BaseModel):
    tamanho: PositiveInt
    quartos: PositiveInt
    n_vagas: PositiveInt

class EspecificacoesCasaRequest(EspecificacoesCasa):
    email: EmailStr = None

class EspecificacoesCasaResponse(BaseModel):
    preco_estimado: PositiveFloat
    dados: EspecificacoesCasaRequest

# Configuração do MongoDB
client = MongoClient('mongodb://root:example@mongodb:27017')
db = client['dbmongo']
collection = db['dbscrm']

@app.post("/quanto-cobrar-de-casa/", response_model=EspecificacoesCasaResponse)
def prever_preco(especificacoes_cada: EspecificacoesCasaRequest):
    
    dados_entrada = [[especificacoes_cada.tamanho, especificacoes_cada.quartos, especificacoes_cada.n_vagas]]
    preco_estimado = modelo.predict(dados_entrada)[0]
    response = EspecificacoesCasaResponse(preco_estimado=preco_estimado, dados=especificacoes_cada)
    collection.insert_one(response.model_dump())
    return response

@app.get("/mkt/", response_model=List[EspecificacoesCasaResponse])
def read_all_leads():
    casas = list(collection.find({}))
    return [EspecificacoesCasaResponse(preco_estimado=casa['preco_estimado'], dados=casa['dados']) for casa in casas]

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/src/frontend.py
================================================
import streamlit as st
import requests

# Função para enviar dados para a API e receber a previsão
def get_previsao(tamanho, quartos, n_vagas, email):
    url = 'http://backend:8000/quanto-cobrar-de-casa/'  # Endereço da API FastAPI
    data = {
        "tamanho": tamanho,
        "quartos": quartos,
        "n_vagas": n_vagas,
        "email": email if email else None  # Inclui o email na requisição apenas se for fornecido
    }
    response = requests.post(url, json=data)
    print("Status Code:", response.status_code)
    print("Response Body:", response.text)  # Print or log the raw response
    try:
        return response.json()  # Attempt to parse JSON
    except ValueError:
        # Handle the case where parsing JSON fails
        print("Failed to decode JSON from response:")
        print(response.text)
        return None

# Interface do usuário no Streamlit
st.title('Quanto vale o seu imóvel')

# Entrada de dados pelo usuário
tamanho = st.number_input('Insira o tamanho da casa (em m²):', min_value=10.0, step=0.1, format="%.1f")
quartos = st.number_input('Insira o número de quartos:', min_value=1, step=1)
n_vagas = st.number_input('Insira o número de vagas de garagem:', min_value=0, max_value=4, step=1)
email = st.text_input('Insira seu email (opcional):', '')

# Botão para fazer a previsão
if st.button('Prever preço'):
    resposta = get_previsao(tamanho, quartos, n_vagas, email)
    if 'preco_estimado' in resposta:
        st.success(f'O preço estimado da casa é: R${resposta["preco_estimado"]:.2f}')
    else:
        st.error('Erro ao obter previsão.')


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/src/modelo.py
================================================
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import joblib

# Exemplo de dados: [tamanho, quartos, vagas], preço
X = np.array([
    [100, 3, 2],
    [150, 4, 3],
    [120, 2, 1],
    [300, 5, 4],
    [200, 3, 2],
    [250, 4, 3],
    [180, 3, 2],
    [140, 2, 1],
    [320, 5, 4],
    [210, 3, 0]  # Exemplo de uma casa sem vaga de garagem
])
y = np.array([200000, 300000, 180000, 500000, 400000, 450000, 360000, 220000, 520000, 410000])

# Dividindo os dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando o modelo de regressão linear
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Avaliando o modelo com o conjunto de teste
y_pred = modelo.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"MSE: {mse}")

# Salvando o modelo treinado
joblib.dump(modelo, 'modelo_casas.pkl')


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/README.md
================================================
```bash
curl "https://oaidalleapiprodscus.blob.core.windows.net/private/org-IZAyac7K1DAkImDFavXsGhdj/user-ZydpZOHfvGETDmONdXQjgDCG/img-Qo5w2dx6pp8EJe7XYsbFnAYu.png?st=2024-04-25T03%3A45%3A19Z&se=2024-04-25T05%3A45%3A19Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-04-24T19%3A33%3A09Z&ske=2024-04-25T19%3A33%3A09Z&sks=b&skv=2021-08-06&sig=69MjVktvwm2F3J7SwdqsoPHI7NnL3qZRHSLG39BUM3U%3D" -o saved_image.png
```

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/exemplo_01.py
================================================
import requests
import json
from dotenv import load_dotenv
import os

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# URL da API
url = "https://api.openai.com/v1/images/generations"


# Cabeçalhos da requisição
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {OPENAI_API_KEY}"  # Substitua $OPENAI_API_KEY pelo seu token real
}

# Dados a serem enviados como corpo da requisição
body = {
    "model": "dall-e-3",
    "prompt": "a white siamese cat",
    "n": 1,
    "size": "1024x1024"
}

# Realizando a requisição POST
response = requests.post(
    url, 
    headers=headers, 
    data=json.dumps(body))

print(response.status_code)
print(response.json())

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/exemplo_02.py
================================================
import requests
import json
from dotenv import load_dotenv
import os

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# URL da API
url = "https://api.openai.com/v1/images/generations"


# Cabeçalhos da requisição
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {OPENAI_API_KEY}"  # Substitua $OPENAI_API_KEY pelo seu token real
}

# Dados a serem enviados como corpo da requisição
body = {
    "model": "dall-e-3",
    "prompt": "a white siamese cat",
    "n": 1,
    "size": "1024x1024"
}

# Realizando a requisição POST
response = requests.post(
    url, 
    headers=headers, 
    data=json.dumps(body))

print(response.status_code)
print(response.json())

URL_IMAGE = response.json()["data"][0]["url"]

imagem_path = "download_image.jpg"

print(requests.get(URL_IMAGE).content)

with open(imagem_path, "wb") as file:
    file.write(requests.get(URL_IMAGE).content)


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/exemplo_03.py
================================================
import streamlit as st
import requests
import json
import os
from dotenv import load_dotenv

# Carrega variáveis de ambiente
load_dotenv()

# Configuração inicial da chave da API
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# URL da API
url = "https://api.openai.com/v1/images/generations"

# Cabeçalhos da requisição
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {OPENAI_API_KEY}"
}

# Interface do Streamlit
st.title('Gerador de Imagens')
prompt = st.text_area('Digite o prompt para gerar uma imagem:', value='Descrição da imagem aqui')

if st.button('Gerar Imagem'):
    # Corpo da requisição
    body = {
        "model": "dall-e-3",
        "prompt": prompt,
        "n": 1,
        "size": "1024x1024"
    }

    # Realiza a requisição POST
    response = requests.post(url, headers=headers, data=json.dumps(body))

    if response.status_code == 200:
        # Extrai a URL da imagem
        url_image = response.json()["data"][0]["url"]
        # Exibe a imagem na interface do Streamlit
        st.image(url_image)
    else:
        st.error("Erro ao gerar a imagem. Por favor, tente novamente.")


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/exemplo_04.py
================================================
import streamlit as st
import requests
import json
import os
from dotenv import load_dotenv

# Carrega variáveis de ambiente
load_dotenv()

# Configuração inicial da chave da API
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# URL da API para completar texto
url = "https://api.openai.com/v1/completions"

# Cabeçalhos da requisição
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {OPENAI_API_KEY}"
}

# Interface do Streamlit
st.title('Conversor de SQL: SQL Server para PostgreSQL')
query_sql_server = st.text_area('Digite a consulta SQL Server aqui:')
print(query_sql_server)

if st.button('Converter para PostgreSQL'):
    # Prompt de conversão
    prompt = f"Convert this SQL Server query to a PostgreSQL query: {query_sql_server}"
    
    # Corpo da requisição
    body = {
        "model": "text-davinci-003",
        "prompt": prompt,
        "max_tokens": 1000
    }

    # Realiza a requisição POST
    response = requests.post(url, headers=headers, data=json.dumps(body))

    if response.status_code == 200:
        # Extrai a resposta e apresenta a consulta convertida
        postgres_query = response.json()["choices"][0]["text"].strip()
        st.text_area('Consulta PostgreSQL:', value=postgres_query, height=700)
    else:
        st.error("Erro na conversão. Por favor, tente novamente.")


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/async/main_asy.py
================================================
import asyncio
import aiohttp

async def fetch_url(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.text()

async def main():
    urls = ["https://example.com"] * 10  # Lista de URLs para fetch
    tasks = [fetch_url(url) for url in urls]
    responses = await asyncio.gather(*tasks)
    for response in responses:
        print(response[:100])  # Printa os primeiros 100 caracteres de cada resposta

asyncio.run(main())


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/async/main_sync.py
================================================
import requests

def fetch_url(url):
    response = requests.get(url)
    return response.text

def main():
    urls = ["https://example.com"] * 10  # Lista de URLs para fetch
    responses = [fetch_url(url) for url in urls]
    for response in responses:
        print(response[:100])  # Printa os primeiros 100 caracteres de cada resposta

main()


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/response_json_html_xml/readme.md
================================================
https://medium.com/@denniskoko/implementing-content-negotiation-in-fastapi-371d03c59c02

Aceitar diferentes tipos de comunicação

### Exemplo com a API da NASA:

Para acessar a API da NASA, você precisa passar sua chave de API como um parâmetro na solicitação. Aqui está um exemplo usando cURL:

```bash
curl -X GET 'https://api.nasa.gov/planetary/apod?api_key=aJ87xjxneJaiBejYAfFrSSPJXR4Ce2RdMnlrYP3x' \
  -H 'Accept: application/json'
```

Neste exemplo, a chave da API (`api_key=aJ87xjxneJaiBejYAfFrSSPJXR4Ce2RdMnlrYP3x`) é passada como um parâmetro na URL.

### Exemplo com a API da Wikipedia:

A API da Wikipedia usa um método de autenticação diferente, onde você não precisa de uma chave de API, mas pode especificar o formato da resposta usando uma flag na solicitação. Aqui está um exemplo usando cURL:

```bash
curl -X GET 'https://en.wikipedia.org/w/api.php?action=query&format=xml&titles=Albert%20Einstein'
```

Neste exemplo, estamos fazendo uma solicitação para obter informações sobre "Albert Einstein" da Wikipedia. A flag `format=xml` é usada para especificar que queremos a resposta no formato XML.

### Diferença entre as duas APIs:

* A API da NASA requer uma chave de API para autenticação e utiliza o cabeçalho `Accept` para especificar o tipo de conteúdo desejado na resposta (JSON neste caso).
* A API da Wikipedia não requer uma chave de API, mas permite que você especifique o formato da resposta usando uma flag na própria URL da solicitação.


### API da NASA - Solicitação JSON:

```bash
# JSON
curl -X GET 'https://api.nasa.gov/planetary/apod?api_key=aJ87xjxneJaiBejYAfFrSSPJXR4Ce2RdMnlrYP3x' \
  -H 'Accept: application/json'
```

### API da NASA - Solicitação XML:

curl -X GET 'https://api.nasa.gov/planetary/apod' \                                                 
  -H 'Accept: application/xml'
<?xml version="1.0" encoding="UTF-8"?>
<response>
  <error>
    <code>API_KEY_MISSING</code>
    <message>No api_key was supplied. Get one at https://api.nasa.gov:443</message>
  </error>
</response>%  


```bash
# XML
curl -X GET 'https://api.nasa.gov/planetary/apod?api_key=aJ87xjxneJaiBejYAfFrSSPJXR4Ce2RdMnlrYP3x' -H 'Accept: application/xml'
```

### API da Wikipedia - Solicitação JSON:

```bash
# JSON
curl -X GET 'https://en.wikipedia.org/w/api.php?action=query&format=json&titles=Albert%20Einstein'
```

### API da Wikipedia - Solicitação XML:

```bash
# XML
curl -X GET 'https://en.wikipedia.org/w/api.php?action=query&format=xml&titles=Albert%20Einstein'
```

Esses exemplos demonstram como fazer solicitações para ambas as APIs especificando o formato da resposta desejada (JSON ou XML).


Aqui está um exemplo de como você pode usar o cURL para fazer solicitações para o endpoint `get_user_by_extension` com diferentes tipos de formato:

```bash
# Solicitar dados no formato JSON
curl -X GET 'http://localhost:8000/users?format=json'

# Solicitar dados no formato HTML
curl -X GET 'http://localhost:8000/users?format=html'

# Solicitar dados no formato XML
curl -X GET 'http://localhost:8000/users?format=xml'
```

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/response_json_html_xml/multiple.py
================================================
from fastapi import FastAPI, Request, Response, Query, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse
from typing import Optional

app = FastAPI()

# Sample data
data = {
    "name": "John Doe",
    "age": 30,
    "email": "john.doe@example.com"
}

# Define function to generate HTML content from data
def generate_html(data):
    html_content = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>User Info</title>
    </head>
    <body>
        <h1>User Info</h1>
        <ul>
            <li>Name: {name}</li>
            <li>Age: {age}</li>
            <li>Email: {email}</li>
        </ul>
    </body>
    </html>
    """.format(**data)
    return html_content

# Define function to generate XML content from data
def generate_xml(data):
    xml_content = f"<user>\n" # noqa
    for key, value in data.items():
        xml_content += f"    <{key}>{value}</{key}>\n"
    xml_content += "</user>"
    return xml_content

# Define endpoint to handle content negotiation for Accept and query
# Define endpoint to handle content negotiation for Accept and query
@app.get("/user", responses={
    200: {"description": "User information", "content": {
        "application/json": {},
        "text/html": {},
        "application/xml": {}
    }}
})
async def get_user(request: Request, format: Optional[str] = Query(None, pattern="^(json|html|xml)$")):
    # Check if query parameter is present
    if format:
        if format == "json":
            return JSONResponse(content=data)
        elif format == "html":
            return HTMLResponse(content=generate_html(data))
        elif format == "xml":
            return Response(content=generate_xml(data), media_type="application/xml")

    # Check client's preferred content type from Accept header
    accept_header = request.headers.get("Accept")
    
    # Default to JSON if Accept header is not provided
    if not accept_header:
        return JSONResponse(content=data)
    
    # Parse Accept header to determine client's preferred content type
    if "application/json" in accept_header:
        return JSONResponse(content=data)
    elif "text/html" in accept_header:
        return HTMLResponse(content=generate_html(data))
    elif "application/xml" in accept_header:
        return Response(content=generate_xml(data), media_type="application/xml")
    else:
        # If no matching content type is found, return a 406 Not Acceptable response
        return Response(status_code=406, content="406 Not Acceptable: Unsupported Media Type")

# Define endpoint to handle content negotiation for file extension
@app.get("/user.{format}")
async def get_user_by_extension(format: str):
    """
    Retorna informações do usuário no formato especificado.

    Parâmetros:
    - format: Formato desejado da resposta. Opções disponíveis: json, html, xml.
    """
    if format == "json":
        return JSONResponse(content=data)
    elif format == "html":
        return HTMLResponse(content=generate_html(data))
    elif format == "xml":
        return Response(content=generate_xml(data), media_type="application/xml")
    else:
        raise HTTPException(status_code=406, detail="Unsupported Media Type")
    
@app.get("/users")
async def get_user_by_extension_final(format: str = None):
    if format == "json":
        return JSONResponse(content=data)
    elif format == "html":
        return HTMLResponse(content=generate_html(data))
    elif format == "xml":
        return Response(content=generate_xml(data), media_type="application/xml")
    else:
        raise HTTPException(status_code=406, detail="Unsupported Media Type")

# Run the application
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/README.md
================================================
## Repositório Rinha

https://github.com/zanfranceschi/rinha-de-backend-2024-q1

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/Dockerfile
================================================
FROM python:3.12

WORKDIR /app

COPY ./requirements.txt /app/requirements.txt

RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

COPY ./app /app

CMD ["uvicorn", "app.main:app", "--reload", "--host", "0.0.0.0", "--port", "80"]

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/docker-compose.yml
================================================
version: "3.9"

services:
  api-1:
    hostname: api-1
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./:/app
    ports:
      - "8001:80"

  api-2:
    hostname: api-2
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./:/app
    ports:
      - "8002:80"

  nginx:
    image: nginx:latest
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - api-1
      - api-2

  db:
    image: postgres:latest
    hostname: db
    environment:
      - POSTGRES_PASSWORD=123
      - POSTGRES_USER=admin
      - POSTGRES_DB=rinha
    ports:
      - "5432:5432"
    volumes:
      - ./script.sql:/docker-entrypoint-initdb.d/script.sql


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/nginx.conf
================================================
events {}

http {
    upstream api_servers {
        server api-1:80;
        server api-2:80;
    }

    server {
        listen 80;

        location / {
            proxy_pass http://api_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/pyproject.toml
================================================
[tool.poetry]
name = "app"
version = "0.1.0"
description = ""
authors = ["Your Name <you@example.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
aiohttp = "^3.9.5"
requests = "^2.31.0"
fastapi = "^0.110.2"
uvicorn = {extras = ["standard"], version = "^0.29.0"}
sqlalchemy = "^2.0.29"
psycopg2 = "^2.9.9"
asyncpg = "^0.29.0"
scikit-learn = "^1.4.2"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/requirements.txt
================================================
aiohttp==3.9.5
aiosignal==1.3.1
annotated-types==0.6.0
anyio==4.3.0
asyncpg==0.29.0
attrs==23.2.0
certifi==2024.2.2
charset-normalizer==3.3.2
click==8.1.7
colorama==0.4.6
fastapi==0.110.2
frozenlist==1.4.1
greenlet==3.0.3
h11==0.14.0
httptools==0.6.1
idna==3.7
multidict==6.0.5
psycopg2==2.9.9
pydantic==2.7.1
pydantic_core==2.18.2
python-dotenv==1.0.1
PyYAML==6.0.1
requests==2.31.0
sniffio==1.3.1
SQLAlchemy==2.0.29
starlette==0.37.2
typing_extensions==4.11.0
urllib3==2.2.1
uvicorn==0.29.0
watchfiles==0.21.0
websockets==12.0
yarl==1.9.4


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/script.sql
================================================
CREATE UNLOGGED TABLE IF NOT EXISTS clients (
    id SERIAL PRIMARY KEY NOT NULL,
    limite INTEGER NOT NULL,
    saldo INTEGER NOT NULL
);

CREATE UNLOGGED TABLE IF NOT EXISTS transactions (
    id SERIAL PRIMARY KEY NOT NULL,
    tipo CHAR(1) NOT NULL,
    descricao VARCHAR(10) NOT NULL,
    valor INTEGER NOT NULL,
    cliente_id INTEGER NOT NULL,
    realizada_em TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_cliente_id
ON transactions(cliente_id);

INSERT INTO clients (limite, saldo)
VALUES
    (100000, 0),
    (80000, 0),
    (1000000, 0),
    (10000000, 0),
    (500000, 0);

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/.python-version
================================================
3.12.1


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/app/database.py
================================================
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.orm import declarative_base

# URL de conexão assíncrona para PostgreSQL, ajuste conforme necessário
SQLALCHEMY_DATABASE_URL = "postgresql+asyncpg://admin:123@db/rinha"

# Criar um engine assíncrono
engine = create_async_engine(SQLALCHEMY_DATABASE_URL)

# Configurar sessionmaker para criar sessões assíncronas
async_session_local = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine,
    class_=AsyncSession  # Isso especifica que a sessão deve ser assíncrona
)

Base = declarative_base()


================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/app/main.py
================================================
import time

from fastapi import FastAPI, HTTPException, Response
from fastapi.params import Depends
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from . import schemas
from .database import async_session_local
from .models import Cliente, Transacao

app = FastAPI()

async def get_session():
    async with async_session_local() as session:
        yield session


@app.post("/clientes/{cliente_id}/transacoes", response_model=schemas.ClienteResponse)
async def post_transacao(cliente_id: int, 
                         transacao: schemas.TransactionCreateRequest, 
                         session: AsyncSession = Depends(get_session)):
    
    result = await session.execute(select(Cliente).filter_by(id=cliente_id))
    cliente = result.scalars().one_or_none()

    if not cliente:
        raise HTTPException(status_code=404, detail="Cliente não encontrado")

    if transacao.tipo == "d":
        if cliente.saldo - transacao.valor < -cliente.limite:
            raise HTTPException(status_code=422, detail="Saldo insuficiente")
        cliente.saldo -= transacao.valor
    else:  # ou seja do tipo == "c"
        cliente.saldo += transacao.valor

    nova_transacao = Transacao(**transacao.model_dump(), cliente_id=cliente_id)
    session.add(nova_transacao)
    await session.commit()
    await session.refresh(cliente)

    return cliente


@app.get("/clientes/{cliente_id}/extrato")
async def get_extrato(cliente_id: int, session: AsyncSession = Depends(get_session)):
    # Obtém o cliente de forma assíncrona usando select e scalars
    result = await session.execute(select(Cliente).filter_by(id=cliente_id))
    client = result.scalars().one_or_none()

    if not client:
        raise HTTPException(status_code=404, detail="Cliente não encontrado")

    # Executa a consulta para obter as últimas transações de forma assíncrona
    transactions_result = await session.execute(
        select(Transacao)
        .where(Transacao.cliente_id == cliente_id)
        .order_by(Transacao.id.desc())
        .limit(10)
    )
    transactions = transactions_result.scalars().all()

    return {
        "saldo": {
            "total": client.saldo,
            "data_extrato": time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime()),
            "limite": client.limite
        },
        "ultimas_transacoes": [
            t for t in transactions
        ]
    }

@app.delete("/clientes/{cliente_id}", status_code=204)
async def delete_cliente(cliente_id: int, session: AsyncSession = Depends(get_session)):
    result = await session.execute(select(Cliente).filter_by(id=cliente_id))
    cliente = result.scalars().one_or_none()

    if not cliente:
        raise HTTPException(status_code=404, detail="Cliente não encontrado")

    await session.delete(cliente)
    await session.commit()
    return Response(status_code=204)

@app.put("/clientes/{cliente_id}", response_model=schemas.ClienteResponse)
async def update_cliente(cliente_id: int, update_data: schemas.ClientCreateRequest, session: AsyncSession = Depends(get_session)):
    result = await session.execute(select(Cliente).filter_by(id=cliente_id))
    cliente = result.scalars().one_or_none()

    if not cliente:
        raise HTTPException(status_code=404, detail="Cliente não encontrado")

    for var, value in update_data.model_dump(exclude_unset=True).items():
        setattr(cliente, var, value)

    await session.commit()
    await session.refresh(cliente)
    return cliente

@app.post("/clientes/", response_model=schemas.ClienteResponse, status_code=201)
async def create_cliente(cliente_data: schemas.ClientCreateRequest, 
                         session: AsyncSession = Depends(get_session)):
    
    novo_cliente = Cliente(
        saldo=cliente_data.saldo,
        limite=cliente_data.limite
    )
    session.add(novo_cliente)
    await session.commit()
    await session.refresh(novo_cliente)
    return novo_cliente

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/app/models.py
================================================
from datetime import datetime

from sqlalchemy import Boolean, Column, ForeignKey, Integer, String, DateTime
from sqlalchemy.orm import relationship
from .database import Base

# Models represent the tables in the database

class Cliente(Base):
    __tablename__ = "clients"

    id = Column(Integer, primary_key=True)
    limite = Column(Integer)
    saldo = Column(Integer)

    transacoes = relationship("Transacao", back_populates="cliente")

class Transacao(Base):
    __tablename__ = "transactions"

    id = Column(Integer, primary_key=True)
    cliente_id = Column(Integer, ForeignKey("clients.id"))
    valor = Column(Integer)
    tipo = Column(String)
    descricao = Column(String)
    realizada_em = Column(DateTime, default=datetime.now())

    cliente = relationship("Cliente", back_populates="transacoes")

================================================
File: /06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/app/schemas.py
================================================
from enum import Enum
from typing import Optional

from pydantic import BaseModel,Field

class TransactionTypeEnum(str, Enum):
    credito = 'c'
    debito = 'd'

class TransactionBase(BaseModel):
    valor: int
    tipo: TransactionTypeEnum
    descricao: str = Field(max_length=10, min_length=1)

class TransactionCreateRequest(TransactionBase):
    pass

class ClienteBase(BaseModel):
    limite: int
    saldo: int

class ClienteResponse(ClienteBase):
    pass

class ClientRequest(ClienteBase):
    pass

class ClientCreateRequest(ClienteBase):
    id: int

class ClienteUpdateRequest(BaseModel):
    saldo: Optional[float] = None
    limite: Optional[float] = None


================================================
File: /06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/README.md
================================================
# api


================================================
File: /06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/docker-compose.yml
================================================
version: '3.8'

services:
  db:
    image: postgres:latest
    restart: always
    environment:
      POSTGRES_DB: mydatabase
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

volumes:
  pgdata:


================================================
File: /06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/pyproject.toml
================================================
[tool.poetry]
name = "workshop-api"
version = "0.1.0"
description = ""
authors = ["Fabio Cantarim <fabio.cantarim@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.12"
fastapi = "^0.111.0"
uvicorn = "^0.29.0"
psycopg2-binary = "^2.9.9"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/src/controller/user.py
================================================

from src.db.connection import PostgreSQLConnection
from src.models.user import User


db = PostgreSQLConnection(dbname='postgres', user='myuser', password='mypassword')


async def c_get_user(user_id:int):
    db.connect()
    user = db.select_user(f"SELECT * FROM users WHERE id = {user_id}")
    db.close()
    return user

async def c_create_user(user: User):
    db.connect()
    db.insert_user(user.id, user.name, user.area, user.job_description, user.role, user.salary, user.is_active, user.last_evaluation)
    db.close()
    return user


async def c_delete_user(user_id: int):
    db.connect()
    user = db.delete_user(user_id)
    db.close()
    return user

async def c_update_user(user: User):
    db.connect()
    user = db.update_user(user.id, user.name, user.area, user.job_description, user.role, user.salary, user.is_active, user.last_evaluation)
    db.close()
    return user

================================================
File: /06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/src/db/connection.py
================================================
import psycopg2

class PostgreSQLConnection:
    def __init__(self, dbname, user, password, host='localhost', port='5432'):
        self.dbname = dbname
        self.user = user
        self.password = password
        self.host = host
        self.port = port
        self.conn = None

    def connect(self):
        try:
            self.conn =  psycopg2.connect(dbname = self.dbname, user=self.user, password=self.password, host=self.host, port=self.port)
            print("Conexão com sucesso!")
        except psycopg2.Error as e:
            print("Erro ao conectar ao Postgre:", e)


    def select_user(self, query):
        if not self.conn:
            print("Você não esta conectado")
            return None
        try:
            cursor = self.conn.cursor()
            cursor.execute(query)
            rows = cursor.fetchall()
            cursor.close()
            return rows
        except psycopg2 as e:
            print("Erro ao executar a consulta", e)
            return None

    def insert_user(self, id, name, area, job_description, role, salary, is_active, last_evaluation):
        if not self.conn:
            print("Você nao esta conectado")
            return None
        try:
            cursor = self.conn.cursor()
            cursor.execute(
                "INSERT INTO users (id, name, area, job_description, role, salary, is_active, last_evaluation) VALUES (%s, %s, %s,%s, %s, %s,%s,%s)",
                (id, name, area, job_description, role, salary, is_active, last_evaluation)
            )
            self.conn.commit()
            cursor.close()
            print("Usário registrado!")
        except psycopg2.Error as e:
            print("Não foi possível registrar! - ",e)

    def delete_user(self, user_id: int):
        if not self.conn:
            print("Você não esta conectado")
            return None
        try:
            cursor = self.conn.cursor()
            cursor.execute(
                "DELETE FROM users WHERE id = %s",
                (user_id,)
            )
            self.conn.commit()
            cursor.close()
            print("Registro Deletado")
        except psycopg2.Error as e:
            print("Não foi possível deletar! - ", e)

    def update_user(self, id, name=None, area=None, job_description=None, role=None, salary=None, is_active=None, last_evaluation=None):
        if not self.conn:
            print("Você não esta conectado")
            return None

        try:
            cursor = self.conn.cursor()
            update_query = "UPDATE users SET"
            update_values = []

            if name is not None:
                update_query += " name = %s,"
                update_values.append(name)
            
            if area is not None:
                update_query += " area = %s,"
                update_values.append(area)

            if job_description is not None:
                update_query += " job_description = %s,"
                update_values.append(job_description)

            if role is not None:
                update_query += " role = %s,"
                update_values.append(role)
            
            if salary is not None:
                update_query += " salary = %s,"
                update_values.append(salary)
            
            if is_active is not None:
                update_query += " is_active = %s,"
                update_values.append(is_active)

            if last_evaluation is not None:
                update_query += " last_evaluation = %s,"
                update_values.append(last_evaluation)

            update_query = update_query.rstrip(',') + " WHERE id = %s"
            update_values.append(id)

            cursor.execute(update_query, tuple(update_values))
            self.conn.commit()
            cursor.close()
            print("Usuário Atualizado")
        except psycopg2.Error as e:
            print("Não foi possível atualizar o user: ", e)

    def close(self):
        if self.conn:
            self.conn.close()
            print("Conexão Encerrada!")
    

================================================
File: /06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/src/models/user.py
================================================
from typing import Optional
from pydantic import BaseModel

class  User(BaseModel):
    id: int
    name: Optional[str]
    area: Optional[str]
    job_description: Optional[str]
    role: Optional[int] 
    salary: Optional[float]
    is_active: Optional[bool]
    last_evaluation: Optional[str]

================================================
File: /06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/src/routes/user.py
================================================

from fastapi import FastAPI, HTTPException

from src.controller.user import c_create_user, c_delete_user, c_get_user, c_update_user
from src.models.user import User

app = FastAPI()

@app.get("/users/{user_id}")
async def get_user(user_id: int):
    db_user = await c_get_user(user_id)
    if db_user is None:
        raise HTTPException(status_code=404, detail="Usuário Não Encontrado")
    return db_user

@app.post("/users")
async def create_user(user: User):
    db_user = await c_get_user(user.id)
    if db_user:
        raise HTTPException(status_code=400, detail="O usuário já existe")
    user = await c_create_user(user)
    return user

@app.delete("/users/{user_id}")
async def delete_user(user_id: int):
    db_user = await c_get_user(user_id)
    if db_user is None:
        raise HTTPException(status_code=404, detail="O usuário não existe")
    await c_delete_user(user_id)
    return {"message": "Usuário foi deletado"}

@app.put("/users/{user_id}")
async def update_user(user_id: int, user: User):
    db_user = await c_get_user(user_id)
    if db_user is None:
        raise HTTPException(status_code=404, detail="O usuário não existe")
    await c_update_user(user)
    return {"message": "Usuário foi alterado"}


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/README.md
================================================
# FastAPI and Docker Workshop by Sebastián Ramírez (Tiangolo)

Join Docker Captain Sebastián Ramírez as he uses memes to teach you how to create an API ready for production in very little time using FastAPI.

================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/Dockerfile
================================================
FROM python:3.12 as builder

ENV PIP_YES=1 \
    POETRY_VIRTUALENVS_IN_PROJECT=1 \
    POETRY_VIRTUALENVS_CREATE=1 \
    POETRY_CACHE_DIR=/tmp/poetry_cache

WORKDIR /app

COPY pyproject.toml poetry.lock /app/
RUN touch README.md

RUN --mount=type=cache,target=$POETRY_CACHE_DIR \
    pip install poetry==1.8.2 && \
    poetry install --only main --no-root

FROM python:3.12-slim as runtime

ENV VIRTUAL_ENV=/app/.venv \
    PATH="/app/.venv/bin:$PATH"

WORKDIR /app

COPY --from=builder $VIRTUAL_ENV $VIRTUAL_ENV
COPY src ./src
EXPOSE 8000
#ENTRYPOINT [ "gunicorn", "src.main:app", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000" ]
ENTRYPOINT [ "uvicorn", "src.main:app", "--host", "127.0.0.1", "--port", "8000" ]


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/Makefile
================================================
.PHONY: dev-run

dev-run:
	@poetry run python -m uvicorn src.main:app --reload --host localhost --port 8000


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/docker-compose.yml
================================================
services:
  web:
    build: .
    environment:
      - UVICORN_RELOAD=True
    ports:
      - 5000:8000
    volumes:
      - ./src:/app/src


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/mkdocs.yml
================================================
site_name: My Docs


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/pyproject.toml
================================================
[tool.poetry]
name = "fastapi-docker"
version = "0.1.0"
description = "Join Docker Captain Sebastián Ramírez as he uses memes to teach you how to create an API ready for production in very little time using FastAPI."
authors = ["Kaio Silva <kaiohp.silva1@gmail.com>"]
license = "MIT"
readme = "README.md"
packages = [{include = "src"}]


[tool.poetry.dependencies]
python = "^3.12"
fastapi = {extras = ["all"], version = "^0.110.1"}
gunicorn = "^22.0.0"
python-jose = {extras = ["cryptography"], version = "^3.3.0"}
passlib = {extras = ["bcrypt"], version = "^1.7.4"}
bcrypt = "4.0.1"
sqlalchemy = "^2.0.29"


[tool.poetry.group.docs.dependencies]
mkdocs = "^1.5.3"


[tool.poetry.group.test.dependencies]
pytest = "^8.1.1"


[tool.poetry.group.dev.dependencies]
black = "^24.4.0"
isort = "^5.13.2"
flake8 = "^7.0.0"
pre-commit = "^3.7.0"
commitizen = "^3.22.0"
mypy = "^1.10.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.black]
line-length = 79


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/.env.example
================================================
SECRET_KEY="<your hex 32>"
ALGORITHM="<algorithm>"
ACCESS_TOKEN_EXPIRE_MINUTES="<time>"


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/.flake8
================================================
[flake8]
max-line-length = 79
extend-ignore = E203,E701


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/.pre-commit-config.yaml
================================================
# See https://pre-commit.com for more information
# See https://pre-commit.com/hooks.html for more hooks
repos:
    - repo: https://github.com/pre-commit/pre-commit-hooks
      rev: v4.6.0
      hooks:
          - id: trailing-whitespace
          - id: end-of-file-fixer
          - id: check-yaml
          - id: check-added-large-files


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/.python-version
================================================
3.12.2


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/docs/index.md
================================================
# Welcome to MkDocs

For full documentation visit [mkdocs.org](https://www.mkdocs.org).

## Commands

* `mkdocs new [dir-name]` - Create a new project.
* `mkdocs serve` - Start the live-reloading docs server.
* `mkdocs build` - Build the documentation site.
* `mkdocs -h` - Print help message and exit.

## Project layout

    mkdocs.yml    # The configuration file.
    docs/
        index.md  # The documentation homepage.
        ...       # Other markdown pages, images and other files.


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/main.py
================================================
import time
from datetime import timedelta
from typing import Annotated

from fastapi import Depends, FastAPI, HTTPException, Request, status
from fastapi.security import OAuth2PasswordRequestForm

from src.core import crud, models, schemas, security
from src.core.config import settings
from src.core.db import engine
from src.core.deps import CurrentUser, SessionDep
from src.core.schemas import Token

models.Base.metadata.create_all(bind=engine)

app = FastAPI()


@app.post("/login/access-token/")
async def login(
    session: SessionDep,
    form_data: Annotated[OAuth2PasswordRequestForm, Depends()],
) -> Token:

    user = crud.authenticate(
        session=session, email=form_data.username, password=form_data.password
    )
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token_expires = timedelta(
        minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES
    )
    return Token(
        access_token=security.create_access_token(
            user.id, expires_delta=access_token_expires
        )
    )


@app.post("/singup", response_model=schemas.User)
async def create_user(session: SessionDep, user: schemas.UserCreate):
    db_user = crud.get_user(session, email=user.email)
    if db_user:
        raise HTTPException(status_code=400, detail="Email already registered")
    return crud.create_user(session, user)


@app.get("/users/me/", response_model=schemas.User)
async def read_users_me(current_user: CurrentUser, session: SessionDep):
    db_user = crud.get_user(session, user_id=current_user.id)
    return db_user


@app.get("/users/me/items/", response_model=list[schemas.Item])
async def read_own_items(
    session: SessionDep,
    current_user: CurrentUser,
    skip: int = 0,
    limit: int = 100,
):
    db_items = crud.get_items(
        session, user_id=current_user.id, skip=skip, limit=limit
    )

    return db_items


@app.middleware("http")
async def add_process_time_header_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/app_log.py
================================================
import logging
import os

from fastapi.logger import logger as fastapi_logger

if "gunicorn" in os.environ.get("SERVER_SOFTWARE", ""):
    """
    When running with gunicorn the log handlers get suppressed instead of
    passed along to the container manager. This forces the gunicorn handlers
    to be used throughout the project.
    """

    gunicorn_logger = logging.getLogger("gunicorn")
    log_level = gunicorn_logger.level

    root_logger = logging.getLogger()
    gunicorn_error_logger = logging.getLogger("gunicorn.error")
    uvicorn_access_logger = logging.getLogger("uvicorn.access")

    # Use gunicorn error handlers for root, uvicorn, and fastapi loggers
    root_logger.handlers = gunicorn_error_logger.handlers
    uvicorn_access_logger.handlers = gunicorn_error_logger.handlers
    fastapi_logger.handlers = gunicorn_error_logger.handlers

    # Pass on logging levels for root, uvicorn, and fastapi loggers
    root_logger.setLevel(log_level)
    uvicorn_access_logger.setLevel(log_level)
    fastapi_logger.setLevel(log_level)


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/config.py
================================================
from typing import Literal

from pydantic import PostgresDsn, computed_field
from pydantic_core import MultiHostUrl
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        env_ignore_empty=True,
        extra="ignore",
    )

    SECRET_KEY: str
    ALGORITHM: str
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 7  # 7 days in minutes
    DOMAIN: str = "localhost"
    ENVIROMENT: Literal["dev", "prod"] = "dev"

    @computed_field  # type: ignore
    @property
    def server_host(self) -> str:
        if self.ENVIROMENT == "dev":
            return f"http://{self.DOMAIN}"
        return f"https://{self.DOMAIN}"

    if ENVIROMENT == "prod":
        POSTGRES_SCHEME: str = "postgresql+psycopg"
        POSTGRES_USER: str
        POSTGRES_PASSWORD: str
        POSTGRES_SERVER: str
        POSTGRES_PORT: int = 5432
        POSTGRES_DB: str

    @computed_field  # type: ignore
    @property
    def SQLALCHEMY_DATABASE_URI(self) -> str | PostgresDsn:
        if self.ENVIROMENT == "dev":
            return "sqlite:///database/app.db"
        return MultiHostUrl.build(
            scheme=self.POSTGRES_SCHEME,
            username=self.POSTGRES_USER,
            password=self.POSTGRES_PASSWORD,
            host=self.POSTGRES_SERVER,
            port=self.POSTGRES_PORT,
            path=self.POSTGRES_DB,
        )


settings = Settings()  # type: ignore


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/crud.py
================================================
from sqlalchemy.exc import ArgumentError
from sqlalchemy.orm import Session

from src.core import models, schemas
from src.core.security import get_password_hash, verify_password


def get_user(
    db: Session,
    user_id: int | None = None,
    email: str | None = None,
):
    if not any([user_id, email]):
        raise ArgumentError("Must provide user_id or email.")

    query = db.query(models.User)
    if user_id:
        query = query.filter(models.User.id == user_id)
    if email:
        query = query.filter(models.User.email == email)

    return query.first()


def get_users(db: Session, skip: int = 0, limit: int = 100):
    return db.query(models.User).offset(skip).limit(limit).all()


def create_user(db: Session, user: schemas.UserCreate):
    db_user = models.User(
        email=user.email,
        hashed_password=get_password_hash(user.password),
    )
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return db_user


def get_items(db: Session, user_id: int, skip: int = 0, limit: int = 100):
    return (
        db.query(models.Item)
        .where(models.Item.owner_id == user_id)
        .offset(skip)
        .limit(limit)
        .all()
    )


def create_user_item(db: Session, item: schemas.ItemCreate, user_id: int):
    db_item = models.Item(**item.model_dump(), owner_id=user_id)
    db.add(db_item)
    db.commit()
    db.refresh(db_item)
    return db_item


def authenticate(session: Session, email: str, password: str):
    db_user = get_user(db=session, email=email)
    if not db_user or not verify_password(password, db_user.hashed_password):
        return None
    return db_user


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/db.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

from src.core.config import settings

SQLALCHEMY_DATABASE_URL = settings.SQLALCHEMY_DATABASE_URI

engine = create_engine(
    str(SQLALCHEMY_DATABASE_URL), connect_args={"check_same_thread": False}
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/deps.py
================================================
from typing import Annotated

from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from pydantic import ValidationError
from sqlalchemy.orm import Session

from src.core.config import settings
from src.core.db import SessionLocal
from src.core.models import User
from src.core.schemas import TokenPayload

oauth2 = OAuth2PasswordBearer(tokenUrl="login/access-token")


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


SessionDep = Annotated[Session, Depends(get_db)]
TokenDep = Annotated[str, Depends(oauth2)]


async def get_current_user(session: SessionDep, token: TokenDep) -> User:
    try:
        payload = jwt.decode(
            token, settings.SECRET_KEY, algorithms=[settings.ALGORITHM]
        )
        token_data = TokenPayload(**payload)
    except (JWTError, ValidationError):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Could not validate credentials",
        )

    user = session.get(User, token_data.sub)
    if user is None:
        raise HTTPException(status_code=404, detail="User not found")
    if not user.is_active:  # type: ignore
        raise HTTPException(status_code=400, detail="Inactive user")
    return user


CurrentUser = Annotated[User, Depends(get_current_user)]


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/models.py
================================================
from sqlalchemy import Boolean, Column, ForeignKey, Integer, String
from sqlalchemy.orm import relationship

from src.core.db import Base


class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    email = Column(String, unique=True, index=True)
    hashed_password = Column(String)
    is_active = Column(Boolean, default=True)

    items = relationship("Item", back_populates="owner")


class Item(Base):
    __tablename__ = "items"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, index=True)
    description = Column(String, index=True)
    owner_id = Column(Integer, ForeignKey("users.id"))

    owner = relationship("User", back_populates="items")


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/schemas.py
================================================
from pydantic import BaseModel, EmailStr


class ItemBase(BaseModel):
    title: str
    description: str | None = None


class ItemCreate(ItemBase):
    pass


class Item(ItemBase):
    id: int
    owner_id: int

    class Config:
        from_attributes = True


class UserBase(BaseModel):
    email: EmailStr


class UserCreate(UserBase):
    password: str


class User(UserBase):
    id: int
    is_active: bool
    items: list[Item] = []

    class Config:
        from_attributes = True


class Token(BaseModel):
    access_token: str
    token_type: str = "bearer"


class TokenPayload(BaseModel):
    sub: int | None = None


================================================
File: /06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/security.py
================================================
from datetime import datetime, timedelta, timezone
from typing import Any

from jose import jwt
from passlib.context import CryptContext

from src.core.config import settings

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


def create_access_token(subject: str | Any, expires_delta: timedelta):
    expire = datetime.now(timezone.utc) + expires_delta
    to_encode = {"exp": expire, "sub": str(subject)}
    encoded_jwt = jwt.encode(
        to_encode, settings.SECRET_KEY, algorithm=settings.ALGORITHM
    )
    return encoded_jwt


def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)


def get_password_hash(password):
    return pwd_context.hash(password)


================================================
File: /06-restAPI-fastAPI-deploy/segundo/luciano-starter/README.md
================================================
# api_aovivo


================================================
File: /06-restAPI-fastAPI-deploy/segundo/luciano-starter/main.py
================================================
from fastapi import FastAPI
import random

servidor = FastAPI()

@servidor.get("/recursos")
def numero_aleatorio():
    num = random.randint(1,95)
    print(num)
    return num

================================================
File: /06-restAPI-fastAPI-deploy/segundo/luciano-starter/old.py
================================================
import random

def numero_aleatorio():
    num = random.randint(1,95)
    print(num)
    return num

def dobra_o_numero(numero: int):
    return numero * 2

def main():
    num = numero_aleatorio()
    return dobra_o_numero(num)

if __name__ == "__main__":
    numero_dobrado = main()
    print(numero_dobrado)

================================================
File: /06-restAPI-fastAPI-deploy/segundo/luciano-starter/programa_a.py
================================================
import random
import time

def numero_aleatorio():
    num = random.randint(1,95)
    print(num)
    with open("recursos/arquivo.txt", "a") as file:
        file.write(f"{num}\n")


if __name__ == "__main__":
    while True:
        numero_aleatorio()
        time.sleep(1)

================================================
File: /06-restAPI-fastAPI-deploy/segundo/luciano-starter/programa_b.py
================================================
import time

def dobra_o_numero(numero: int):
    return numero * 2

def le_o_ultimo_numero_do_arquivo():
    with open("recursos/arquivo.txt", "r") as file:
        ultimo_numero = int(file.readlines()[-1])
        return ultimo_numero

if __name__ == "__main__":
    while True: 
        ultimo_numero = le_o_ultimo_numero_do_arquivo()
        print(ultimo_numero)
        numero_dobrado = dobra_o_numero(ultimo_numero)
        print(numero_dobrado)
        time.sleep(1)

================================================
File: /06-restAPI-fastAPI-deploy/segundo/luciano-starter/programa_c.py
================================================
import time

def dobra_o_numero(numero: int):
    return numero * 2

def le_o_ultimo_numero_do_arquivo():
    with open("recursos/arquivo.txt", "r") as file:
        ultimo_numero = int(file.readlines()[-1])
        return ultimo_numero

if __name__ == "__main__":
    while True: 
        ultimo_numero = le_o_ultimo_numero_do_arquivo()
        print(ultimo_numero)
        numero_dobrado = dobra_o_numero(ultimo_numero)
        print(numero_dobrado)
        time.sleep(1)

================================================
File: /06-restAPI-fastAPI-deploy/segundo/luciano-starter/programa_d.py
================================================
import random
import time

def numero_aleatorio():
    num = random.randint(1,95)
    print(num)
    with open("recursos/arquivo.txt", "a") as file:
        file.write(f"{num}\n")


if __name__ == "__main__":
    while True:
        numero_aleatorio()
        time.sleep(1)

================================================
File: /06-restAPI-fastAPI-deploy/segundo/luciano-starter/requirements.txt
================================================
annotated-types==0.6.0
anyio==4.3.0
certifi==2024.2.2
click==8.1.7
colorama==0.4.6
dnspython==2.6.1
email_validator==2.1.1
fastapi==0.111.0
fastapi-cli==0.0.3
h11==0.14.0
httpcore==1.0.5
httptools==0.6.1
httpx==0.27.0
idna==3.7
Jinja2==3.1.4
markdown-it-py==3.0.0
MarkupSafe==2.1.5
mdurl==0.1.2
orjson==3.10.3
pydantic==2.7.1
pydantic_core==2.18.2
Pygments==2.18.0
python-dotenv==1.0.1
python-multipart==0.0.9
PyYAML==6.0.1
rich==13.7.1
shellingham==1.5.4
sniffio==1.3.1
starlette==0.37.2
typer==0.12.3
typing_extensions==4.11.0
ujson==5.9.0
uvicorn==0.29.0
watchfiles==0.21.0
websockets==12.0


================================================
File: /06-restAPI-fastAPI-deploy/segundo/luciano-starter/recursos/arquivo.txt
================================================
67
41
67
15
25
94
43
93
12
21
4
26
50
91
13
64
47
69
58
83
88
58
45
1
48
72
50
53
40
75
90
18
43
89
32
29
57
66
51
64
79
61
60
32
34
65
53
12
17
45
10
44
20
94
20
76
10
46
53
19
58
17
52
30
24
94
65
34
46
74
92
9
10
13
77
77
24
79
92
40
94
52
24
73
70
72
14
77
20
85
83
63
57
13
11
58
47
4
11
79
48
36
1
31
31
78
85
29
18
76
84
63
83
2
31
67
25
45
34
80
31
46
41
25
93
17
40
85
77
86
63
61
29
39
40
34
80
15
71
3
80
63
7
12
41
59
24
39
58
68
65
86
94
53
76
51
16
17
78
34
56
20
50
38
17
83
57
62
2
43
40
6
18
67
11
10
18
24
70
69
59
40
40
85
19
92
80
91
30
54
63
36
85
58
38
44
45
34
81
38
13
65
6
20
57
61
23
63
52
58
36
64
87
86
5
88
64
63
32
34
89
43
6
64
51
83
55
81
54
49
94
3
73
75
34
34
35
32
25
87
95
13
14
49
26
86
89
21
88
92
45
37
32
68
2
30
82
86
56
23
72
85
20
44
87
21
33
83
85
44
91
64
20
72
83
42
13
54
4
72
87
29
19
74
64
23
59
25
51
45
4
46
43
63
84
52
49
51
93
20
17
68
7
74
23
79
51
22
83
88
22
30
78
34
39
43
50
39
81
10
72
81
45
81
32
51
19
45
36
70
75
17
39
89
69
53
14
86
37
41
42
50
43
51
58
6
77
13
21
80
87
53
30
24
50
49
11
69
54
63
50
78
3
11
51
9
74
21
64
29
85
55
46
8
11
93
40
33
69
25
71
13
54
32
94
91
47
47
23
33
9
41
51
26
57
32
2
58
60
68
93
37
44
51
33
81
2
56
94
84
73
68
63
2
59
30
46
81
19
40
88
89
65
1
6
25
43
41


================================================
File: /08-kafka-pubsub-streaming/README.md
================================================
# kafka-workshop

Bem-vindo ao projeto de monitoramento de transações financeiras em tempo real com Apache Kafka! Este projeto faz parte de um workshop prático exclusivo para os alunos da **Escola Jornada de Dados**.

[Excalidraw](https://link.excalidraw.com/l/8pvW6zbNUnD/1FWXp1AaTy3)

## Sobre o Workshop

* **Data:** Sábado, 13 de julho
* **Horário:** 9:00 da manhã
* **Duração:** 4 horas
* **Público:** Exclusivo para alunos da Jornada de Dados

### Overview Kafka

Kafka-demo

Kafka-Topics-Partitions

Kafka-Brokers-Replications

Kafka-partitions

Kafka-producers

Kafka-connect

Schema-Registry


### Descrição do Workshop

## Escopo do Projeto: Simulação de 50 Refrigeradores Espalhados pelo Brasil

Claro! Aqui está o README atualizado com o comando para criar o tópico `marketing-project`.

## Projeto: Monitoramento de Refrigeração no Brasil

### Objetivo

Este projeto tem como objetivo simular 50 refrigeradores espalhados pelo Brasil. Cada refrigerador irá reportar a temperatura a cada segundo. Queremos entender como os refrigeradores estão operando em diferentes regiões e monitorar temperaturas anômalas que podem indicar falhas ou problemas.

### Componentes do Projeto

1. **Producers**:
    - 50 produtores simulando refrigeradores.
    - Cada produtor envia dados de temperatura a cada segundo.
    - As temperaturas são geradas com base em dois intervalos: 
        - 45 refrigeradores com temperaturas entre 0°C e 5°C.
        - 5 refrigeradores com temperaturas entre 20°C e 40°C.
    - Cada produtor tem um ID único gerado com UUID.

2. **Consumer**:
    - Um consumidor que lê os dados em tempo real e exibe as temperaturas usando Streamlit.

3. **Kafka Topic**:
    - Tópico chamado `marketing-project`.

### Passos para Configuração

#### 1. Configurar o Confluent CLI

Certifique-se de que você tenha configurado o Confluent CLI e esteja autenticado:

```bash
confluent login
```

#### 2. Selecionar o Ambiente e o Cluster

```bash
confluent environment use <environment_id>
confluent kafka cluster use <cluster_id>
```

#### 3. Criar o Tópico `marketing-project`

```bash
confluent kafka topic create marketing-project --partitions 6
```

### Objetivo

O objetivo deste projeto é simular 50 refrigeradores espalhados pelo Brasil, gerando dados de temperatura em tempo real. Essa simulação visa monitorar e analisar as variações de temperatura, permitindo uma melhor compreensão do comportamento térmico dos refrigeradores em diferentes regiões e condições climáticas. Com este projeto, pretendemos:

1. **Monitorar Temperaturas em Tempo Real**: Capturar e exibir dados de temperatura de 50 refrigeradores espalhados pelo Brasil, atualizados a cada segundo.
2. **Analisar Distribuição de Temperaturas**: Identificar padrões de temperatura e comportamentos anômalos entre os refrigeradores.
3. **Avaliar Eficiência Térmica**: Verificar a eficiência térmica dos refrigeradores em manter a temperatura dentro de uma faixa desejada.
4. **Identificar Problemas Potenciais**: Detectar refrigeradores que possam estar operando fora da faixa de temperatura esperada, sinalizando possíveis falhas ou necessidade de manutenção.

### Descrição do Projeto

1. **Simulação dos Refrigeradores**:
    - **Quantidade**: 50 refrigeradores.
    - **Localização**: Cada refrigerador terá uma latitude e longitude fixas, representando diferentes regiões do Brasil.
    - **Distribuição de Temperatura**:
        - 45 refrigeradores manterão temperaturas entre 0 e 5 graus Celsius (95%).
        - 5 refrigeradores terão temperaturas entre 20 e 40 graus Celsius (5%), representando possíveis falhas ou condições extremas.

2. **Produção de Dados**:
    - **Intervalo de Produção**: Cada refrigerador gerará uma leitura de temperatura a cada segundo.
    - **Formato dos Dados**: Os dados serão enviados em formato JSON, contendo a latitude, longitude, temperatura e identificador do refrigerador.

3. **Tópico Kafka**:
    - **Nome do Tópico**: `marketing-project`.
    - **Produção e Consumo**: Os dados gerados pelos 50 refrigeradores serão publicados neste tópico Kafka.

4. **Consumidor de Dados**:
    - **Visualização em Tempo Real**: Um consumidor será implementado usando Streamlit para exibir as últimas leituras de temperatura de cada refrigerador em tempo real.
    - **Monitoramento e Análise**: A aplicação Streamlit permitirá monitorar os dados e identificar rapidamente qualquer comportamento anômalo.

### Componentes do Projeto

1. **Producers**:
    - 50 instâncias de produtores, cada uma simulando um refrigerador.
    - Utilização da biblioteca `Faker` para gerar dados de localização e temperatura.
    - Publicação dos dados no tópico `marketing-project` no Kafka.

2. **Consumer**:
    - Uma aplicação Streamlit que consome os dados do tópico `marketing-project`.
    - Exibição dos dados em um dashboard, mostrando a última leitura de temperatura de cada refrigerador.

### Ferramentas e Tecnologias

1. **Apache Kafka**: Para transmissão e ingestão de dados em tempo real.
2. **Confluent Cloud**: Plataforma gerenciada de Kafka para simplificar a configuração e o gerenciamento do cluster.
3. **Faker**: Biblioteca para geração de dados fictícios.
4. **Docker**: Para containerização dos produtores e consumidor.
5. **Streamlit**: Para visualização dos dados em tempo real.

### Implementação

1. **Configuração do Tópico Kafka**:
    - Criar o tópico `marketing-project` no Confluent Cloud.

2. **Desenvolvimento dos Producers**:
    - Implementar o script de produção utilizando `Faker` para gerar os dados de temperatura e localização.
    - Containerizar os produtores usando Docker.

3. **Desenvolvimento do Consumer**:
    - Implementar a aplicação Streamlit para consumir e exibir os dados em tempo real.
    - Containerizar o consumidor usando Docker.

4. **Execução**:
    - Iniciar os 50 produtores e o consumidor utilizando `docker-compose`.

## Executar os Containers
Certifique-se de que o arquivo docker-compose.yml está configurado corretamente e execute os seguintes comandos no terminal para construir a imagem Docker e iniciar os containers:

```bash
docker-compose build
docker-compose up -d
```

### Resultado Esperado

Ao final do projeto, espera-se ter uma simulação funcional de 50 refrigeradores espalhados pelo Brasil, com dados de temperatura sendo gerados e exibidos em tempo real. A aplicação Streamlit fornecerá uma interface intuitiva para monitorar e analisar esses dados, possibilitando a identificação de padrões e anomalias de temperatura, além de fornecer insights sobre a eficiência térmica dos refrigeradores em diferentes condições climáticas.

### Bizu para apagar

confluent kafka topic update marketing-project --config retention.ms=1000

confluent kafka topic delete marketing-project


###

```bash
confluent kafka topic consume marketing-project --from-beginning
```



================================================
File: /08-kafka-pubsub-streaming/docker-compose.yml
================================================
---
version: "3.9"

services:
  zookeeper:
    platform: linux/amd64
    image: confluentinc/cp-zookeeper:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: zookeeper
    restart: unless-stopped
    ports:
      - '32181:32181'
      - '2888:2888'
      - '3888:3888'
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper:2888:3888
    healthcheck:
      test: echo stat | nc localhost 32181
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  kafka-ui: # https://github.com/provectus/kafka-ui/blob/master/documentation/compose/kafka-ui.yaml
    container_name: kafka-ui 
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8087:8080
    depends_on:
      - broker-1
      - broker-2
      - broker-3
    environment:
      KAFKA_CLUSTERS_0_NAME: broker-1
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker-1:29091
      KAFKA_CLUSTERS_0_METRICS_PORT: 19101
      #KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schemaregistry0:8085
      #KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: first
      #KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect0:8083
      KAFKA_CLUSTERS_1_NAME: broker-2
      KAFKA_CLUSTERS_1_BOOTSTRAPSERVERS: broker-2:29092
      KAFKA_CLUSTERS_1_METRICS_PORT: 19102
      #KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schemaregistry0:8085
      KAFKA_CLUSTERS_2_NAME: broker-3
      KAFKA_CLUSTERS_2_BOOTSTRAPSERVERS: broker-3:29093
      KAFKA_CLUSTERS_2_METRICS_PORT: 19103
      #KAFKA_CLUSTERS_1_SCHEMAREGISTRY: http://schemaregistry1:8085
      DYNAMIC_CONFIG_ENABLED: 'true'
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  broker-1:
    platform: linux/amd64
    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: broker-1
    restart: unless-stopped
    ports:
      - '9091:9091'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-1:29091,EXTERNAL://localhost:9091
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      #KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      #KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      #KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      #KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 19101
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9091
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  broker-2:
    platform: linux/amd64
    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: broker-2
    restart: unless-stopped
    ports:
      - '9092:9092'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-2:29092,EXTERNAL://localhost:9092
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      #KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      #KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      #KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      #KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 19102
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9092
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  broker-3:
    platform: linux/amd64
    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
    container_name: broker-3
    restart: unless-stopped
    ports:
      - '9093:9093'
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-3:29093,EXTERNAL://localhost:9093
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 3
      #KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      #KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      #KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      #KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 19103
      KAFKA_JMX_HOSTNAME: localhost
    healthcheck:
      test: nc -vz localhost 9093
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - kafka
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
  
  # kafka-shell:
  #   image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}
  #   container_name: kafka-shell
  #   restart: unless-stopped
  #   depends_on:
  #     - zookeeper
  #     - broker-1
  #     - broker-2
  #     - broker-3
  #   entrypoint: ""
  #   command: "sleep 10000000"
  #   networks:
  #     - kafka

  producer:
    platform: linux/amd64
    container_name: producer
    build: ./python-client/
    restart: always
    environment:
      - ACTION=producer
      - BOOTSTRAP_SERVERS=broker-1:29091,broker-2:29092,broker-3:29093
      - TOPIC=my-topic
      - PYTHONUNBUFFERED=1 # https://github.com/docker/compose/issues/4837#issuecomment-302765592
    networks:
      - kafka
    depends_on:
      - zookeeper
      - broker-1
      - broker-2
      - broker-3
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  consumer:
    platform: linux/amd64
    container_name: consumer
    build: ./python-client/
    restart: always
    environment:
      - ACTION=consumer
      - BOOTSTRAP_SERVERS=broker-1:29091,broker-2:29092,broker-3:29093
      - TOPIC=my-topic
      - CONSUMER_GROUP=cg-group-id
      - PYTHONUNBUFFERED=1 # https://github.com/docker/compose/issues/4837#issuecomment-302765592
    networks:
      - kafka
    depends_on:
      - zookeeper
      - broker-1
      - broker-2
      - broker-3
      - producer
    logging:
      driver: "json-file"
      options:
        max-size: "1m"

  # shell:
  #   container_name: shell
  #   build: ./python-client/
  #   restart: always
  #   environment:
  #     - ACTION=shell
  #     - BOOTSTRAP_SERVERS=broker-1:29091,broker-2:29092,broker-3:29093
  #     - TOPIC=my-topic
  #   networks:
  #     - kafka
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "1m"

networks:
  kafka:
    name: kafka

================================================
File: /08-kafka-pubsub-streaming/pyproject.toml
================================================
[tool.poetry]
name = "kafka-workshop"
version = "0.1.0"
description = ""
authors = ["lvgalvaofilho <lvgalvaofilho@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "3.12.1"
kafka-python = "^2.0.2"
six = "^1.16.0"
confluent-kafka = "^2.5.0"
python-dotenv = "^1.0.1"


[tool.poetry.group.dev.dependencies]
ruff = "^0.5.1"
taskipy = "^1.13.0"

[tool.taskipy.tasks]

lint = 'ruff check . && ruff check . --diff'
format = 'ruff check . --fix && ruff format . '

[tool.ruff]
line-length = 79

[tool.ruff.lint]
preview = true
select = ['I', 'F', 'E', 'W', 'PL', 'PT']
ignore = ['E402', 'F811', 'E501']

[tool.ruff.format]
preview = true
quote-style = 'single'


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /08-kafka-pubsub-streaming/.python-version
================================================
3.12.1


================================================
File: /08-kafka-pubsub-streaming/kafka-brokers/README.md
================================================
## Resumo: Kafka Brokers e Replicação

### Kafka Brokers

**Conceito Básico:**
- Kafka é um sistema distribuído composto por várias máquinas chamadas brokers.
- Cada broker hospeda um conjunto de partições do Kafka e lida com solicitações de leitura e escrita de eventos.

**Infraestrutura Física:**
- Os brokers podem ser servidores físicos, instâncias de nuvem, contêineres em pods, ou servidores virtualizados.
- Independentemente da forma de implementação, cada broker executa o processo do Kafka broker.

**Funções dos Brokers:**
- Cada broker gerencia algumas partições do Kafka, aceitando eventos novos para essas partições ou lendo eventos delas.
- Brokers também gerenciam a replicação de partições entre si.

**Simplicidade:**
- Brokers são projetados para serem simples, garantindo que eles sejam facilmente escaláveis, compreensíveis, modificáveis e extensíveis.

### Kafka Replication

**Necessidade de Replicação:**
- Armazenar cada partição em um único broker seria arriscado devido à suscetibilidade a falhas dos brokers e seus sistemas de armazenamento.
- Portanto, os dados das partições são copiados para vários brokers para garantir a segurança dos dados.

**Tipos de Réplicas:**
- **Líder**: A principal réplica da partição.
- **Seguidoras**: Réplicas adicionais que seguem a líder.

**Processo de Replicação:**
- Cada partição replicada tem uma réplica líder e várias seguidoras.
- Escrita e leitura de dados são feitas principalmente na réplica líder.
- Após a escrita na líder, a líder e as seguidoras trabalham juntas para replicar os dados.

### Introdução

A replicação é um aspecto crucial do Apache Kafka que garante a durabilidade e disponibilidade dos dados. Neste exercício, vamos entender como a replicação funciona e por que ela é essencial para um sistema de mensageria distribuído como o Kafka.

### Conceitos Básicos

- **Partition**: Uma partição é uma unidade de paralelismo em Kafka. Cada tópico pode ter várias partições.
- **Leader Replica**: A réplica líder é a partição principal onde todas as gravações ocorrem.
- **Follower Replicas**: As réplicas seguidoras são cópias da partição que residem em outros brokers para garantir a durabilidade e a disponibilidade dos dados.

### Funcionamento da Replicação

1. **Escrita no Leader**:
   - Quando os dados são produzidos para uma partição, eles são inicialmente escritos na réplica líder.

2. **Sincronização com Followers**:
   - Após a escrita, a réplica líder replica os dados para as réplicas seguidoras. Este processo garante que todas as réplicas tenham os mesmos dados.

3. **Falha e Recuperação**:
   - Se um broker que hospeda a réplica líder falhar, uma das réplicas seguidoras é promovida a líder. Isso garante a continuidade das operações sem perda de dados.

### Diagrama de Replicação

```mermaid
graph TD;
    Producer -->|Escreve| Leader;
    Leader -->|Replica Dados| Follower1;
    Leader -->|Replica Dados| Follower2;
    Follower1 -->|Sincroniza| Leader;
    Follower2 -->|Sincroniza| Leader;
    Leader -->|Failover| Follower1;
    Follower1 -->|Promovido a Líder| Leader;
```

### Passos para Configurar e Verificar a Replicação

#### 1. Criar um Tópico com Réplicas
Para criar um tópico com múltiplas réplicas, use o comando:

```bash
confluent kafka topic create temperatura --partitions 3 --config min.insync.replicas=2 --config replication.factor=2
```

#### 2. Produzir Dados para o Tópico
Produza mensagens para o tópico `temperatura` com valores aleatórios entre 35 e 40:

```bash
echo "1:$(shuf -i 35-40 -n 1)" | confluent kafka topic produce temperatura
echo "2:$(shuf -i 35-40 -n 1)" | confluent kafka topic produce temperatura
echo "3:$(shuf -i 35-40 -n 1)" | confluent kafka topic produce temperatura
```

#### 3. Verificar a Replicação
Para verificar a replicação, use o comando `describe` no tópico:

```bash
confluent kafka topic describe temperatura
```

Isso mostrará informações sobre as partições e suas réplicas.

### Número Ideal de Réplicas no Kafka

Determinar o número ideal de réplicas para suas partições no Kafka é uma decisão que envolve considerar trade-offs entre durabilidade, disponibilidade e custo. Aqui estão algumas diretrizes gerais e como calcular a quantidade adequada de réplicas:

### Diretrizes Gerais

1. **Durabilidade e Alta Disponibilidade**:
   - Mais réplicas aumentam a durabilidade e a disponibilidade dos dados.
   - Em um cenário comum, ter 2 réplicas (um líder e um seguidor) pode ser suficiente para muitas aplicações, pois garante que os dados permaneçam disponíveis se um broker falhar.

2. **Custo**:
   - Cada réplica adicional aumenta o uso de armazenamento e recursos de rede, o que pode aumentar os custos operacionais.
   - Avalie o trade-off entre a necessidade de alta disponibilidade e os custos adicionais.

3. **Requisitos de Tolerância a Falhas**:
   - Se a aplicação exige uma alta tolerância a falhas (por exemplo, sistemas financeiros ou críticos), considere usar 3 réplicas ou mais.
   - Para ambientes menos críticos, 2 réplicas podem ser adequadas.

### Como Calcular o Número de Réplicas

Para calcular o número ideal de réplicas, você pode seguir estas etapas:

1. **Analisar a Criticidade dos Dados**:
   - Determine o impacto da perda temporária de acesso aos dados.
   - Quanto mais críticos os dados, mais réplicas você deve considerar.

2. **Avaliar a Infraestrutura**:
   - Considere o número de brokers no cluster. O número de réplicas não deve exceder o número de brokers.
   - Para garantir alta disponibilidade, o número de réplicas deve ser menor que o número total de brokers.

3. **Calcular o Fator de Replicação**:
   - Um fator de replicação de 2 (1 líder + 1 seguidor) é o mínimo recomendado para garantir alta disponibilidade.
   - Um fator de replicação de 3 (1 líder + 2 seguidores) oferece uma boa combinação de durabilidade e custo.

### Exemplo Prático

Vamos criar um tópico com 3 partições e 2 réplicas (1 líder e 1 seguidor) no Confluent Cloud.

#### 1. Criar um Tópico com Replicação

```bash
confluent kafka topic create temperatura --partitions 3 --config replication.factor=2
```

#### 2. Produzir Mensagens para o Tópico

Produza mensagens para o tópico `temperatura` com valores aleatórios entre 35 e 40:

```bash
for i in {1..8}; do echo "$i:$(shuf -i 35-40 -n 1)" | confluent kafka topic produce temperatura; done
```

#### 3. Verificar a Replicação

Use o comando `describe` no tópico para verificar a configuração das partições e réplicas:

```bash
confluent kafka topic describe temperatura
```

#### 4. Monitorar e Ajustar

- **Verificação na Interface Web**:
  - Navegue até a interface do Confluent Cloud e selecione seu cluster.
  - Na guia "Topics", clique no tópico `temperatura` para ver detalhes das partições e réplicas.

- **Monitoramento de Métricas**:
  - Use as ferramentas de monitoramento do Confluent Cloud para observar o comportamento do cluster, latências e disponibilidade.

### Conclusão

O número ideal de réplicas depende dos requisitos específicos de durabilidade, disponibilidade e custo da sua aplicação. Embora 2 réplicas possam ser suficientes para muitas aplicações, ambientes mais críticos podem exigir 3 réplicas ou mais. Use as diretrizes acima para tomar uma decisão informada.

### Diferença entre Partição e Replicação no Apache Kafka

### Partição (Partition)

#### Conceito
- **Definição**: Uma partição é uma unidade de paralelismo em Kafka. Cada tópico pode ter várias partições.
- **Objetivo**: Aumentar a capacidade de processamento e a paralelização das operações de leitura e escrita.
- **Distribuição**: As mensagens são distribuídas entre as partições de um tópico.
- **Ordenação**: Mensagens dentro de uma partição são ordenadas de forma estrita, mas não há garantia de ordenação entre partições diferentes do mesmo tópico.

#### Como Funciona
- Quando um produtor envia mensagens para um tópico, essas mensagens são distribuídas entre as partições.
- Cada partição é tratada de forma independente e pode ser armazenada em diferentes brokers no cluster.
- Consumidores podem ler mensagens de várias partições em paralelo, melhorando o throughput.

#### Exemplo
- Suponha um tópico `temperatura` com 3 partições:
    - Partição 0: Contém mensagens de sensor A
    - Partição 1: Contém mensagens de sensor B
    - Partição 2: Contém mensagens de sensor C

```mermaid
graph TD;
    Producer -->|Particiona| Partition0;
    Producer -->|Particiona| Partition1;
    Producer -->|Particiona| Partition2;
```

### Replicação (Replication)

#### Conceito
- **Definição**: Replicação é o processo de criar cópias de uma partição em diferentes brokers para garantir a durabilidade e a disponibilidade dos dados.
- **Objetivo**: Garantir a disponibilidade e a integridade dos dados em caso de falha de um broker.
- **Tipos de Réplicas**: 
    - **Leader Replica**: A réplica principal que recebe todas as operações de escrita e leitura.
    - **Follower Replicas**: Réplicas secundárias que mantêm uma cópia dos dados da réplica líder.

#### Como Funciona
- Quando uma mensagem é escrita em uma partição líder, essa mensagem é replicada para as partições seguidoras.
- Se o broker que hospeda a réplica líder falhar, uma das réplicas seguidoras é promovida a líder.
- Isso garante que os dados estejam disponíveis mesmo se um ou mais brokers falharem.

#### Exemplo
- Suponha um tópico `temperatura` com 3 partições e cada partição replicada em 3 brokers (1 líder e 2 seguidores).

```mermaid
graph TD;
    Producer -->|Escreve| Leader0;
    Leader0 -->|Replica| Follower0-1;
    Leader0 -->|Replica| Follower0-2;

    Producer -->|Escreve| Leader1;
    Leader1 -->|Replica| Follower1-1;
    Leader1 -->|Replica| Follower1-2;

    Producer -->|Escreve| Leader2;
    Leader2 -->|Replica| Follower2-1;
    Leader2 -->|Replica| Follower2-2;
```

### Comparação

| Aspecto       | Partição                          | Replicação                            |
|---------------|-----------------------------------|---------------------------------------|
| **Definição** | Unidade de paralelismo            | Cópia de dados para durabilidade      |
| **Objetivo**  | Aumentar a capacidade de processamento | Garantir disponibilidade e integridade dos dados |
| **Operação**  | Mensagens distribuídas entre partições | Mensagens replicadas entre réplicas  |
| **Falha**     | Se uma partição falhar, apenas uma parte dos dados é afetada | Se o líder falhar, um seguidor é promovido a líder |

### Conclusão

- **Partições**: São usadas para melhorar a escalabilidade e a paralelização no Kafka. Elas permitem que várias operações de leitura e escrita ocorram simultaneamente, aumentando o throughput.
- **Replicação**: Garante a alta disponibilidade e a durabilidade dos dados, criando cópias das partições em múltiplos brokers. Isso assegura que os dados estejam sempre acessíveis, mesmo em caso de falha de um broker.

Compreender a diferença entre partições e replicação é crucial para projetar sistemas Kafka escaláveis e resilientes. Se precisar de mais detalhes ou ajuda, estou à disposição!

================================================
File: /08-kafka-pubsub-streaming/kafka-consumers/README.md
================================================
# Consumidores com Confluent Kafka Python no Confluent Cloud

## Introdução

Nessa parte, vamos explorar como criar e configurar um consumidor utilizando a biblioteca Python `confluent_kafka` para ler mensagens de um broker Kafka hospedado no Confluent Cloud. O objetivo é consumir mensagens de um tópico que simula leituras de temperatura de um sensor de geladeira.

## Conceitos Básicos

- **Consumidor (Consumer)**: Aplicações que leem (consomem) dados do Kafka.

## Diagrama de Arquitetura

```mermaid
graph TD;
    A[Sensor de Geladeira] -->|Produz Dados| B[Kafka Producer];
    B -->|Envia Dados| C[Confluent Cloud Broker];
    C -->|Distribui Dados| D[Partições de Tópicos];
    D --> E[Kafka Consumer];
    E --> F[Armazenamento em CSV];
```

## Código do Consumidor

Crie um arquivo chamado `kafka_consumers.py` com o seguinte conteúdo:

```python
from confluent_kafka import Consumer
from dotenv import load_dotenv
import os
import csv

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Configurações do consumidor
conf = {
    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),
    'sasl.mechanisms': 'PLAIN',
    'security.protocol': 'SASL_SSL',
    'sasl.username': os.getenv('SASL_USERNAME'),
    'sasl.password': os.getenv('SASL_PASSWORD'),
    'group.id': 'python-consumer-group',
    'auto.offset.reset': 'earliest'
}

# Criação do consumidor
consumer = Consumer(**conf)

# Inscrição no tópico
topic = os.getenv('TOPIC_PRODUCER')
consumer.subscribe([topic])

# Função para processar mensagens
def consume_messages():
    # Nome do arquivo CSV
    script_dir = os.path.dirname(os.path.abspath(__file__))  # Diretório do script atual
    csv_file = os.path.join(script_dir, 'messages.csv')  # Caminho completo para o arquivo CSV
    
    # Abrir o arquivo CSV em modo de escrita
    with open(csv_file, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Key', 'Value'])  # Escrever cabeçalho no CSV
        
        try:
            while True:
                msg = consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem
                # Exibe a mensagem recebida
                key = msg.key().decode('utf-8')
                value = msg.value().decode('utf-8')
                print(f"Received message: Key: {key}, Value: {value}")
                # Escreve a mensagem no CSV
                writer.writerow([key, value])
        except KeyboardInterrupt:
            pass
        finally:
            print("Closing consumer.")
            consumer.close()

if __name__ == "__main__":
    consume_messages()
```

## Executando o Consumidor

Para executar o consumidor e começar a ler mensagens do tópico, use o seguinte comando:

```bash
python kafka_consumers.py
```

As mensagens lidas serão exibidas no console e salvas em um arquivo `messages.csv` no mesmo diretório do script Python.

## Conclusão

Com este guia, você configurou e executou um consumidor Kafka utilizando Python e a biblioteca `confluent_kafka`. Isso permite que você leia e processe mensagens de um tópico Kafka hospedado no Confluent Cloud, armazenando os dados consumidos em um arquivo CSV para análise posterior.

================================================
File: /08-kafka-pubsub-streaming/kafka-consumers/kafka_consumers.py
================================================
from confluent_kafka import Consumer, KafkaError
from dotenv import load_dotenv
import os
import csv

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Configurações do consumidor
conf = {
    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),
    'sasl.mechanisms': 'PLAIN',
    'security.protocol': 'SASL_SSL',
    'sasl.username': os.getenv('SASL_USERNAME'),
    'sasl.password': os.getenv('SASL_PASSWORD'),
    'group.id': 'novo-python-consumer-group',
    'auto.offset.reset': 'earliest'
}

# Criação do consumidor
consumer = Consumer(**conf)

# Inscrição no tópico
topic = os.getenv('TOPIC_PRODUCER')
consumer.subscribe([topic])

# Função para processar mensagens
def consume_messages():
    # Nome do arquivo CSV
    script_dir = os.path.dirname(os.path.abspath(__file__))  # Diretório do script atual
    csv_file = os.path.join(script_dir, 'messages.csv')  # Caminho completo para o arquivo CSV
    
    # Abrir o arquivo CSV em modo de escrita
    with open(csv_file, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Key', 'Value'])  # Escrever cabeçalho no CSV
        
        try:
            while True:
                msg = consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem
                # Exibe a mensagem recebida
                if msg is None:  # Se nenhuma mensagem foi recebida
                    continue
                if msg.error():  # Se houve um erro ao receber a mensagem
                    continue
                key = msg.key().decode('utf-8')
                value = msg.value().decode('utf-8')
                print(f"Received message: Key: {key}, Value: {value}")
                # Escreve a mensagem no CSV
                writer.writerow([key, value])
        except KeyboardInterrupt:
            pass
        finally:
            print("Closing consumer.")
            consumer.close()

if __name__ == "__main__":
    consume_messages()


================================================
File: /08-kafka-pubsub-streaming/kafka-consumers/kafka_consumers_02.py
================================================
from confluent_kafka import Consumer, KafkaError
from dotenv import load_dotenv
import os
import csv

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Configurações do consumidor
conf = {
    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),
    'sasl.mechanisms': 'PLAIN',
    'security.protocol': 'SASL_SSL',
    'sasl.username': os.getenv('SASL_USERNAME'),
    'sasl.password': os.getenv('SASL_PASSWORD'),
    'group.id': 'outro-python-consumer-group',
    'auto.offset.reset': 'earliest'
}

# Criação do consumidor
consumer = Consumer(**conf)

# Inscrição no tópico
topic = os.getenv('TOPIC_PRODUCER')
consumer.subscribe([topic])

# Função para processar mensagens
def consume_messages():
    # Nome do arquivo CSV
    script_dir = os.path.dirname(os.path.abspath(__file__))  # Diretório do script atual
    csv_file = os.path.join(script_dir, 'messages_02.csv')  # Caminho completo para o arquivo CSV
    
    # Abrir o arquivo CSV em modo de escrita
    with open(csv_file, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Key', 'Value'])  # Escrever cabeçalho no CSV
        
        try:
            while True:
                msg = consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem
                # Exibe a mensagem recebida
                if msg is None:  # Se nenhuma mensagem foi recebida
                    continue
                if msg.error():  # Se houve um erro ao receber a mensagem
                    continue
                key = msg.key().decode('utf-8')
                value = msg.value().decode('utf-8')
                print(f"Received message: Key: {key}, Value: {value}")
                # Escreve a mensagem no CSV
                writer.writerow([key, value])
        except KeyboardInterrupt:
            pass
        finally:
            print("Closing consumer.")
            consumer.close()

if __name__ == "__main__":
    consume_messages()


================================================
File: /08-kafka-pubsub-streaming/kafka-demo/README.md
================================================
# Workshop de Kafka - Exercício de Configuração e Produção de Mensagens

## Pré-requisitos

1. **Confluent Cloud Cluster:** Certifique-se de ter configurado o Confluent Cloud Cluster e o CLI conforme os exercícios anteriores.
2. **Kafka CLI:** Tenha o Kafka CLI instalado e configurado para se conectar ao seu cluster Confluent Cloud.

## Passos para o Exercício

### 1. Criar uma Conta no Confluent Cloud
- Vá para a [URL do Confluent Cloud](https://confluent.cloud).
- Insira seu nome, email e senha.
- Clique no botão "Start Free" e espere receber um email de confirmação.
- Confirme seu email para prosseguir com a criação do cluster.

### 2. Configurar o Cluster
- Após confirmar o email, siga as instruções para configurar seu cluster.
- Escolha entre um cluster básico, padrão ou dedicado. Para este exercício, escolha o **cluster básico**.

### 3. Aplicar Código Promocional
- Navegue até "Settings" > "Billing and Payment".
- Insira o código promocional `kafka101` para obter $101 adicionais de uso gratuito.

### 4. Criar um Tópico

#### Criar um Tópico Usando a Interface Web
- Na página inicial do Confluent Cloud, selecione a guia "Topics".
- Clique em "Create Topic" e nomeie o tópico como `tecnologias`.
- Mantenha o número padrão de partições (6) e crie o tópico.

#### Criar um Tópico Usando o CLI
- No terminal, após configurar o CLI conforme os passos seguintes, crie um tópico:

    ```bash
    confluent kafka topic create tecnologias --partitions 6
    ```

### 5. Produzir Mensagens Usando a Interface Web
- Navegue até a guia "Messages" para visualizar a produção e o consumo de mensagens em tempo real.
- Clique em "Produce a new message to this topic".
- Insira `1` como chave e `Python` como valor, e clique em "Produce".

### 6. Configurar a Interface de Linha de Comando (CLI)
- Na página do Confluent Cloud, vá até "CLI and Tools" para baixar e configurar as ferramentas de linha de comando.
- No terminal, faça login no Confluent Cloud:

    ```bash
    confluent login --save
    ```

- Use o mesmo email e senha que você usou para criar sua conta.

### 7. Selecionar Ambiente e Cluster
- Liste os ambientes disponíveis:

    ```bash
    confluent environment list
    ```

- Use o ID do ambiente padrão:

    ```bash
    confluent environment use <environment_id>
    ```

- Liste os clusters Kafka disponíveis:

    ```bash
    confluent kafka cluster list
    ```

- Use o ID do cluster:

    ```bash
    confluent kafka cluster use <cluster_id>
    ```

### 8. Criar e Configurar Chave da API
- Crie uma chave da API:

    ```bash
    confluent api-key create --resource <cluster_id>
    ```

- Salve a chave da API e o segredo fornecidos.
- Use a chave da API:

    ```bash
    confluent api-key use <api_key> --resource <cluster_id>
    ```

### 9. Produzir Mensagens Usando o CLI
- Liste os tópicos disponíveis:

    ```bash
    confluent kafka topic list
    ```

- Para produzir mensagens para o tópico `tecnologias`, abra um terminal CLI e execute:

    ```bash
    confluent kafka topic produce tecnologias
    ```

- No prompt, insira uma mensagem de cada vez e pressione Enter:

    ```plaintext
    1:Python
    2:SQL
    3:Kafka
    4:Spark
    5:Airflow
    6:Kubernetes
    7:Terraform
    8:Docker
    ```

### 10. Consumir Mensagens Usando o CLI
- Abra outro terminal CLI para consumir as mensagens desde o início do tópico:

    ```bash
    confluent kafka topic consume tecnologias --from-beginning
    ```

### 11. Verificar Mensagens no Console Web
- Volte para a interface web do Confluent Cloud e verifique as mensagens produzidas.
- Para ver mensagens na interface web, defina o deslocamento (offset) para zero e verifique cada partição.

### Conclusão

Se você seguiu todos os passos, realizou várias atividades importantes:
- Criou uma conta no Confluent Cloud e configurou seu primeiro cluster Kafka.
- Criou um tópico e produziu mensagens usando a interface web.
- Instalou o CLI, criou uma chave da API e produziu/consumiu mensagens usando o CLI.

### Gráfico Mermaid

```mermaid
graph TD;
    A[Criar Conta no Confluent Cloud] --> B[Configurar Cluster]
    B --> C[Aplicar Código Promocional]
    C --> D[Criar Tópico]
    D --> E[Produzir Mensagens (Web)]
    E --> F[Configurar CLI]
    F --> G[Selecionar Ambiente e Cluster]
    G --> H[Criar e Configurar Chave da API]
    H --> I[Produzir Mensagens (CLI)]
    I --> J[Consumir Mensagens (CLI)]
    J --> K[Verificar Mensagens (Web)]
```

### Referências
- [Confluent Cloud Documentation](https://docs.confluent.io/cloud/current/get-started/index.html)
- [Kafka Documentation](https://kafka.apache.org/documentation/)

---

Espero que este README e o gráfico ajudem no entendimento e execução do exercício sobre configuração e produção de mensagens no Kafka. Se precisar de mais detalhes ou ajuda, estou à disposição!

================================================
File: /08-kafka-pubsub-streaming/kafka-ecosystem/README.md
================================================
Aqui está um README melhorado com a explicação sobre o ecossistema Kafka e um diagrama Mermaid mais detalhado e sem erros:

# Ecossistema Kafka

### Introdução

Nessa parte, vamos explorar o ecossistema mais amplo do Kafka. Mesmo que os brokers gerenciem tópicos particionados e replicados, e que tenhamos uma coleção crescente de produtores e consumidores escrevendo e lendo eventos, certos padrões surgem na forma como esses componentes interagem. Esses padrões incentivam os desenvolvedores a construir funcionalidades recorrentes em torno do Kafka.

### Ecosistema Kafka

O ecossistema Kafka inclui várias ferramentas e componentes que resolvem problemas comuns e aumentam as capacidades do Kafka. Alguns dos principais componentes são:

- **Kafka Connect**: Uma ferramenta para mover dados entre Kafka e outros sistemas de forma robusta e escalável.
- **Confluent Schema Registry**: Um serviço para gerenciar e aplicar versões de esquemas de dados para Kafka.
- **Kafka Streams**: Uma biblioteca para processar fluxos de dados em tempo real.
- **ksqlDB**: Um banco de dados de streaming que oferece uma interface SQL para processar dados em Kafka.

### Diagrama do Ecossistema Kafka

![imagem](./kafka_eco.webp)

### Componentes do Ecossistema Kafka

#### Kafka Connect

Kafka Connect é uma ferramenta para mover grandes coleções de dados dentro e fora do Kafka. Ele é usado para integrar Kafka com bancos de dados, sistemas de armazenamento e outros sistemas de dados.

#### Confluent Schema Registry

O Schema Registry gerencia e aplica versões de esquemas de dados, garantindo que os dados enviados para o Kafka estejam em conformidade com os esquemas esperados. Ele ajuda a manter a compatibilidade e a integridade dos dados.

#### Kafka Streams

Kafka Streams é uma biblioteca cliente para construir aplicações de streaming que processam e analisam dados em Kafka. Ele permite o processamento em tempo real de fluxos de dados.

#### ksqlDB

ksqlDB é um banco de dados de streaming que fornece uma interface SQL para processar e analisar dados em Kafka. Ele permite consultas ad-hoc e análise contínua de fluxos de dados.

### Conclusão

Entender o ecossistema Kafka é essencial para aproveitar ao máximo suas capacidades. Além dos produtores e consumidores básicos, ferramentas como Kafka Connect, Schema Registry, Kafka Streams e ksqlDB fornecem funcionalidades adicionais que simplificam e ampliam o uso do Kafka.

Continue explorando esses componentes para construir soluções robustas e escaláveis com Kafka.

================================================
File: /08-kafka-pubsub-streaming/kafka-ecosystem/kafka-connect/README.md
================================================
# Kafka Connect com Confluent Cloud: Geração e Consumo de Dados

## Visão Geral

Neste exercício, vamos criar um conector de origem para o Kafka Connect no Confluent Cloud, que produzirá dados para um tópico Kafka. Uma vez configurado o conector, consumiremos esses dados a partir da linha de comando.

## Passos

1. **Criar um Tópico**
2. **Configurar o DataGen Connector**
3. **Consumir os Dados**

## Pré-requisitos

- Uma conta no Confluent Cloud
- CLI do Confluent instalado e autenticado

## Passo 1: Criar um Tópico

Primeiro, precisamos fornecer um tópico para o conector DataGen produzir dados. Podemos criar esse tópico usando a interface do Confluent Cloud ou o Confluent CLI.

### Usando a Interface do Confluent Cloud

1. Acesse o painel do Confluent Cloud.
2. Navegue até a seção "Tópicos".
3. Clique em "Criar Tópico".
4. Nomeie o tópico como `inventory` e use as configurações padrão.
5. Clique em "Criar" para finalizar.

### Usando o Confluent CLI

```bash
confluent kafka topic create inventory --partitions 1 --cluster <cluster-id>
```

Substitua `<cluster-id>` pelo ID real do seu cluster Kafka.

## Passo 2: Configurar o DataGen Connector

Agora, precisamos configurar o conector DataGen para gerar dados de amostra de acordo com um esquema predefinido.

### Usando a Interface do Confluent Cloud

1. Navegue até a seção "Connect".
2. Clique em "Adicionar Conector".
3. Selecione "DataGen" da lista de conectores disponíveis.
4. Configure o conector:
    - **Nome do Conector**: DataGen-Inventory
    - **Tópico**: `inventory`
    - **Formato da Mensagem de Saída**: JSON
    - **API Key**: Crie ou selecione uma API Key e um segredo existentes.
    - **Modelo de Dados**: Selecione "Inventory" dos modelos disponíveis.
5. Clique em "Continuar" e revise a configuração.
6. Clique em "Lançar" para iniciar o conector.

### Exemplo de Configuração

```json
{
  "name": "DataGen-Inventory",
  "config": {
    "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
    "kafka.topic": "inventory",
    "quickstart": "Inventory",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false",
    "max.interval": 1000,
    "iterations": -1,
    "tasks.max": 1
  }
}
```

Você pode usar a interface do Confluent Cloud para inserir essa configuração.

## Passo 3: Consumir os Dados

Uma vez que o conector DataGen estiver em execução, podemos consumir os dados gerados usando o Confluent CLI.

### Usando o Confluent CLI

```bash
confluent kafka topic consume inventory --from-beginning --cluster <cluster-id>
```

Substitua `<cluster-id>` pelo ID real do seu cluster Kafka.

## Conclusão

Ao longo deste exercício, aprendemos como iniciar rapidamente com o Kafka Connect criando um conector de origem simples que gera dados. Isso é apenas o começo do que o Kafka Connect tem a oferecer. Incentivamos você a explorar mais recursos para entender todas as capacidades do Kafka Connect.

## Recursos Adicionais

- [Documentação do Confluent Kafka Connect](https://docs.confluent.io/platform/current/connect/index.html)
- [Guia de Início Rápido do Confluent Cloud](https://docs.confluent.io/cloud/current/get-started/index.html)
- [Deep Dive no Kafka Connect](https://docs.confluent.io/platform/current/connect/concepts.html)

Seguindo esses passos, você pode configurar facilmente um conector de origem do Kafka Connect, gerar dados de amostra e consumir esses dados usando o Confluent Cloud.

================================================
File: /08-kafka-pubsub-streaming/kafka-ecosystem/kafka-connect/kafka_consumer_connect.py
================================================
from confluent_kafka import Consumer, KafkaError
from dotenv import load_dotenv
import os
import csv

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Configurações do consumidor
conf = {
    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),
    'sasl.mechanisms': 'PLAIN',
    'security.protocol': 'SASL_SSL',
    'sasl.username': os.getenv('SASL_USERNAME'),
    'sasl.password': os.getenv('SASL_PASSWORD'),
    'group.id': 'python-consumer-group',
    'auto.offset.reset': 'earliest'
}

# Criação do consumidor
consumer = Consumer(**conf)

# Inscrição no tópico
topic = os.getenv('GET_POSTGRES')
consumer.subscribe([topic])

# Função para processar mensagens
def consume_messages():
    # Nome do arquivo CSV
    script_dir = os.path.dirname(os.path.abspath(__file__))  # Diretório do script atual
    csv_file = os.path.join(script_dir, 'messages.csv')  # Caminho completo para o arquivo CSV
    
    # Abrir o arquivo CSV em modo de escrita
    with open(csv_file, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Key', 'Value'])  # Escrever cabeçalho no CSV
        
        try:
            while True:
                msg = consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem
                # Exibe a mensagem recebida
                if msg is None:  # Se nenhuma mensagem foi recebida
                    continue
                if msg.error():  # Se houve um erro ao receber a mensagem
                    continue
                key = msg.key()
                value = msg.value()
                print(f"Received message: Key: {key}, Value: {value}")
                # Escreve a mensagem no CSV
                writer.writerow([key, value])
        except KeyboardInterrupt:
            pass
        finally:
            print("Closing consumer.")
            consumer.close()

if __name__ == "__main__":
    consume_messages()


================================================
File: /08-kafka-pubsub-streaming/kafka-ecosystem/ksqlDB/README.md
================================================
Claro! Vou explicar cada comando e passo detalhadamente para que você entenda o objetivo de cada um deles.

# Hands On: ksqlDB

Neste exercício, você aprenderá como manipular seus dados usando o ksqlDB. Até agora, estivemos produzindo dados e lendo dados de um tópico do Apache Kafka sem quaisquer etapas intermediárias. Com a transformação e agregação de dados, podemos fazer muito mais!

No último exercício, criamos um Conector Datagen Source para produzir um fluxo de dados de pedidos para um tópico Kafka, serializando-o em Avro usando o Confluent Schema Registry. Este exercício depende dos dados que nosso conector está produzindo, então, se você não completou o exercício anterior, encorajamos você a fazê-lo antes de prosseguir.

Antes de começarmos, certifique-se de que seu Conector Datagen Source ainda está ativo e em execução.

## Passo a Passo

### 1. Criar Cluster ksqlDB
Na página inicial do cluster no Confluent Cloud Console, selecione ksqlDB no menu à esquerda. 
- **Objetivo**: Iniciar o ksqlDB, que é a plataforma de streaming SQL para Apache Kafka, permitindo consultas e transformações em tempo real dos dados no Kafka.

### 2. Configurar Acesso
Escolha "Global access" na página de controle de acesso e continue para dar um nome ao cluster ksqlDB e depois inicie o cluster.
- **Objetivo**: Configurar o acesso ao cluster ksqlDB para garantir que todos os serviços e aplicações tenham acesso ao ksqlDB.

### 3. Registrar Tópico
Após a provisão do cluster ksqlDB, você será levado ao editor ksqlDB. Adicione o tópico `orders` do exercício anterior à aplicação ksqlDB. Registre-o como um stream executando:
```sql
CREATE STREAM orders_stream WITH (
  KAFKA_TOPIC='orders', 
  VALUE_FORMAT='AVRO',
  PARTITIONS=6,
  TIMESTAMP='ordertime');
```
- **Objetivo**: Criar um stream no ksqlDB a partir do tópico `orders` para que possamos realizar consultas SQL sobre os dados em tempo real.

### 4. Visualizar Stream
Navegue até a aba Streams e selecione `orders_stream` para visualizar mais detalhes sobre o stream.
- **Objetivo**: Inspecionar os detalhes do stream, incluindo os campos e o formato dos dados, para entender melhor a estrutura do fluxo de dados.

### 5. Transformar Dados
Execute a seguinte consulta para transformar o campo `ordertime` em um formato mais legível e extrair os dados aninhados do struct `address`:
```sql
SELECT 
    TIMESTAMPTOSTRING(ORDERTIME, 'yyyy-MM-dd HH:mm:ss.SSS') AS ORDERTIME_FORMATTED,
    orderid,
    itemid,
    orderunits,
    address->city, 
    address->state,
    address->zipcode
FROM ORDERS_STREAM;
```
- **Objetivo**: Transformar os dados no stream `orders_stream` para melhorar a legibilidade do campo `ordertime` e extrair campos aninhados do endereço.

### 6. Persistir Resultados
Para persistir os resultados em um novo stream, adicione uma linha `CREATE STREAM` no início da consulta:
```sql
CREATE STREAM ORDERS_STREAM_TS AS
SELECT 
    TIMESTAMPTOSTRING(ORDERTIME, 'yyyy-MM-dd HH:mm:ss.SSS') AS ORDERTIME_FORMATTED,
    orderid,
    itemid,
    orderunits,
    address->city, 
    address->state,
    address->zipcode 
FROM ORDERS_STREAM;
```
- **Objetivo**: Criar um novo stream `ORDERS_STREAM_TS` que contém os dados transformados do stream original `orders_stream`.

### 7. Agregação de Dados
Crie uma tabela para contar quantos pedidos são feitos por estado. O ksqlDB facilita a segmentação dos dados em janelas de tempo:
```sql
CREATE TABLE STATE_COUNTS AS 
SELECT 
  address->state,
  COUNT_DISTINCT(ORDERID) AS DISTINCT_ORDERS
FROM ORDERS_STREAM
WINDOW TUMBLING (SIZE 7 DAYS) 
GROUP BY address->state;
```
- **Objetivo**: Agregar os dados no stream `ORDERS_STREAM` para contar o número distinto de pedidos por estado em uma janela de 7 dias, criando uma tabela `STATE_COUNTS`.

### 8. Consultar Tabela
Navegue até a aba Tables para ver os dados relacionados à tabela.
- **Objetivo**: Visualizar e consultar os dados agregados na tabela `STATE_COUNTS`.

### 9. Consulta de Pull
Para ver apenas os valores mais recentes de uma tabela, execute uma consulta de pull. Veja um instantâneo atual contendo os estados que tiveram mais de dois pedidos por período de uma semana:
```sql
SELECT
    *
FROM STATE_COUNTS
WHERE DISTINCT_ORDERS > 2;
```
- **Objetivo**: Executar uma consulta de pull para obter um snapshot dos dados na tabela `STATE_COUNTS`, filtrando os estados que tiveram mais de dois pedidos na última semana.

Esses passos fornecem uma visão detalhada de como configurar, transformar e consultar dados em tempo real usando o ksqlDB com Apache Kafka.

================================================
File: /08-kafka-pubsub-streaming/kafka-ecosystem/schema_registry/README.md
================================================
# Confluent Schema Registry

Quando aplicações estão produzindo e consumindo mensagens do Kafka, duas coisas acontecem:

1. **Novos Consumidores Emergentes**: Novas aplicações, potencialmente desenvolvidas por diferentes equipes, precisarão entender o formato das mensagens nos tópicos existentes.
2. **Evolução do Formato das Mensagens**: O formato das mensagens vai evoluir à medida que o negócio evolui. Por exemplo, objetos de pedido podem ganhar novos campos de status ou nomes de usuário podem ser divididos em nome e sobrenome.

## O Problema

O formato dos objetos de domínio é um alvo em constante movimento. Precisamos de uma maneira de concordar sobre o esquema das mensagens nos tópicos. É aí que entra o Confluent Schema Registry.

## O que é o Schema Registry?

O Schema Registry é um processo de servidor independente que roda em uma máquina externa aos brokers do Kafka. Ele mantém um banco de dados de todos os esquemas que foram escritos nos tópicos no cluster. Esse banco de dados é persistido em um tópico interno do Kafka e é armazenado em cache no Schema Registry para acesso de baixa latência.

### Funcionalidades

- **Manutenção de Esquemas**: O Schema Registry mantém um banco de dados de esquemas persistido em um tópico do Kafka.
- **API REST**: Produtores e consumidores podem usar a API REST do Schema Registry para verificar a compatibilidade dos esquemas.
- **Suporte a Alta Disponibilidade**: Pode ser configurado em uma configuração redundante para alta disponibilidade.
- **Compatibilidade de Esquemas**: Permite verificar a compatibilidade dos esquemas em tempo de produção e consumo.

## Exemplo de Uso

### Configurando um Produtor para Usar o Schema Registry

Quando um produtor é configurado para usar o Schema Registry, ele chama a API do Schema Registry no momento da produção de uma mensagem. Se o esquema da mensagem for compatível com as regras de compatibilidade definidas para o tópico, a produção terá sucesso. Caso contrário, a produção falhará, permitindo que o código da aplicação trate essa condição.

### Suporte a Diferentes Formatos de Serialização

O Schema Registry suporta três formatos de serialização:
- **JSON Schema**
- **Avro**
- **Protobuf**

## Vantagens

- **Gestão de Evolução de Esquemas**: Facilita a gestão da evolução de esquemas, evitando falhas de runtime quando possível.
- **Colaboração e Controle de Versão**: Permite a colaboração em torno das mudanças de esquema, utilizando ferramentas de controle de versão como arquivos IDL (Interface Description Language).

## Conclusão

Em sistemas não triviais, o uso do Schema Registry é essencial. Ele oferece uma maneira padronizada e automatizada de aprender sobre os esquemas e gerenciar suas evoluções internamente, facilitando a vida dos desenvolvedores e garantindo a compatibilidade das mensagens.

## Recursos Adicionais

- [Documentação do Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/index.html)
- [Guia de Início Rápido do Confluent Cloud](https://docs.confluent.io/cloud/current/get-started/index.html)
- [Deep Dive no Schema Registry](https://docs.confluent.io/platform/current/schema-registry/concepts.html)

## Diagrama de Arquitetura

```mermaid
graph TD;
    A[Aplicação Produtora] -->|Produz Dados| B[Kafka Topic];
    B -->|Esquemas| C[Schema Registry];
    D[Aplicação Consumidora] -->|Consome Dados| B;
    C -->|Verifica Esquemas| D;
    E[Confluent CLI] -->|Gerencia| C;
```

## Exemplo de Configuração de Esquema

```json
{
  "type": "record",
  "namespace": "com.mycorp.mynamespace",
  "name": "vendas",
  "fields": [
    { "name": "id", "type": "int", "doc": "Identificador da venda." },
    { "name": "data_venda", "type": "string", "doc": "Data da venda." },
    { "name": "produto_id", "type": "int", "doc": "Identificador do produto." },
    { "name": "quantidade", "type": "int", "doc": "Quantidade vendida." },
    { "name": "valor_total", "type": "float", "doc": "Valor total da venda." }
  ]
}
```

Use o Schema Registry para garantir que as mensagens produzidas e consumidas estejam de acordo com este esquema, permitindo uma gestão eficiente e robusta da evolução dos dados.

## Hands-on: Confluent Schema Registry

### Configuração e Uso do Schema Registry

1. **Habilitar o Schema Registry**: No Confluent Cloud Console, selecione Schema Registry no canto inferior esquerdo.
2. **Configurar Credenciais de API**: Crie uma chave de API e um segredo na seção "API credentials" da página do Schema Registry.
3. **Criar Conector Datagen**: Navegue até "Data integration" e selecione "Connectors". Adicione um conector Datagen Source para gerar dados de exemplo.
4. **Configurar o Tópico e o Conector**: Crie um novo tópico chamado `orders` e configure o conector para usar o formato Avro.
5. **Consumir Mensagens**: Use o comando abaixo para consumir mensagens do tópico `orders`.

Passo 1: Armazenar a Chave da API
Armazene a chave da API e o segredo:

Ótimo! Agora que você tem a chave da API e o segredo, vamos seguir os passos para armazenar a chave da API e consumir mensagens do tópico `orders`.

### Passo 0: Criar uma nova chave

```bash
confluent api-key create --resource lkc-7zoqpw
```

### Passo 1: Armazenar a Chave da API

Armazene a chave da API e o segredo:

```bash
confluent api-key store CKTUPV72A52DME6K mVETdpD41gScm+njQJcljcm5M7IcFWWjcE3TXvfxTenxmzBHbM3syzxRii3iQoWC --resource lkc-7zoqpw
```

### Passo 2: Selecionar a Chave da API

Depois de armazenar a chave da API, selecione-a:

```bash
confluent api-key use CKTUPV72A52DME6K --resource lkc-7zoqpw
```

### Passo 3: Consumir Mensagens do Tópico

Finalmente, consuma as mensagens do tópico `orders`:

```bash
confluent kafka topic consume orders
```

### Código Completo

1. Armazene a chave da API:

    ```bash
    confluent api-key store CKTUPV72A52DME6K mVETdpD41gScm+njQJcljcm5M7IcFWWjcE3TXvfxTenxmzBHbM3syzxRii3iQoWC --resource lkc-7zoqpw
    ```

2. Selecione a chave da API:

    ```bash
    confluent api-key use CKTUPV72A52DME6K --resource lkc-7zoqpw
    ```

3. Consuma mensagens do tópico:

    ```bash
    confluent kafka topic consume orders
    ```

Seguindo esses passos, você deve conseguir consumir as mensagens do tópico `orders` usando o Confluent CLI.

![schemas_img](/kafka-ecosystem/schema_registry/schemas_key.png)
```bash
confluent kafka topic consume --value-format avro --schema-registry-api-key N7CUKKPO4QRVRIXW --schema-registry-api-secret k4PPzQTYqMBDIV4es9P8rww5Kb48XlLWFwvP+nH1TPDkf6XqTYM3qmhOXHevoMYA orders
```

================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/README.md
================================================
## Escopo do Projeto: Simulação de 50 Refrigeradores Espalhados pelo Brasil

Claro! Aqui está o README atualizado com o comando para criar o tópico `marketing-project`.

## Projeto: Monitoramento de Refrigeração no Brasil

### Objetivo

Este projeto tem como objetivo simular 50 refrigeradores espalhados pelo Brasil. Cada refrigerador irá reportar a temperatura a cada segundo. Queremos entender como os refrigeradores estão operando em diferentes regiões e monitorar temperaturas anômalas que podem indicar falhas ou problemas.

### Componentes do Projeto

1. **Producers**:
    - 50 produtores simulando refrigeradores.
    - Cada produtor envia dados de temperatura a cada segundo.
    - As temperaturas são geradas com base em dois intervalos: 
        - 45 refrigeradores com temperaturas entre 0°C e 5°C.
        - 5 refrigeradores com temperaturas entre 20°C e 40°C.
    - Cada produtor tem um ID único gerado com UUID.

2. **Consumer**:
    - Um consumidor que lê os dados em tempo real e exibe as temperaturas usando Streamlit.

3. **Kafka Topic**:
    - Tópico chamado `marketing-project`.

### Passos para Configuração

#### 1. Configurar o Confluent CLI

Certifique-se de que você tenha configurado o Confluent CLI e esteja autenticado:

```bash
confluent login
```

#### 2. Selecionar o Ambiente e o Cluster

```bash
confluent environment use <environment_id>
confluent kafka cluster use <cluster_id>
```

#### 3. Criar o Tópico `marketing-project`

```bash
confluent kafka topic create marketing-project --partitions 6
```

### Objetivo

O objetivo deste projeto é simular 50 refrigeradores espalhados pelo Brasil, gerando dados de temperatura em tempo real. Essa simulação visa monitorar e analisar as variações de temperatura, permitindo uma melhor compreensão do comportamento térmico dos refrigeradores em diferentes regiões e condições climáticas. Com este projeto, pretendemos:

1. **Monitorar Temperaturas em Tempo Real**: Capturar e exibir dados de temperatura de 50 refrigeradores espalhados pelo Brasil, atualizados a cada segundo.
2. **Analisar Distribuição de Temperaturas**: Identificar padrões de temperatura e comportamentos anômalos entre os refrigeradores.
3. **Avaliar Eficiência Térmica**: Verificar a eficiência térmica dos refrigeradores em manter a temperatura dentro de uma faixa desejada.
4. **Identificar Problemas Potenciais**: Detectar refrigeradores que possam estar operando fora da faixa de temperatura esperada, sinalizando possíveis falhas ou necessidade de manutenção.

### Descrição do Projeto

1. **Simulação dos Refrigeradores**:
    - **Quantidade**: 50 refrigeradores.
    - **Localização**: Cada refrigerador terá uma latitude e longitude fixas, representando diferentes regiões do Brasil.
    - **Distribuição de Temperatura**:
        - 45 refrigeradores manterão temperaturas entre 0 e 5 graus Celsius (95%).
        - 5 refrigeradores terão temperaturas entre 20 e 40 graus Celsius (5%), representando possíveis falhas ou condições extremas.

2. **Produção de Dados**:
    - **Intervalo de Produção**: Cada refrigerador gerará uma leitura de temperatura a cada segundo.
    - **Formato dos Dados**: Os dados serão enviados em formato JSON, contendo a latitude, longitude, temperatura e identificador do refrigerador.

3. **Tópico Kafka**:
    - **Nome do Tópico**: `marketing-project`.
    - **Produção e Consumo**: Os dados gerados pelos 50 refrigeradores serão publicados neste tópico Kafka.

4. **Consumidor de Dados**:
    - **Visualização em Tempo Real**: Um consumidor será implementado usando Streamlit para exibir as últimas leituras de temperatura de cada refrigerador em tempo real.
    - **Monitoramento e Análise**: A aplicação Streamlit permitirá monitorar os dados e identificar rapidamente qualquer comportamento anômalo.

### Componentes do Projeto

1. **Producers**:
    - 50 instâncias de produtores, cada uma simulando um refrigerador.
    - Utilização da biblioteca `Faker` para gerar dados de localização e temperatura.
    - Publicação dos dados no tópico `marketing-project` no Kafka.

2. **Consumer**:
    - Uma aplicação Streamlit que consome os dados do tópico `marketing-project`.
    - Exibição dos dados em um dashboard, mostrando a última leitura de temperatura de cada refrigerador.

### Ferramentas e Tecnologias

1. **Apache Kafka**: Para transmissão e ingestão de dados em tempo real.
2. **Confluent Cloud**: Plataforma gerenciada de Kafka para simplificar a configuração e o gerenciamento do cluster.
3. **Faker**: Biblioteca para geração de dados fictícios.
4. **Docker**: Para containerização dos produtores e consumidor.
5. **Streamlit**: Para visualização dos dados em tempo real.

### Implementação

1. **Configuração do Tópico Kafka**:
    - Criar o tópico `marketing-project` no Confluent Cloud.

2. **Desenvolvimento dos Producers**:
    - Implementar o script de produção utilizando `Faker` para gerar os dados de temperatura e localização.
    - Containerizar os produtores usando Docker.

3. **Desenvolvimento do Consumer**:
    - Implementar a aplicação Streamlit para consumir e exibir os dados em tempo real.
    - Containerizar o consumidor usando Docker.

4. **Execução**:
    - Iniciar os 50 produtores e o consumidor utilizando `docker-compose`.

## Executar os Containers
Certifique-se de que o arquivo docker-compose.yml está configurado corretamente e execute os seguintes comandos no terminal para construir a imagem Docker e iniciar os containers:

```bash
docker-compose build
docker-compose up -d
```

### Resultado Esperado

Ao final do projeto, espera-se ter uma simulação funcional de 50 refrigeradores espalhados pelo Brasil, com dados de temperatura sendo gerados e exibidos em tempo real. A aplicação Streamlit fornecerá uma interface intuitiva para monitorar e analisar esses dados, possibilitando a identificação de padrões e anomalias de temperatura, além de fornecer insights sobre a eficiência térmica dos refrigeradores em diferentes condições climáticas.

### Bizu para apagar

confluent kafka topic update marketing-project --config retention.ms=1000

confluent kafka topic delete marketing-project


###

```bash
confluent kafka topic consume marketing-project --from-beginning
```



================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/docker-compose.yml
================================================
version: '3.8'

services:
  producer:
    build: ./producer
    environment:
      - BOOTSTRAP_SERVERS=${BOOTSTRAP_SERVERS}
      - SASL_USERNAME=${SASL_USERNAME}
      - SASL_PASSWORD=${SASL_PASSWORD}
      - TOPIC_NAME=${TOPIC_NAME}
      - PRODUCER_ID=${PRODUCER_ID}
    deploy:
      replicas: 50

  consumer:
    build: ./consumer
    environment:
      - BOOTSTRAP_SERVERS=${BOOTSTRAP_SERVERS}
      - SASL_USERNAME=${SASL_USERNAME}
      - SASL_PASSWORD=${SASL_PASSWORD}
      - TOPIC_NAME=${TOPIC_NAME}
    volumes:
      - ./data:/app/data

  dashboard:
    build: ./dashboard
    environment:
      - BOOTSTRAP_SERVERS=${BOOTSTRAP_SERVERS}
      - SASL_USERNAME=${SASL_USERNAME}
      - SASL_PASSWORD=${SASL_PASSWORD}
      - TOPIC_NAME=${TOPIC_NAME}
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data


================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/consumer/Dockerfile
================================================
FROM python:3.9-slim

WORKDIR /app

COPY consumer.py .

RUN pip install confluent_kafka sqlalchemy pandas psycopg2-binary

CMD ["python", "consumer.py"]


================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/consumer/consumer.py
================================================
import time
import random
from confluent_kafka import Consumer, KafkaError
import json
import os
import pandas as pd
from sqlalchemy import create_engine
from datetime import datetime

# Configuração do Consumer
consumer_conf = {
    'bootstrap.servers': os.environ['BOOTSTRAP_SERVERS'],
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'PLAIN',
    'sasl.username': os.environ['SASL_USERNAME'],
    'sasl.password': os.environ['SASL_PASSWORD'],
    'group.id': 'temperature-consumer-group',
    'auto.offset.reset': 'earliest'
}

consumer = Consumer(consumer_conf)
consumer.subscribe(['marketing-project'])

# Configuração do PostgreSQL
db_url = "postgresql://postgres_kafka_user:UQ4PAxSQbukeWVcFEDpdNquS6zhbt8zs@dpg-cq8vi35ds78s7396a4og-a.oregon-postgres.render.com/postgres_kafka_sink"
engine = create_engine(db_url)

def consume_messages():
    batch_size = 100  # Número de mensagens para processar por batch
    batch_interval = 5  # Intervalo de tempo (em segundos) entre os batches
    messages = []
    start_time = time.time()  # Inicializa start_time dentro da função

    try:
        while True:
            msg = consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem
            if msg is None:
                continue
            if msg.error():
                continue

            value = json.loads(msg.value().decode('utf-8'))  # Converte o valor para JSON
            # Desnormalizar a mensagem
            data = {
                'id': value['id'],
                'latitude': value['latitude'],
                'longitude': value['longitude'],
                'temperature': value['temperature'],
                'time': datetime.now()  # Captura o tempo atual da mensagem
            }
            messages.append(data)

            # Processar batch quando atingir o tamanho ou o intervalo de tempo
            if len(messages) >= batch_size or (time.time() - start_time) >= batch_interval:
                df = pd.DataFrame(messages)
                df.to_sql('temperature_data', engine, if_exists='append', index=False)
                messages = []  # Limpar lista de mensagens após salvar no banco
                start_time = time.time()  # Resetar o contador de tempo

    except KeyboardInterrupt:
        pass
    finally:
        print("Closing consumer.")
        consumer.close()

if __name__ == "__main__":
    start_time = time.time()
    consume_messages()


================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/consumer/requirements.txt
================================================
confluent_kafka
faker
streamlit
sqlalchemy

================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/dashboard/Dockerfile
================================================
FROM python:3.9-slim

WORKDIR /app

COPY dashboard.py .

RUN pip install streamlit sqlalchemy pandas psycopg2-binary python-dotenv plotly

CMD ["streamlit", "run", "dashboard.py"]


================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/dashboard/dashboard.py
================================================
import streamlit as st
import pandas as pd
from sqlalchemy import create_engine
from dotenv import load_dotenv
import os
import time
import plotly.express as px

# Carregar variáveis de ambiente
load_dotenv()

# Configuração do PostgreSQL
db_url = 'postgresql://postgres_kafka_user:UQ4PAxSQbukeWVcFEDpdNquS6zhbt8zs@dpg-cq8vi35ds78s7396a4og-a.oregon-postgres.render.com/postgres_kafka_sink'
engine = create_engine(db_url)

# Função para carregar dados do banco de dados
def load_data():
    query = """
    SELECT DISTINCT ON (id) *
    FROM temperature_data
    ORDER BY id, time DESC;
    """
    try:
        df = pd.read_sql(query, engine)
        return df
    except Exception as e:
        st.error(f"Erro ao carregar dados: {e}")
        return pd.DataFrame()  # Retorna um DataFrame vazio em caso de erro

# Função para exibir o mapa
def display_map(df):
    df['color'] = df['temperature'].apply(lambda x: 'red' if x > 5 else 'blue')
    fig = px.scatter_mapbox(
        df,
        lat="latitude",
        lon="longitude",
        color="color",
        color_discrete_map={"red": "red", "blue": "blue"},
        hover_name="id",
        hover_data={"temperature": True, "latitude": False, "longitude": False},
        zoom=3,
        height=600,
    )
    fig.update_layout(mapbox_style="open-street-map")
    st.plotly_chart(fig)

# Loop para atualizar a página a cada 5 segundos
while True:
    st.title('Dashboard de Temperaturas')
    data = load_data()
    if not data.empty:
        st.write(data)
        display_map(data)
    else:
        st.write('A tabela "temperature_data" ainda não existe ou não foi possível carregar os dados.')
    
    time.sleep(10)
    st.experimental_rerun


================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/producer/Dockerfile
================================================
FROM python:3.9-slim

WORKDIR /app

COPY producer.py .

RUN pip install faker confluent_kafka

CMD ["python", "producer.py"]


================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/producer/producer.py
================================================
import time
import random
from confluent_kafka import Producer
import uuid
from faker import Faker
import json
import os

fake = Faker('pt_BR')

def delivery_report(err, msg):
    if err is not None:
        print(f"Message delivery failed: {err}")
    else:
        print(f"Message delivered to {msg.topic()} [{msg.partition()}]")

# Configuração do Producer
producer_conf = {
    'bootstrap.servers': os.environ['BOOTSTRAP_SERVERS'],
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'PLAIN',
    'sasl.username': os.environ['SASL_USERNAME'],
    'sasl.password': os.environ['SASL_PASSWORD']
}

def generate_unique_id():
    return int(str(uuid.uuid4().int)[:8])

producer = Producer(producer_conf)

def generate_initial_data(id):
    lat = random.uniform(-33.0, 5.0)
    lon = random.uniform(-73.0, -34.0)
    address = fake.address()
    temperature_range = 'low' if random.random() < 0.95 else 'high'
    data = {
        'id': id,
        'latitude': lat,
        'longitude': lon,
        'address': address,
        'temperature_range': temperature_range
    }
    return data

def generate_temperature(temperature_range):
    if temperature_range == 'low':
        temperature = random.gauss(2.5, 1.0)
    else:
        temperature = random.uniform(20.0, 40.0)
    return round(temperature, 2)

if __name__ == '__main__':
    producer_id = generate_unique_id()
    initial_data = generate_initial_data(producer_id)
    while True:
        temperature = generate_temperature(initial_data['temperature_range'])
        data = initial_data.copy()
        data['temperature'] = temperature
        producer.produce('marketing-project', key=str(producer_id), value=json.dumps(data), callback=delivery_report)
        producer.poll(1)
        time.sleep(5)


================================================
File: /08-kafka-pubsub-streaming/kafka-marketing-project/producer/requirements.txt
================================================
confluent_kafka
faker

================================================
File: /08-kafka-pubsub-streaming/kafka-producers/README-producers.md
================================================
### Produtores com Confluent Kafka Python no Confluent Cloud

### Introdução

Nessa parte, vamos explorar como criar e configurar um produtor utilizando a biblioteca Python `confluent_kafka` para enviar mensagens para um broker Kafka hospedado no Confluent Cloud. Nosso objetivo é criar um sensor de geladeira para a Ambev, permitindo monitorar a temperatura do freezer e garantir que ele esteja na temperatura ideal.

### Conceitos Básicos

- **Produtor (Producer)**: Aplicações que enviam (produzem) dados para o Kafka.

### Diagrama de Arquitetura

```mermaid
graph TD;
    A[Sensor de Geladeira] -->|Produz Dados| B[Kafka Producer];
    B -->|Envia Dados| C[Confluent Cloud Broker];
    C -->|Distribui Dados| D[Partições de Tópicos];
```

### Estrutura do Produtor

No Kafka, os produtores são responsáveis por enviar mensagens para tópicos específicos. Usaremos a biblioteca `confluent_kafka` para criar e configurar um produtor em Python.

### Instalação da Biblioteca

Primeiro, instale a biblioteca `confluent_kafka`:

```bash
pip install confluent_kafka python-dotenv
```

### Configuração do Produtor para Confluent Cloud

#### Passos para Obter Configurações Necessárias

1. **Obter o Bootstrap Server**:
   - No Confluent Cloud Console, vá para seu cluster Kafka e encontre o endereço do bootstrap server.

2. **Obter as Credenciais de API (API Key e Secret)**:
   - No Confluent Cloud Console, vá para "API keys" e crie uma nova chave API. Anote a API Key e o Secret.

3. **Criar o Tópico Usando a CLI do Confluent Cloud**:

```bash
confluent kafka topic create freezer-temperatura --partitions 3 --cluster lkc-k08pop
```

### Arquivo `.env`

Crie um arquivo chamado `.env` no mesmo diretório do seu script Python e adicione as seguintes linhas:

```plaintext
BOOTSTRAP_SERVERS=pkc-12576z.us-west2.gcp.confluent.cloud:9092
SASL_USERNAME=YOUR_API_KEY  # Substitua pela sua API Key
SASL_PASSWORD=YOUR_API_SECRET  # Substitua pelo seu API Secret
CLIENT_ID=python-producer
TOPIC_PRODUCER=freezer-temperatura
```

### Exemplo de Configuração

Aqui está um exemplo completo de como configurar e usar o produtor em Python com a biblioteca `confluent_kafka` para o Confluent Cloud.

```python
from confluent_kafka import Producer
from dotenv import load_dotenv
import os
import random
import time

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Configurações do produtor
conf = {
    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),
    'sasl.mechanisms': 'PLAIN',
    'security.protocol': 'SASL_SSL',
    'sasl.username': os.getenv('SASL_USERNAME'),
    'sasl.password': os.getenv('SASL_PASSWORD'),
    'client.id': os.getenv('CLIENT_ID')
}

# Criação do produtor
producer = Producer(**conf)

# Função de callback para entrega de mensagens
def delivery_report(err, msg):
    if err is not None:
        print(f"Message delivery failed: {err}")
    else:
        print(f"Message delivered to {msg.topic()} [{msg.partition()}]")

# Produção de mensagens simulando um sensor de geladeira
topic = os.getenv('TOPIC_PRODUCER')
for i in range(10):
    temperature = random.uniform(-5, 5)  # Temperatura aleatória entre -5 e 5 graus Celsius
    key = f"sensor{i % 3}"  # Usar diferentes chaves para distribuir entre partições
    producer.produce(topic, key=key, value=f"{temperature:.2f}", callback=delivery_report)
    producer.poll(0)
    time.sleep(1)  # Simula leitura de temperatura a cada segundo

# Espera até todas as mensagens serem entregues
producer.flush()
```

### Diagrama de Processo de Produção

```mermaid
sequenceDiagram
    participant S as Sensor de Geladeira
    participant P as Kafka Producer
    participant B as Confluent Cloud Broker
    participant T as Tópico: freezer-temperatura
    S->>P: Gera Leitura de Temperatura
    P->>B: Envia Mensagem
    B->>T: Armazena na Partição Correspondente
    T->>C: Consumidores Analisam Dados
```

### Como o Produtor Escolhe a Partição

O produtor decide a qual partição enviar cada mensagem, seja de forma round-robin (sem chave) ou computando a partição destino através do hash da chave.

1. **Round-Robin**:
   - Mensagens sem chave são distribuídas de forma round-robin entre as partições.

2. **Hash da Chave**:
   - Mensagens com chave têm a partição destino calculada através do hash da chave.

### Funcionamento Interno do Produtor

- **Gerenciamento de Pools de Conexão**: O produtor gerencia pools de conexão para otimizar a comunicação com o cluster.
- **Bufferização de Rede**: Mensagens são bufferizadas antes de serem enviadas para os brokers.
- **Acknowledge de Mensagens**: O produtor espera os acknowledgements dos brokers para liberar espaço no buffer.
- **Retransmissão de Mensagens**: Retransmissão de mensagens ocorre quando necessário.

### Recomendações

Para realmente entender como os produtores funcionam, é altamente recomendável que você escreva e execute algum código. Veja a API em ação, digite os comandos você mesmo e observe o comportamento do Kafka.

### Conclusão

Os produtores são uma parte essencial do Kafka, permitindo que você envie dados para o cluster de forma eficiente e escalável. Compreender como configurá-los e como eles interagem com as partições é crucial para aproveitar ao máximo o Apache Kafka.

### Execução do Script

Para executar o script, certifique-se de que o arquivo `.env` está no mesmo diretório e execute o seguinte comando:

```bash
python kafka_producers.py
```

Isso iniciará o produtor Kafka que enviará leituras de temperatura simuladas para o tópico `freezer-temperatura` no Confluent Cloud.

### Lendo os valores gerados

```bash
confluent kafka topic consume freezer-temperatura --cluster lkc-k08pop --from-beginning
```

================================================
File: /08-kafka-pubsub-streaming/kafka-producers/kafka_producers.py
================================================
from confluent_kafka import Producer
from dotenv import load_dotenv
import os
import random
import time

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Configurações do produtor
conf = {
    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),
    'sasl.mechanisms': 'PLAIN',
    'security.protocol': 'SASL_SSL',
    'sasl.username': os.getenv('SASL_USERNAME'),
    'sasl.password': os.getenv('SASL_PASSWORD'),
    'client.id': os.getenv('CLIENT_ID')
}

# Criação do produtor
producer = Producer(**conf)

# Função de callback para entrega de mensagens
def delivery_report(err, msg):
    if err is not None:
        print(f"Message delivery failed: {err}")
    else:
        print(f"Message delivered to {msg.topic()} [{msg.partition()}]")

# Produção de mensagens simulando um sensor de geladeira
topic = os.getenv('TOPIC_PRODUCER')
for i in range(10):
    temperature = random.uniform(-5, 5)  # Temperatura aleatória entre -5 e 5 graus Celsius
    key = f"sensor{(i % 3) + 1}"  # Usar diferentes chaves para distribuir entre partições
    producer.produce(topic, key=key, value=f"{temperature:.2f}", callback=delivery_report)
    producer.poll(0)
    time.sleep(1)  # Simula leitura de temperatura a cada segundo

# Espera até todas as mensagens serem entregues
producer.flush()


================================================
File: /08-kafka-pubsub-streaming/kafka-topics-partitions/README.md
================================================
# Exercício de Particionamento de Tópicos

## Pré-requisitos

1. **Confluent Cloud Cluster:** Certifique-se de ter configurado o Confluent Cloud Cluster e o CLI conforme os exercícios anteriores.
2. **Kafka CLI:** Tenha o Kafka CLI instalado e configurado para se conectar ao seu cluster Confluent Cloud.

## Passos para o Exercício

### 1. Listar Tópicos Existentes
Para ver uma lista de tópicos no seu cluster, execute o comando abaixo no terminal:

```bash
confluent kafka topic list
```

### 2. Descrever o Tópico Existente
Para ver mais detalhes sobre a configuração do tópico `tecnologias`, use o comando:

```bash
confluent kafka topic describe tecnologias
```

### Principais Pontos do Comando `describe tecnologias`

Aqui estão os detalhes atualizados com os valores convertidos para segundos, megabytes (MB), e dias, conforme aplicável:

1. **cleanup.policy**: 
   - Valor: `delete`
   - Descrição: Define a política de limpeza de logs. `delete` significa que os logs antigos serão excluídos quando o tempo de retenção expirar.

2. **compression.type**: 
   - Valor: `producer`
   - Descrição: Especifica o tipo de compressão usada. `producer` indica que a compressão é definida pelo produtor.

3. **delete.retention.ms**: 
   - Valor: `86400000` (1 dia)
   - Descrição: Tempo de retenção em milissegundos para mensagens deletadas. (1 dia)

4. **file.delete.delay.ms**: 
   - Valor: `60000` (60 segundos)
   - Descrição: Atraso antes que um segmento de log deletado seja realmente removido do disco. (60 segundos)

5. **flush.messages**: 
   - Valor: `9223372036854775807`
   - Descrição: Número de mensagens que devem ser escritas antes que os dados sejam forçados a serem liberados no disco (influencia a durabilidade).

6. **flush.ms**: 
   - Valor: `9223372036854775807`
   - Descrição: Tempo em milissegundos antes que os dados sejam forçados a serem liberados no disco (influencia a durabilidade).

7. **index.interval.bytes**: 
   - Valor: `4096` (4 KB)
   - Descrição: Número de bytes entre entradas de índice. (4 KB)

8. **max.message.bytes**: 
   - Valor: `2097164` (2 MB)
   - Descrição: Tamanho máximo de uma mensagem (em bytes) que o tópico pode aceitar. (2 MB)

9. **message.downconversion.enable**: 
   - Valor: `true`
   - Descrição: Permite a conversão descendente de mensagens para versões anteriores de formato.

10. **message.format.version**: 
    - Valor: `3.0-IV1`
    - Descrição: Versão do formato da mensagem usada pelo tópico.

11. **message.timestamp.type**: 
    - Valor: `CreateTime`
    - Descrição: Tipo de timestamp usado para mensagens (tempo de criação da mensagem).

12. **min.cleanable.dirty.ratio**: 
    - Valor: `0.5`
    - Descrição: Proporção mínima de dados "sujos" em um log de segmento que aciona a limpeza de compactação.

13. **min.insync.replicas**: 
    - Valor: `2`
    - Descrição: Número mínimo de réplicas que devem estar sincronizadas para produzir mensagens com sucesso.

14. **num.partitions**: 
    - Valor: `6`
    - Descrição: Número de partições no tópico.

15. **retention.bytes**: 
    - Valor: `-1`
    - Descrição: Limite de retenção de bytes para o tópico. `-1` indica que não há limite.

16. **retention.ms**: 
    - Valor: `604800000` (7 dias)
    - Descrição: Tempo de retenção em milissegundos para mensagens no tópico. (7 dias)

17. **segment.bytes**: 
    - Valor: `104857600` (100 MB)
    - Descrição: Tamanho do segmento de log em bytes. (100 MB)

18. **segment.ms**: 
    - Valor: `604800000` (7 dias)
    - Descrição: Tempo em milissegundos antes que um novo segmento de log seja criado. (7 dias)

19. **unclean.leader.election.enable**: 
    - Valor: `false`
    - Descrição: Se `false`, desativa a eleição de líderes que não estão completamente sincronizados, melhorando a durabilidade à custa da disponibilidade.

### 3. Criar Novos Tópicos com Diferentes Partições
Vamos criar dois novos tópicos chamados `tecnologias1` e `tecnologias4` com 1 e 4 partições, respectivamente.

#### Criar o Tópico `tecnologias1` com 1 Partição
```bash
confluent kafka topic create tecnologias1 --partitions 1
```

#### Criar o Tópico `tecnologias4` com 4 Partições
```bash
confluent kafka topic create tecnologias4 --partitions 4
```

### 4. Verificar a Contagem de Partições dos Novos Tópicos
Para verificar a contagem de partições dos novos tópicos, use o comando `describe` novamente:

```bash
confluent kafka topic describe tecnologias1
confluent kafka topic describe tecnologias4
```

### 5. Produzir Dados para os Tópicos

#### Produzir Dados para o Tópico `tecnologias1`
```bash
confluent kafka topic produce tecnologias1
```

No prompt, insira uma mensagem de cada vez e pressione Enter:

```plaintext
1:Python
2:SQL
3:Kafka
4:Spark
5:Airflow
6:Kubernetes
7:Terraform
8:Docker
```

#### Produzir Dados para o Tópico `tecnologias4`
```bash
confluent kafka topic produce tecnologias4
```

No prompt, insira uma mensagem de cada vez e pressione Enter:

```plaintext
1:Python
2:SQL
3:Kafka
4:Spark
5:Airflow
6:Kubernetes
7:Terraform
8:Docker
```

### 6. Verificar as Mensagens Produzidas
No console web do Confluent Cloud, visualize as mensagens produzidas. Observe que o tópico `tecnologias1` tem todas as 8 mensagens em uma única partição, enquanto o tópico `tecnologias4` tem uma distribuição ligeiramente diferente.

### Conclusão
Com isso, você deve ter uma boa ideia de como a contagem de partições pode afetar a distribuição de dados nos seus tópicos. No próximo exercício, continuaremos explorando mais recursos do Kafka.

### Gráfico Mermaid

```mermaid
graph TD;
    A[Configurar CLI] --> B[Listar Tópicos]
    B --> C[Descrever Tópico]
    C --> D[Criar Tópicos tecnologias1 e tecnologias4]
    D --> E[Verificar Partições]
    E --> F[Produzir Dados para Tópicos]
    F --> G[Verificar Mensagens no Console Web]
```

### Vantagens e Desvantagens de Trabalhar com um Número de Partições

## Vantagens

### 1. **Paralelismo**
   - **Vantagem**: Aumentar o número de partições permite maior paralelismo. Cada partição pode ser processada por um consumidor diferente, aumentando a capacidade de processamento.
   - **Exemplo**: Em um cenário de alta carga, múltiplos consumidores podem processar mensagens simultaneamente, reduzindo o tempo de processamento.

### 2. **Escalabilidade**
   - **Vantagem**: Maior número de partições facilita a escalabilidade horizontal, permitindo adicionar mais consumidores conforme a demanda aumenta.
   - **Exemplo**: Em sistemas distribuídos, adicionar novas instâncias de consumidores para lidar com picos de carga sem reconfigurar o cluster.

### 3. **Balanceamento de Carga**
   - **Vantagem**: Mensagens são distribuídas entre várias partições, o que ajuda a balancear a carga de trabalho entre diferentes consumidores.
   - **Exemplo**: Diferentes consumidores podem trabalhar em paralelo, cada um processando uma partição específica, evitando gargalos.

### 4. **Melhoria no Desempenho de Escrita**
   - **Vantagem**: Várias partições podem melhorar a taxa de transferência de escrita, já que múltiplos produtores podem escrever em diferentes partições simultaneamente.
   - **Exemplo**: Em aplicações com alta taxa de escrita, a capacidade de distribuir escritas entre múltiplas partições pode aumentar a eficiência geral.

## Desvantagens

### 1. **Complexidade na Gerência de Partições**
   - **Desvantagem**: Mais partições podem aumentar a complexidade da gerência, incluindo monitoramento, manutenção e configuração.
   - **Exemplo**: Necessidade de ferramentas avançadas para gerenciar grandes quantidades de partições, aumentando a carga administrativa.

### 2. **Rebalanceamento**
   - **Desvantagem**: Rebalancear partições entre consumidores pode ser custoso em termos de desempenho, especialmente em clusters grandes.
   - **Exemplo**: Durante o rebalanceamento, pode haver um tempo de inatividade ou degradação de desempenho à medida que as partições são redistribuídas.

### 3. **Overhead de Recursos**
   - **Desvantagem**: Mais partições exigem mais recursos do Kafka broker, incluindo memória, armazenamento e processamento.
   - **Exemplo**: Cada partição tem uma sobrecarga associada, e um número excessivo pode levar ao consumo excessivo de recursos do servidor.

### 4. **Problemas de Ordenação**
   - **Desvantagem**: Garantir a ordenação das mensagens pode ser mais difícil com múltiplas partições, pois cada partição mantém sua própria ordem.
   - **Exemplo**: Para manter a ordenação global de mensagens, pode ser necessário um processamento adicional ou uma lógica complexa no consumidor.

### 5. **Latência de Rebalanceamento**
   - **Desvantagem**: Quando um novo consumidor é adicionado, ou um consumidor existente falha, o tempo para rebalancear as partições pode adicionar latência.
   - **Exemplo**: Em um sistema com alta disponibilidade, a latência de rebalanceamento pode afetar a entrega de mensagens em tempo real.

### Conclusão
Escolher o número adequado de partições é uma decisão crítica que deve equilibrar as necessidades de desempenho e paralelismo com a complexidade de gerenciamento e os recursos disponíveis. Avaliar cuidadosamente os requisitos do sistema e realizar testes de carga podem ajudar a determinar a configuração ideal para um cenário específico.

Não há um número "ideal" de partições que funcione para todos os cenários, pois o número apropriado de partições para um tópico Kafka depende de diversos fatores, incluindo a taxa de produção e consumo de mensagens, o tamanho das mensagens, e a arquitetura geral do seu sistema. No entanto, aqui estão algumas diretrizes e considerações que podem ajudar a determinar um número adequado de partições:

## Fatores para Determinar o Número de Partições

### 1. **Taxa de Produção e Consumo**
   - **Alto Volume de Dados**: Se você está lidando com um alto volume de dados, mais partições podem ajudar a distribuir a carga entre vários consumidores.
   - **Concorrência de Consumidores**: O número de consumidores em um grupo deve idealmente corresponder ou ser menor que o número de partições para garantir que cada partição seja consumida eficientemente.

### 2. **Tamanho das Mensagens**
   - **Mensagens Grandes**: Se suas mensagens são grandes, mais partições podem ajudar a evitar que uma única partição se torne um gargalo.

### 3. **Desempenho e Latência**
   - **Paralelismo**: Mais partições permitem maior paralelismo, aumentando o throughput geral do sistema.
   - **Latência**: Muito poucas partições podem levar a gargalos, enquanto muitas partições podem introduzir sobrecarga administrativa.

### 4. **Escalabilidade**
   - **Crescimento Futuro**: Considere o crescimento futuro do seu sistema. Pode ser mais fácil começar com mais partições do que necessário e crescer nelas, do que aumentar o número de partições posteriormente.

### 5. **Replicação e Tolerância a Falhas**
   - **Número de Réplicas**: Partições adicionais podem ajudar a garantir maior resiliência e disponibilidade.

### 6. **Limitações Operacionais**
   - **Limites do Kafka**: Tenha em mente que cada partição adiciona uma certa quantidade de sobrecarga administrativa ao Kafka e ao Zookeeper.
   - **Recursos de Hardware**: Certifique-se de que seus brokers têm recursos suficientes (CPU, memória, disco) para lidar com o número de partições desejado.

## Implementação em Instâncias EC2

### 1. **Distribuição de Partições**
   - **Balanceamento de Carga**: As partições podem ser distribuídas entre várias instâncias EC2 para garantir balanceamento de carga e alta disponibilidade.
   - **Dimensionamento**: Dependendo do tamanho e da carga de trabalho, você pode alocar várias partições em uma única instância EC2, mas isso deve ser feito com cautela para evitar gargalos.

### 2. **Considerações de Deploy**
   - **Tipo de Instância**: Escolha instâncias EC2 que correspondam aos requisitos de desempenho do Kafka. Instâncias com alto I/O, como as séries I3, podem ser vantajosas.
   - **Storage**: Utilize armazenamento de alta performance, como SSDs, para partições Kafka.
   - **Rede**: Garanta que a latência da rede entre instâncias EC2 seja baixa, especialmente se elas estão em diferentes zonas de disponibilidade.

### 3. **Escalabilidade e Gerenciamento**
   - **Auto Scaling**: Configure grupos de auto scaling para instâncias EC2 para lidar com variações na carga de trabalho.
   - **Monitoramento e Alertas**: Use ferramentas de monitoramento para observar o desempenho das partições e instâncias, e configure alertas para identificar problemas rapidamente.

## Exemplo de Configuração

### Criar Tópico com Partições via CLI

```bash
confluent kafka topic create tecnologias --partitions 10
```

### Produzir e Consumir Mensagens

#### Produzir Mensagens

```bash
# Abra um terminal para produzir mensagens
confluent kafka topic produce tecnologias

# Insira mensagens manualmente
1:Python
2:Kafka
3:AWS
4:Docker
5:Spark
6:Hadoop
7:Kubernetes
8:TensorFlow
9:Pandas
10:SQL
```

#### Consumir Mensagens

```bash
# Abra um segundo terminal para consumir mensagens
confluent kafka topic consume tecnologias --from-beginning
```

## Conclusão

O número ideal de partições depende das necessidades específicas do seu sistema, incluindo a taxa de produção e consumo, o tamanho das mensagens e a necessidade de escalabilidade futura. Distribuir partições entre várias instâncias EC2 pode ajudar a garantir alta disponibilidade e desempenho, mas deve ser feito com planejamento cuidadoso para evitar gargalos e garantir eficiência operacional.


### Referências
- [Confluent Cloud Documentation](https://docs.confluent.io/cloud/current/get-started/index.html)
- [Kafka Documentation](https://kafka.apache.org/documentation/)


================================================
File: /08-kafka-pubsub-streaming/python-client/Dockerfile
================================================
FROM --platform=amd64 python:3.8-alpine
RUN pip install kafka-python Faker
ENV ACTION produce
ENV BOOTSTRAP_SERVERS "broker-1:29091,broker-2:29092,broker-3:29093"
ADD consume.py /src/consume.py
ADD produce.py /src/produce.py
ADD run.sh /src/run.sh
CMD ["sh", "/src/run.sh", "$ACTION"]


================================================
File: /08-kafka-pubsub-streaming/python-client/consume.py
================================================
# https://kafka-python.readthedocs.io/en/master/#kafkaproducer
import json
import os
import time
from datetime import datetime

from kafka import KafkaConsumer

TOPIC = os.environ.get('TOPIC', 'foobar')
CONSUMER_GROUP = os.environ.get('CONSUMER_GROUP', 'cg-group-id')
BOOTSTRAP_SERVERS = os.environ.get(
    'BOOTSTRAP_SERVERS', 'localhost:9091,localhost:9092,localhost:9093'
).split(',')

print('iniciando')


def configurar_consumidor():
    try:
        consumidor = KafkaConsumer(
            TOPIC,
            bootstrap_servers=BOOTSTRAP_SERVERS,
            auto_offset_reset='latest',
            enable_auto_commit=True,
            group_id=CONSUMER_GROUP,
            value_deserializer=lambda x: json.loads(x.decode('utf-8')),
        )
        return consumidor

    except Exception as e:
        if e == 'NoBrokersAvailable':
            print('aguardando os brokers ficarem disponíveis')
        return 'nao-disponivel'


def diferenca_tempo(received_time):
    agora = datetime.now().strftime('%s')
    return int(agora) - received_time


print('iniciando consumidor, verificando se os brokers estão disponíveis')
consumidor = 'nao-disponivel'

while consumidor == 'nao-disponivel':
    print('brokers ainda não disponíveis')
    time.sleep(5)
    consumidor = configurar_consumidor()

print('brokers estão disponíveis e prontos para consumir mensagens')

for mensagem in consumidor:
    try:
        print(mensagem.value)
        # print(f"Mensagem recebida em: {mensagem.timestamp}")
        # agora = datetime.now().strftime("%s")
        # print(f"Timestamp atual {agora}")
    except Exception as e:
        print('ocorreu uma exceção no consumo')
        print(e)

# Fechar o consumidor
print('fechando consumidor')
consumidor.close()


================================================
File: /08-kafka-pubsub-streaming/python-client/produce.py
================================================
# https://kafka-python.readthedocs.io/en/master/#kafkaproducer
import json
import os
import random
import time
import uuid

from faker import Faker
from kafka import KafkaProducer

fake = Faker()

TOPIC = os.environ.get('TOPIC', 'foobar')
BOOTSTRAP_SERVERS = os.environ.get(
    'BOOTSTRAP_SERVERS', 'localhost:9091,localhost:9092,localhost:9093'
).split(',')


def criar_transacao(contador):
    mensagem = {
        'id_sequencia': contador,
        'id_usuario': str(fake.random_int(min=20000, max=100000)),
        'id_transacao': str(uuid.uuid4()),
        'id_produto': str(uuid.uuid4().fields[-1])[:5],
        'endereco': str(
            fake.street_address()
            + ' | '
            + fake.city()
            + ' | '
            + fake.country_code()
        ),
        'cadastro_em': str(fake.date_time_this_month()),
        'id_plataforma': str(random.choice(['Mobile', 'Laptop', 'Tablet'])),
        'mensagem': 'transacao feita pelo usuario {}'.format(
            str(uuid.uuid4().fields[-1])
        ),
    }
    return mensagem


def configurar_produtor():
    try:
        produtor = KafkaProducer(
            bootstrap_servers=BOOTSTRAP_SERVERS,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
        )
        return produtor
    except Exception as e:
        if e == 'NoBrokersAvailable':
            print('aguardando os brokers ficarem disponíveis')
        return 'nao-disponivel'


print('configurando produtor, verificando se os brokers estão disponíveis')
produtor = 'nao-disponivel'

while produtor == 'nao-disponivel':
    print('brokers ainda não disponíveis')
    time.sleep(5)
    produtor = configurar_produtor()

print('brokers estão disponíveis e prontos para produzir mensagens')
contador = 0

while True:
    contador += 1
    mensagem_json = criar_transacao(contador)
    produtor.send(TOPIC, mensagem_json)
    print(
        'mensagem enviada para o kafka com id de sequência {}'.format(contador)
    )
    time.sleep(2)

produtor.close()


================================================
File: /08-kafka-pubsub-streaming/python-client/requirements.txt
================================================
kafka-python
Faker

================================================
File: /08-kafka-pubsub-streaming/python-client/run.sh
================================================
#!/usr/bin/env sh
set -x

if [ "$ACTION" == "producer" ] 
then
  echo "starting $ACTION"
  env | grep BOOTSTRAP
  python3 /src/produce.py
fi

if [ "$ACTION" == "consumer" ]
then
  echo "starting $ACTION"
  env | grep BOOTSTRAP
  python3 /src/consume.py
fi

if [ "$ACTION" == "shell" ]
then
  sleep 10000000
fi

================================================
File: /09-streamlit-dashboard-realtime/README.md
================================================
# Workshop Streamlit

![imagem](./pic/foto.png)

## Roteiro da Aula

- **9h00** Boas vindas
- **9h30** Hello World
- **10h00** Principais comandos
- **10h30** Projeto Survey
- **11h30** Intervalo
- **12h00** Projeto Dashboard Realtime 

## O que vamos fazer hoje

```mermaid
graph TD
    subgraph Coleta[Aplicação de Coleta de Dados]
        A[Iniciar Aplicação de Coleta de Dados] --> B[Conectar ao Banco de Dados]
        B --> C{Conexão bem-sucedida?}
        C -- Sim --> D[Verificar Tabela]
        D --> E{Tabela Existe?}
        E -- Não --> F[Criar Tabela]
        E -- Sim --> G[Exibir Formulário]
        G --> H[Preencher Formulário]
        H --> I[Submeter Formulário]
        I --> J[Salvar Dados no Banco de Dados]
        J --> K[Exibir Mensagem de Sucesso]
        C -- Não --> L[Exibir Mensagem de Erro]
    end

    subgraph DB[Banco de Dados PostgreSQL]
        DB_PostgreSQL[(PostgreSQL)]
        J --> DB_PostgreSQL
        P --> DB_PostgreSQL
    end
    
    subgraph Visualização[Aplicação de Visualização de Dados]
        M[Iniciar Aplicação de Visualização de Dados] --> N[Conectar ao Banco de Dados]
        N --> O{Conexão bem-sucedida?}
        O -- Sim --> P[Carregar Dados do Banco]
        P --> Q[Exibir Tabela de Dados]
        Q --> R[Exibir Gráficos]
        R --> S[Exibir Nuvem de Palavras]
        S --> T[Exibir Mapa]
        T --> U[Exibir Imagem]
        O -- Não --> V[Exibir Mensagem de Erro]
    end
    
    Coleta --> DB
    DB --> Visualização
```

![coleta](./pic/coleta.png)
[APP de Coleta](https://workshop-jornada-coleta.streamlit.app/)

![coleta](./pic/dash.png)
[APP de Dashboard]https://workshop-jornada-dash.streamlit.app/

## A Maneira Mais Simples de Criar um Aplicativo Web com Python

Criar aplicativos web geralmente envolve o uso de frameworks web Python como Django e Flask. Embora esses frameworks sejam poderosos e flexíveis, eles possuem uma curva de aprendizado significativa e podem exigir um investimento substancial de tempo para desenvolver algo funcional.

### Por que usar Streamlit?

Desenvolver com Streamlit torna o processo de criação de aplicativos web muito mais rápido e fácil. Com Streamlit, você pode transformar scripts Python em aplicativos web interativos em questão de minutos. Aqui estão alguns benefícios:

- **Rápida Prototipagem**: Streamlit permite que você crie protótipos rapidamente sem a necessidade de escrever código HTML, CSS ou JavaScript.
- **Foco em Dados**: Ideal para cientistas de dados e analistas, Streamlit facilita a visualização e a interação com dados diretamente em Python.
- **Simplicidade**: A API intuitiva do Streamlit permite que você escreva aplicativos web de forma natural, usando construções familiares do Python.
- **Interatividade**: Adicione widgets interativos como sliders, botões e seletores com facilidade para tornar suas análises de dados mais dinâmicas.

### Comparação com Outros Frameworks

| Característica        | Django & Flask                      | Streamlit                               |
|-----------------------|-------------------------------------|-----------------------------------------|
| **Curva de Aprendizado** | Alta                                | Baixa                                   |
| **Tempo de Desenvolvimento** | Lento (configuração manual de roteamento, templates, etc.) | Rápido (scripts Python para web apps)  |
| **Conhecimento Necessário**  | Python, HTML, CSS, JavaScript   | Apenas Python                           |
| **Interatividade**    | Necessário configurar manualmente  | Integrado e fácil de usar               |
| **Foco Principal**    | Desenvolvimento web geral           | Aplicações de dados e visualizações     |

### Exemplo de Código com Streamlit

Aqui está um exemplo simples de como é fácil criar uma aplicação web com Streamlit:

```python
import streamlit as st
import pandas as pd
import numpy as np

# Título da aplicação
st.title("Exemplo de Aplicação Streamlit")

# Adiciona um cabeçalho
st.header("Introdução ao Streamlit")

# Adiciona um texto
st.text("Esta é uma aplicação web criada com Streamlit!")

# Cria um dataframe
data = pd.DataFrame({
    'Coluna 1': [1, 2, 3, 4],
    'Coluna 2': [10, 20, 30, 40]
})

# Exibe o dataframe
st.dataframe(data)

# Adiciona um gráfico de linha
st.line_chart(data)

# Adiciona um botão e uma resposta ao clique
if st.button("Clique aqui"):
    st.write("Botão clicado!")

# Adiciona um seletor
opcao = st.selectbox("Escolha uma opção", ['Opção A', 'Opção B', 'Opção C'])
st.write("Você selecionou:", opcao)
```

### Iniciando com Streamlit

Para começar a usar Streamlit, siga estes passos:

1. **Instale Streamlit**:
   ```bash
   pip install streamlit
   ```

2. **Crie um script Python** com o conteúdo do exemplo acima e salve-o como `app.py`.

3. **Execute o Streamlit**:
   ```bash
   streamlit run app.py
   ```

4. **Abra o navegador** e acesse `http://localhost:8501` para ver sua aplicação web em ação!

Streamlit é uma ferramenta poderosa e simples que transforma a maneira como você desenvolve e compartilha suas análises de dados. Experimente e veja como pode facilitar o seu trabalho!

## O que é o Streamlit?

- **Streamlit é um Web Framework Open Source em Python** que transforma scripts de dados em aplicativos web interativos e compartilháveis em questão de minutos.
  
- **Streamlit não requer experiência em front-end**. Você pode criar aplicações completas usando apenas Python, o que permite que você foque no desenvolvimento do seu modelo e análise de dados, sem se preocupar com HTML, CSS ou JavaScript.

### Principais Vantagens do Streamlit

- **Fácil de Usar**: A simplicidade do Streamlit permite que você desenvolva rapidamente aplicativos web sem a necessidade de aprender tecnologias de front-end. Se você conhece Python, já sabe o suficiente para criar aplicativos web com Streamlit.
  
- **Rápido**: Com Streamlit, você pode transformar seus scripts Python em aplicativos web em minutos. A configuração é mínima, e você pode ver as mudanças em tempo real enquanto desenvolve.
  
- **Interativo**: Adicionar interatividade às suas análises de dados é simples com Streamlit. Você pode facilmente incluir widgets como sliders, botões, seletores e muito mais para tornar suas aplicações dinâmicas e responsivas.
  
- **Open Source**: Streamlit é gratuito e open source, apoiado por uma comunidade ativa. Isso significa que você pode contribuir para o projeto e se beneficiar das contribuições de outros desenvolvedores.

## Iniciando um Projeto Streamlit

Começar um projeto com Streamlit é simples e rápido. Aqui estão os passos básicos:

1. **Crie um Script Python**: O primeiro passo é criar um arquivo Python, por exemplo, `app.py`.

2. **Importe a Biblioteca Streamlit**: Dentro do seu script, importe a biblioteca Streamlit e use uma série de métodos do Streamlit para criar widgets de entrada e saída.

### Exemplo de Código

Aqui está um exemplo simples de como começar:

```python
import streamlit as st

# Título da aplicação
st.title("Meu Primeiro App com Streamlit")

# Adiciona um cabeçalho
st.header("Introdução ao Streamlit")

# Adiciona um texto
st.text("Streamlit facilita a criação de aplicações web interativas com Python!")

# Widgets de entrada
nome = st.text_input("Digite seu nome")
idade = st.slider("Selecione sua idade", 0, 100, 25)

# Exibe os dados de entrada
st.write(f"Nome: {nome}")
st.write(f"Idade: {idade}")

# Adiciona um gráfico simples
st.line_chart([1, 2, 3, 4, 5])
```

### Executando o Aplicativo

Para executar sua aplicação Streamlit, use o comando abaixo no terminal:

```bash
streamlit run app.py
```

Abra seu navegador e acesse `http://localhost:8501` para ver sua aplicação em ação!

# Aula Introdutória ao Streamlit

Este repositório contém um exemplo prático que explora os principais métodos do Streamlit, uma biblioteca em Python que facilita a criação de aplicações web interativas. O objetivo é fornecer uma base sólida para iniciantes aprenderem a utilizar os recursos básicos do Streamlit.

## Objetivo

O objetivo deste exemplo é explorar os principais métodos do Streamlit, abordando diferentes tipos de exibição de texto, dados, métricas, gráficos, mapas, mídia e widgets interativos. 

## Estrutura do Projeto

- **exemplo/main.py**: Contém o código principal que demonstra os métodos do Streamlit.
- **exemplo/main_exercicio.py**: Contém somente os comentários, servindo como exercício para preencher os métodos do Streamlit.
- **requirements.txt**: Lista de dependências necessárias para executar o projeto.
- **watch.py**: Script opcional para reiniciar automaticamente o Streamlit ao detectar alterações no código.

## Como Executar o Projeto

1. **Clone o repositório**:

   ```sh
   git clone https://github.com/seu-usuario/streamlit-intro.git
   cd streamlit-intro
   ```

2. **Crie um ambiente virtual e ative-o**:

   ```sh
   python -m venv venv
   source venv/bin/activate  # Para Windows: venv\Scripts\activate
   ```

3. **Instale as dependências**:

   ```sh
   pip install -r requirements.txt
   ```

4. **Execute o Streamlit**:

   ```sh
   streamlit run exemplo/main.py
   ```

   O Streamlit irá automaticamente observar mudanças no arquivo `main.py` e recarregar a aplicação.

# Conteúdo do Exemplo

### 1. Títulos e Texto

- **Título da aplicação**: `st.title(titulo)`
- **Cabeçalho**: `st.header(cabecalho)`
- **Subcabeçalho**: `st.subheader(subcabecalho)`
- **Texto simples**: `st.text(texto)`
- **Markdown**: `st.markdown(markdown_texto)`
- **Fórmula LaTeX**: `st.latex(latex_formula)`
- **Código com destaque de sintaxe**: `st.code(codigo)`

### 2. Exibição de Dados

- **Exibe um DataFrame**: `st.write(df)`
- **DataFrame com redimensionamento**: `st.dataframe(df)`
- **Tabela estática**: `st.table(df)`
- **Objeto JSON**: `st.json(json_obj)`
- **CSV como string**: `st.write(csv_string)`


- **Lista de números**: `st.write(lista)`

### 3. Métricas

- **Métrica com delta**: `st.metric(label, value, delta)`
- **Métricas diversas**: `st.metric(label, value)`

### 4. Gráficos

- **Gráfico de linha**: `st.line_chart(data)`
- **Gráfico de área**: `st.area_chart(data)`
- **Gráfico de barra**: `st.bar_chart(data)`
- **Gráfico de dispersão**: `st.plotly_chart(scatter_plot)`
- **Histograma**: `st.plotly_chart(histogram)`

### 5. Mapas

- **Exibe um mapa**: `st.map(map_data)`

### 6. Mídia

- **Imagem com legenda**: `st.image(url, caption)`
- **Reprodutor de áudio**: `st.audio(url)`
- **Reprodutor de vídeo**: `st.video(url)`

### 7. Widgets

- **Botão**: `st.button(label)`
- **Caixa de seleção**: `st.checkbox(label)`
- **Opções de escolha única**: `st.radio(label, options)`
- **Menu suspenso**: `st.selectbox(label, options)`
- **Menu suspenso múltiplo**: `st.multiselect(label, options)`
- **Barra deslizante**: `st.slider(label, min_value, max_value, value)`
- **Barra deslizante com opções de texto**: `st.select_slider(label, options, value)`
- **Caixa de entrada de texto**: `st.text_input(label)`
- **Caixa de entrada de número**: `st.number_input(label, min_value, max_value)`
- **Área de texto**: `st.text_area(label)`
- **Seletor de data**: `st.date_input(label, value)`

### 8. Barra Lateral

- **Título da barra lateral**: `st.sidebar.title(title)`
- **Botão na barra lateral**: `st.sidebar.button(label)`

### 9. Carregamento de CSV e Downloads

- **Carregamento de arquivo CSV**: `st.file_uploader(label, type)`
- **Barra de progresso durante o upload**: `st.progress(progress)`
- **Download de arquivo Parquet**: `st.download_button(label, data, file_name, mime)`

# Setup do nosso projeto

Este projeto utiliza Streamlit para criar aplicações web interativas em Python. Vamos configurar o ambiente de desenvolvimento com as seguintes ferramentas:

- Git e Github (utilizando o Github CLI) para versionamento do código
- Python 3.12.1 utilizando o Pyenv
- Poetry para gerenciamento de pacotes e ambiente virtual
- Streamlit para desenvolver a aplicação
- Ruff para linting
- Taskipy para tarefas automatizadas
- Pytest para testes

## Configurando o ambiente de desenvolvimento

### 1. Criando o repositório no Github

Utilize o Github CLI para criar um novo repositório:

```bash
gh repo create
```

### 2. Criando o arquivo .gitignore

Utilize o ignr para criar um arquivo .gitignore específico para projetos Python:

```bash
ignr -n python
```

### 3. Criando o README.md

Crie um arquivo README.md (este arquivo) para documentar o projeto.

```bash
touch README.md
```

### 4. Instalando Python 3.12.1 com Pyenv

```bash
pyenv install 3.12.1
pyenv local 3.12.1
```

### 5. Configurando o ambiente com Poetry

Inicialize um novo projeto com Poetry:

```bash
poetry init
```

### 6. Instalando Streamlit

Adicione o Streamlit ao seu projeto:

```bash
poetry add streamlit
```

### 7. Instalando e Configurando Ruff

Ruff é uma ferramenta de linting rápida para Python, que ajuda a identificar e corrigir problemas no código, como formatação incorreta, erros de sintaxe e práticas de codificação inadequadas. A análise estática de código, como a realizada pelo Ruff, é uma prática essencial para garantir a qualidade e a manutenção do código. Ela verifica o código-fonte sem executá-lo, encontrando erros comuns e garantindo conformidade com padrões de codificação.

#### 7.1. Adicionando Ruff ao grupo de desenvolvimento

Para adicionar Ruff ao grupo de desenvolvimento, execute:

```bash
poetry add --group dev ruff
```

#### 7.2. Configurando Ruff no `pyproject.toml`

Adicione a configuração do Ruff no arquivo `pyproject.toml` para personalizar seu comportamento:

```toml
[tool.ruff]
# Configurações de linting
line-length = 88  # Comprimento máximo de linha
select = ["E", "F", "W"]  # Seleciona as categorias de erros e avisos
ignore = ["E203", "W503"]  # Ignora regras específicas

# Adicione paths específicos se necessário
exclude = ["build", "dist"]

# Para especificar o conjunto de regras do Black para formatação
extend-select = ["B"]
```

### 7.3. Realizando análise estática

Para realizar a análise estática do código com Ruff, execute o seguinte comando:

```bash
ruff check .
```

### 7.4. Corrigindo problemas automaticamente

Ruff também pode corrigir problemas automaticamente quando possível. Para fazer isso, utilize o comando:

```bash
ruff check . --fix
```

### Exemplo de Configuração Completa no `pyproject.toml`

Aqui está um exemplo de como o `pyproject.toml` deve ficar com a configuração do Ruff:

```toml
[tool.poetry]
name = "projeto-streamlit"
version = "0.1.0"
description = "Projeto Streamlit para criar aplicações web interativas em Python."
authors = ["Seu Nome <seu.email@example.com>"]

[tool.poetry.dependencies]
python = "^3.12"
streamlit = "^1.0.0"

[tool.poetry.dev-dependencies]
ruff = "^0.0.255"
pytest = "^6.2.4"
taskipy = "^1.9.0"
pre-commit = "^2.14.0"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.ruff]
line-length = 88
select = ["E", "F", "W", "C", "N", "B", "I", "PL", "PT"]
ignore = ["E203", "W503"]
exclude = ["build", "dist"]
extend-select = ["B"]

[tool.taskipy.tasks]
lint = "ruff check ."
test = "pytest"
```

Com essas configurações, você terá um ambiente de desenvolvimento bem configurado, incluindo a análise estática do código com Ruff, garantindo que seu código esteja sempre em conformidade com as melhores práticas de desenvolvimento em Python.

### 8. Instalando Taskipy

Adicione Taskipy ao seu projeto:

```bash
poetry add --group dev taskipy
```

Adicione o seguinte ao seu `pyproject.toml` para configurar o Taskipy:

```toml
[tool.taskipy.tasks]
lint = "ruff check ."
```

# Integração Streamlit e outras bibliotecas gráficas

Streamlit é uma biblioteca de código aberto em Python que torna extremamente fácil criar e compartilhar aplicativos web de dados. Uma das grandes vantagens do Streamlit é a sua capacidade de integrar diversas bibliotecas gráficas populares do Python, proporcionando uma experiência visual rica e interativa. Aqui estão algumas vantagens e destaques dessas integrações:

### 1. Matplotlib

#### O que é?
Matplotlib é uma biblioteca de plotagem 2D extremamente popular em Python, usada para criar gráficos estáticos, animados e interativos. É altamente configurável e permite criar visualizações complexas com facilidade.

#### Vantagens no Streamlit
- **Facilidade de Integração**: Com Streamlit, você pode exibir gráficos Matplotlib de forma simples utilizando `st.pyplot()`.
- **Interatividade**: Streamlit permite adicionar interatividade aos seus gráficos Matplotlib sem a necessidade de configurar um ambiente web completo.
- **Visualizações Poderosas**: Combine a flexibilidade do Matplotlib com a simplicidade do Streamlit para criar dashboards poderosos e visualizações de dados detalhadas.

### Comando para Instalação
```bash
poetry add matplotlib
```

### 2. Folium

#### O que é?
Folium é uma biblioteca que facilita a visualização de dados geoespaciais utilizando Leaflet.js. Com Folium, você pode criar mapas interativos e adicionar marcadores, camadas e outras funcionalidades.

#### Vantagens no Streamlit
- **Mapas Interativos**: Streamlit e Folium juntos permitem a criação de mapas interativos dentro de aplicativos web, ideais para análises geoespaciais.
- **Visualização de Dados Geográficos**: Exiba dados geográficos e geolocalizados diretamente no navegador, facilitando a compreensão e análise espacial.
- **Simplicidade**: Adicione mapas interativos aos seus aplicativos com poucas linhas de código usando `folium_static()`.

### Comando para Instalação
```bash
poetry add folium streamlit-folium
```

### 3. WordCloud

#### O que é?
WordCloud é uma biblioteca para a geração de nuvens de palavras a partir de texto. As nuvens de

 palavras são uma forma visual de representar a frequência ou importância de palavras em um texto, com palavras mais frequentes aparecendo maiores.

#### Vantagens no Streamlit
- **Visualização de Texto**: Utilize WordCloud com Streamlit para criar representações visuais atraentes de dados textuais.
- **Facilidade de Uso**: Crie e exiba nuvens de palavras rapidamente usando `WordCloud` e `st.pyplot()`.
- **Análise Textual**: Ideal para visualizar e explorar grandes volumes de texto de maneira intuitiva.

### Comando para Instalação
```bash
poetry add wordcloud
```

### Conclusão

Streamlit se destaca como uma ferramenta poderosa para criar aplicativos de dados interativos, integrando facilmente bibliotecas gráficas populares como Matplotlib, Folium e WordCloud. Estas integrações permitem aos desenvolvedores focar na análise e visualização de dados, sem a necessidade de se preocupar com a infraestrutura web subjacente. Com Streamlit, você pode transformar scripts de dados em aplicativos web compartilháveis em minutos, facilitando a disseminação e compreensão de insights valiosos.

### Exemplo de Código

```python
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import folium
from streamlit_folium import folium_static
from wordcloud import WordCloud

# Exemplo de gráfico Matplotlib
st.header("Gráfico Matplotlib")
fig, ax = plt.subplots()
ax.plot([1, 2, 3, 4], [10, 20, 25, 30])
st.pyplot(fig)

# Exemplo de mapa Folium
st.header("Mapa Folium")
m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)
folium.Marker([45.5236, -122.6750], popup="The Waterfront").add_to(m)
folium_static(m)

# Exemplo de nuvem de palavras WordCloud
st.header("Nuvem de Palavras")
text = "Python Streamlit Matplotlib Folium WordCloud"
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
fig, ax = plt.subplots()
ax.imshow(wordcloud, interpolation='bilinear')
ax.axis("off")
st.pyplot(fig)
```

Streamlit transforma a maneira como você trabalha com dados, tornando a criação de visualizações e aplicativos de dados mais acessível e eficiente.

## Survey App com Streamlit e PostgreSQL

### Descrição

Este projeto demonstra como criar uma aplicação web de enquete utilizando Streamlit para a interface do usuário e PostgreSQL para o armazenamento de dados. O objetivo é coletar dados dos participantes através de um formulário, salvar esses dados em um banco de dados PostgreSQL e posteriormente visualizá-los e analisá-los em uma outra aplicação web.

### Estrutura do Projeto

O projeto está dividido em duas partes principais, cada uma localizada em uma pasta separada:

1. **Aplicação de Coleta de Dados** (`projeto_coleta`):
   - Coleta os dados dos participantes através de um formulário web.
   - Salva os dados coletados em um banco de dados PostgreSQL.

2. **Aplicação de Visualização de Dados** (`projeto_dash`):
   - Carrega os dados do banco de dados PostgreSQL.
   - Exibe os dados em gráficos interativos e tabelas.

### Arquivos

- `projeto_coleta/main.py`: Contém a implementação da aplicação de coleta de dados.
- `projeto_dash/visualization.py`: Contém a implementação da aplicação de visualização de dados.
- `.env`: Arquivo de configuração com as variáveis de ambiente para conexão ao banco de dados PostgreSQL.
- `requirements.txt`: Lista de dependências do projeto.

### Instruções para Executar

#### 1. Clonar o Repositório

Clone o repositório para sua máquina local:

```bash
git clone https://github.com/seu-usuario/seu-repositorio.git
cd seu-repositorio
```

#### 2. Configurar o Ambiente Virtual

Crie e ative um ambiente virtual:

```bash
python -m venv venv
source venv/bin/activate  # No Windows use `venv\Scripts\activate`
```

#### 3. Instalar Dependências

Instale as dependências necessárias:

```bash
pip install -r requirements.txt
```

#### 4. Configurar Variáveis de Ambiente

Crie um arquivo `.env` na raiz do projeto e adicione as configurações de conexão ao banco de dados PostgreSQL:

```env
DB_HOST=seu_host
DB_DATABASE=seu_database
DB_USER=seu_usuario
DB_PASSWORD=sua_senha
```

#### 5. Executar a Aplicação de Coleta de Dados

Entre na pasta `projeto_coleta` e inicie a aplicação de coleta de dados:

```bash
cd projeto_coleta
streamlit run main.py
```

#### 6. Executar a Aplicação de Visualização de Dados

Em um novo terminal, entre na pasta `projeto_dash` e inicie a aplicação de visualização de dados:

```bash
cd projeto_dash
streamlit run visualization.py
```

### Funcionalidades

#### Aplicação de Coleta de Dados (`main.py`)

1. **Conexão ao Banco de Dados**:
   - Conecta ao banco de dados PostgreSQL utilizando as variáveis de ambiente configuradas.

2. **Criação da Tabela**:
   - Cria a tabela `survey_data` no banco de dados, caso ainda não exista.

3. **Formulário de Enquete**:
   - Coleta informações dos participantes, como estado, área de atuação, bibliotecas utilizadas, horas de estudo, conforto com dados e experiência em Python, SQL e Cloud.

4. **Salvar Dados**:
   - Salva os dados coletados no banco de dados PostgreSQL.

#### Aplicação de Visualização de Dados (`visualization.py`)

1. **Conexão ao Banco de Dados**:
   - Conecta ao banco de dados PostgreSQL utilizando as variáveis de ambiente configuradas.

2. **Carregar Dados**:
   - Carrega os dados da tabela `survey_data` do banco de dados.

3. **Exibir Dados**:
   - Exibe os dados em uma tabela.

4. **Gráficos Interativos**:
   - Exibe gráficos de área mostrando o nível de conforto com dados versus horas de estudo.
   - Exibe gráficos de linha mostrando a experiência técnica dos participantes em Python, SQL e Cloud.
   - Exibe um mapa do Brasil com a distribuição dos participantes por estado.
   - Exibe uma nuvem de palavras com as bibliotecas utilizadas pelos participantes.
   - Exibe as top 3 bibliotecas utilizadas por área de atuação.

5. **Exibir Imagem**:
   - Exibe uma imagem ao final da página.

### Workflow

#### Workflow da Aplicação de Coleta de Dados

```mermaid
graph TD
    A[Iniciar Aplicação de Coleta de Dados] --> B[Conectar ao Banco de Dados]
    B --> C{Conexão bem-sucedida?}
    C -- Sim --> D[Verificar Tabela]
    D --> E{Tabela Existe?}
    E -- Não --> F[Criar Tabela]
    E -- Sim --> G[Exibir Formulário]
    G --> H[Preencher Formulário]
    H --> I[Submeter Formulário]
    I --> J[Salvar Dados no Banco de Dados]
    J --> K[Exibir Mensagem de Sucesso]
    C -- Não --> L[Exibir Mensagem de Erro]
```

#### Workflow da Aplicação de Visualização de Dados

```mermaid
graph TD
    A[Iniciar Aplicação de Visualização de Dados] --> B[Conectar ao Banco de Dados]
    B --> C{Conexão bem-sucedida?}
    C -- Sim --> D[Carregar Dados do Banco]
    D --> E[Exibir Tabela de Dados]
    E --> F[Exibir Gráficos]
    F --> G[Exibir Nuvem de Palavras]
    G --> H[Exibir Mapa]
    H --> I[Exibir Imagem]
    C -- Não --> J[Exibir Mensagem de Erro]
```

Com esse guia, você estará preparado para configurar, desenvolver e executar um projeto completo utilizando Streamlit e PostgreSQL, desde a coleta até a visualização de dados.

================================================
File: /09-streamlit-dashboard-realtime/app.py
================================================
import pandas as pd
import streamlit as st

# Título da aplicação
st.title("Exemplo de Aplicação Streamlit")

# Adiciona um cabeçalho
st.header("Introdução ao Streamlit")

# Adiciona um texto
st.text("Esta é uma aplicação web criada com Streamlit!")

# Cria um dataframe
data = pd.DataFrame({"Coluna 1": [1, 2, 3, 4], "Coluna 2": [10, 20, 30, 40]})

# Exibe o dataframe
st.dataframe(data)

# Adiciona um gráfico de linha
st.line_chart(data)

# Adiciona um botão e uma resposta ao clique
if st.button("Clique aqui"):
    st.write("Botão clicado!")

# Adiciona um seletor
opcao = st.selectbox("Escolha uma opção", ["Opção A", "Opção B", "Opção C"])
st.write("Você selecionou:", opcao)


================================================
File: /09-streamlit-dashboard-realtime/pyproject.toml
================================================
[tool.poetry]
name = "workshop-streamlit"
version = "0.1.0"
description = ""
authors = ["lvgalvaofilho <lvgalvaofilho@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "3.12.1"
streamlit = "^1.37.0"
folium = "^0.17.0"
streamlit-folium = "^0.22.0"
matplotlib = "^3.9.1"
seaborn = "^0.13.2"
pydeck = "^0.9.1"
wordcloud = "^1.9.3"
psycopg2-binary = "^2.9.9"
python-dotenv = "^1.0.1"
sqlalchemy = "^2.0.31"
xlsxwriter = "^3.2.0"
plotly = "^5.23.0"

[tool.poetry.group.dev.dependencies]
ruff = "^0.5.5"
taskipy = "^1.13.0"
pre-commit = "^3.7.1"
pytest = "^8.3.2"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.ruff]
line-length = 200
indent-width = 4
exclude = ["build","exemplo","ruff_erros"]


[tool.ruff.lint]
select = ["E", "F", "W", "C", "N", "B", "I", "PL", "PT"]

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = ["ALL"]
unfixable = []

[tool.ruff.format]
# Like Black, use double quotes for strings.
quote-style = "double"

[tool.taskipy.tasks]
lint = "ruff check ."
format = "ruff format ."
ruff = "ruff check . --fix && ruff format ."

================================================
File: /09-streamlit-dashboard-realtime/.python-version
================================================
3.12.1


================================================
File: /09-streamlit-dashboard-realtime/exemplo/main.py
================================================
import io
import time
from datetime import datetime

import numpy as np
import pandas as pd
import streamlit as st

# 1. Títulos e Texto
titulo = "Aula Introdutória ao Streamlit"
st.title(titulo)  # Exibe o título da aplicação

cabecalho = "Aprendendo os Principais Métodos"
st.header(cabecalho)  # Exibe um cabeçalho

# Exibição de Texto
subcabecalho_texto = "Métodos de Exibição de Texto"
st.subheader(subcabecalho_texto)  # Exibe um subcabeçalho

# Exibe um texto simples
texto_simples = (
    "Streamlit facilita a criação de aplicações web interativas com Python."
)
st.text(texto_simples)  # Exibe um texto simples

# Exibe um texto em markdown
texto_markdown = "### Este é um texto em **markdown**!"
st.markdown(texto_markdown)  # Exibe texto formatado usando Markdown

# Exibe uma fórmula em LaTeX
formula_latex = r""" e^{i\pi} + 1 = 0 """
st.latex(formula_latex)  # Exibe uma fórmula matemática usando LaTeX

# Exibe um código
codigo_exemplo = "x = 42"
st.code(
    codigo_exemplo, language="python"
)  # Exibe um trecho de código com destaque de sintaxe

# 2. Exibição de Dados
subcabecalho_dados = "Exibição de Dados"
st.subheader(subcabecalho_dados)  # Exibe um subcabeçalho

# Cria um DataFrame e exibe-o de várias formas
data = {"A": [1, 2, 3, 4], "B": [10, 20, 30, 40]}
df = pd.DataFrame(data)

# Exibe o DataFrame usando o método write
st.write(
    "Aqui está um dataframe:", df
)  # Exibe o DataFrame com formatação padrão

# Exibe o DataFrame usando o método dataframe
st.dataframe(df)  # Exibe o DataFrame com opções de redimensionamento

# Exibe o DataFrame usando o método table
st.table(df)  # Exibe o DataFrame como uma tabela estática

# Exibe um JSON
json_exemplo = {"name": "Streamlit", "type": "Web Framework"}
st.json(json_exemplo)  # Exibe um objeto JSON

# Exibe um CSV como string
csv_exemplo = df.to_csv(index=False)
st.write("Exibindo CSV:", csv_exemplo)  # Exibe o DataFrame como CSV

# Exibe uma lista
lista_exemplo = [1, 2, 3, 4, 5]
st.write("Lista de números:", lista_exemplo)  # Exibe uma lista

# 3. Métricas
subcabecalho_metricas = "Métricas"
st.subheader(subcabecalho_metricas)  # Exibe um subcabeçalho

# Exibe uma métrica com delta (diferença)
st.metric(
    label="Temperatura", value="70 °F", delta="1.2 °F"
)  # Exibe uma métrica com delta (mudança)

# Exibe mais métricas com diferentes valores e deltas
st.metric(label="Umidade", value="60%", delta="-5%")
st.metric(label="Velocidade do Vento", value="15 km/h", delta="2 km/h")
st.metric(label="Nível de Ruído", value="40 dB", delta="1.5 dB")
st.metric(label="Pressão Atmosférica", value="1013 hPa", delta="2 hPa")

# Exibe uma métrica sem delta
st.metric(label="População", value="8 bilhões")

# 4. Gráficos
subcabecalho_graficos = "Gráficos"
st.subheader(subcabecalho_graficos)  # Exibe um subcabeçalho

# Cria e exibe gráficos de linha, área e barra
chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])
st.line_chart(chart_data)  # Exibe um gráfico de linha
st.area_chart(chart_data)  # Exibe um gráfico de área
st.bar_chart(chart_data)  # Exibe um gráfico de barra

# Mais exemplos de gráficos
mais_graficos = "Mais Exemplos de Gráficos"
st.subheader(mais_graficos)  # Exibe um subcabeçalho

# Gráfico de dispersão
scatter_data = pd.DataFrame(np.random.randn(100, 2), columns=["x", "y"])
st.plotly_chart(
    {
        "data": [
            {
                "x": scatter_data["x"],
                "y": scatter_data["y"],
                "type": "scatter",
                "mode": "markers",
            }
        ],
        "layout": {"title": "Gráfico de Dispersão"},
    }
)  # Exibe um gráfico de dispersão

# Histograma
hist_data = np.random.randn(1000)
st.plotly_chart(
    {
        "data": [{"x": hist_data, "type": "histogram"}],
        "layout": {"title": "Histograma"},
    }
)  # Exibe um histograma

# 5. Mapas
subcabecalho_mapas = "Mapas"
st.subheader(subcabecalho_mapas)  # Exibe um subcabeçalho

# Cria e exibe um mapa
map_data = pd.DataFrame(
    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
    columns=["lat", "lon"],
)
st.map(map_data)  # Exibe um mapa com pontos aleatórios

# 6. Mídia
subcabecalho_midia = "Mídia"
st.subheader(subcabecalho_midia)  # Exibe um subcabeçalho

# Exibe uma imagem
imagem_url = "https://www.streamlit.io/images/brand/streamlit-logo-primary-colormark-darktext.png"
imagem_legenda = "Streamlit Logo"
st.image(imagem_url, caption=imagem_legenda)  # Exibe uma imagem com legenda

# Exibe um áudio
audio_url = "https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3"
st.audio(audio_url)  # Exibe um reprodutor de áudio

# Exibe um vídeo
video_url = "https://www.youtube.com/watch?v=B2iAodr0fOo"
st.video(video_url)  # Exibe um reprodutor de vídeo

# 7. Widgets
subcabecalho_widgets = "Widgets"
st.subheader(subcabecalho_widgets)  # Exibe um subcabeçalho

# Botão - Exibe um botão que, ao ser clicado, mostra uma mensagem
if st.button("Clique aqui"):
    st.write("Botão clicado!")  # Mensagem exibida ao clicar no botão

# Checkbox - Exibe uma caixa de seleção
aceita_termos = st.checkbox("Eu aceito os termos e condições")
st.write("Aceita os termos:", aceita_termos)  # Exibe o valor selecionado

# Radio - Exibe opções de escolha única
opcao_radio = st.radio("Escolha uma opção", ("Opção 1", "Opção 2", "Opção 3"))
st.write("Opção escolhida:", opcao_radio)  # Exibe a opção selecionada

# Selectbox - Exibe um menu suspenso para selecionar uma opção
opcao_selectbox = st.selectbox(
    "Selecione uma opção", ["Opção A", "Opção B", "Opção C"]
)
st.write("Opção selecionada:", opcao_selectbox)  # Exibe a opção selecionada

# Multiselect - Exibe um menu suspenso para selecionar várias opções
opcoes_multiselect = st.multiselect(
    "Selecione múltiplas opções", ["Opção 1", "Opção 2", "Opção 3"]
)
st.write(
    "Opções selecionadas:", opcoes_multiselect
)  # Exibe as opções selecionadas

# Slider - Exibe uma barra deslizante para selecionar um valor
valor_slider = st.slider("Selecione um valor", 0, 100, 50)
st.write("Valor selecionado:", valor_slider)  # Exibe o valor selecionado

# Select Slider - Exibe uma barra deslizante com opções de texto
intervalo_slider = st.select_slider(
    "Selecione um intervalo", options=["a", "b", "c", "d"], value=("b", "c")
)
st.write(
    "Intervalo selecionado:", intervalo_slider
)  # Exibe o intervalo selecionado

# Text Input - Exibe uma caixa de entrada de texto
nome = st.text_input("Digite seu nome")
st.write("Nome digitado:", nome)  # Exibe o texto digitado

# Number Input - Exibe uma caixa de entrada de número
numero = st.number_input("Selecione um número", 0, 100)
st.write("Número selecionado:", numero)  # Exibe o número selecionado

# Text Area - Exibe uma área de texto
texto = st.text_area("Escreva um texto")
st.write("Texto digitado:", texto)  # Exibe o texto digitado

# Date Input - Exibe um seletor de data
data = st.date_input("Selecione uma data", datetime.now())
st.write("Data selecionada:", data)  # Exibe a data selecionada

# Sidebar
st.sidebar.title("Barra Lateral")  # Exibe o título da barra lateral
botao_sidebar = st.sidebar.button("Botão na Barra Lateral")
if botao_sidebar:
    st.sidebar.write(
        "Botão na barra lateral clicado!"
    )  # Mensagem exibida ao clicar no botão da barra lateral

# Carregar CSV
subcabecalho_csv = "Carregar CSV"
st.subheader(subcabecalho_csv)  # Exibe um subcabeçalho
uploaded_file = st.file_uploader("Escolha um arquivo CSV", type="csv")

if uploaded_file is not None:
    # Barra de progresso durante o upload
    progress_bar = st.progress(0)

    # Simulação do progresso de carregamento
    for i in range(100):
        time.sleep(0.01)
        progress_bar.progress(i + 1)

    # Lê o CSV
    csv_data = pd.read_csv(uploaded_file)
    st.write("Dados do CSV:")
    st.dataframe(csv_data)  # Exibe o conteúdo do arquivo CSV

    # Soltar balões após o upload
    st.balloons()

    # Função para converter DataFrame para Parquet
    @st.cache_data
    def convert_df_to_parquet(df):
        output = io.BytesIO()
        df.to_parquet(output, index=False)
        return output.getvalue()

    st.download_button(
        label="Baixar dados como Parquet",
        data=convert_df_to_parquet(csv_data),
        file_name="dados.parquet",
        mime="application/octet-stream",
    )


================================================
File: /09-streamlit-dashboard-realtime/exemplo/main_exercicio.py
================================================
import io
from datetime import datetime

import numpy as np
import pandas as pd
import streamlit as st

# 1. Títulos e Texto
titulo = "Aula Introdutória ao Streamlit"
cabecalho = "Aprendendo os Principais Métodos"

# Exibição de Texto
subcabecalho_texto = "Métodos de Exibição de Texto"

# Texto simples
texto_simples = (
    "Streamlit facilita a criação de aplicações web interativas com Python."
)

# Texto em markdown
texto_markdown = "### Este é um texto em **markdown**!"

# Fórmula em LaTeX
formula_latex = r""" e^{i\pi} + 1 = 0 """

# Código
codigo_exemplo = "x = 42"

# 2. Exibição de Dados
subcabecalho_dados = "Exibição de Dados"

# Dados do DataFrame
data = {"A": [1, 2, 3, 4], "B": [10, 20, 30, 40]}
df = pd.DataFrame(data)

# JSON
json_exemplo = {"name": "Streamlit", "type": "Web Framework"}

# CSV como string
csv_exemplo = df.to_csv(index=False)

# Lista
lista_exemplo = [1, 2, 3, 4, 5]

# 3. Métricas
subcabecalho_metricas = "Métricas"

# Métricas com delta
temperatura = {"label": "Temperatura", "value": "70 °F", "delta": "1.2 °F"}
umidade = {"label": "Umidade", "value": "60%", "delta": "-5%"}
vento = {"label": "Velocidade do Vento", "value": "15 km/h", "delta": "2 km/h"}
ruido = {"label": "Nível de Ruído", "value": "40 dB", "delta": "1.5 dB"}
pressao = {
    "label": "Pressão Atmosférica",
    "value": "1013 hPa",
    "delta": "2 hPa",
}

# Métrica sem delta
populacao = {"label": "População", "value": "8 bilhões"}

# 4. Gráficos
subcabecalho_graficos = "Gráficos"

# Dados dos gráficos
chart_data = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])

# Mais exemplos de gráficos
mais_graficos = "Mais Exemplos de Gráficos"

# Dados do gráfico de dispersão
scatter_data = pd.DataFrame(np.random.randn(100, 2), columns=["x", "y"])

# Dados do histograma
hist_data = np.random.randn(1000)

# 5. Mapas
subcabecalho_mapas = "Mapas"

# Dados do mapa
map_data = pd.DataFrame(
    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],
    columns=["lat", "lon"],
)

# 6. Mídia
subcabecalho_midia = "Mídia"

# URL da imagem
imagem_url = "https://www.streamlit.io/images/brand/streamlit-logo-primary-colormark-darktext.png"
imagem_legenda = "Streamlit Logo"

# URL do áudio
audio_url = "https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3"

# URL do vídeo
video_url = "https://www.youtube.com/watch?v=B2iAodr0fOo"

# 7. Widgets
subcabecalho_widgets = "Widgets"

# Botão
botao = "Clique aqui"
botao_mensagem = "Botão clicado!"

# Checkbox
checkbox_label = "Eu aceito os termos e condições"

# Radio
radio_label = "Escolha uma opção"
radio_options = ("Opção 1", "Opção 2", "Opção 3")

# Selectbox
selectbox_label = "Selecione uma opção"
selectbox_options = ["Opção A", "Opção B", "Opção C"]

# Multiselect
multiselect_label = "Selecione múltiplas opções"
multiselect_options = ["Opção 1", "Opção 2", "Opção 3"]

# Slider
slider_label = "Selecione um valor"
slider_min = 0
slider_max = 100
slider_default = 50

# Select Slider
select_slider_label = "Selecione um intervalo"
select_slider_options = ["a", "b", "c", "d"]
select_slider_default = ("b", "c")

# Text Input
text_input_label = "Digite seu nome"

# Number Input
number_input_label = "Selecione um número"
number_input_min = 0
number_input_max = 100

# Text Area
text_area_label = "Escreva um texto"

# Date Input
date_input_label = "Selecione uma data"
date_input_default = datetime.now()

# Sidebar
sidebar_title = "Barra Lateral"
sidebar_button_label = "Botão na Barra Lateral"
sidebar_button_message = "Botão na barra lateral clicado!"

# Carregar CSV
subcabecalho_csv = "Carregar CSV"
file_uploader_label = "Escolha um arquivo CSV"
file_uploader_type = "csv"


# Função para converter DataFrame para Parquet
@st.cache_data
def convert_df_to_parquet(df):
    output = io.BytesIO()
    df.to_parquet(output, index=False)
    return output.getvalue()


download_button_label_parquet = "Baixar dados como Parquet"
download_button_filename_parquet = "dados.parquet"
download_button_mime_parquet = "application/octet-stream"


================================================
File: /09-streamlit-dashboard-realtime/exemplo_erros_ruff/erros_ruff.py
================================================
import os  # Importação não utilizada
import sys  # Importação não utilizada

def function_one():
    pass

def function_two():
    pass

def example_function(a,b):  # Falta espaço após a vírgula
    """Exemplo de função com vários erros"""
    print("Esta é uma linha de código que é muito longa e ultrapassa o limite de 79 caracteres, o que não está de acordo com o PEP8.")  # Linha muito longa (E501)
    result = 1 \
        + 2  # Quebra de linha desnecessária (W503)
    myVariable = 10  # Nome de variável não está no estilo snake_case (N806)
    return result + a + b

def another_function():
    print(undefined_variable)  # Variável não definida (F821)

def function_with_trailing_whitespace():  
    pass  # Espaço em branco à direita (W291)

def function_without_docstring():
    pass  # Deveria ter uma docstring explicando a função (D103)

def function_with_missing_type_annotations(a, b):  # Deveria ter anotação de tipo (ANN001)
    return a + b

def unused_function_example():  # Função definida mas não utilizada (F841)
    pass

x = 5
y =  10  # Dois espaços antes da atribuição (E222)

def function_with_no_space_in_def(param1,param2):  # Falta de espaço após a vírgula (E231)
    return param1 + param2

def inconsistent_indentation():
  a = 5  # Indentação inconsistente (E111)
    b = 6
    return a + b

def mixed_tabs_and_spaces():
    a = 10  # Mistura de espaços e tabulações (E101)
	b = 20
	return a + b

def incorrect_default_argument(value=[]):  # Uso de lista mutável como valor padrão (B006)
    value.append(1)
    return value

def function_with_multiple_returns(a):  # Múltiplos retornos (C901)
    if a > 10:
        return True
    elif a == 10:
        return False
    else:
        return None

class ExampleClass:
    def __init__(self):
        self.value = 10  # Atributo não documentado (D107)

    def get_value(self):
        return self.value

    def set_value(self, value):
        self.value = value  # Método setter sem validação (B008)


================================================
File: /09-streamlit-dashboard-realtime/exemplo_libs_grafico/exemplo.py
================================================
import folium
import matplotlib.pyplot as plt
import streamlit as st
from streamlit_folium import folium_static
from wordcloud import WordCloud

# Exemplo de gráfico Matplotlib
st.header("Gráfico Matplotlib")
fig, ax = plt.subplots()
ax.plot([1, 2, 3, 4], [10, 20, 25, 30])
st.pyplot(fig)

# Exemplo de mapa Folium
st.header("Mapa Folium")
m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)
folium.Marker([45.5236, -122.6750], popup="The Waterfront").add_to(m)
folium_static(m)

# Exemplo de nuvem de palavras WordCloud
st.header("Nuvem de Palavras")
text = "Python Streamlit Matplotlib Folium WordCloud"
wordcloud = WordCloud(
    width=800, height=400, background_color="white"
).generate(text)
fig, ax = plt.subplots()
ax.imshow(wordcloud, interpolation="bilinear")
ax.axis("off")
st.pyplot(fig)


================================================
File: /09-streamlit-dashboard-realtime/projeto_coleta/coleta.py
================================================
import streamlit as st
import pandas as pd
import os

# Nome do arquivo CSV onde os dados serão armazenados
data_file = "survey_data.csv"

# Opções de estados
estados = [
    "Acre", "Alagoas", "Amapá", "Amazonas", "Bahia", "Ceará",
    "Distrito Federal", "Espírito Santo", "Goiás", "Maranhão",
    "Mato Grosso", "Mato Grosso do Sul", "Minas Gerais", "Pará",
    "Paraíba", "Paraná", "Pernambuco", "Piauí", "Rio de Janeiro",
    "Rio Grande do Norte", "Rio Grande do Sul", "Rondônia", "Roraima",
    "Santa Catarina", "São Paulo", "Sergipe", "Tocantins"
]

# Opções de áreas de atuação
areas_atuacao = ["Analista de Dados", "Cientista de Dados", "Engenheiro de Dados"]

# Opções de bibliotecas
bibliotecas = [
    "Pandas", "Pydantic", "scikit-learn", "Git", "Pandera", "streamlit",
    "postgres", "databricks", "AWS", "Azure", "airflow", "dbt",
    "Pyspark", "Polars", "Kafka", "Duckdb", "PowerBI", "Excel", "Tableau", "storm"
]

# Opções de horas codando
horas_codando = ["Menos de 5", "5-10", "10-20", "Mais de 20"]

# Opções de conforto com dados
conforto_dados = ["Desconfortável", "Neutro", "Confortável", "Muito Confortável"]

# Criação do formulário
with st.form("dados_enquete"):
    estado = st.selectbox("Estado", estados)
    area_atuacao = st.selectbox("Área de Atuação", areas_atuacao)
    bibliotecas_selecionadas = st.multiselect("Bibliotecas e ferramentas mais utilizadas", bibliotecas)
    horas_codando = st.selectbox("Horas Codando ao longo da semana", horas_codando)
    conforto_dados = st.selectbox("Conforto ao programar e trabalhar com dados", conforto_dados)
    experiencia_python = st.slider("Experiência de Python", 0, 10)
    experiencia_sql = st.slider("Experiência de SQL", 0, 10)
    experiencia_cloud = st.slider("Experiência em Cloud", 0, 10)

    # Botão para submeter o formulário
    submit_button = st.form_submit_button("Enviar")

# Se o botão foi clicado, salvar os dados no DataFrame e no CSV
if submit_button:
    novo_dado = {
        "Estado": estado,
        "Bibliotecas e ferramentas": ", ".join(bibliotecas_selecionadas),
        "Área de Atuação": area_atuacao,
        "Horas de Estudo": horas_codando,
        "Conforto com Dados": conforto_dados,
        "Experiência de Python": experiencia_python,
        "Experiência de SQL": experiencia_sql,
        "Experiência em Cloud": experiencia_cloud,
    }
    new_data = pd.DataFrame([novo_dado])

    # Verificar se o arquivo existe antes de tentar ler
    if os.path.exists(data_file):
        existing_data = pd.read_csv(data_file)
        updated_data = existing_data.append(new_data, ignore_index=True)
    else:
        updated_data = new_data
    
    # Salvar os dados no arquivo CSV
    updated_data.to_csv(data_file, index=False)
    st.success("Dados enviados com sucesso!")

st.write("Outside the form")


================================================
File: /09-streamlit-dashboard-realtime/projeto_coleta/coleta_postgres_psycopg2.py
================================================
import streamlit as st
import pandas as pd
import psycopg2
import os
from psycopg2 import sql
from dotenv import load_dotenv

# Carregar variáveis de ambiente
load_dotenv()

# Configuração do banco de dados
DB_HOST = os.getenv("DB_HOST")
DB_DATABASE = os.getenv("DB_DATABASE")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")

# Função para conectar ao banco de dados
def conectar_banco():
    try:
        conn = psycopg2.connect(
            host=DB_HOST,
            database=DB_DATABASE,
            user=DB_USER,
            password=DB_PASSWORD,
        )
        return conn
    except Exception as e:
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        return None

# Função para criar a tabela caso não exista
def criar_tabela_se_nao_existir(conn):
    try:
        with conn.cursor() as cur:
            create_table_query = """
            CREATE TABLE IF NOT EXISTS survey_data (
                id SERIAL PRIMARY KEY,
                estado VARCHAR(50),
                bibliotecas TEXT,
                area_atuacao VARCHAR(50),
                horas_estudo VARCHAR(20),
                conforto_dados VARCHAR(50),
                experiencia_python INTEGER,
                experiencia_sql INTEGER,
                experiencia_cloud INTEGER
            )
            """
            cur.execute(create_table_query)
            conn.commit()
    except Exception as e:
        st.error(f"Erro ao criar a tabela: {e}")

# Função para salvar dados no banco de dados
def salvar_dados_banco(conn, dados):
    try:
        with conn.cursor() as cur:
            insert_query = sql.SQL("""
                INSERT INTO survey_data (estado, bibliotecas, area_atuacao, horas_estudo, conforto_dados, experiencia_python, experiencia_sql, experiencia_cloud)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """)
            cur.execute(
                insert_query,
                (
                    dados["Estado"],
                    dados["Bibliotecas e ferramentas"],
                    dados["Área de Atuação"],
                    dados["Horas de Estudo"],
                    dados["Conforto com Dados"],
                    dados["Experiência de Python"],
                    dados["Experiência de SQL"],
                    dados["Experiência de Cloud"],
                ),
            )
            conn.commit()
    except Exception as e:
        st.error(f"Erro ao salvar os dados no banco de dados: {e}")
        conn.rollback()

# Conectando ao banco de dados e criando a tabela se necessário
conn = conectar_banco()
if conn is not None:
    criar_tabela_se_nao_existir(conn)

# Opções de estados, áreas de atuação, bibliotecas, horas codando e conforto com dados
estados = ["Acre", "Alagoas", "Amapá", "Amazonas", "Bahia", "Ceará",
           "Distrito Federal", "Espírito Santo", "Goiás", "Maranhão",
           "Mato Grosso", "Mato Grosso do Sul", "Minas Gerais", "Pará",
           "Paraíba", "Paraná", "Pernambuco", "Piauí", "Rio de Janeiro",
           "Rio Grande do Norte", "Rio Grande do Sul", "Rondônia", "Roraima",
           "Santa Catarina", "São Paulo", "Sergipe", "Tocantins"]

areas_atuacao = ["Analista de Dados", "Cientista de Dados", "Engenheiro de Dados"]

bibliotecas = ["Pandas", "Pydantic", "scikit-learn", "Git", "Pandera", "streamlit",
               "postgres", "databricks", "AWS", "Azure", "airflow", "dbt",
               "Pyspark", "Polars", "Kafka", "Duckdb", "PowerBI", "Excel", "Tableau", "storm"]

horas_codando = ["Menos de 5", "5-10", "10-20", "Mais de 20"]

conforto_dados = ["Desconfortável", "Neutro", "Confortável", "Muito Confortável"]

# Criação do formulário
with st.form("dados_enquete"):
    estado = st.selectbox("Estado", estados)
    area_atuacao = st.selectbox("Área de Atuação", areas_atuacao)
    bibliotecas_selecionadas = st.multiselect("Bibliotecas e ferramentas mais utilizadas", bibliotecas)
    horas_codando = st.selectbox("Horas Codando ao longo da semana", horas_codando)
    conforto_dados = st.selectbox("Conforto ao programar e trabalhar com dados", conforto_dados)
    experiencia_python = st.slider("Experiência de Python", 0, 10)
    experiencia_sql = st.slider("Experiência de SQL", 0, 10)
    experiencia_cloud = st.slider("Experiência em Cloud", 0, 10)

    # Botão para submeter o formulário
    submit_button = st.form_submit_button("Enviar")

# Se o botão foi clicado, salvar os dados no banco de dados
if submit_button:
    novo_dado = {
        "Estado": estado,
        "Bibliotecas e ferramentas": ", ".join(bibliotecas_selecionadas),
        "Área de Atuação": area_atuacao,
        "Horas de Estudo": horas_codando,
        "Conforto com Dados": conforto_dados,
        "Experiência de Python": experiencia_python,
        "Experiência de SQL": experiencia_sql,
        "Experiência de Cloud": experiencia_cloud,
    }
    salvar_dados_banco(conn, novo_dado)
    st.success("Dados enviados com sucesso!")

st.write("Outside the form")


================================================
File: /09-streamlit-dashboard-realtime/projeto_coleta/coleta_postgres_sqlalchemy.py
================================================
import streamlit as st
import pandas as pd
import os
from sqlalchemy import create_engine, Column, Integer, String, Text
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.orm import declarative_base, sessionmaker
from dotenv import load_dotenv

# Carregar variáveis de ambiente
load_dotenv()

# Construir a URL do banco de dados
DB_HOST = os.getenv("DB_HOST")
DB_DATABASE = os.getenv("DB_DATABASE")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_DATABASE}"

# Definindo a base e a tabela usando SQLAlchemy ORM
Base = declarative_base()

class SurveyData(Base):
    __tablename__ = 'survey_data'
    id = Column(Integer, primary_key=True, autoincrement=True)
    estado = Column(String(50))
    bibliotecas = Column(Text)
    area_atuacao = Column(String(50))
    horas_estudo = Column(String(20))
    conforto_dados = Column(String(50))
    experiencia_python = Column(Integer)
    experiencia_sql = Column(Integer)
    experiencia_cloud = Column(Integer)

def get_engine():
    try:
        engine = create_engine(DATABASE_URL)
        return engine
    except SQLAlchemyError as e:
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        return None

# Função para criar a tabela caso não exista
def criar_tabela_se_nao_existir(engine):
    try:
        Base.metadata.create_all(engine)
    except SQLAlchemyError as e:
        st.error(f"Erro ao criar a tabela: {e}")

# Função para salvar dados no banco de dados
def salvar_dados_banco(session, dados):
    try:
        novo_dado = SurveyData(
            estado=dados["Estado"],
            bibliotecas=dados["Bibliotecas e ferramentas"],
            area_atuacao=dados["Área de Atuação"],
            horas_estudo=dados["Horas de Estudo"],
            conforto_dados=dados["Conforto com Dados"],
            experiencia_python=dados["Experiência de Python"],
            experiencia_sql=dados["Experiência de SQL"],
            experiencia_cloud=dados["Experiência de Cloud"],
        )
        session.add(novo_dado)
        session.commit()
    except SQLAlchemyError as e:
        st.error(f"Erro ao salvar os dados no banco de dados: {e}")
        session.rollback()

# Obter a instância do engine e criar a tabela se necessário
engine = get_engine()
if engine is not None:
    criar_tabela_se_nao_existir(engine)

# Configurar a sessão do SQLAlchemy
Session = sessionmaker(bind=engine)

# Opções de estados, áreas de atuação, bibliotecas, horas codando e conforto com dados
estados = ["Acre", "Alagoas", "Amapá", "Amazonas", "Bahia", "Ceará",
           "Distrito Federal", "Espírito Santo", "Goiás", "Maranhão",
           "Mato Grosso", "Mato Grosso do Sul", "Minas Gerais", "Pará",
           "Paraíba", "Paraná", "Pernambuco", "Piauí", "Rio de Janeiro",
           "Rio Grande do Norte", "Rio Grande do Sul", "Rondônia", "Roraima",
           "Santa Catarina", "São Paulo", "Sergipe", "Tocantins"]

areas_atuacao = ["Analista de Dados", "Cientista de Dados", "Engenheiro de Dados"]

bibliotecas = ["Pandas", "Pydantic", "scikit-learn", "Git", "Pandera", "streamlit",
               "postgres", "databricks", "AWS", "Azure", "airflow", "dbt",
               "Pyspark", "Polars", "Kafka", "Duckdb", "PowerBI", "Excel", "Tableau", "storm"]

horas_codando = ["Menos de 5", "5-10", "10-20", "Mais de 20"]

conforto_dados = ["Desconfortável", "Neutro", "Confortável", "Muito Confortável"]

# Criação do formulário
with st.form("dados_enquete"):
    estado = st.selectbox("Estado", estados)
    area_atuacao = st.selectbox("Área de Atuação", areas_atuacao)
    bibliotecas_selecionadas = st.multiselect("Bibliotecas e ferramentas mais utilizadas", bibliotecas)
    horas_codando = st.selectbox("Horas Codando ao longo da semana", horas_codando)
    conforto_dados = st.selectbox("Conforto ao programar e trabalhar com dados", conforto_dados)
    experiencia_python = st.slider("Experiência de Python", 0, 10)
    experiencia_sql = st.slider("Experiência de SQL", 0, 10)
    experiencia_cloud = st.slider("Experiência em Cloud", 0, 10)

    # Botão para submeter o formulário
    submit_button = st.form_submit_button("Enviar")

# Se o botão foi clicado, salvar os dados no banco de dados
if submit_button:
    novo_dado = {
        "Estado": estado,
        "Bibliotecas e ferramentas": ", ".join(bibliotecas_selecionadas),
        "Área de Atuação": area_atuacao,
        "Horas de Estudo": horas_codando,
        "Conforto com Dados": conforto_dados,
        "Experiência de Python": experiencia_python,
        "Experiência de SQL": experiencia_sql,
        "Experiência de Cloud": experiencia_cloud,
    }
    session = Session()
    salvar_dados_banco(session, novo_dado)
    st.success("Dados enviados com sucesso!")

st.write("Outside the form")


================================================
File: /09-streamlit-dashboard-realtime/projeto_dash/dash.py
================================================
from collections import Counter

import folium
import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
from streamlit_folium import folium_static
from wordcloud import WordCloud

# Caminho para o arquivo CSV
DATA_PATH = "survey_data.csv"
IMAGE_PATH = "foto.png"

# Configurações iniciais
data = pd.read_csv(DATA_PATH)

# Listas e constantes
COMFORT_ORDER = [
    "Muito Desconfortável",
    "Desconfortável",
    "Neutro",
    "Confortável",
    "Muito Confortável",
]

STATES_COORDS = {
    "São Paulo": [-23.5505, -46.6333],
    "Rio de Janeiro": [-22.9068, -43.1729],
    "Minas Gerais": [-19.9167, -43.9345],
    "Bahia": [-12.9714, -38.5014],
    "Paraná": [-25.4284, -49.2733],
    "Rio Grande do Sul": [-30.0346, -51.2177],
    "Santa Catarina": [-27.5954, -48.5480],
    "Ceará": [-3.7172, -38.5434],
    "Distrito Federal": [-15.8267, -47.9218],
    "Pernambuco": [-8.0476, -34.8770],
    "Goiás": [-16.6869, -49.2648],
    "Pará": [-1.4558, -48.4902],
    "Mato Grosso": [-15.6014, -56.0979],
    "Amazonas": [-3.1190, -60.0217],
    "Espírito Santo": [-20.3155, -40.3128],
    "Paraíba": [-7.1195, -34.8450],
    "Acre": [-9.97499, -67.8243],
    # Adicione outras coordenadas dos estados aqui
}

# Funções de análise e visualização
def top_bibliotecas_por_area(data):
    st.header("Top 3 Bibliotecas por Área de Atuação")
    areas = data["Área de Atuação"].unique()
    area_selecionada = st.selectbox(
        "Selecione a Área de Atuação",
        ["Nenhuma área selecionada"] + list(areas),
    )

    if area_selecionada != "Nenhuma área selecionada":
        st.subheader(area_selecionada)
        bibliotecas_area = (
            data[data["Área de Atuação"] == area_selecionada]["Bibliotecas"]
            .str.cat(sep=",")
            .split(",")
        )
        bibliotecas_contagem = Counter(
            [biblioteca.strip() for biblioteca in bibliotecas_area]
        )
        top_3_bibliotecas = bibliotecas_contagem.most_common(3)

        col1, col2, col3 = st.columns(3)
        colunas = [col1, col2, col3]
        for i, (biblioteca, count) in enumerate(top_3_bibliotecas):
            with colunas[i]:
                st.metric(label=biblioteca, value=f"{count} vezes")


def plotar_grafico_area(data):
    data["Conforto com Dados"] = pd.Categorical(
        data["Conforto com Dados"], categories=COMFORT_ORDER, ordered=True
    )
    comfort_vs_study_hours = (
        data.groupby(["Conforto com Dados", "Horas de Estudo"], observed=True)
        .size()
        .unstack(fill_value=0)
    )
    comfort_vs_study_hours = comfort_vs_study_hours.reindex(
        columns=["Menos de 5", "5-10", "10-20", "Mais de 20"], fill_value=0
    )
    colors = [
        "#00008B",
        "#87CEEB",
        "#FF6347",
        "#FF0000",
    ]
    st.header("Nível de Conforto com Dados vs. Horas de Estudo por Semana")
    st.area_chart(comfort_vs_study_hours, color=colors)


def plotar_graficos_experiencia(data):
    st.header("Experiência Técnica dos Participantes")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.subheader("Experiência de Python")
        experiencia_python_count = (
            data["Experiência de Python"].value_counts().sort_index()
        )
        st.line_chart(experiencia_python_count)

    with col2:
        st.subheader("Experiência de SQL")
        experiencia_sql_count = (
            data["Experiência de SQL"].value_counts().sort_index()
        )
        st.bar_chart(experiencia_sql_count)

    with col3:
        st.subheader("Experiência em Cloud")
        experiencia_cloud_count = (
            data["Experiência em Cloud"].value_counts().sort_index()
        )
        st.area_chart(experiencia_cloud_count)


def plotar_mapa(data):
    st.header("Mapa do Brasil com Distribuição dos Participantes")
    state_participants = Counter(data["Estado"])
    map_data = pd.DataFrame(
        [
            {
                "Estado": state,
                "lat": coord[0],
                "lon": coord[1],
                "Participantes": state_participants[state],
            }
            for state, coord in STATES_COORDS.items()
            if state_participants[state] > 0  # Filtrar apenas estados com participantes
        ]
    )
    m = folium.Map(location=[-15.7801, -47.9292], zoom_start=4)
    for _, row in map_data.iterrows():
        folium.CircleMarker(
            location=[row["lat"], row["lon"]],
            radius=row["Participantes"] * 3,
            popup=f"{row['Estado']}: {row['Participantes']} participantes",
            color="crimson",
            fill=True,
            fill_color="crimson",
            weight=1,
        ).add_to(m)
    folium_static(m)


def plotar_nuvem_palavras(data):
    st.header("Nuvem de Palavras das Bibliotecas Utilizadas")
    all_libs = " ".join(data["Bibliotecas"].dropna().str.replace(",", " "))
    wordcloud = WordCloud(
        width=800, height=400, background_color="white"
    ).generate(all_libs)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud)
    ax.axis("off")
    st.pyplot(fig)


def exibir_imagem_final(image_path):
    st.header("Foto da Jornada")
    st.image(image_path, use_column_width=True)


# Execução das funções
st.title("Dados dos Participantes")
top_bibliotecas_por_area(data)
plotar_grafico_area(data)
plotar_graficos_experiencia(data)
plotar_mapa(data)
plotar_nuvem_palavras(data)
exibir_imagem_final(IMAGE_PATH)


================================================
File: /09-streamlit-dashboard-realtime/projeto_dash/dash_postgres.py
================================================
from collections import Counter

import folium
import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
from streamlit_folium import folium_static
from wordcloud import WordCloud
from sqlalchemy import create_engine
from dotenv import load_dotenv
import os

# Carregar variáveis de ambiente
load_dotenv()

# Configuração do banco de dados
DB_HOST = os.getenv("DB_HOST")
DB_DATABASE = os.getenv("DB_DATABASE")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")

# URL do banco de dados
DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_DATABASE}"

# Função para conectar ao banco de dados PostgreSQL usando SQLAlchemy
def conectar_banco():
    try:
        engine = create_engine(DATABASE_URL)
        return engine
    except Exception as e:
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        return None

# Função para carregar dados do banco de dados
def carregar_dados(engine):
    if engine:
        try:
            query = """
            SELECT
                estado AS "Estado",
                bibliotecas AS "Bibliotecas",
                area_atuacao AS "Área de Atuação",
                horas_estudo AS "Horas de Estudo",
                conforto_dados AS "Conforto com Dados",
                experiencia_python AS "Experiência de Python",
                experiencia_sql AS "Experiência de SQL",
                experiencia_cloud AS "Experiência em Cloud"
            FROM
                survey_data
            """
            data = pd.read_sql(query, engine)
            return data
        except Exception as e:
            st.error(f"Erro ao carregar os dados do banco de dados: {e}")
            return pd.DataFrame()
    return pd.DataFrame()

# Conectar ao banco de dados e carregar os dados
engine = conectar_banco()
data = carregar_dados(engine)

IMAGE_PATH = "foto.png"

# Listas e constantes
COMFORT_ORDER = [
    "Muito Desconfortável",
    "Desconfortável",
    "Neutro",
    "Confortável",
    "Muito Confortável",
]

STATES_COORDS = {
    "São Paulo": [-23.5505, -46.6333],
    "Rio de Janeiro": [-22.9068, -43.1729],
    "Minas Gerais": [-19.9167, -43.9345],
    "Bahia": [-12.9714, -38.5014],
    "Paraná": [-25.4284, -49.2733],
    "Rio Grande do Sul": [-30.0346, -51.2177],
    "Santa Catarina": [-27.5954, -48.5480],
    "Ceará": [-3.7172, -38.5434],
    "Distrito Federal": [-15.8267, -47.9218],
    "Pernambuco": [-8.0476, -34.8770],
    "Goiás": [-16.6869, -49.2648],
    "Pará": [-1.4558, -48.4902],
    "Mato Grosso": [-15.6014, -56.0979],
    "Amazonas": [-3.1190, -60.0217],
    "Espírito Santo": [-20.3155, -40.3128],
    "Paraíba": [-7.1195, -34.8450],
    "Acre": [-9.97499, -67.8243],
    # Adicione outras coordenadas dos estados aqui
}

# Funções de análise e visualização
def top_bibliotecas_por_area(data):
    st.header("Top 3 Bibliotecas por Área de Atuação")
    areas = data["Área de Atuação"].unique()
    area_selecionada = st.selectbox(
        "Selecione a Área de Atuação",
        ["Nenhuma área selecionada"] + list(areas),
    )

    if area_selecionada != "Nenhuma área selecionada":
        st.subheader(area_selecionada)
        bibliotecas_area = (
            data[data["Área de Atuação"] == area_selecionada]["Bibliotecas"]
            .str.cat(sep=",")
            .split(",")
        )
        bibliotecas_contagem = Counter(
            [biblioteca.strip() for biblioteca in bibliotecas_area]
        )
        top_3_bibliotecas = bibliotecas_contagem.most_common(3)

        col1, col2, col3 = st.columns(3)
        colunas = [col1, col2, col3]
        for i, (biblioteca, count) in enumerate(top_3_bibliotecas):
            with colunas[i]:
                st.metric(label=biblioteca, value=f"{count} vezes")


def plotar_grafico_area(data):
    data["Conforto com Dados"] = pd.Categorical(
        data["Conforto com Dados"], categories=COMFORT_ORDER, ordered=True
    )
    comfort_vs_study_hours = (
        data.groupby(["Conforto com Dados", "Horas de Estudo"], observed=True)
        .size()
        .unstack(fill_value=0)
    )
    comfort_vs_study_hours = comfort_vs_study_hours.reindex(
        columns=["Menos de 5", "5-10", "10-20", "Mais de 20"], fill_value=0
    )
    colors = [
        "#00008B",
        "#87CEEB",
        "#FF6347",
        "#FF0000",
    ]
    st.header("Nível de Conforto com Dados vs. Horas de Estudo por Semana")
    st.area_chart(comfort_vs_study_hours, color=colors)


def plotar_graficos_experiencia(data):
    st.header("Experiência Técnica dos Participantes")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.subheader("Experiência de Python")
        experiencia_python_count = (
            data["Experiência de Python"].value_counts().sort_index()
        )
        st.line_chart(experiencia_python_count)

    with col2:
        st.subheader("Experiência de SQL")
        experiencia_sql_count = (
            data["Experiência de SQL"].value_counts().sort_index()
        )
        st.bar_chart(experiencia_sql_count)

    with col3:
        st.subheader("Experiência em Cloud")
        experiencia_cloud_count = (
            data["Experiência em Cloud"].value_counts().sort_index()
        )
        st.area_chart(experiencia_cloud_count)


def plotar_mapa(data):
    st.header("Mapa do Brasil com Distribuição dos Participantes")
    state_participants = Counter(data["Estado"])
    map_data = pd.DataFrame(
        [
            {
                "Estado": state,
                "lat": coord[0],
                "lon": coord[1],
                "Participantes": state_participants[state],
            }
            for state, coord in STATES_COORDS.items()
            if state_participants[state] > 0  # Filtrar apenas estados com participantes
        ]
    )
    m = folium.Map(location=[-15.7801, -47.9292], zoom_start=4)
    for _, row in map_data.iterrows():
        folium.CircleMarker(
            location=[row["lat"], row["lon"]],
            radius=row["Participantes"] * 3,
            popup=f"{row['Estado']}: {row['Participantes']} participantes",
            color="crimson",
            fill=True,
            fill_color="crimson",
            weight=1,
        ).add_to(m)
    folium_static(m)


def plotar_nuvem_palavras(data):
    st.header("Nuvem de Palavras das Bibliotecas Utilizadas")
    all_libs = " ".join(data["Bibliotecas"].dropna().str.replace(",", " "))
    wordcloud = WordCloud(
        width=800, height=400, background_color="white"
    ).generate(all_libs)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud)
    ax.axis("off")
    st.pyplot(fig)


def exibir_imagem_final(image_path):
    st.header("Foto da Jornada")
    st.image(image_path, use_column_width=True)


# Execução das funções
st.title("Dados dos Participantes")
top_bibliotecas_por_area(data)
plotar_grafico_area(data)
plotar_graficos_experiencia(data)
plotar_mapa(data)
plotar_nuvem_palavras(data)
#exibir_imagem_final(IMAGE_PATH)


================================================
File: /09-streamlit-dashboard-realtime/projeto_dash/dash_postgres_cache.py
================================================
import os
from collections import Counter

import folium
import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from sqlalchemy import create_engine
from streamlit_folium import folium_static
from wordcloud import WordCloud

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()


# Função para conectar ao banco de dados PostgreSQL usando SQLAlchemy
@st.cache_resource
def conectar_banco():
    try:
        engine = create_engine(
            f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_DATABASE')}"
        )
        return engine
    except Exception as e:
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        return None


# Função para carregar dados do banco de dados
@st.cache_data(ttl=300)
def carregar_dados(_engine):
    if _engine:
        try:
            query = """
            SELECT
                estado AS "Estado",
                bibliotecas AS "Bibliotecas",
                area_atuacao AS "Área de Atuação",
                horas_estudo AS "Horas de Estudo",
                conforto_dados AS "Conforto com Dados",
                experiencia_python AS "Experiência de Python",
                experiencia_sql AS "Experiência de SQL",
                experiencia_cloud AS "Experiência em Cloud"
            FROM
                survey_data
            """
            data = pd.read_sql(query, _engine)
            return data
        except Exception as e:
            st.error(f"Erro ao carregar os dados do banco de dados: {e}")
            return pd.DataFrame()
    return pd.DataFrame()


# Função para exibir os dados
def exibir_dados(data):
    st.header("Dados dos Participantes")
    st.dataframe(data)


# Função para plotar gráfico de área
def plotar_grafico_area(data):
    comfort_order = [
        "Muito Desconfortável",
        "Desconfortável",
        "Neutro",
        "Confortável",
        "Muito Confortável",
    ]
    data["Conforto com Dados"] = pd.Categorical(
        data["Conforto com Dados"], categories=comfort_order, ordered=True
    )
    comfort_vs_study_hours = (
        data.groupby(["Conforto com Dados", "Horas de Estudo"], observed=True)
        .size()
        .unstack(fill_value=0)
    )
    comfort_vs_study_hours = comfort_vs_study_hours.reindex(
        columns=["Menos de 5", "5-10", "10-20", "Mais de 20"], fill_value=0
    )
    colors = [
        "#00008B",
        "#87CEEB",
        "#FF6347",
        "#FF0000",
    ]  # Dark Blue, Light Blue, Light Red, Red
    st.header("Nível de Conforto com Dados vs. Horas de Estudo por Semana")
    st.area_chart(comfort_vs_study_hours, color=colors)


# Função para plotar gráficos de experiência técnica
def plotar_graficos_experiencia(data):
    st.header("Experiência Técnica dos Participantes")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.subheader("Experiência de Python")
        experiencia_python_count = (
            data["Experiência de Python"].value_counts().sort_index()
        )
        st.line_chart(experiencia_python_count)

    with col2:
        st.subheader("Experiência de SQL")
        experiencia_sql_count = (
            data["Experiência de SQL"].value_counts().sort_index()
        )
        st.line_chart(experiencia_sql_count)

    with col3:
        st.subheader("Experiência em Cloud")
        experiencia_cloud_count = (
            data["Experiência em Cloud"].value_counts().sort_index()
        )
        st.line_chart(experiencia_cloud_count)


# Função para plotar o mapa do Brasil
def plotar_mapa(data):
    st.header("Mapa do Brasil com Distribuição dos Participantes")
    state_coords = {
        "Acre": [-9.97499, -67.8243],
        "Alagoas": [-9.5713, -36.7820],
        "Amapá": [0.0370, -51.0504],
        "Amazonas": [-3.1190, -60.0217],
        "Bahia": [-12.9714, -38.5014],
        "Ceará": [-3.7172, -38.5434],
        "Distrito Federal": [-15.8267, -47.9218],
        "Espírito Santo": [-20.3155, -40.3128],
        "Goiás": [-16.6869, -49.2648],
        "Maranhão": [-2.5307, -44.3068],
        "Mato Grosso": [-15.6014, -56.0979],
        "Mato Grosso do Sul": [-20.4697, -54.6201],
        "Minas Gerais": [-19.9167, -43.9345],
        "Pará": [-1.4558, -48.4902],
        "Paraíba": [-7.1195, -34.8450],
        "Paraná": [-25.4284, -49.2733],
        "Pernambuco": [-8.0476, -34.8770],
        "Piauí": [-5.0892, -42.8016],
        "Rio de Janeiro": [-22.9068, -43.1729],
        "Rio Grande do Norte": [-5.7945, -35.2110],
        "Rio Grande do Sul": [-30.0346, -51.2177],
        "Rondônia": [-8.7612, -63.9039],
        "Roraima": [2.8235, -60.6758],
        "Santa Catarina": [-27.5954, -48.5480],
        "São Paulo": [-23.5505, -46.6333],
        "Sergipe": [-10.9472, -37.0731],
        "Tocantins": [-10.1844, -48.3336],
    }
    state_participants = Counter(data["Estado"])
    map_data = pd.DataFrame(
        [
            {
                "Estado": state,
                "lat": coord[0],
                "lon": coord[1],
                "Participantes": state_participants[state],
            }
            for state, coord in state_coords.items()
            if state_participants[state]
            > 0  # Filtrar apenas estados com participantes
        ]
    )
    m = folium.Map(location=[-15.7801, -47.9292], zoom_start=4)
    for _, row in map_data.iterrows():
        folium.CircleMarker(
            location=[row["lat"], row["lon"]],
            radius=row["Participantes"]
            * 10,  # Ajustar o raio proporcionalmente
            popup=f"{row['Estado']}: {row['Participantes']} participantes",
            color="crimson",
            fill=True,
            fill_color="crimson",
            weight=1,  # Ajustar a espessura do contorno do círculo
        ).add_to(m)
    folium_static(m)


# Função para plotar nuvem de palavras
def plotar_nuvem_palavras(data):
    st.header("Nuvem de Palavras das Bibliotecas Utilizadas")
    all_libs = " ".join(data["Bibliotecas"].dropna().str.replace(",", " "))
    wordcloud = WordCloud(
        width=800, height=400, background_color="white"
    ).generate(all_libs)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation="bilinear")
    ax.axis("off")  # remove o quadro em volta
    st.pyplot(fig)  # gera a imagem no streamlit
    plt.close(fig)


def top_bibliotecas_por_area(data):
    st.header("Top 3 Bibliotecas por Área de Atuação")
    areas = data["Área de Atuação"].unique()
    area_selecionada = st.selectbox(
        "Selecione a Área de Atuação",
        ["Nenhuma área selecionada"] + list(areas),
    )

    if area_selecionada != "Nenhuma área selecionada":
        st.subheader(area_selecionada)
        bibliotecas_area = (
            data[data["Área de Atuação"] == area_selecionada]["Bibliotecas"]
            .str.cat(sep=",")
            .split(",")
        )
        bibliotecas_contagem = Counter(bibliotecas_area)
        top_3_bibliotecas = bibliotecas_contagem.most_common(3)
        col1, col2, col3 = st.columns(3)
        colunas = [col1, col2, col3]
        for col, (biblioteca, count) in zip(colunas, top_3_bibliotecas):
            with col:
                st.metric(label=biblioteca.strip(), value=f"{count} vezes")


# Função para exibir uma imagem no final
def exibir_imagem_final(image_path):
    st.header("Foto da Jornada")
    st.image(image_path, use_column_width=True)


# Carregar os dados do banco de dados
engine = conectar_banco()
data = carregar_dados(engine)

# Chamar as funções para exibir os dados e gráficos
st.title("Geração de Gráficos da Enquete")
exibir_dados(data)
plotar_grafico_area(data)
plotar_graficos_experiencia(data)
plotar_mapa(data)
plotar_nuvem_palavras(data)
top_bibliotecas_por_area(data)
exibir_imagem_final("foto.png")


================================================
File: /09-streamlit-dashboard-realtime/survey/README.md
================================================
## Vantagens de Utilizar o Streamlit

Streamlit é uma biblioteca de código aberto em Python que torna extremamente fácil criar e compartilhar aplicativos web de dados. Uma das grandes vantagens do Streamlit é a sua capacidade de integrar diversas bibliotecas gráficas populares do Python, proporcionando uma experiência visual rica e interativa. Aqui estão algumas vantagens e destaques dessas integrações:

### 1. Matplotlib

#### O que é?
Matplotlib é uma biblioteca de plotagem 2D extremamente popular em Python, usada para criar gráficos estáticos, animados e interativos. É altamente configurável e permite criar visualizações complexas com facilidade.

#### Vantagens no Streamlit
- **Facilidade de Integração**: Com Streamlit, você pode exibir gráficos Matplotlib de forma simples utilizando `st.pyplot()`.
- **Interatividade**: Streamlit permite adicionar interatividade aos seus gráficos Matplotlib sem a necessidade de configurar um ambiente web completo.
- **Visualizações Poderosas**: Combine a flexibilidade do Matplotlib com a simplicidade do Streamlit para criar dashboards poderosos e visualizações de dados detalhadas.

### 2. Folium

#### O que é?
Folium é uma biblioteca que facilita a visualização de dados geoespaciais utilizando Leaflet.js. Com Folium, você pode criar mapas interativos e adicionar marcadores, camadas e outras funcionalidades.

#### Vantagens no Streamlit
- **Mapas Interativos**: Streamlit e Folium juntos permitem a criação de mapas interativos dentro de aplicativos web, ideais para análises geoespaciais.
- **Visualização de Dados Geográficos**: Exiba dados geográficos e geolocalizados diretamente no navegador, facilitando a compreensão e análise espacial.
- **Simplicidade**: Adicione mapas interativos aos seus aplicativos com poucas linhas de código usando `folium_static()`.

### 3. WordCloud

#### O que é?
WordCloud é uma biblioteca para a geração de nuvens de palavras a partir de texto. As nuvens de palavras são uma forma visual de representar a frequência ou importância de palavras em um texto, com palavras mais frequentes aparecendo maiores.

#### Vantagens no Streamlit
- **Visualização de Texto**: Utilize WordCloud com Streamlit para criar representações visuais atraentes de dados textuais.
- **Facilidade de Uso**: Crie e exiba nuvens de palavras rapidamente usando `WordCloud` e `st.pyplot()`.
- **Análise Textual**: Ideal para visualizar e explorar grandes volumes de texto de maneira intuitiva.

### Conclusão

Streamlit se destaca como uma ferramenta poderosa para criar aplicativos de dados interativos, integrando facilmente bibliotecas gráficas populares como Matplotlib, Folium e WordCloud. Estas integrações permitem aos desenvolvedores focar na análise e visualização de dados, sem a necessidade de se preocupar com a infraestrutura web subjacente. Com Streamlit, você pode transformar scripts de dados em aplicativos web compartilháveis em minutos, facilitando a disseminação e compreensão de insights valiosos.

### Exemplo de Código

```python
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import folium
from streamlit_folium import folium_static
from wordcloud import WordCloud

# Exemplo de gráfico Matplotlib
st.header("Gráfico Matplotlib")
fig, ax = plt.subplots()
ax.plot([1, 2, 3, 4], [10, 20, 25, 30])
st.pyplot(fig)

# Exemplo de mapa Folium
st.header("Mapa Folium")
m = folium.Map(location=[45.5236, -122.6750], zoom_start=13)
folium.Marker([45.5236, -122.6750], popup="The Waterfront").add_to(m)
folium_static(m)

# Exemplo de nuvem de palavras WordCloud
st.header("Nuvem de Palavras")
text = "Python Streamlit Matplotlib Folium WordCloud"
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
fig, ax = plt.subplots()
ax.imshow(wordcloud, interpolation='bilinear')
ax.axis("off")
st.pyplot(fig)
```

Streamlit transforma a maneira como você trabalha com dados, tornando a criação de visualizações e aplicativos de dados mais acessível e eficiente.

================================================
File: /09-streamlit-dashboard-realtime/survey/dash.py
================================================
from collections import Counter

import folium
import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
from streamlit_folium import folium_static
from wordcloud import WordCloud

# Carregar dados do CSV
data_path = "survey_data.csv"  # caminho ajustado para o seu arquivo CSV
data = pd.read_csv(data_path)


def top_bibliotecas_por_area(data):
    st.header("Top 3 Bibliotecas por Área de Atuação")
    areas = data["Área de Atuação"].unique()
    area_selecionada = st.selectbox(
        "Selecione a Área de Atuação",
        ["Nenhuma área selecionada"] + list(areas),
    )

    if area_selecionada != "Nenhuma área selecionada":
        st.subheader(area_selecionada)
        bibliotecas_area = (
            data[data["Área de Atuação"] == area_selecionada]["Bibliotecas"]
            .str.cat(sep=",")
            .split(",")
        )
        bibliotecas_contagem = Counter(
            [biblioteca.strip() for biblioteca in bibliotecas_area]
        )
        top_3_bibliotecas = bibliotecas_contagem.most_common(3)

        col1, col2, col3 = st.columns(3)
        colunas = [col1, col2, col3]
        for i, (biblioteca, count) in enumerate(top_3_bibliotecas):
            with colunas[i]:
                st.metric(label=biblioteca, value=f"{count} vezes")


# Função para plotar gráfico de área
def plotar_grafico_area(data):
    comfort_order = [
        "Muito Desconfortável",
        "Desconfortável",
        "Neutro",
        "Confortável",
        "Muito Confortável",
    ]
    data["Conforto com Dados"] = pd.Categorical(
        data["Conforto com Dados"], categories=comfort_order, ordered=True
    )
    comfort_vs_study_hours = (
        data.groupby(["Conforto com Dados", "Horas de Estudo"], observed=True)
        .size()
        .unstack(fill_value=0)
    )
    comfort_vs_study_hours = comfort_vs_study_hours.reindex(
        columns=["Menos de 5", "5-10", "10-20", "Mais de 20"], fill_value=0
    )
    colors = [
        "#00008B",
        "#87CEEB",
        "#FF6347",
        "#FF0000",
    ]  # Dark Blue, Light Blue, Light Red, Red
    st.header("Nível de Conforto com Dados vs. Horas de Estudo por Semana")
    st.area_chart(comfort_vs_study_hours, color=colors)


# Função para plotar gráficos de experiência técnica
def plotar_graficos_experiencia(data):
    st.header("Experiência Técnica dos Participantes")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.subheader("Experiência de Python")
        experiencia_python_count = (
            data["Experiência de Python"].value_counts().sort_index()
        )
        st.line_chart(experiencia_python_count)

    with col2:
        st.subheader("Experiência de SQL")
        experiencia_sql_count = (
            data["Experiência de SQL"].value_counts().sort_index()
        )
        st.bar_chart(experiencia_sql_count)

    with col3:
        st.subheader("Experiência em Cloud")
        experiencia_cloud_count = (
            data["Experiência em Cloud"].value_counts().sort_index()
        )
        st.area_chart(experiencia_cloud_count)


# Função para plotar o mapa do Brasil
def plotar_mapa(data):
    st.header("Mapa do Brasil com Distribuição dos Participantes")
    state_coords = {
        "São Paulo": [-23.5505, -46.6333],
        "Rio de Janeiro": [-22.9068, -43.1729],
        "Minas Gerais": [-19.9167, -43.9345],
        "Bahia": [-12.9714, -38.5014],
        "Paraná": [-25.4284, -49.2733],
        "Rio Grande do Sul": [-30.0346, -51.2177],
        "Santa Catarina": [-27.5954, -48.5480],
        "Ceará": [-3.7172, -38.5434],
        "Distrito Federal": [-15.8267, -47.9218],
        "Pernambuco": [-8.0476, -34.8770],
        "Goiás": [-16.6869, -49.2648],
        "Pará": [-1.4558, -48.4902],
        "Mato Grosso": [-15.6014, -56.0979],
        "Amazonas": [-3.1190, -60.0217],
        "Espírito Santo": [-20.3155, -40.3128],
        "Paraíba": [-7.1195, -34.8450],
        "Acre": [-9.97499, -67.8243],
        # Adicione outras coordenadas dos estados aqui
    }
    state_participants = Counter(data["Estado"])
    map_data = pd.DataFrame(
        [
            {
                "Estado": state,
                "lat": coord[0],
                "lon": coord[1],
                "Participantes": state_participants[state],
            }
            for state, coord in state_coords.items()
        ]
    )
    m = folium.Map(location=[-15.7801, -47.9292], zoom_start=4)
    for _, row in map_data.iterrows():
        folium.CircleMarker(
            location=[row["lat"], row["lon"]],
            radius=row["Participantes"]
            * 3,  # Ajustar o raio proporcionalmente
            popup=f"{row['Estado']}: {row['Participantes']} participantes",
            color="crimson",
            fill=True,
            fill_color="crimson",
            weight=1,  # Ajustar a espessura do contorno do círculo
        ).add_to(m)
    folium_static(m)


# Função para plotar nuvem de palavras
def plotar_nuvem_palavras(data):
    st.header("Nuvem de Palavras das Bibliotecas Utilizadas")
    all_libs = " ".join(data["Bibliotecas"].dropna().str.replace(",", " "))
    wordcloud = WordCloud(
        width=800, height=400, background_color="white"
    ).generate(all_libs)
    fig, ax = plt.subplots(figsize=(10, 5))
    print(ax)
    ax.imshow(wordcloud)
    ax.axis("off")  # remove o quadro em volta
    st.pyplot(fig)  # gera a imagem no streamlit


# Função para exibir uma imagem no final
def exibir_imagem_final(image_path):
    st.header("Foto da Jornada")
    st.image(image_path, use_column_width=True)


# Chamar as funções
st.title("Dados dos Participantes")
top_bibliotecas_por_area(data)
plotar_grafico_area(data)
plotar_graficos_experiencia(data)
plotar_mapa(data)
plotar_nuvem_palavras(data)
exibir_imagem_final("foto.png")


================================================
File: /09-streamlit-dashboard-realtime/survey/dash_postgres.py
================================================
import os
from collections import Counter

import folium
import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from sqlalchemy import create_engine
from streamlit_folium import folium_static
from wordcloud import WordCloud

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()


# Função para conectar ao banco de dados PostgreSQL usando SQLAlchemy
def conectar_banco():
    try:
        engine = create_engine(
            f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_DATABASE')}"
        )
        return engine
    except Exception as e:
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        return None


# Função para carregar dados do banco de dados
def carregar_dados(engine):
    if engine:
        try:
            query = """
            SELECT
                estado AS "Estado",
                bibliotecas AS "Bibliotecas",
                area_atuacao AS "Área de Atuação",
                horas_estudo AS "Horas de Estudo",
                conforto_dados AS "Conforto com Dados",
                experiencia_python AS "Experiência de Python",
                experiencia_sql AS "Experiência de SQL",
                experiencia_cloud AS "Experiência em Cloud"
            FROM
                survey_data
            """
            data = pd.read_sql(query, engine)
            return data
        except Exception as e:
            st.error(f"Erro ao carregar os dados do banco de dados: {e}")
            return pd.DataFrame()
    return pd.DataFrame()


# Função para exibir os dados
def exibir_dados(data):
    st.header("Dados dos Participantes")
    st.dataframe(data)


# Função para plotar gráfico de área
def plotar_grafico_area(data):
    comfort_order = [
        "Muito Desconfortável",
        "Desconfortável",
        "Neutro",
        "Confortável",
        "Muito Confortável",
    ]
    data["Conforto com Dados"] = pd.Categorical(
        data["Conforto com Dados"], categories=comfort_order, ordered=True
    )
    comfort_vs_study_hours = (
        data.groupby(["Conforto com Dados", "Horas de Estudo"], observed=True)
        .size()
        .unstack(fill_value=0)
    )
    comfort_vs_study_hours = comfort_vs_study_hours.reindex(
        columns=["Menos de 5", "5-10", "10-20", "Mais de 20"], fill_value=0
    )
    colors = [
        "#00008B",
        "#87CEEB",
        "#FF6347",
        "#FF0000",
    ]  # Dark Blue, Light Blue, Light Red, Red
    st.header("Nível de Conforto com Dados vs. Horas de Estudo por Semana")
    st.area_chart(comfort_vs_study_hours, color=colors)


# Função para plotar gráficos de experiência técnica
def plotar_graficos_experiencia(data):
    st.header("Experiência Técnica dos Participantes")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.subheader("Experiência de Python")
        experiencia_python_count = (
            data["Experiência de Python"].value_counts().sort_index()
        )
        st.line_chart(experiencia_python_count)

    with col2:
        st.subheader("Experiência de SQL")
        experiencia_sql_count = (
            data["Experiência de SQL"].value_counts().sort_index()
        )
        st.line_chart(experiencia_sql_count)

    with col3:
        st.subheader("Experiência em Cloud")
        experiencia_cloud_count = (
            data["Experiência em Cloud"].value_counts().sort_index()
        )
        st.line_chart(experiencia_cloud_count)


# Função para plotar o mapa do Brasil
def plotar_mapa(data):
    st.header("Mapa do Brasil com Distribuição dos Participantes")
    state_coords = {
        "São Paulo": [-23.5505, -46.6333],
        "Rio de Janeiro": [-22.9068, -43.1729],
        "Minas Gerais": [-19.9167, -43.9345],
        "Bahia": [-12.9714, -38.5014],
        "Paraná": [-25.4284, -49.2733],
        "Rio Grande do Sul": [-30.0346, -51.2177],
        "Santa Catarina": [-27.5954, -48.5480],
        "Ceará": [-3.7172, -38.5434],
        "Distrito Federal": [-15.8267, -47.9218],
        "Pernambuco": [-8.0476, -34.8770],
        "Goiás": [-16.6869, -49.2648],
        "Pará": [-1.4558, -48.4902],
        "Mato Grosso": [-15.6014, -56.0979],
        "Amazonas": [-3.1190, -60.0217],
        "Espírito Santo": [-20.3155, -40.3128],
        "Paraíba": [-7.1195, -34.8450],
        "Acre": [-9.97499, -67.8243],
        # Adicione outras coordenadas dos estados aqui
    }
    state_participants = Counter(data["Estado"])
    map_data = pd.DataFrame(
        [
            {
                "Estado": state,
                "lat": coord[0],
                "lon": coord[1],
                "Participantes": state_participants[state],
            }
            for state, coord in state_coords.items()
        ]
    )
    m = folium.Map(location=[-15.7801, -47.9292], zoom_start=4)
    for _, row in map_data.iterrows():
        folium.CircleMarker(
            location=[row["lat"], row["lon"]],
            radius=row["Participantes"]
            * 3,  # Ajustar o raio proporcionalmente
            popup=f"{row['Estado']}: {row['Participantes']} participantes",
            color="crimson",
            fill=True,
            fill_color="crimson",
            weight=1,  # Ajustar a espessura do contorno do círculo
        ).add_to(m)
    folium_static(m)


# Função para plotar nuvem de palavras
def plotar_nuvem_palavras(data):
    st.header("Nuvem de Palavras das Bibliotecas Utilizadas")
    all_libs = " ".join(data["Bibliotecas"].dropna().str.replace(",", " "))
    wordcloud = WordCloud(
        width=800, height=400, background_color="white"
    ).generate(all_libs)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation="bilinear")
    ax.axis("off")  # remove o quadro em volta
    st.pyplot(fig)  # gera a imagem no streamlit


# Função para exibir o top 3 bibliotecas por área de atuação
def top_bibliotecas_por_area(data):
    st.header("Top 3 Bibliotecas por Área de Atuação")
    areas = data["Área de Atuação"].unique()
    area_selecionada = st.selectbox(
        "Selecione a Área de Atuação",
        ["Nenhuma área selecionada"] + list(areas),
    )

    if area_selecionada != "Nenhuma área selecionada":
        st.subheader(area_selecionada)
        bibliotecas_area = (
            data[data["Área de Atuação"] == area_selecionada]["Bibliotecas"]
            .str.cat(sep=",")
            .split(",")
        )
        bibliotecas_contagem = Counter(bibliotecas_area)
        top_3_bibliotecas = bibliotecas_contagem.most_common(3)
        col1, col2, col3 = st.columns(3)
        colunas = [col1, col2, col3]
        for col, (biblioteca, count) in zip(colunas, top_3_bibliotecas):
            with col:
                st.metric(label=biblioteca.strip(), value=f"{count} vezes")


# Função para exibir uma imagem no final
def exibir_imagem_final(image_path):
    st.header("Foto da Jornada")
    st.image(image_path, use_column_width=True)


# Carregar os dados do banco de dados
engine = conectar_banco()
data = carregar_dados(engine)

# Chamar as funções para exibir os dados e gráficos
st.title("Geração de Gráficos da Enquete")
exibir_dados(data)
plotar_grafico_area(data)
plotar_graficos_experiencia(data)
plotar_mapa(data)
plotar_nuvem_palavras(data)
top_bibliotecas_por_area(data)
exibir_imagem_final("foto.png")


================================================
File: /09-streamlit-dashboard-realtime/survey/dash_postgres_cache.py
================================================
import os
from collections import Counter

import folium
import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from sqlalchemy import create_engine
from streamlit_folium import folium_static
from wordcloud import WordCloud

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()


# Função para conectar ao banco de dados PostgreSQL usando SQLAlchemy
@st.cache_resource
def conectar_banco():
    try:
        engine = create_engine(
            f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_DATABASE')}"
        )
        return engine
    except Exception as e:
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        return None


# Função para carregar dados do banco de dados
@st.cache_data(ttl=300)
def carregar_dados(_engine):
    if _engine:
        try:
            query = """
            SELECT
                estado AS "Estado",
                bibliotecas AS "Bibliotecas",
                area_atuacao AS "Área de Atuação",
                horas_estudo AS "Horas de Estudo",
                conforto_dados AS "Conforto com Dados",
                experiencia_python AS "Experiência de Python",
                experiencia_sql AS "Experiência de SQL",
                experiencia_cloud AS "Experiência em Cloud"
            FROM
                survey_data
            """
            data = pd.read_sql(query, _engine)
            return data
        except Exception as e:
            st.error(f"Erro ao carregar os dados do banco de dados: {e}")
            return pd.DataFrame()
    return pd.DataFrame()


# Função para exibir os dados
def exibir_dados(data):
    st.header("Dados dos Participantes")
    st.dataframe(data)


# Função para plotar gráfico de área
def plotar_grafico_area(data):
    comfort_order = [
        "Muito Desconfortável",
        "Desconfortável",
        "Neutro",
        "Confortável",
        "Muito Confortável",
    ]
    data["Conforto com Dados"] = pd.Categorical(
        data["Conforto com Dados"], categories=comfort_order, ordered=True
    )
    comfort_vs_study_hours = (
        data.groupby(["Conforto com Dados", "Horas de Estudo"], observed=True)
        .size()
        .unstack(fill_value=0)
    )
    comfort_vs_study_hours = comfort_vs_study_hours.reindex(
        columns=["Menos de 5", "5-10", "10-20", "Mais de 20"], fill_value=0
    )
    colors = [
        "#00008B",
        "#87CEEB",
        "#FF6347",
        "#FF0000",
    ]  # Dark Blue, Light Blue, Light Red, Red
    st.header("Nível de Conforto com Dados vs. Horas de Estudo por Semana")
    st.area_chart(comfort_vs_study_hours, color=colors)


# Função para plotar gráficos de experiência técnica
def plotar_graficos_experiencia(data):
    st.header("Experiência Técnica dos Participantes")
    col1, col2, col3 = st.columns(3)

    with col1:
        st.subheader("Experiência de Python")
        experiencia_python_count = (
            data["Experiência de Python"].value_counts().sort_index()
        )
        st.line_chart(experiencia_python_count)

    with col2:
        st.subheader("Experiência de SQL")
        experiencia_sql_count = (
            data["Experiência de SQL"].value_counts().sort_index()
        )
        st.line_chart(experiencia_sql_count)

    with col3:
        st.subheader("Experiência em Cloud")
        experiencia_cloud_count = (
            data["Experiência em Cloud"].value_counts().sort_index()
        )
        st.line_chart(experiencia_cloud_count)


# Função para plotar o mapa do Brasil
def plotar_mapa(data):
    st.header("Mapa do Brasil com Distribuição dos Participantes")
    state_coords = {
        "Acre": [-9.97499, -67.8243],
        "Alagoas": [-9.5713, -36.7820],
        "Amapá": [0.0370, -51.0504],
        "Amazonas": [-3.1190, -60.0217],
        "Bahia": [-12.9714, -38.5014],
        "Ceará": [-3.7172, -38.5434],
        "Distrito Federal": [-15.8267, -47.9218],
        "Espírito Santo": [-20.3155, -40.3128],
        "Goiás": [-16.6869, -49.2648],
        "Maranhão": [-2.5307, -44.3068],
        "Mato Grosso": [-15.6014, -56.0979],
        "Mato Grosso do Sul": [-20.4697, -54.6201],
        "Minas Gerais": [-19.9167, -43.9345],
        "Pará": [-1.4558, -48.4902],
        "Paraíba": [-7.1195, -34.8450],
        "Paraná": [-25.4284, -49.2733],
        "Pernambuco": [-8.0476, -34.8770],
        "Piauí": [-5.0892, -42.8016],
        "Rio de Janeiro": [-22.9068, -43.1729],
        "Rio Grande do Norte": [-5.7945, -35.2110],
        "Rio Grande do Sul": [-30.0346, -51.2177],
        "Rondônia": [-8.7612, -63.9039],
        "Roraima": [2.8235, -60.6758],
        "Santa Catarina": [-27.5954, -48.5480],
        "São Paulo": [-23.5505, -46.6333],
        "Sergipe": [-10.9472, -37.0731],
        "Tocantins": [-10.1844, -48.3336],
    }
    state_participants = Counter(data["Estado"])
    map_data = pd.DataFrame(
        [
            {
                "Estado": state,
                "lat": coord[0],
                "lon": coord[1],
                "Participantes": state_participants[state],
            }
            for state, coord in state_coords.items()
            if state_participants[state]
            > 0  # Filtrar apenas estados com participantes
        ]
    )
    m = folium.Map(location=[-15.7801, -47.9292], zoom_start=4)
    for _, row in map_data.iterrows():
        folium.CircleMarker(
            location=[row["lat"], row["lon"]],
            radius=row["Participantes"]
            * 10,  # Ajustar o raio proporcionalmente
            popup=f"{row['Estado']}: {row['Participantes']} participantes",
            color="crimson",
            fill=True,
            fill_color="crimson",
            weight=1,  # Ajustar a espessura do contorno do círculo
        ).add_to(m)
    folium_static(m)


# Função para plotar nuvem de palavras
def plotar_nuvem_palavras(data):
    st.header("Nuvem de Palavras das Bibliotecas Utilizadas")
    all_libs = " ".join(data["Bibliotecas"].dropna().str.replace(",", " "))
    wordcloud = WordCloud(
        width=800, height=400, background_color="white"
    ).generate(all_libs)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation="bilinear")
    ax.axis("off")  # remove o quadro em volta
    st.pyplot(fig)  # gera a imagem no streamlit
    plt.close(fig)


def top_bibliotecas_por_area(data):
    st.header("Top 3 Bibliotecas por Área de Atuação")
    areas = data["Área de Atuação"].unique()
    area_selecionada = st.selectbox(
        "Selecione a Área de Atuação",
        ["Nenhuma área selecionada"] + list(areas),
    )

    if area_selecionada != "Nenhuma área selecionada":
        st.subheader(area_selecionada)
        bibliotecas_area = (
            data[data["Área de Atuação"] == area_selecionada]["Bibliotecas"]
            .str.cat(sep=",")
            .split(",")
        )
        bibliotecas_contagem = Counter(bibliotecas_area)
        top_3_bibliotecas = bibliotecas_contagem.most_common(3)
        col1, col2, col3 = st.columns(3)
        colunas = [col1, col2, col3]
        for col, (biblioteca, count) in zip(colunas, top_3_bibliotecas):
            with col:
                st.metric(label=biblioteca.strip(), value=f"{count} vezes")


# Função para exibir uma imagem no final
def exibir_imagem_final(image_path):
    st.header("Foto da Jornada")
    st.image(image_path, use_column_width=True)


# Carregar os dados do banco de dados
engine = conectar_banco()
data = carregar_dados(engine)

# Chamar as funções para exibir os dados e gráficos
st.title("Geração de Gráficos da Enquete")
exibir_dados(data)
plotar_grafico_area(data)
plotar_graficos_experiencia(data)
plotar_mapa(data)
plotar_nuvem_palavras(data)
top_bibliotecas_por_area(data)
exibir_imagem_final("foto.png")


================================================
File: /09-streamlit-dashboard-realtime/survey/exemplo.env
================================================
# .env
DB_HOST=seu_host
DB_DATABASE=seu_banco
DB_USER=seu_usuario
DB_PASSWORD=sua_senha

================================================
File: /Bootcamp - Cloud para dados/README.md
================================================
## Próximos treinamentos

![pics](/pics/bootcamp_cloud.png)

Para entregar valor ao negócio, é fundamental que nossas aplicações, dashboards, bancos de dados e modelos estejam em produção, ou seja, em uso pelo cliente.

Se você já tentou implementar um serviço em Cloud na AWS, Azure ou GCP, sabe que há muitos desafios envolvidos: desde padrões a seguir, passando por configurações de IAM (acesso), redes privadas, até a implantação de máquinas virtuais e instalação de Docker.

Muitas vezes, profissionais focados em dados e aplicações não possuem esse conhecimento especializado.

O Bootcamp de Cloud para dados foi criado para preencher essa lacuna. É um curso que fornece os elementos essenciais de Cloud para que você possa implantar suas aplicações de forma independente.

Começamos do zero, com foco em quem não possui experiência em Cloud, e seguimos passo a passo até a implantação das suas primeiras aplicações. Durante o curso, construiremos cinco aplicações em diferentes plataformas de Cloud.

Se você deseja entender VPC, EC2, e toda a infraestrutura necessária para subir suas aplicações, este Bootcamp é ideal para você.

O Bootcamp começará no dia 22 de agosto e terá duração até o dia 04 de outubro.

As aulas serão ao vivo, sempre às 12h (meio-dia), com duração de 1 hora e 20 minutos. Além disso, todas as aulas serão disponibilizadas na nossa plataforma no mesmo dia, para que você possa assisti-las quantas vezes desejar.

Para fazer parte desse bootcamp, a inscrição será liberada no dia 20/08 ás 20h em nosso canal no Youtube

[![abertura](/pics/capa_youtube_bootcamp_cloud.png)](https://www.youtube.com/watch?v=GqgWGZtC-3w)

## Conteúdo completo Bootcamp de cloud

Aqui está o calendário completo de aula (podendo sofrer alterações ao longo do curso)

Aqui está a tabela atualizada com os nomes dos serviços específicos do Azure e GCP nos respectivos projetos:

| Acesso e início   | Tema da Aula                                            | Objetivo da Aula                                                                                      | Principais Tecnologias Abordadas                          | Carga Horária |
|-------------------|---------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------|---------------|
| 23/08/2024 (Quinta) | Introdução a Cloud, criando nossa conta e publicando um site na AWS | Introduzir os conceitos básicos de computação em nuvem, criar uma conta AWS e publicar um site simples. | Amazon S3, Amazon Route 53                                | 1h20          |
| 24/08/2024 (Sexta)  | Serviços de Armazenamento na Nuvem                    | Entender e gerenciar serviços de armazenamento na nuvem, garantindo segurança e escalabilidade para os dados empresariais. | Amazon S3                                                 | 1h20          |
| 26/08/2024 (Segunda)| Computação em Nuvem - VMs                             | Explorar a criação e uso de VMs para aplicações escaláveis na nuvem.                                    | Amazon EC2                                                | 1h20          |
| 27/08/2024 (Terça)  | Projeto EC2 e S3                                      | Implementar um projeto integrando EC2 e S3 para armazenar e processar dados de forma eficiente.         | Amazon EC2, Amazon S3                                     | 1h20          |
| 29/08/2024 (Quinta) | Gerenciamento de Segurança IAM                        | Configurar e gerenciar identidades e acessos com segurança usando IAM.                                  | AWS IAM                                                   | 1h20          |
| 30/08/2024 (Sexta)  | Gerenciamento de Redes VPC                            | Configurar redes virtuais na nuvem para otimizar a comunicação entre serviços.                          | Amazon VPC                                                | 1h20          |
| 02/09/2024 (Segunda)| Banco de Dados Relacional                             | Aprender a configurar e gerenciar bancos de dados relacionais na nuvem.                                 | Amazon RDS                                                | 1h20          |
| 03/09/2024 (Terça)  | Projeto Frontend e RDS                                | Desenvolver um frontend que interage com um banco de dados relacional na nuvem.                         | Amazon EC2, Amazon RDS                                    | 1h20          |
| 05/09/2024 (Quinta) | Arquitetura de Eventos Parte 1                        | Entender e criar arquiteturas baseadas em eventos para comunicação entre serviços na nuvem.             | Amazon SNS, Amazon SQS, AWS EventBridge                   | 1h20          |
| 06/09/2024 (Sexta)  | Arquitetura de Eventos Parte 2                        | Continuar o desenvolvimento de arquiteturas baseadas em eventos, integrando serviços adicionais.        | Amazon SNS, Amazon SQS, AWS EventBridge                   | 1h20          |
| 09/09/2024 (Segunda)| AWS Lambda Parte 1                                    | Explorar a execução de funções serverless usando AWS Lambda para automatizar processos na nuvem.        | AWS Lambda                                                | 1h20          |
| 10/09/2024 (Terça)  | AWS Lambda Parte 2                                    | Implementar um projeto completo utilizando AWS Lambda para criar uma arquitetura serverless.            | AWS Lambda                                                | 1h20          |
| 12/09/2024 (Quinta) | Automatizando Tudo com Terraform                      | Aprender a automatizar a criação e o gerenciamento de infraestruturas na nuvem usando Terraform.        | Terraform, AWS                                            | 1h20          |
| 13/09/2024 (Sexta)  | Terraform - Avançado                                  | Explorar funcionalidades avançadas do Terraform para gerenciar infraestruturas complexas.               | Terraform, AWS                                            | 1h20          |
| 16/09/2024 (Segunda)| Projeto PDF Eventos Parte 1                           | Implementar um projeto que processa PDFs em eventos usando AWS Lambda, S3 e SQS.                        | AWS Lambda, Amazon S3, Amazon SQS                         | 1h20          |
| 17/09/2024 (Terça)  | Projeto PDF Eventos Parte 2                           | Continuar o desenvolvimento do projeto de processamento de PDFs, integrando mais funcionalidades.       | AWS Lambda, Amazon S3, Amazon SQS                         | 1h20          |
| 19/09/2024 (Quinta) | Introdução ao Azure                                   | Entender os conceitos básicos do Azure e configurar os serviços equivalentes à AWS (Blob Storage, VMs). | Azure Blob Storage, Azure Virtual Machines (VMs)          | 1h20          |
| 20/09/2024 (Sexta)  | Projeto Azure: PostgreSQL e VNet                      | Implementar um projeto no Azure usando serviços equivalentes ao RDS, IAM e VPC da AWS.                  | Azure Database for PostgreSQL, Azure IAM, Azure Virtual Network (VNet) | 1h20          |
| 23/09/2024 (Segunda)| Projeto Azure: Functions e Event Grid                 | Criar uma arquitetura serverless no Azure, utilizando funções e eventos, similar ao AWS Lambda.         | Azure Functions, Azure Event Grid                         | 1h20          |
| 24/09/2024 (Terça)  | Revisão e De-Para AWS-Azure                           | Revisar o aprendizado e discutir o mapeamento de serviços AWS para Azure.                               | AWS, Azure                                                | 1h20          |
| 26/09/2024 (Quinta) | Introdução ao GCP                                     | Entender os conceitos básicos do Google Cloud Platform e configurar os serviços equivalentes à AWS.     | Google Cloud Storage, Google Compute Engine (VMs)         | 1h20          |
| 27/09/2024 (Sexta)  | Projeto GCP: Cloud SQL e VPC                          | Implementar um projeto no GCP usando serviços equivalentes ao RDS, IAM e VPC da AWS.                    | Cloud SQL (PostgreSQL), Google IAM, Google Virtual Private Cloud (VPC) | 1h20          |
| 30/09/2024 (Segunda)| Projeto GCP: Cloud Functions e Pub/Sub                | Criar uma arquitetura serverless no GCP, utilizando funções e eventos, similar ao AWS Lambda.           | Google Cloud Functions, Google Pub/Sub                    | 1h20          |
| 01/10/2024 (Terça)  | Revisão e De-Para AWS-GCP                             | Revisar o aprendizado e discutir o mapeamento de serviços AWS para GCP.                                 | AWS, Google Cloud Platform (GCP)                          | 1h20          |
| 03/10/2024 (Quinta) | Containers na AWS Parte 1                             | Introdução ao uso de containers na AWS, utilizando Amazon ECS e Docker.                                 | Amazon ECS (Elastic Container Service), Docker            | 1h20          |
| 04/10/2024 (Sexta)  | Containers na AWS Parte 2                             | Implementar um projeto utilizando containers na AWS com Amazon ECS e Docker.                            | Amazon ECS, Docker                                        | 1h20          |
| 07/10/2024 (Segunda)| Containers na AWS Parte 3                             | Introdução ao Kubernetes na AWS, utilizando Amazon EKS.                                                 | Amazon EKS (Elastic Kubernetes Service), Kubernetes       | 1h20          |
| 08/10/2024 (Terça)  | Containers na AWS Parte 4                             | Implementar um projeto utilizando Kubernetes na AWS com Amazon EKS.                                     | Amazon EKS, Kubernetes                                    | 1h20          |
| 10/10/2024 (Quinta) | Airflow em Projeto Completo Parte 1                   | Introdução ao Apache Airflow, configurando e criando um pipeline de dados na AWS.                       | Apache Airflow, AWS                                       | 1h20          |
| 11/10/2024 (Sexta)  | Airflow em Projeto Completo Parte 2                   | Implementar um projeto completo de pipeline de dados usando Apache Airflow na AWS.                      | Apache Airflow, AWS                                       | 1h20          |

================================================
File: /Bootcamp - Cloud para dados/Aula_01/README.md
================================================
# Bootcamp Cloud: Aula 01

## Introdução à AWS e Cloud Computing

**Objetivo:** Fornecer uma introdução prática ao uso da AWS, abordando a criação de contas, controle de custos e navegação na interface gráfica, com foco em aplicações na área de dados.

---

## 1. Por que Utilizar Cloud?

1. **Escalabilidade:** A cloud permite escalar recursos conforme necessário, sem a necessidade de grandes investimentos iniciais.
2. **Custo-eficiência:** Com a cloud, você paga apenas pelo que utiliza, reduzindo custos operacionais.
3. **Acessibilidade:** Serviços de cloud podem ser acessados de qualquer lugar, facilitando a colaboração e o trabalho remoto.
4. **Segurança:** Provedores de cloud como AWS oferecem robustos recursos de segurança, incluindo criptografia e autenticação multifator.

## 2. O que Não é Cloud

1. **Não é um Data Center On-Premise:** Embora a cloud envolva servidores, não é o mesmo que ter um data center físico na empresa.
2. **Não é apenas armazenamento:** Cloud envolve muito mais do que apenas guardar dados. É um ecossistema completo de serviços.
3. **Não é sempre barato:** Se mal gerido, o uso da cloud pode resultar em custos inesperados.

## 3. Acessando o Site da AWS

1. **Site da AWS:** Visite [aws.amazon.com](https://aws.amazon.com).
2. **Trocando o Idioma:** 
   - No canto superior direito, clique no ícone de globo e selecione “English”.
   - **Motivo:** O inglês é o idioma predominante em certificações, documentação, vagas de emprego, e na própria interface da AWS.

## 4. Cadastro e Configuração Inicial da Conta AWS

**Criar uma Conta na AWS:**
1. Acesse o site oficial da AWS (aws.amazon.com) e clique em "Create an AWS Account".
2. Insira as informações solicitadas, como e-mail, senha e nome da conta.
3. Selecione o tipo de conta e forneça as informações de pagamento (cartão de crédito/débito).
4. Verifique a conta via e-mail e complete o processo de criação.

**Configurações Iniciais:**
- Opte pelo **AWS Free Tier**, que oferece uma camada gratuita com serviços limitados por 12 meses, ideal para quem está começando.
- Após o cadastro, revise as configurações de segurança e habilite a autenticação multifator (MFA) para proteger a conta.

## 5. Controle de Custos na AWS

**Evitar Custos Inesperados:**
- **AWS Budgets:** Use o AWS Budgets para configurar alertas de custo. Acesse o serviço pelo console, defina um orçamento mensal e configure notificações por e-mail.
- **AWS Cost Explorer:** Monitore os gastos em tempo real utilizando o AWS Cost Explorer, que permite visualizar detalhadamente onde e como os recursos estão sendo utilizados.

**Dicas Práticas:**
- Defina **limites de serviço** para evitar que recursos sejam provisionados além do necessário.
- Revise regularmente as instâncias e serviços em execução e encerre aqueles que não estão em uso.

## 6. Navegação na Interface Gráfica da AWS

**AWS Management Console:**
- O **AWS Management Console** é o painel de controle principal da AWS. Ele oferece acesso a todos os serviços disponíveis na plataforma, como S3, EC2 e RDS.

**Serviços Comuns:**
- **S3 (Simple Storage Service):** Armazenamento escalável de objetos, ideal para dados brutos, backups e arquivos de grande volume.
- **EC2 (Elastic Compute Cloud):** Serviço que permite criar e gerenciar instâncias de máquinas virtuais na nuvem, altamente configuráveis.
- **RDS (Relational Database Service):** Serviço gerenciado de banco de dados relacional, que suporta mecanismos como MySQL, PostgreSQL, e SQL Server.

**Documentação e Suporte:**
- A AWS oferece documentação abrangente e suporte direto pelo console. Acesse a documentação para obter informações detalhadas sobre cada serviço e suas configurações.

## 7. Visão Geral dos Produtos AWS

1. **Explorando Todos os Produtos:**
   - Navegue até a seção "Products" no menu principal.
   - Explore as diversas categorias: Computação, Armazenamento, Banco de Dados, etc.

2. **EC2 (Elastic Compute Cloud):**
   - **Visão Geral:** Serviço que permite criar instâncias de servidores virtuais.
   - **Instâncias de Todos os Tipos:** De uso geral, otimizadas para memória, para computação, etc.
   - **Democratização:** A cloud, ao permitir acesso fácil a essas tecnologias, democratiza o acesso à internet e ao conhecimento técnico.

3. **Visão Geral de Vários Tipos de Banco de Dados:**
   - **RDS (Relational Database Service):** Gerencia bancos de dados como MySQL, PostgreSQL, e SQL Server.
   - **DynamoDB:** Banco de dados NoSQL totalmente gerenciado.
   - **Redshift:** Armazenamento e análise de dados em larga escala.
   
   **Motivação para Estudo:** Ao explorar os diferentes tipos de bancos de dados, você é motivado a estudar e entender novas tecnologias.

## 8. Tipos de Serviço AWS

1. **Free Tier:**
   - **12 Meses Gratuitos:** Oferece acesso gratuito a muitos serviços durante o primeiro ano.
   - **Trial Limitado:** Alguns serviços têm um período de trial limitado a 1-3 meses.

## 9. Configuração de Regiões

**Regiões e Zonas de Disponibilidade:**
- **Regiões** são locais geográficos onde a AWS mantém seus data centers. Cada região é composta por várias **Zonas de Disponibilidade**, que são grupos de data centers fisicamente separados.
- A escolha da região influencia a latência dos serviços e pode ter implicações legais e de conformidade, dependendo dos dados armazenados.

**Escolha da Região:**
- Selecione a região mais próxima dos seus usuários ou que atenda aos requisitos específicos de conformidade do seu projeto. No console da AWS, é possível alternar facilmente entre diferentes regiões.

## 10. Configuração de IAM (Identity and Access Management)

**Importância do IAM:**
- Criação de usuários e grupos com permissões específicas.
- Segurança aprimorada com autenticação multifator (MFA).

**Criar um Fator de Autenticação:**
- Acesse o IAM no console AWS.
- Vá para “Users” e selecione seu usuário root.
- Clique em “Security credentials” e ative o MFA.

**Uso do Authy:**
- Utilize o Authy como aplicativo MFA para adicionar uma camada extra de segurança.

**Criar um Acesso Admin no IAM:**
- Vá para "Users" e clique em "Add user".
- Crie um usuário com "Programmatic access" e "AWS Management Console access".
- Defina a senha e anote a Access Key.
- **Add User to Group:** Adicione o usuário ao grupo "Administrators".
- **Attach Permissions:** Associe a permissão "AdministratorAccess".
- **Tags:** Adicione tags para identificar e gerenciar o usuário.
- **Regions:** Defina as regiões de uso para o usuário.

## 11. Criando o Nosso Primeiro Serviço S3

Nesta seção, vamos aprender a criar e configurar um bucket S3 na AWS, além de explorar como armazenar arquivos, configurar permissões e servir um site estático.

### 11.1 Criando o Bucket S3

1. **Nome do Bucket:**
   - Escolha um nome único para o seu bucket. Lembre-se que o nome deve ser globalmente único dentro da AWS.
  
2. **Zona (Region):**
   - Selecione a região onde o bucket será criado. É recomendável escolher a região mais próxima dos seus usuários para reduzir latência.

3. **ACLs (Access Control Lists):**
   - Configure as listas de controle de acesso (ACLs) para definir permissões de leitura e escrita. Por padrão, é melhor manter o controle de acesso restrito.

4. **Bucket Público:**
   - Inicialmente, o bucket deve ser privado por questões de segurança. Se for necessário torná-lo público, isso pode ser feito posteriormente.

5. **Bucket Versioning:**
   - Habilite o versionamento do bucket para manter várias versões de um objeto. Isso pode ser útil para recuperação de dados e auditoria.

6. **Tags:**
   - Adicione tags para identificar e organizar seu bucket. Tags podem incluir informações como ambiente (produção, desenvolvimento), proprietário, e finalidade.

7. **Default Encryption:**
   - Configure a criptografia padrão para garantir que todos os objetos armazenados no bucket sejam criptografados automaticamente.

### 11.2 Acessando o Bucket

- Depois de criar o bucket, acesse-o através do console da AWS. Na interface do bucket, você pode ver e gerenciar todos os objetos armazenados.

### 11.3 Colocando uma Imagem no Bucket

1. **Upload de Arquivo:**
   - Faça upload de uma imagem ou qualquer outro arquivo no bucket.
  
2. **Verificando o Arquivo:**
   - Após o upload, tente acessar o arquivo diretamente. Você notará que, por padrão, o arquivo não será acessível publicamente.

### 11.4 Configurando Permissões: Tornando o Arquivo Público

1. **Padrão Privado:**
   - Por padrão, todos os arquivos em um bucket S3 são privados, ou seja, somente você pode acessá-los.
  
2. **Tornar o Arquivo Público:**
   - Para tornar o arquivo acessível publicamente, você pode modificar as permissões do arquivo ou configurar uma política de bucket.

### 11.5 Bucket Policy: Exemplo de Permissão de Leitura para Usuário Anônimo

- **Bucket Policy:**
   - Adicione uma política ao bucket para permitir que usuários anônimos leiam os arquivos. Um exemplo de política de leitura pública para um bucket S3 pode ser adicionado através do editor de políticas no console da AWS.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::nome-do-bucket/*"
        }
    ]
}
```

### 11.6 Adicionando um Arquivo `index.html`

- **Upload do Arquivo `index.html`:**
   - Adicione um arquivo `index.html` ao seu bucket. Esse arquivo servirá como a página principal do seu site estático.

### 11.7 Abrindo um Website Estático

1. **Configuração de Website Estático:**
   - No console do S3, habilite a opção de hospedagem de site estático. Defina o `index.html` como o documento de índice.
  
2. **Acessando o Website:**
   - Após a configuração, você poderá acessar o site via URL fornecida pelo S3.

### 11.8 Retornando o `index.html` Diretamente na Raiz

- **Configurando o Redirecionamento:**
   - Configure para que o `index.html` seja servido diretamente na raiz do site, garantindo que o usuário veja o conteúdo imediatamente ao acessar o URL do bucket.

### 11.9 O que é um Site Estático?

- **Definição:**
   - Um site estático é um site cujas páginas são servidas exatamente como são armazenadas, sem processamento dinâmico. Isso significa que não há interação com banco de dados ou scripts do lado do servidor, tornando-o ideal para conteúdos simples, como blogs ou páginas de portfólio.

   Aqui está o passo a passo atualizado, começando com o comando `sudo su` para obter acesso root, definindo uma senha para o root, e então seguindo com a instalação do Python e Streamlit, além da execução do aplicativo.

## 12. Criando e Configurando uma Instância EC2

Nesta seção, vamos aprender a configurar uma instância EC2, obter acesso root, definir uma senha para o root, e depois configurar o ambiente com Python e Streamlit para rodar um aplicativo básico de "Hello World", incluindo a renderização de uma imagem armazenada no S3.

### 12.1 Conectando-se à Instância EC2 e Obtendo Acesso Root

1. **Conectar-se via SSH:**
   - Use o comando SSH no terminal para conectar-se à instância. O comando será algo assim:

   ```bash
   ssh -i "caminho-para-sua-chave.pem" ec2-user@seu-endereco-publico-ec2
   ```

2. **Obter Acesso Root:**
   - Assim que estiver conectado, troque para o usuário root usando o comando:

   ```bash
   sudo su
   ```

3. **Definir uma Senha para o Root (se necessário):**
   - Se você ainda não configurou uma senha para o root, ou deseja alterá-la, execute:

   ```bash
   passwd
   ```

   - Você será solicitado a digitar a nova senha para o root e confirmá-la.

### 12.2 Instalando Python, `pip3`, e Streamlit

1. **Atualizar Pacotes e Instalar Python:**
   - Primeiro, atualize os pacotes e instale Python 3 com os seguintes comandos:

   ```bash
   yum update -y
   yum install python3 -y
   ```

2. **Instalando o `pip3`:**
   - Instale o `pip3`, que é o gerenciador de pacotes para Python 3:

   ```bash
   yum install python3-pip -y
   ```

3. **Verificando a Instalação do `pip3`:**
   - Verifique se o `pip3` foi instalado corretamente:

   ```bash
   pip3 --version
   ```

4. **Instalando o Streamlit:**
   - Agora que o `pip3` está instalado, você pode instalar o Streamlit com o comando:

   ```bash
   pip3 install streamlit
   ```

### 12.3 Criando o Código do Streamlit com a Imagem

1. **Criar um Arquivo Python para o Streamlit:**
   - Use o editor de texto `nano` para criar e editar o arquivo `app.py`. No terminal, execute:

   ```bash
   nano app.py
   ```

2. **Inserir o Código para Renderizar a Imagem:**
   - Insira o seguinte código no arquivo `app.py`:

   ```python
   import streamlit as st

   # Título do aplicativo
   st.title("Hello, World!")

   # Descrição
   st.write("Este é um simples aplicativo Streamlit rodando em uma instância EC2 na AWS.")
   ```

3. **Salvar o Arquivo e Sair do Editor:**
   - Para salvar o arquivo, pressione `Ctrl + O` e depois `Enter`.
   - Para sair do editor `nano`, pressione `Ctrl + X`.

### 12.4 Executando o Streamlit na Porta 80

1. **Executar o Streamlit na Porta 80:**
   - Agora que você está logado como root, execute o Streamlit na porta 80:

   ```bash
   streamlit run app.py --server.port 80 --server.enableCORS false
   ```

2. **Acessar a Aplicação:**
   - Acesse o aplicativo no navegador através do endereço IP público da sua instância EC2:

   ```http
   http://seu-endereco-publico-ec2
   ```

### Resultado Esperado

Ao acessar o aplicativo, você verá o título "Hello, World!", uma descrição, e a imagem "fotodaturma.png" renderizada diretamente do S3, com a legenda "Foto da Turma - Primeiro Dia".

================================================
File: /Bootcamp - Cloud para dados/Aula_01/app.py
================================================
# https://docs.streamlit.io/get-started/tutorials/create-an-app

import streamlit as st
import pandas as pd
import numpy as np

st.title('Uber pickups in NYC')

DATE_COLUMN = 'date/time'
DATA_URL = ('https://s3.amazonaws.com/www.bootcampdecloudjornadadedados.com/uber-raw-data-sep14.csv.gz')

@st.cache_data
def load_data(nrows):
    data = pd.read_csv(DATA_URL, nrows=nrows)
    lowercase = lambda x: str(x).lower()
    data.rename(lowercase, axis='columns', inplace=True)
    data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])
    return data

data_load_state = st.text('Loading data...')
data = load_data(10000)
data_load_state.text("Done! (using st.cache_data)")

if st.checkbox('Show raw data'):
    st.subheader('Raw data')
    st.write(data)

st.subheader('Number of pickups by hour')
hist_values = np.histogram(data[DATE_COLUMN].dt.hour, bins=24, range=(0,24))[0]
st.bar_chart(hist_values)

# Some number in the range 0-23
hour_to_filter = st.slider('hour', 0, 23, 17)
filtered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]

st.subheader('Map of all pickups at %s:00' % hour_to_filter)
st.map(filtered_data)

================================================
File: /Bootcamp - Cloud para dados/Aula_01/index.html
================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Primeira Aula - Bootcamp Cloud</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: auto;
            background: #fff;
            padding: 20px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1 {
            color: #0066cc;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px 0;
        }
        p {
            font-size: 18px;
            line-height: 1.6;
        }
        .summary {
            background-color: #e6f7ff;
            padding: 15px;
            margin-top: 20px;
            border-left: 5px solid #0099cc;
        }
        .links {
            margin-top: 20px;
            font-size: 18px;
        }
        .links a {
            color: #0066cc;
            text-decoration: none;
        }
        .links a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Primeira Aula - Bootcamp Cloud</h1>
        <img src="fotodaturma.png" alt="Foto da Turma">
        <p>Hoje foi o primeiro passo de uma jornada de 2 meses! Estamos animados para ver o progresso de todos ao longo deste bootcamp, que vai cobrir os principais aspectos da cloud com foco na AWS.</p>
        <p>Aproveite cada momento e vamos juntos construir uma base sólida em cloud computing!</p>
        
        <div class="summary">
            <h2>Resumo do Bootcamp</h2>
            <p>O Bootcamp de Cloud para Dados foi criado para ajudar você a implantar suas aplicações de forma independente. Durante os próximos 30 dias, vamos explorar conceitos essenciais de Cloud, incluindo VPC, EC2, IAM, e muito mais. Construiremos cinco aplicações em diferentes plataformas de Cloud, como AWS, Azure e GCP, sempre com foco em entregar valor ao negócio.</p>
            <p>Se você deseja entender toda a infraestrutura necessária para subir suas aplicações, este Bootcamp é ideal para você. As aulas serão ao vivo, com duração de 1h20 cada, e todas estarão disponíveis na nossa plataforma para revisões. Prepare-se para uma imersão completa na nuvem!</p>
        </div>

        <div class="links">
            <p>Para saber mais, visite nosso site: <a href="https://www.suajornadadedados.com.br" target="_blank">www.suajornadadedados.com.br</a></p>
            <p>Para realizar a inscrição, clique aqui: <a href="https://chk.eduzz.com/e2d3br2f" target="_blank">Inscrição Bootcamp Cloud</a></p>
        </div>
    </div>
</body>
</html>


================================================
File: /Bootcamp - Cloud para dados/Aula_02/README.md
================================================
# Bootcamp Cloud: Aula 02

## S3: Armazenamento de Dados na AWS

**Objetivo:** Explorar as diversas aplicações do Amazon S3 no contexto de engenharia, ciência e análise de dados, com um foco prático em como configurar e utilizar o serviço via script Python usando o boto3. Além disso, abordaremos a criação de um IAM exclusivo e um grupo de recursos para gerenciar acesso e segurança.

---

### 1. Revisão da Aula Anterior: Verificando Gastos de Ontem e Usando Tags para Filtragem

**Objetivo:** Aprender a utilizar o AWS Cost Explorer para verificar os gastos do dia anterior e aplicar filtros utilizando tags criadas durante o gerenciamento dos recursos.

#### Passo 1: Acessar o AWS Cost Explorer

1. **Login no AWS Management Console:**
   - Acesse sua conta AWS e no console, digite "Cost Explorer" na barra de pesquisa superior e selecione "Cost Explorer" nos resultados.

2. **Ativando o Cost Explorer (se necessário):**
   - Se esta for a primeira vez que você está usando o Cost Explorer, será necessário ativá-lo. Clique em "Enable Cost Explorer".

#### Passo 2: Verificar os Gastos de Ontem

1. **Configurar o Período de Tempo:**
   - No Cost Explorer, selecione o intervalo de datas. No campo "Filter by Date", escolha a data de ontem como início e fim.

2. **Visualizar Gastos:**
   - O Cost Explorer mostrará os gastos do dia anterior, distribuídos por serviço, região, ou qualquer outro filtro padrão da AWS.

#### Passo 3: Usar Tags para Filtragem de Custos

1. **Aplicar Filtros de Tag:**
   - Para ver os custos associados a um grupo específico de recursos, clique em "Add Filter" e selecione "Tag".

2. **Selecionar Tags Relevantes:**
   - Escolha a tag que você criou anteriormente, como `Environment: Production` ou `Project: DataAnalysis`. Isso permitirá que você visualize apenas os custos associados a recursos marcados com essas tags.

3. **Analisar Resultados:**
   - O Cost Explorer atualizará os resultados para mostrar apenas os gastos relacionados aos recursos que possuem a tag selecionada.

#### Passo 4: Configurar Relatórios Personalizados

1. **Criar um Relatório Customizado:**
   - Se você quiser acompanhar esses custos regularmente, pode criar um relatório customizado clicando em "Save as Report".
   - Dê um nome ao relatório (ex: "Gastos Diários por Tag") e defina as configurações para receber atualizações automáticas por e-mail.

#### Passo 5: Boas Práticas ao Usar Tags e Monitorar Custos

1. **Consistência na Aplicação de Tags:**
   - Certifique-se de que todas as equipes utilizem as mesmas convenções de tags para facilitar a filtragem e monitoramento dos custos.

2. **Revisão Regular de Custos:**
   - Incorpore a revisão diária ou semanal dos custos como parte do seu workflow para evitar surpresas no faturamento.

3. **Alertas de Orçamento:**
   - Use o AWS Budgets para criar alertas de orçamento baseados em tags, garantindo que você seja notificado se os gastos superarem os limites previstos.

---

## 2. O que é Storage?

1. **Definição:**
   - Storage, ou armazenamento, refere-se ao espaço onde dados digitais são guardados. No contexto da AWS, o S3 é um serviço de storage de objetos, onde arquivos de todos os tipos e tamanhos podem ser armazenados e acessados com alta disponibilidade e durabilidade.

2. **Tipos de Storage na AWS:**
   - Além do S3, a AWS oferece outros tipos de storage, como EBS (Elastic Block Store) para armazenamento de blocos, e o Amazon EFS (Elastic File System) para armazenamento de arquivos. O S3 é focado no armazenamento de objetos, ideal para grandes volumes de dados não estruturados.

### 2.1 Vantagem da Redundância

1. **O que é Redundância?**
   - Redundância em storage se refere à duplicação de dados em múltiplos locais para garantir a disponibilidade e durabilidade dos dados, mesmo em caso de falhas.

2. **Vantagem da Redundância no S3:**
   - O Amazon S3 armazena automaticamente os dados em pelo menos três zonas de disponibilidade (AZs) dentro de uma região, oferecendo proteção contra falhas em qualquer uma dessas zonas.
   - Isso garante que os dados estejam sempre acessíveis e seguros, mesmo em situações de desastres naturais ou falhas em data centers.

3. **Como Funciona a Redundância no S3?**
   - Ao enviar um arquivo para um bucket S3, o dado é replicado automaticamente em múltiplas AZs. Essa replicação é transparente para o usuário, que não precisa se preocupar com a configuração ou gerenciamento desse processo.
   - A redundância é uma das principais razões pelas quais o S3 oferece uma durabilidade de 99.999999999% (11 noves), garantindo que os dados sejam preservados a longo prazo.

---

## 3. Criação de um Bucket S3: Passo a Passo e Detalhes

### 3.1 Criando um Bucket S3 via AWS Management Console

1. **Acessando o AWS Management Console:**
   - Faça login no [AWS Management Console](https://aws.amazon.com) e navegue até o serviço S3. Isso pode ser feito digitando "S3" na barra de pesquisa superior e selecionando "S3" nos resultados.

2. **Iniciando a Criação do Bucket:**
   - Na página do S3, clique em “Create bucket” para iniciar o processo de criação.

3. **Nome do Bucket:**
   - **Bucket name:** Insira um nome único para o bucket. O nome deve ser globalmente exclusivo em todas as regiões da AWS e seguir as regras de nomenclatura (por exemplo, sem espaços, apenas caracteres minúsculos).
   - **Exemplo:** `meu-bucket-dados-2024`

4. **Escolhendo a Região:**
   - **Region:** Selecione a região onde o bucket será criado. A escolha da região pode afetar a latência e os custos de transferência de dados. É recomendável escolher uma região próxima dos usuários finais ou do local onde os dados serão processados.
   - **Exemplo:** `US West (Oregon)`

5. **Configurações de Controle de Acesso:**
   - **Block Public Access settings for this bucket:** Por padrão, a AWS bloqueia o acesso público a novos buckets. Deixe essa configuração ativada para garantir a segurança dos dados.
   - **Block all public access:** Mantendo essa opção ativada, você evita que qualquer pessoa fora da sua conta AWS tenha acesso aos dados do bucket.

6. **Configurações de Versionamento:**
   - **Bucket Versioning:** Habilitar o versionamento permite que o S3 mantenha várias versões de um objeto, o que é útil para proteção contra exclusões acidentais e para auditoria.
   - **Opções:**
     - **Enable:** Ativa o versionamento.
     - **Disable:** Desativa o versionamento (padrão).
     - **Suspend:** Suspende o versionamento, mantendo versões anteriores sem criar novas versões.
   - **Recomendação:** Habilitar o versionamento se o bucket for usado para armazenar dados críticos ou históricos.

7. **Configurações de Logs de Acesso:**
   - **Server access logging:** Essa opção permite registrar todas as solicitações de acesso ao bucket, o que é útil para auditoria e monitoramento de segurança.
   - **Opções:**
     - **Enable:** Ativa o registro de logs.
     - **Target bucket:** Especifique um bucket para armazenar os logs de acesso.
     - **Target prefix:** Defina um prefixo para os logs, organizando-os dentro do bucket de destino.
   - **Recomendação:** Ativar se precisar monitorar acessos ao bucket.

8. **Criptografia:**
   - **Default encryption:** A criptografia padrão garante que todos os objetos armazenados no bucket sejam criptografados automaticamente.
   - **Opções:**
     - **AES-256:** Criptografia usando a chave gerenciada pela AWS (AES-256).
     - **AWS-KMS:** Criptografia com uma chave gerenciada pelo AWS Key Management Service (KMS), o que oferece maior controle sobre as chaves de criptografia.
   - **Recomendação:** Habilitar a criptografia padrão, especialmente se você estiver armazenando dados sensíveis.

9. **Etiquetas (Tags):**
   - **Add tags:** As tags são pares chave-valor que ajudam a organizar e gerenciar seus buckets.
   - **Exemplo:** 
     - **Key:** `Environment`
     - **Value:** `Production`
   - **Recomendação:** Usar tags para facilitar a identificação e gerenciamento de buckets em ambientes grandes ou complexos.

10. **Gerenciamento de Ciclo de Vida:**
    - **Object Lock:** Esse recurso protege os objetos de serem excluídos ou modificados por um período especificado. É útil para conformidade regulatória e proteção contra exclusão acidental.
    - **Opções:**
      - **Enable:** Ativa o Object Lock (requer habilitação de versionamento).
    - **Recomendação:** Ativar apenas se houver necessidade específica de reten

ção de dados.

11. **Configurações Avançadas:**
    - **Transfer acceleration:** Acelera transferências de dados para e do bucket, usando pontos de presença da AWS globalmente.
    - **Requester Pays:** Esta configuração faz com que o solicitante pague pelos custos de download de dados do bucket, útil em cenários de compartilhamento de dados públicos.
    - **Intelligent-Tiering:** Automatiza o movimento de objetos entre camadas de armazenamento de custo mais baixo com base nos padrões de acesso.
    - **Lifecycle rules:** Crie regras de ciclo de vida para transitar objetos entre diferentes classes de armazenamento (ex: de Standard para Glacier) com base em políticas de retenção.
    - **Recomendação:** Avaliar cada uma dessas opções com base nas necessidades do projeto.

12. **Revisão e Criação do Bucket:**
    - Revise todas as configurações que você escolheu para o bucket.
    - Clique em "Create bucket" para finalizar a criação.

13. **Verificação e Acesso:**
    - Após a criação, o novo bucket aparecerá na lista de buckets do S3. Clique no nome do bucket para acessar a interface de gerenciamento, onde você pode começar a carregar arquivos, configurar permissões adicionais, ou ajustar configurações.

### 3.2 Recursos Avançados no Painel S3

1. **Intelligent-Tiering Archive Configurations:**
   - **Descrição:** O Intelligent-Tiering é uma classe de armazenamento que move automaticamente os objetos entre diferentes camadas de armazenamento (frequentemente acessado e raramente acessado) com base em padrões de acesso. É ideal para otimizar custos sem sacrificar a performance.
   - **Archive Configurations:** Permite a configuração de arquivamento de objetos para camadas de armazenamento ainda mais econômicas, como Deep Archive.

2. **Server Access Logging:**
   - **Descrição:** Essa propriedade permite registrar todas as solicitações feitas ao bucket S3, incluindo acessos e modificações. Esses logs podem ser armazenados em outro bucket S3 para auditoria e análise.
   - **Recomendação:** Habilitar logging pode ser útil para monitoramento de segurança e compliance.

3. **AWS CloudTrail Data Events:**
   - **Descrição:** AWS CloudTrail pode ser configurado para registrar eventos de dados do S3, como acessos a objetos específicos ou mudanças em permissões. **Este serviço pode ter custos associados**, especialmente se muitos eventos forem registrados.
   - **Recomendação:** Usar o CloudTrail para monitorar eventos críticos, como alterações em buckets sensíveis.

4. **Event Notifications:**
   - **Descrição:** Configura notificações para eventos específicos no bucket, como a criação de novos objetos ou exclusão de objetos. Essas notificações podem acionar funções Lambda, filas SQS, ou tópicos SNS.
   - **Exemplo de Uso:** Automatizar a geração de thumbnails quando uma nova imagem é carregada no bucket.

5. **Object Lock:**
   - **Descrição:** O Object Lock impede que objetos sejam excluídos ou sobrescritos por um período determinado, o que é útil para conformidade regulatória e prevenção de perda de dados.
   - **Recomendação:** Usar em buckets que armazenam dados sensíveis que não podem ser alterados ou excluídos.

6. **Static Website Hosting:**
   - **Descrição:** Permite configurar o bucket S3 para hospedar um site estático, servindo arquivos como HTML, CSS e JavaScript diretamente da nuvem.
   - **Exemplo de Uso:** Hospedagem de páginas web simples, como portfólios ou blogs estáticos.

### 3.3 Exemplo Prático: Bucket Temporário com Regras de Ciclo de Vida

1. **Cenário:** Vamos supor que você esteja gerenciando um pipeline de ETL (Extract, Transform, Load) onde os dados são extraídos de várias fontes e carregados em um bucket temporário para processamento.

2. **Processo:**
   - **Passo 1:** Criação do Bucket Temporário
     - Nomeie o bucket como `temp-etl-bucket`.
     - Este bucket armazena os dados extraídos antes do processamento.
   
   - **Passo 2:** Processamento dos Dados
     - Uma vez que os dados estejam no bucket temporário, um job de processamento (como um script Python ou um AWS Lambda) é acionado para transformar os dados.
     - Após o processamento, os dados transformados são movidos para um bucket final, `processed-data-bucket`.

   - **Passo 3:** Aplicação de Regras de Ciclo de Vida
     - Configure uma regra de ciclo de vida para o bucket `temp-etl-bucket`:
       - **Transição de Armazenamento:** Após 7 dias, mova os dados para uma classe de armazenamento mais econômica, como Glacier.
       - **Exclusão Automática:** Após 30 dias, exclua permanentemente os objetos do bucket.

3. **Configuração da Regra de Ciclo de Vida:**
   - No console do S3, vá para a aba “Management” e selecione “Lifecycle Rules”.
   - Clique em “Create lifecycle rule”.
   - Nomeie a regra (ex: `TempDataCleanup`).
   - Defina as condições:
     - **Prefix:** Se necessário, limite a regra a um determinado prefixo (por exemplo, `/temp`).
     - **Transition:** Mova os dados para o Glacier após 7 dias.
     - **Expiration:** Exclua os dados após 30 dias.

4. **Benefícios:**
   - **Redução de Custos:** Ao mover os dados para o Glacier e posteriormente excluí-los, você reduz custos de armazenamento.
   - **Automação:** As regras de ciclo de vida garantem que o bucket temporário seja gerenciado automaticamente, sem necessidade de intervenção manual.
   - **Segurança:** Garantir que os dados temporários sejam excluídos ajuda a minimizar riscos de segurança e conformidade.

---

## 4. Exemplo de Uso do S3 via Python (boto3)

### 4.1 Instalando o boto3

1. **Instalação do boto3:**
   - Para interagir com o S3 via Python, é necessário instalar o boto3, o SDK da AWS para Python:

   ```bash
   pip install boto3
   ```

### 4.2 Configurando Credenciais AWS

1. **Configuração das Credenciais:**
   - Antes de utilizar o boto3, é necessário configurar as credenciais da AWS. Isso pode ser feito através do `aws configure` ou definindo variáveis de ambiente.

   ```bash
   aws configure
   ```

2. **Configuração Manual (Opcional):**
   - As credenciais também podem ser configuradas manualmente:

   ```python
   import boto3

   session = boto3.Session(
       aws_access_key_id='YOUR_ACCESS_KEY',
       aws_secret_access_key='YOUR_SECRET_KEY',
       region_name='us-west-2'
   )
   s3 = session.resource('s3')
   ```

### 4.3 Criando e Gerenciando Buckets

1. **Criando um Bucket:**
   - Use o boto3 para criar um novo bucket S3:

   ```python
   import boto3

   s3 = boto3.client('s3')
   bucket_name = 'meu-novo-bucket'

   s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={
       'LocationConstraint': 'us-west-2'})
   ```

2. **Listando Buckets:**
   - Verifique todos os buckets existentes:

   ```python
   response = s3.list_buckets()

   for bucket in response['Buckets']:
       print(bucket['Name'])
   ```

### 4.4 Upload e Download de Arquivos

1. **Upload de Arquivos:**
   - Carregue um arquivo local para o S3:

   ```python
   s3.upload_file('local-file.txt', bucket_name, 's3-file.txt')
   ```

2. **Download de Arquivos:**
   - Baixe um arquivo do S3 para o sistema local:

   ```python
   s3.download_file(bucket_name, 's3-file.txt', 'local-file.txt')
   ```

### 4.5 Configurando ACLs e Políticas de Bucket

1. **Definindo Permissões de Acesso:**
   - Use boto3 para definir permissões de leitura/escrita:

   ```python
   s3.put_object_acl(Bucket=bucket_name, Key='s3-file.txt', ACL='public-read')
   ```

2. **Políticas de Bucket:**
   - Configure uma política de bucket para permitir acesso público:

   ```python
   bucket_policy = {
       'Version': '2012-10-17',
       'Statement': [{
           'Sid': 'PublicReadGetObject',
           'Effect': 'Allow',
           'Principal': '*',
           'Action': 's3:GetObject',
           'Resource': f'arn:aws:s3:::{bucket_name}/*'
       }]
   }

   s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(bucket_policy))
   ```

### 4.6 Excluindo Objetos e Buckets

1. **Excluir um Objeto:**
   - Remova um arquivo do S3:

   ```python
   s3.delete_object(Bucket=bucket_name, Key='s3-file.txt')
   ```

2. **Excluir um Bucket:**
   - Delete um bucket (após excluir todos os objetos):

   ```python
   s3.delete_bucket(Bucket=bucket_name)
   ```

---

## 5.

 Casos de Uso do S3 em Engenharia, Ciência e Análise de Dados

### 5.1 Engenharia de Dados

1. **Data Lake:**
   - O S3 é frequentemente utilizado como uma camada de armazenamento em data lakes, onde dados brutos de diferentes fontes são armazenados antes de serem processados e analisados.
   - Vantagens incluem escalabilidade, baixo custo, e integração com serviços como AWS Glue e Amazon Athena.

2. **ETL (Extract, Transform, Load):**
   - O S3 é essencial em pipelines ETL, onde dados são extraídos de fontes diversas, transformados, e armazenados no S3 antes de serem carregados em um data warehouse ou banco de dados.
   - Exemplos incluem armazenar arquivos CSV ou Parquet que são processados periodicamente por ferramentas como Apache Spark ou AWS Glue.

3. **Backup e Arquivamento:**
   - O S3 é uma solução robusta para backup e arquivamento de dados críticos, com suporte para versionamento e regras de ciclo de vida para mover dados entre diferentes classes de armazenamento, como S3 Glacier.

### 5.2 Ciência de Dados

1. **Armazenamento de Datasets:**
   - Cientistas de dados podem utilizar o S3 para armazenar grandes datasets que são utilizados em modelagem, treinamento de algoritmos de machine learning, e análise exploratória de dados.

2. **Integração com Amazon SageMaker:**
   - O S3 é integrado ao Amazon SageMaker, permitindo que datasets sejam carregados diretamente do S3 para notebooks e treinamentos de modelos de machine learning.
   - Modelos treinados também podem ser armazenados no S3 para versionamento e reutilização.

3. **Data Sharing e Colaboração:**
   - O S3 permite o compartilhamento de grandes datasets com equipes distribuídas ou com outros pesquisadores, através da configuração de permissões e políticas de acesso.

### 5.3 Análise de Dados

1. **Análise com Amazon Athena:**
   - O Amazon S3 pode ser usado como o local de armazenamento para dados analisados com Amazon Athena, que permite executar queries SQL diretamente sobre dados armazenados no S3 sem a necessidade de carregar esses dados para um banco de dados relacional.

2. **Dashboards e Reporting:**
   - Dados armazenados no S3 podem ser facilmente integrados a ferramentas de BI e reporting, como Amazon QuickSight ou Tableau, para criar dashboards e relatórios atualizados em tempo real.

3. **Log Storage e Análise:**
   - Logs de aplicações, servidores, e eventos podem ser armazenados no S3 e analisados usando Amazon Athena ou serviços de terceiros, ajudando a identificar padrões e insights operacionais.

---

**Conclusão:** Nesta aula, exploramos o Amazon S3 como um serviço fundamental para a gestão de dados na AWS, cobrindo desde a criação e configuração de buckets, até exemplos práticos de uso com Python e casos de uso em diferentes áreas. Além disso, discutimos boas práticas de segurança e gerenciamento de dados, incluindo o uso de IAM e grupos de recursos, para garantir que seus dados estejam seguros e bem gerenciados.

================================================
File: /Bootcamp - Cloud para dados/Aula_02/projeto/README.md
================================================
## Projeto Python: Backup de Arquivos Locais para o S3

### Introdução

O objetivo deste projeto é criar um sistema automatizado de backup utilizando Python e o serviço de armazenamento de objetos Amazon S3. O projeto será responsável por ler arquivos de uma pasta local no seu computador, enviar esses arquivos para um bucket S3 na AWS e, em seguida, deletar os arquivos locais após o upload bem-sucedido.

Essa solução é ideal para cenários onde é necessário garantir que os dados locais sejam salvos em um ambiente seguro e acessível, liberando espaço de armazenamento no dispositivo local e protegendo contra perdas de dados.

### Diagrama do Fluxo do Projeto

```mermaid
graph TD
    A[Início do Backup] --> B{Listar Arquivos na Pasta Local}
    B -->|Arquivos Encontrados| C[Upload dos Arquivos para S3]
    B -->|Nenhum Arquivo Encontrado| F[Fim do Processo]
    C --> D{Upload Bem-Sucedido?}
    D -->|Sim| E[Deletar Arquivos Locais]
    D -->|Não| G[Registrar Erro]
    E --> F[Fim do Processo]
    G --> F[Fim do Processo]
```

### Explicação do Fluxo:

1. **Início do Backup:** O processo começa com a execução do script Python.
2. **Listar Arquivos na Pasta Local:** O script verifica a presença de arquivos na pasta local especificada.
   - Se arquivos forem encontrados, o processo continua.
   - Se nenhum arquivo for encontrado, o processo termina.
3. **Upload dos Arquivos para S3:** Os arquivos listados são enviados para o bucket S3 designado.
4. **Upload Bem-Sucedido?:** Verifica se o upload dos arquivos foi realizado com sucesso.
   - Se o upload for bem-sucedido, os arquivos locais são deletados.
   - Se ocorrer algum erro durante o upload, o erro é registrado para análise posterior.
5. **Fim do Processo:** O processo de backup é concluído, seja após o upload e deleção dos arquivos ou após a detecção de um erro.

### 1. Configuração Inicial do Projeto

#### Passo 1: Criação do Ambiente de Desenvolvimento

1. **Criar uma Pasta para o Projeto:**
   - Crie uma nova pasta para o seu projeto, por exemplo: `backup_s3`.

2. **Criar um Ambiente Virtual (Opcional, mas Recomendado):**
   - Navegue até a pasta do projeto e crie um ambiente virtual:
   
   ```bash
   python -m venv venv
   ```

   - Ative o ambiente virtual:
     - **Windows:** `venv\Scripts\activate`
     - **Linux/MacOS:** `source venv/bin/activate`

3. **Instalar Dependências Necessárias:**
   - Instale o boto3 para interagir com o S3 e outras bibliotecas necessárias:
   
   ```bash
   pip install boto3
   ```

4. **Criar um Arquivo `requirements.txt`:**
   - Para facilitar a instalação de dependências no futuro, crie um arquivo `requirements.txt`:
   
   ```bash
   boto3
   ```

   - Você pode gerar este arquivo automaticamente com:
   
   ```bash
   pip freeze > requirements.txt
   ```

### 2. Configuração do AWS IAM e S3

1. **Criar um Bucket S3:**
   - Siga os passos mencionados na seção anterior para criar um bucket S3 que será utilizado para armazenar os backups.

2. **Criar um Usuário IAM para o Projeto:**
   - Crie um usuário IAM com permissões para acessar o bucket S3 específico.
   - Obtenha as credenciais (Access Key ID e Secret Access Key) que serão usadas no código.

### 3. Implementação do Código Python

#### Passo 1: Configurar as Credenciais AWS

1. **Criar um Arquivo de Configuração:**
   - Crie um arquivo `.env` para armazenar as credenciais do S3 de forma segura:
   
   ```
   AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY
   AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY
   AWS_REGION=YOUR_REGION
   BUCKET_NAME=YOUR_BUCKET_NAME
   ```

   - Instale a biblioteca `python-dotenv` para carregar essas variáveis de ambiente:
   
   ```bash
   pip install python-dotenv
   ```

2. **Carregar as Credenciais no Código:**

   - No arquivo principal do projeto (`backup.py`), carregue as variáveis de ambiente:
   
   ```python
   import os
   from dotenv import load_dotenv

   load_dotenv()

   AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')
   AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')
   AWS_REGION = os.getenv('AWS_REGION')
   BUCKET_NAME = os.getenv('BUCKET_NAME')
   ```

#### Passo 2: Listar e Fazer Upload dos Arquivos

1. **Listar Arquivos em uma Pasta Local:**

   - No mesmo arquivo `backup.py`, adicione o código para listar todos os arquivos em uma pasta específica:
   
   ```python
   import os

   def listar_arquivos(pasta):
       arquivos = []
       for nome_arquivo in os.listdir(pasta):
           caminho_completo = os.path.join(pasta, nome_arquivo)
           if os.path.isfile(caminho_completo):
               arquivos.append(caminho_completo)
       return arquivos
   ```

2. **Fazer Upload dos Arquivos para o S3:**

   - Utilize o boto3 para fazer o upload dos arquivos listados para o S3:
   
   ```python
   import boto3

   s3_client = boto3.client(
       's3',
       aws_access_key_id=AWS_ACCESS_KEY_ID,
       aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
       region_name=AWS_REGION
   )

   def upload_arquivos_para_s3(arquivos):
       for arquivo in arquivos:
           nome_arquivo = os.path.basename(arquivo)
           s3_client.upload_file(arquivo, BUCKET_NAME, nome_arquivo)
           print(f'{nome_arquivo} foi enviado para o S3.')
   ```

3. **Deletar Arquivos Locais Após o Upload:**

   - Após o upload, delete os arquivos locais para liberar espaço:
   
   ```python
   def deletar_arquivos_locais(arquivos):
       for arquivo in arquivos:
           os.remove(arquivo)
           print(f'{arquivo} foi deletado do local.')
   ```

#### Passo 3: Integrar Tudo e Rodar o Backup

1. **Função Principal do Projeto:**

   - Combine todas as funções em uma função principal que executa o backup:
   
   ```python
   def executar_backup(pasta):
       arquivos = listar_arquivos(pasta)
       if arquivos:
           upload_arquivos_para_s3(arquivos)
           deletar_arquivos_locais(arquivos)
       else:
           print("Nenhum arquivo encontrado para backup.")

   if __name__ == "__main__":
       PASTA_LOCAL = 'caminho/para/sua/pasta'
       executar_backup(PASTA_LOCAL)
   ```

2. **Rodar o Projeto:**

   - Salve o código e execute o script:
   
   ```bash
   python backup.py
   ```

### 4. Testar e Implementar Melhorias

1. **Testar o Backup:**
   - Coloque alguns arquivos na pasta local e execute o script. Verifique se os arquivos foram enviados corretamente para o S3 e se foram deletados do seu computador.

2. **Melhorias Futuras:**
   - **Logs:** Adicione logging para monitorar as operações.
   - **Exceções:** Implemente tratamento de exceções para lidar com erros durante o upload ou a exclusão de arquivos.
   - **Agendamento:** Considere utilizar o `cron` no Linux ou o Agendador de Tarefas no Windows para agendar execuções automáticas do script.

================================================
File: /Bootcamp - Cloud para dados/Aula_02/projeto/main.py
================================================
import os
from typing import List
import boto3
from dotenv import load_dotenv

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Configurações da AWS a partir do .env
AWS_ACCESS_KEY_ID: str = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_ACCESS_KEY: str = os.getenv('AWS_SECRET_ACCESS_KEY')
AWS_REGION: str = os.getenv('AWS_REGION')
BUCKET_NAME: str = os.getenv('BUCKET_NAME')

# Configura o cliente S3
s3_client = boto3.client(
    's3',
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    region_name=AWS_REGION
)

def listar_arquivos(pasta: str) -> List[str]:
    """Lista todos os arquivos em uma pasta local."""
    arquivos: List[str] = []
    for nome_arquivo in os.listdir(pasta):
        caminho_completo = os.path.join(pasta, nome_arquivo)
        if os.path.isfile(caminho_completo):
            arquivos.append(caminho_completo)
    return arquivos

def upload_arquivos_para_s3(arquivos: List[str]) -> None:
    """Faz upload dos arquivos listados para o S3."""
    for arquivo in arquivos:
        nome_arquivo: str = os.path.basename(arquivo)
        s3_client.upload_file(arquivo, BUCKET_NAME, nome_arquivo)
        print(f'{nome_arquivo} foi enviado para o S3.')

def deletar_arquivos_locais(arquivos: List[str]) -> None:
    """Deleta os arquivos locais após o upload."""
    for arquivo in arquivos:
        os.remove(arquivo)
        print(f'{arquivo} foi deletado do local.')

def executar_backup(pasta: str) -> None:
    """Executa o processo completo de backup."""
    arquivos: List[str] = listar_arquivos(pasta)
    if arquivos:
        upload_arquivos_para_s3(arquivos)
        deletar_arquivos_locais(arquivos)
    else:
        print("Nenhum arquivo encontrado para backup.")

if __name__ == "__main__":
    PASTA_LOCAL: str = 'caminho/para/sua/pasta'  # Substitua pelo caminho da sua pasta local
    executar_backup(PASTA_LOCAL)


================================================
File: /Bootcamp - Cloud para dados/Aula_02/projeto/main_v2.py
================================================
import os
from typing import List
import boto3
from dotenv import load_dotenv

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Configurações da AWS a partir do .env
AWS_ACCESS_KEY_ID: str = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_ACCESS_KEY: str = os.getenv('AWS_SECRET_ACCESS_KEY')
AWS_REGION: str = os.getenv('AWS_REGION')
BUCKET_NAME: str = os.getenv('BUCKET_NAME')

# Print para verificar se as variáveis de ambiente foram carregadas corretamente
print(f"AWS_ACCESS_KEY_ID: {AWS_ACCESS_KEY_ID}")
print(f"AWS_SECRET_ACCESS_KEY: {AWS_SECRET_ACCESS_KEY}")
print(f"AWS_REGION: {AWS_REGION}")
print(f"BUCKET_NAME: {BUCKET_NAME}")

# Configura o cliente S3
try:
    s3_client = boto3.client(
        's3',
        aws_access_key_id=AWS_ACCESS_KEY_ID,
        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
        region_name=AWS_REGION
    )
    print("Cliente S3 configurado com sucesso.")
except Exception as e:
    print(f"Erro ao configurar o cliente S3: {e}")
    raise

def listar_arquivos(pasta: str) -> List[str]:
    """Lista todos os arquivos em uma pasta local."""
    arquivos: List[str] = []
    try:
        for nome_arquivo in os.listdir(pasta):
            caminho_completo = os.path.join(pasta, nome_arquivo)
            if os.path.isfile(caminho_completo):
                arquivos.append(caminho_completo)
        print(f"Arquivos listados na pasta '{pasta}': {arquivos}")
    except Exception as e:
        print(f"Erro ao listar arquivos na pasta '{pasta}': {e}")
        raise
    return arquivos

def upload_arquivos_para_s3(arquivos: List[str]) -> None:
    """Faz upload dos arquivos listados para o S3."""
    for arquivo in arquivos:
        nome_arquivo: str = os.path.basename(arquivo)
        try:
            print(f"Tentando fazer upload de '{nome_arquivo}' para o bucket '{BUCKET_NAME}'...")
            s3_client.upload_file(arquivo, BUCKET_NAME, nome_arquivo)
            print(f"{nome_arquivo} foi enviado para o S3.")
        except Exception as e:
            print(f"Erro ao enviar '{nome_arquivo}' para o S3: {e}")
            raise

def deletar_arquivos_locais(arquivos: List[str]) -> None:
    """Deleta os arquivos locais após o upload."""
    for arquivo in arquivos:
        try:
            os.remove(arquivo)
            print(f"{arquivo} foi deletado do local.")
        except Exception as e:
            print(f"Erro ao deletar o arquivo '{arquivo}': {e}")
            raise

def executar_backup(pasta: str) -> None:
    """Executa o processo completo de backup."""
    try:
        print(f"Iniciando o processo de backup para a pasta '{pasta}'...")
        arquivos: List[str] = listar_arquivos(pasta)
        if arquivos:
            upload_arquivos_para_s3(arquivos)
            deletar_arquivos_locais(arquivos)
        else:
            print("Nenhum arquivo encontrado para backup.")
    except Exception as e:
        print(f"Erro no processo de backup: {e}")
        raise

if __name__ == "__main__":
    PASTA_LOCAL: str = 'download'  # Substitua pelo caminho da sua pasta local
    try:
        executar_backup(PASTA_LOCAL)
    except Exception as e:
        print(f"Erro ao executar o backup: {e}")


================================================
File: /Bootcamp - Cloud para dados/Aula_02/projeto/.env
================================================
# Credenciais AWS
AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY
AWS_REGION=YOUR_AWS_REGION

# Nome do Bucket S3
BUCKET_NAME=YOUR_BUCKET_NAME


================================================
File: /Bootcamp - Cloud para dados/Aula_03/README.md
================================================
# Bootcamp Cloud: Aula 03

## EC2: Computação Escalável na AWS

**Objetivo:** Introduzir o Amazon EC2 (Elastic Compute Cloud), explorando suas funcionalidades principais, configurações e boas práticas. Durante a aula, criaremos uma instância EC2, configuraremos uma aplicação simples e discutiremos como utilizar o serviço para diferentes cenários na engenharia de dados.

---

### 1. Revisão da Aula Anterior: Configurando e Utilizando o S3

**Objetivo:** Revisar os conceitos e práticas da aula anterior sobre o Amazon S3, focando na criação e configuração de buckets, upload e download de arquivos, e como o S3 pode ser integrado em pipelines de dados.

#### Passo 1: Revisão da Criação de Buckets

1. **Criar um Bucket S3:**
   - Revisamos o processo de criação de um bucket S3, incluindo a escolha da região, configuração de controle de acesso e habilitação de versionamento.

2. **Aplicar Regras de Ciclo de Vida:**
   - Exploramos como configurar regras de ciclo de vida para mover dados entre diferentes classes de armazenamento e excluir objetos automaticamente após um período definido.

#### Passo 2: Revisão do Uso do boto3 para Interagir com o S3

1. **Criar e Listar Buckets:**
   - Utilizamos o boto3 para criar um bucket e listar todos os buckets existentes na conta AWS.

2. **Upload e Download de Arquivos:**
   - Praticamos o upload e download de arquivos entre o sistema local e o S3 utilizando o boto3, além de configurar permissões de acesso.

3. **Excluir Objetos e Buckets:**
   - Exploramos como excluir objetos e buckets utilizando comandos simples do boto3.

#### Passo 3: Revisão de Casos de Uso do S3

1. **Data Lake e ETL:**
   - Discutimos como o S3 é utilizado em arquiteturas de data lake e pipelines ETL, armazenando dados brutos e processados.

2. **Backup e Arquivamento:**
   - Exploramos o uso do S3 para backup e arquivamento de dados críticos, com a utilização de regras de ciclo de vida para otimizar custos.

---

## 2. O que é o EC2?

1. **Definição:**
   - Amazon EC2 (Elastic Compute Cloud) é um serviço de computação em nuvem que oferece capacidade de processamento escalável. Com o EC2, você pode lançar instâncias de máquinas virtuais (VMs) em questão de minutos, configurando-as de acordo com as necessidades específicas do seu projeto.

2. **Componentes Principais do EC2:**
   - **Instâncias:** Máquinas virtuais que rodam sistemas operacionais e aplicativos.
   - **Tipos de Instância:** Variam em termos de CPU, memória, armazenamento e capacidade de rede, permitindo otimizar custos e performance para diferentes workloads.
   - **AMIs (Amazon Machine Images):** Modelos predefinidos que incluem o sistema operacional e software inicial necessário para iniciar uma instância.
   - **Volumes EBS (Elastic Block Store):** Armazenamento persistente que pode ser conectado às instâncias EC2.
   - **Grupos de Segurança:** Firewalls virtuais que controlam o tráfego de entrada e saída das instâncias EC2.

---

## 3. Página Principal do EC2

### 3.1 Nível Gratuito do EC2

O Amazon EC2 oferece uma camada gratuita para novos usuários da AWS, permitindo que explorem e experimentem o serviço sem custos iniciais, dentro de determinados limites.

#### 3.1.1 Uso da Oferta (Mensal)

- **Instâncias do Linux do EC2:**
  - **Utilização:** 11%
  - **Horas Restantes:** 668 horas

Essa oferta inclui até 750 horas de uso de instâncias t2.micro ou t3.micro por mês, tanto para Linux quanto para Windows, durante os primeiros 12 meses após a criação da conta.

#### 3.1.2 Espaço de Armazenamento no EBS

- **Espaço Disponível:** A oferta gratuita também inclui 30 GB de armazenamento em volumes EBS (Elastic Block Store) de propósito geral ou magnéticos.

### 3.2 Integridade do Serviço

A AWS mantém uma alta disponibilidade de seus serviços em diferentes zonas de disponibilidade (AZs) dentro de uma região.

#### 3.2.1 Zonas

As zonas de disponibilidade são locais físicos isolados dentro de uma região da AWS, projetados para operar de forma independente em caso de falhas.

- **Nome da Zona:** `us-east-1a`
  - **ID da Zona:** `use1-az6`

- **Nome da Zona:** `us-east-1b`
  - **ID da Zona:** `use1-az1`

- **Nome da Zona:** `us-east-1c`
  - **ID da Zona:** `use1-az2`

### 3.3 Atributos da Conta

Os atributos da conta fornecem informações sobre a configuração e os recursos padrão disponíveis para sua conta AWS.

#### 3.3.1 VPC Padrão

- **ID da VPC:** `vpc-0cab8f7db8d745f83`

Cada conta AWS tem uma VPC padrão em cada região, que pode ser usada para criar e gerenciar recursos de rede.

#### 3.3.2 Configurações

As configurações de proteção e segurança de dados são fundamentais para garantir que seus recursos na AWS estejam seguros e protegidos.

#### 3.3.3 Console Serial do EC2

O Console Serial do EC2 é uma ferramenta que permite acesso direto ao console de uma instância para solução de problemas em instâncias EC2 que não estão acessíveis pela rede.

---

## 4. Configuração e Lançamento de uma Instância EC2

### 4.1 Criando uma Instância EC2 via AWS Management Console

1. **Acessando o AWS Management Console:**
   - Faça login no [AWS Management Console](https://aws.amazon.com) e navegue até o serviço EC2. Isso pode ser feito digitando "EC2" na barra de pesquisa superior e selecionando "EC2" nos resultados.

2. **Iniciando o Lançamento de uma Instância:**
   - Na página do EC2, clique em “Launch Instance” para iniciar o processo de criação.

3. **Escolhendo uma Amazon Machine Image (AMI):**
   - **AMI:** Selecione uma AMI que contenha o sistema operacional desejado. AMIs podem ser públicas, privadas ou compartilhadas. 
   - **Exemplo:** Escolha uma AMI Amazon Linux 2 ou Ubuntu.

4. **Selecionando o Tipo de Instância:**
   - **Tipo de Instância:** Escolha o tipo de instância que melhor se adequa ao seu workload. Tipos como t2.micro são elegíveis para o nível gratuito, ideal para testes e pequenas aplicações.
   - **Exemplo:** `t2.micro` (1 vCPU, 1 GiB RAM).

5. **Configuração da Instância:**
   - **Número de Instâncias:** Defina quantas instâncias deseja lançar.
   - **Network:** Selecione a VPC e sub-rede onde a instância será criada.
   - **IAM Role:** Atribua uma IAM role se a instância precisar acessar outros serviços AWS.

6. **Configuração de Armazenamento (Volumes EBS):**
   - **Adicionar Volumes:** Configure volumes EBS adicionais se necessário. O volume root é criado automaticamente com a AMI selecionada.
   - **Exemplo:** 8 GiB de General Purpose SSD.

7. **Configuração de Grupos de Segurança:**
   - **Criar/Selecionar Grupo de Segurança:** Configure as regras de firewall para controlar o tráfego de entrada e saída.
   - **Exemplo:** Permitir SSH (porta 22) de um IP específico para acessar a instância.

8. **Revisão e Lançamento:**
   - Revise todas as configurações e clique em “Launch”. Será solicitado que você selecione ou crie um par de chaves para acessar a instância via SSH.

### 4.2 Acessando a Instância via SSH

1. **Obtenção do IP Público da Instância:**
   - Após o lançamento, o EC2 atribui um IP público à instância. Este IP pode ser encontrado no console EC2.

2. **Acessando via SSH:**
   - Use o terminal ou um cliente SSH para acessar a instância.
   - **Exemplo de Comando SSH:**
     ```bash
     ssh -i "minha-chave.pem" ec2-user@ec2-xx-xxx-xx-xx.compute-1.amazonaws.com
     ```

3. **Configuração Inicial:**
   - Uma vez conectado, você pode atualizar o sistema operacional, instalar pacotes adicionais e configurar sua aplicação.

---

### 5. Famílias de Instâncias EC2

O Amazon EC2 oferece várias famílias de instâncias, cada uma projetada para diferentes tipos de workloads. As famílias são categorizadas com base em fatores como uso geral, computação intensiva, memória intensiva e otimização com hardware especializado. Abaixo estão as principais famílias de instâncias EC2 e suas respectivas aplicações em projetos de dados.

#### 5.1 General Purpose (Propósito Geral)

**Descrição:**
As instâncias General Purpose são projetadas para fornecer um equilíbrio de recursos de computação, memória e rede, tornando-as ide

ais para uma ampla variedade de workloads. Elas oferecem uma combinação versátil de recursos, sendo adequadas para a maioria das aplicações.

**Tipos de Instância:**
- **t3, t3a, t4g:** Instâncias com desempenho balanceado que são ideais para aplicações que precisam de uma carga moderada de computação e memória.
- **m6g, m6i, m5:** Oferecem uma proporção mais equilibrada de CPU e memória, sendo ideais para aplicações que exigem um pouco mais de recursos.

**Aplicações em Dados:**
- **Ambientes de Desenvolvimento e Teste:** Instâncias General Purpose são frequentemente usadas para configurar ambientes de desenvolvimento, testar pipelines de dados e executar simulações em pequena escala.
- **Aplicações de Banco de Dados:** Para bancos de dados de uso geral que não requerem otimização extrema de computação ou memória, como pequenos clusters de banco de dados NoSQL.
- **Servidores Web e de Aplicações:** Adequadas para hospedar servidores que processam dados e fornecem APIs para consultas e análises.

#### 5.2 Compute Optimized (Otimização de Computação)

**Descrição:**
As instâncias Compute Optimized são projetadas para workloads que exigem uma alta taxa de computação por núcleo de processador. Elas oferecem alta performance para tarefas intensivas em computação.

**Tipos de Instância:**
- **c7g, c6g, c6i, c5:** Oferecem uma alta relação de vCPU para memória, otimizando o desempenho para aplicações de computação intensiva.

**Aplicações em Dados:**
- **Processamento de Dados em Lote:** Ideal para tarefas como processamento em lote de grandes volumes de dados, onde o tempo de computação é um fator crítico.
- **Analytics e Modelagem de Dados:** Utilizadas em análises avançadas e modelagem estatística que requerem um grande poder de processamento.
- **Simulações e Modelagem Computacional:** Para workloads que envolvem cálculos complexos, como simulações financeiras ou científicas.

#### 5.3 Memory Optimized (Otimização de Memória)

**Descrição:**
As instâncias Memory Optimized são projetadas para workloads que exigem grandes quantidades de memória RAM, oferecendo alta capacidade de memória para processar grandes volumes de dados em memória.

**Tipos de Instância:**
- **r6g, r6i, r5, r5b:** Estas instâncias oferecem uma alta proporção de memória em relação ao número de vCPUs, sendo ideais para aplicações que precisam de grandes volumes de memória.
- **x2gd, x2idn:** São projetadas para cargas de trabalho que necessitam de ainda mais memória, como grandes bancos de dados em memória.

**Aplicações em Dados:**
- **Bancos de Dados em Memória:** Ideal para bancos de dados que mantêm grandes quantidades de dados na RAM, como Redis, Memcached ou bancos de dados em memória do SAP HANA.
- **Análise em Tempo Real:** Utilizadas em aplicações de análise de grandes volumes de dados em tempo real, onde a capacidade de processar dados diretamente na memória é crítica.
- **Big Data e Data Warehousing:** Adequadas para processar grandes volumes de dados em sistemas de data warehousing, como Amazon Redshift, ou para operações de ETL complexas.

#### 5.4 Accelerated Computing (Computação Acelerada)

**Descrição:**
As instâncias Accelerated Computing são projetadas para workloads que podem se beneficiar de hardware especializado, como GPUs ou FPGAs, proporcionando um aumento significativo no desempenho para tarefas específicas.

**Tipos de Instância:**
- **p4, p3:** Instâncias com GPUs NVIDIA, otimizadas para tarefas de deep learning, inferência de machine learning e simulações científicas.
- **g5g, g4ad:** Oferecem GPUs voltadas para renderização gráfica e codificação de vídeo.
- **f1:** Instâncias com FPGAs (Field Programmable Gate Arrays), adequadas para tarefas como criptografia e processamento de sinais.

**Aplicações em Dados:**
- **Treinamento de Modelos de Machine Learning:** Instâncias com GPUs são ideais para treinar modelos complexos de deep learning, permitindo um treinamento mais rápido e eficiente.
- **Inferência em Tempo Real:** Para aplicações de machine learning que requerem inferência em tempo real com latência mínima.
- **Simulações Computacionais e Modelagem 3D:** Utilizadas em simulações que exigem alto poder de processamento gráfico ou operações de matemática intensiva.
- **Análise de Vídeo e Imagem:** Ideal para processar e analisar grandes volumes de dados de vídeo e imagem em tempo real, como em sistemas de vigilância ou análise de conteúdo multimídia.

---

Cada uma dessas famílias de instâncias EC2 é otimizada para diferentes tipos de workloads e oferece recursos específicos que podem ser usados para maximizar a eficiência e o desempenho em projetos de dados. Escolher a instância certa depende dos requisitos específicos do seu projeto, como a quantidade de dados a ser processada, a complexidade das operações e a necessidade de memória, computação ou aceleração de hardware.

---

## 6. Tipos de Preço do EC2

O Amazon EC2 oferece diferentes modelos de precificação para atender às diversas necessidades e orçamentos dos usuários. Compreender esses modelos é essencial para otimizar custos e garantir que você está utilizando os recursos de maneira eficiente. A seguir, são apresentados os principais tipos de preço do EC2:

#### 6.1 Instâncias Sob Demanda (On-Demand)

**Descrição:**
As instâncias Sob Demanda permitem que você pague por capacidade de computação por hora ou por segundo (dependendo do tipo de instância) sem compromisso a longo prazo. Este modelo é ideal para cargas de trabalho que são imprevisíveis ou variáveis.

**Vantagens:**
- **Flexibilidade:** Você pode iniciar, parar e terminar instâncias a qualquer momento sem penalidades.
- **Sem Compromisso Inicial:** Não há necessidade de investir em contratos de longo prazo.
- **Escalabilidade:** Fácil de escalar conforme a demanda aumenta ou diminui.

**Aplicações Comuns:**
- Ambientes de desenvolvimento e teste.
- Aplicações com cargas de trabalho variáveis ou imprevisíveis.
- Projetos de curto prazo que não justificam um compromisso a longo prazo.

#### 6.2 Savings Plans (Planos de Economia)

**Descrição:**
Os Savings Plans oferecem descontos significativos (até 72%) em comparação com as tarifas Sob Demanda em troca de um compromisso de uso consistente (medido em dólares por hora) por um período de 1 ou 3 anos. Existem dois tipos principais de Savings Plans:

- **Compute Savings Plans:** Oferecem maior flexibilidade, aplicando-se a qualquer instância EC2 independente da região, tipo de instância, sistema operacional ou tenancy.
- **EC2 Instance Savings Plans:** Oferecem descontos maiores, mas são aplicáveis apenas a instâncias específicas dentro de uma família de instâncias e região selecionadas.

**Vantagens:**
- **Descontos Significativos:** Economia considerável em comparação com o modelo Sob Demanda.
- **Flexibilidade (no caso dos Compute Savings Plans):** Possibilidade de alterar tipos de instância e regiões sem perder os descontos.
- **Previsibilidade de Custos:** Facilita o planejamento financeiro com base no compromisso de uso.

**Aplicações Comuns:**
- Cargas de trabalho estáveis e previsíveis que podem se comprometer com um nível de uso consistente.
- Empresas que buscam otimizar custos a longo prazo.

#### 6.3 Instâncias Spot

**Descrição:**
As Instâncias Spot permitem que você aproveite a capacidade ociosa da AWS com descontos de até 90% em relação às tarifas Sob Demanda. No entanto, a AWS pode interromper essas instâncias com um aviso prévio de dois minutos quando precisar recuperar a capacidade.

**Vantagens:**
- **Custo Reduzido:** Descontos significativos tornam as Instâncias Spot extremamente econômicas.
- **Escalabilidade:** Ideal para workloads que podem ser interrompidos e retomados sem impacto significativo.

**Desvantagens:**
- **Interrupções:** A possibilidade de interrupção a qualquer momento pode não ser adequada para todas as aplicações.
- **Disponibilidade Variável:** A disponibilidade das Instâncias Spot pode variar conforme a demanda.

**Aplicações Comuns:**
- Processamento em lote e tarefas de ETL.
- Treinamento de modelos de machine learning que podem ser reiniciados.
- Aplicações distribuídas e resilientes que podem lidar com interrupções, como clusters de Hadoop ou Spark.

#### 6.4 Instâncias Dedicadas (Dedicated Instances)

**Descrição:**
As Instâncias Dedicadas são executadas em hardware físico dedicado exclusivamente para a sua conta AWS. Isso garante que suas instâncias não compartilhem hardware com instâncias de outros clientes.

**Vantagens:**
- **Isolamento Físico:** Maior segurança e conformidade para cargas de trabalho sensíveis.
- **Controle de Hardware:** Garantia de que os recursos de hardware não serão compartilhados com outros clientes.

**Desvantagens:**
- **Custo Mais Elevado:** Geralmente, as Instâncias Dedicadas são mais caras do que as instâncias compartilhadas.
- **Menor Flexibilidade de Preço:** Não se beneficiam tanto de descontos como os Savings Plans ou Instâncias Spot.

**Aplicações Comuns:**
- Aplicações que exigem conformidade regulatória específica.
- Cargas de trabalho sensíveis que necessitam de isolamento físico por razões de segurança.
- Ambientes empresariais que requerem controle total sobre o hardware subjacente.

---

### Comparação dos Modelos de Pre

ço

| Modelo de Preço       | Flexibilidade | Desconto em Relação ao On-Demand | Compromisso Necessário | Adequado para                                       |
|-----------------------|---------------|-----------------------------------|------------------------|-----------------------------------------------------|
| Sob Demanda           | Alta          | 0%                                | Nenhum                 | Cargas de trabalho imprevisíveis ou variáveis       |
| Savings Plans         | Média/Alta    | Até 72%                            | Sim (1 ou 3 anos)      | Cargas de trabalho estáveis e previsíveis           |
| Instâncias Spot       | Baixa         | Até 90%                            | Não                    | Processamento em lote, machine learning, workloads resilientes |
| Instâncias Dedicadas  | Média         | 0-30%                             | Nenhum ou contratual    | Aplicações sensíveis, conformidade regulatória      |

### Considerações para Escolha do Modelo de Preço

- **Natureza da Carga de Trabalho:** Se sua aplicação pode tolerar interrupções, as Instâncias Spot podem oferecer enormes economias. Para cargas de trabalho críticas que não podem ser interrompidas, as Instâncias Sob Demanda ou Savings Plans são mais adequadas.
- **Previsibilidade de Uso:** Se você pode prever consistentemente o uso de computação, os Savings Plans proporcionam uma excelente relação custo-benefício.
- **Requisitos de Segurança e Conformidade:** Instâncias Dedicadas são necessárias para certas aplicações que exigem isolamento físico.
- **Orçamento e Otimização de Custos:** Avalie o equilíbrio entre economia e flexibilidade para selecionar o modelo que melhor se alinha ao seu orçamento e necessidades operacionais.

### Estratégias para Otimização de Custos

1. **Mix de Modelos de Preço:** Utilizar uma combinação de diferentes modelos de preço (por exemplo, Sob Demanda para cargas variáveis, Savings Plans para cargas estáveis e Spot para tarefas flexíveis) pode maximizar a economia sem comprometer a performance.
2. **Monitoramento e Ajustes Contínuos:** Utilize ferramentas como AWS Cost Explorer e AWS Trusted Advisor para monitorar o uso e ajustar as estratégias de compra conforme necessário.
3. **Automação com Auto Scaling:** Configure grupos de Auto Scaling para aproveitar automaticamente as Instâncias Spot quando disponíveis e mudar para instâncias Sob Demanda ou reservadas quando necessário.
4. **Escolha de Regiões e Tipos de Instância:** Selecionar regiões com custos mais baixos e tipos de instância que melhor atendem às necessidades de desempenho pode reduzir significativamente os custos.


### 1. **Configurar a Instância EC2**

Aqui está o passo a passo atualizado para configurar o Airflow em uma máquina Linux usando `apt` ao invés de `yum`, seguido por instruções sobre como garantir que sua DAG seja carregada corretamente na interface do Airflow.

### 1. **Atualizar os Pacotes do Sistema**

Primeiro, certifique-se de que todos os pacotes do sistema estão atualizados:

```bash
sudo apt update
sudo apt upgrade -y
```

### 2. **Instalar o `pip3`**

Instale o gerenciador de pacotes `pip3` para Python:

```bash
sudo apt install -y python3-pip
```

### 3. **Instalar o SQLite**

Instale o banco de dados SQLite, que é necessário para o Airflow funcionar corretamente:

```bash
sudo apt install -y sqlite3
```

### 4. **Instalar o Ambiente Virtual do Python**

Instale o módulo `venv` para criar um ambiente virtual isolado para o Airflow:

```bash
sudo apt install -y python3.12-venv
```

### 5. **Instalar as Dependências do PostgreSQL**

Instale a biblioteca `libpq-dev` que é necessária para o suporte ao PostgreSQL no Airflow:

```bash
sudo apt-get install -y libpq-dev
```

### 6. **Criar um Ambiente Virtual**

Crie um ambiente virtual Python para isolar as dependências do Airflow:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 7. **Instalar o Apache Airflow**

Instale o Apache Airflow usando `pip` dentro do ambiente virtual:

```bash
pip install "apache-airflow[celery]==2.10.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.0/constraints-3.8.txt"
```

### 8. **Inicializar o Banco de Dados do Airflow**

Migre o banco de dados do Airflow para garantir que todas as tabelas necessárias sejam criadas:

```bash
airflow db migrate
```

### 9. **Criar um Usuário Administrador**

Crie um usuário administrador para acessar a interface web do Airflow:

```bash
airflow users create \
    --username admin \
    --firstname Peter \
    --lastname Parker \
    --role Admin \
    --email spiderman@superhero.org
```

### 10. **Iniciar o Servidor Web do Airflow**

Inicie o servidor web do Airflow em segundo plano:

```bash
airflow webserver &
```

### 11. **Iniciar o Scheduler do Airflow**

Por fim, inicie o scheduler do Airflow para gerenciar a execução dos DAGs:

```bash
airflow scheduler &
```

### 12. **Adicionar uma Nova DAG**

Para adicionar uma nova DAG:

1. **Criar o Arquivo da DAG:**

   Navegue até o diretório de DAGs do Airflow e crie um novo arquivo Python para a DAG:

   ```bash
   cd ~/airflow/dags
   nano minha_dag.py
   ```

   **Exemplo de Conteúdo da DAG:**

   ```python
   from airflow import DAG
   from airflow.operators.dummy import DummyOperator
   from datetime import datetime

   default_args = {
       'owner': 'airflow',
       'start_date': datetime(2024, 8, 26),
   }

   with DAG('minha_dag',
            default_args=default_args,
            schedule_interval='@daily',
            catchup=False) as dag:

       start = DummyOperator(task_id='start')

       start
   ```

2. **Salvar e Sair:**

   Salve o arquivo no `nano` pressionando `CTRL + O`, e saia pressionando `CTRL + X`.

### 13. **Verificar se a DAG Aparece na UI**

Se a nova DAG não aparecer na UI:

1. **Verifique o Scheduler:**

   Certifique-se de que o scheduler está rodando corretamente:

   ```bash
   airflow scheduler
   ```

2. **Reinicie o Scheduler:**

   Se necessário, reinicie o scheduler para carregar as novas DAGs:

   ```bash
   pkill -f "airflow scheduler"
   airflow scheduler &
   ```

3. **Verifique as Configurações do Airflow:**

   Verifique se o diretório `dags_folder` no `airflow.cfg` aponta para o diretório onde sua DAG está localizada (`~/airflow/dags`).

### 14. **Acessar o Airflow via Interface Web**

Finalmente, acesse a interface web do Airflow no navegador:

================================================
File: /Bootcamp - Cloud para dados/Aula_04/README.md
================================================
# **Bootcamp Cloud: Aula 04: IAM na AWS**

**Objetivo**: Nesta aula, vamos explorar o IAM (Identity and Access Management) da AWS. Vamos entender como proteger a conta AWS, o papel do usuário root, como criar e gerenciar usuários, grupos, políticas, e configurar o MFA.

### **1. Protegendo a Conta AWS**

- **Importância**: A conta AWS é o coração da sua infraestrutura na nuvem. Protegê-la é essencial para evitar acessos não autorizados e garantir a segurança dos seus recursos.

### **2. Usuário Root**

- **O que é o Usuário Root**: 
  - O usuário root é a conta inicial criada ao configurar a AWS. Ele tem acesso total a todos os recursos e configurações.
  - **Riscos**: O uso contínuo do usuário root é arriscado, pois ele tem permissões ilimitadas, o que pode levar a potenciais danos em caso de comprometimento.

- **Boas Práticas**:
  - Evitar o uso diário do usuário root.
  - Criar usuários IAM com permissões específicas para tarefas do dia a dia.
  - Habilitar o MFA (Multi-Factor Authentication) para a conta root.

### **3. IAM (Identity and Access Management)**

- **O que é IAM**:
  - IAM permite criar e gerenciar usuários, grupos, e permissões na AWS.
  - Facilita a implementação do princípio do menor privilégio, garantindo que cada usuário tenha apenas as permissões necessárias.

### **4. Configuração do MFA (Multi-Factor Authentication)**

- **O que é MFA**: 
  - MFA adiciona uma camada extra de segurança, exigindo que o usuário forneça uma segunda forma de autenticação além da senha, como um código gerado por um dispositivo móvel.

- **Passo a Passo para Configurar o MFA no Usuário Root**:
  
  1. **Acessar o Console de Gerenciamento da AWS**:
     - Faça login como o usuário root.
  
  2. **Navegar até a Página de Segurança da Conta**:
     - No canto superior direito, clique no nome da conta e selecione "Minha Conta".
     - Role para baixo até "Configurações de segurança" e clique em "Ativar MFA" na seção de autenticação multifator.

  3. **Escolher o Tipo de Dispositivo MFA**:
     - Selecione "Aplicativo autenticador" para usar um dispositivo móvel como segundo fator de autenticação.

  4. **Configurar o Aplicativo Autenticador**:
     - Abra o aplicativo autenticador no seu dispositivo móvel (ex: Google Authenticator).
     - Escaneie o código QR fornecido pela AWS ou insira a chave manualmente.
  
  5. **Verificar o Código MFA**:
     - Insira os códigos gerados pelo aplicativo para verificar a configuração.
  
  6. **Salvar a Configuração**:
     - Confirme e salve a configuração do MFA.

  7. **Testar a Configuração**:
     - Saia e faça login novamente para verificar se o MFA está funcionando corretamente.

---

### **5. Criando um Usuário Administrativo**

**Passo a Passo para Criar um Usuário Administrativo:**

1. **Acessar o Console IAM**:
   - **Faça login** no console da AWS com o usuário root.
   - Navegue até o serviço **IAM (Identity and Access Management)**.
   
2. **Adicionar Novo Usuário**:
   - Clique em **"Usuários"** e selecione **"Adicionar usuário"**.
   - Insira um nome para o novo usuário administrativo, como "admin-user".

3. **Selecionar o Tipo de Acesso**:
   - **Acesso programático**: Marque essa opção para gerar **chaves de acesso** (Access Key ID e Secret Access Key), necessárias para scripts ou automações que interagem com a AWS via API.
   - **Acesso à AWS Management Console**: Marque essa opção para permitir que o usuário faça login no console da AWS. Defina uma senha inicial (você pode permitir que o usuário a redefina no primeiro login ou definir uma senha permanente).

4. **Configurar Permissões**:
   - **Anexar Políticas Diretamente**: 
     - **AdministratorAccess**: Selecione esta política para conceder ao usuário todas as permissões administrativas, permitindo acesso completo a todos os recursos da AWS.
     - **PowerUserAccess** (Alternativa): Selecione esta política se desejar conceder permissões administrativas amplas, mas sem acesso ao gerenciamento de contas, como a criação de novos usuários IAM ou configuração de billing.

5. **Revisar e Criar o Usuário**:
   - Revise as configurações e clique em **"Criar usuário"**.
   - **Download do CSV**: Faça o download do arquivo CSV contendo as chaves de acesso e a senha do console para uso futuro. Estas informações são essenciais para acessar a AWS programaticamente e via console.

6. **Encerramento do Uso do Usuário Root**:
   - Após a criação do usuário administrativo, saia do console e faça login novamente utilizando as credenciais do novo usuário.
   - A partir deste ponto, o uso do usuário root deve ser restrito a tarefas críticas e configurações de segurança inicial.

---

### **6. Comparação: Root vs AdministratorAccess vs PowerUserAccess**

| **Característica**                  | **Usuário Root**                                      | **AdministratorAccess**                          | **PowerUserAccess**                            |
|-------------------------------------|------------------------------------------------------|-------------------------------------------------|------------------------------------------------|
| **Acesso Total**                    | Sim, acesso total e irrestrito a todos os recursos   | Sim, acesso completo a quase todos os recursos  | Não, acesso restrito a certos recursos        |
| **Gerenciamento de IAM**            | Sim                                                  | Sim                                              | Não, sem acesso ao IAM                        |
| **Configuração de Billing**         | Sim                                                  | Sim                                              | Não                                            |
| **Criação de Recursos**             | Sim                                                  | Sim                                              | Sim                                            |
| **Modificação de Políticas de Conta**| Sim                                                  | Sim                                              | Não                                            |
| **Gerenciamento de Faturamento e Conta** | Sim, incluindo alteração de informações de pagamento e fechamento da conta | Não                                              | Não                                            |
| **Cancelamento de Serviços e Fechamento da Conta** | Sim | Não | Não |
| **Alteração de Suporte AWS**        | Sim, pode alterar o plano de suporte (ex: suporte básico para empresarial) | Não | Não |
| **Exclusão de CloudFront Key Pairs**| Sim, pode criar, gerenciar ou excluir CloudFront key pairs | Não | Não |
| **Uso recomendado**                 | Configuração inicial e tarefas críticas de segurança | Uso diário para administração e operações gerais | Uso para administração sem acesso a IAM e Billing |

---

### **7. Questões Frequentes**

1. **O AdministratorAccess pode alterar o Usuário Root?**

   Não, um usuário com a política AdministratorAccess não pode alterar o usuário root. Somente o próprio usuário root pode alterar suas próprias configurações, como o nome de usuário, senha, chaves de acesso, ou desativar o MFA. Isso é uma medida de segurança importante para proteger a conta AWS, garantindo que apenas o usuário root tenha controle total sobre suas próprias credenciais e configurações.

2. **Eu contratei uma consultoria, qual acesso criar?**

   A escolha entre conceder `AdministratorAccess` ou `PowerUserAccess` à consultoria depende da natureza das tarefas que eles irão realizar e do nível de controle que você deseja manter sobre sua conta AWS.

   - **Quando conceder `AdministratorAccess`**:
     - **Cenário**: A consultoria precisa ter acesso completo para gerenciar todos os recursos da AWS, incluindo a criação e gerenciamento de usuários IAM, configuração de políticas, gerenciamento de billing, e outras tarefas administrativas completas.
     - **Risco**: Eles terão permissão para alterar quase todos os aspectos da sua conta, incluindo ações sensíveis que podem impactar a segurança ou o faturamento.

   - **Quando conceder `PowerUserAccess`**:
     - **Cenário**: A consultoria precisa realizar tarefas administrativas gerais, como criar e gerenciar recursos, mas você deseja limitar o acesso a configurações de conta e gerenciamento de usuários IAM.
     - **Risco**: Eles não poderão criar ou gerenciar usuários IAM, alterar configurações de faturamento, ou fazer mudanças no plano de suporte, o que oferece um nível adicional de segurança e controle sobre aspectos críticos da sua conta.

   - **Recomendação**:
     - **PowerUserAccess** pode ser mais apropriado se você quiser manter controle sobre as configurações mais sensíveis da sua conta, como gerenciamento de usuários IAM e informações de faturamento. 
     - **AdministratorAccess** deve ser concedido somente se a consultoria realmente precisar de controle total sobre todos os aspectos da sua infraestrutura AWS, e você confia plenamente que eles irão gerenciar esses recursos de forma segura e responsável. 

---

### **8. Criando um Grupo de "Engenheiro de Dados" no IAM**

**Passo a Passo para Criar um Grupo de "Engenheiro de Dados" no IAM**

1. **Acessar o Console IAM:**
   - Faça login no console da AWS com um usuário que tenha permissões suficientes para gerenciar o IAM.
   - Navegue até o serviço **IAM (Identity and Access Management)**.

2. **Criar um Novo Grupo:**
   - No painel do IAM, selecione **"Gr

upos"** no menu lateral.
   - Clique em **"Criar Novo Grupo"**.
   - Insira o nome do grupo como **"EngenheiroDeDados"**.

3. **Copiar Permissões de um Grupo Existente (Opcional):**
   - Se você já possui um grupo com permissões similares e deseja copiar suas permissões, selecione essa opção e escolha o grupo de referência.
   - Caso contrário, pule esta etapa e prossiga para anexar políticas diretamente.

4. **Anexar Políticas Diretamente:**
   - Na tela de anexar políticas, você verá uma lista de políticas gerenciadas pela AWS.
   - Para o grupo de "Engenheiro de Dados", selecione as seguintes políticas relevantes:

     - **AmazonS3FullAccess**: Dá ao grupo acesso completo ao Amazon S3, permitindo criar, ler, escrever e excluir buckets e objetos.
     - **AmazonEC2FullAccess**: Permite ao grupo gerenciar instâncias EC2, incluindo o provisionamento, configuração e encerramento de instâncias.
     - **AmazonVPCFullAccess**: Concede ao grupo a capacidade de gerenciar redes VPC, sub-redes, roteamento e segurança de redes.
     - **AmazonRDSFullAccess**: Concede permissão para gerenciar instâncias de bancos de dados no Amazon RDS.
     - **AmazonGlueConsoleFullAccess**: Permite ao grupo acessar e usar o serviço de ETL Amazon Glue para operações de dados.
     - **AmazonAthenaFullAccess**: Dá ao grupo a permissão para consultar dados armazenados no S3 usando o Amazon Athena.

   - Após selecionar as políticas desejadas, clique em **"Next Step"**.

5. **Revisar o Grupo:**
   - Revise as permissões que serão concedidas ao grupo.
   - Verifique se todas as políticas necessárias estão anexadas.

6. **Criar o Grupo:**
   - Clique em **"Create Group"** para finalizar a criação do grupo "EngenheiroDeDados".

7. **Adicionar Usuários ao Grupo:**
   - Após a criação do grupo, você pode adicionar usuários existentes ao grupo "EngenheiroDeDados" ou criar novos usuários e associá-los ao grupo.
   - Isso garantirá que todos os membros do grupo tenham as mesmas permissões para acessar e gerenciar os recursos associados ao projeto de dados.

---

### **9. Tags Sugeridas para o Grupo "Engenheiro de Dados"**

As tags ajudam a organizar e identificar recursos relacionados ao grupo, facilitando a gestão e a automação:

- **Project:** Nome do projeto específico (ex: "DataPipeline2024", "CustomerAnalytics").
- **Department:** Nome do departamento ou equipe (ex: "DataEngineering", "AnalyticsTeam").
- **Role:** Descrição da função (ex: "DataEngineer", "DataOps").
- **Environment:** Ambiente onde o grupo atuará (ex: "Production", "Staging", "Development").
- **Owner:** Nome do proprietário ou responsável pelo grupo (ex: "Luciano", "DataTeamLead").
- **CostCenter:** Código ou nome do centro de custos (ex: "CC1001", "MarketingData").
- **Compliance:** Requisitos de conformidade específicos (ex: "GDPR", "HIPAA").
- **BusinessUnit:** Unidade de negócio relevante (ex: "Sales", "ProductDevelopment").
- **Purpose:** Finalidade do grupo (ex: "DataProcessing", "ETLTasks").
- **SecurityLevel:** Nível de segurança exigido (ex: "High", "Confidential").

---

### **10. Projeto: Criando Grupos, Usuários e Anexando Políticas no IAM**

**Passo a Passo: Criando Grupos, Usuários e Anexando Políticas no IAM**

#### **1. Criar os Grupos no IAM**

1. **Acessar o Console IAM:**
   - Faça login no console da AWS e navegue até o serviço **IAM (Identity and Access Management)**.

2. **Criar o Grupo "EngenheiroDeDados":**
   - No painel do IAM, selecione **"Grupos"** no menu lateral.
   - Clique em **"Criar Novo Grupo"**.
   - Nome do grupo: **EngenheiroDeDados**.
   - **Anexar Políticas Diretamente:**
     - **AmazonS3FullAccess**
     - **AmazonEC2FullAccess**
     - **AmazonVPCFullAccess**
   - Clique em **"Criar Grupo"**.

3. **Criar o Grupo "CientistaDeDados":**
   - Repita o processo anterior.
   - Nome do grupo: **CientistaDeDados**.
   - **Anexar Políticas Diretamente:**
     - **AmazonAthenaFullAccess**
     - **AmazonGlueConsoleFullAccess**
   - Clique em **"Criar Grupo"**.

4. **Criar o Grupo "LambdaExecutors":**
   - Repita o processo novamente.
   - Nome do grupo: **LambdaExecutors**.
   - **Anexar Políticas Diretamente:**
     - **AWSLambdaBasicExecutionRole**
   - Clique em **"Criar Grupo"**.

#### **2. Criar os Usuários no IAM**

1. **Criar Usuário 1 (Engenheiro de Dados):**
   - No IAM, selecione **"Usuários"** no menu lateral.
   - Clique em **"Adicionar Usuário"**.
   - Nome do usuário: **Engenheiro1**.
   - **Selecionar o Tipo de Acesso:**
     - Marque **"Acesso programático"** e **"Acesso à AWS Management Console"**.
     - Defina uma senha para o console.
   - **Atribuir ao Grupo:**
     - Selecione o grupo **"EngenheiroDeDados"**.
   - Clique em **"Próximo"** e **"Criar Usuário"**.

2. **Criar Usuário 2 (Engenheiro de Dados):**
   - Repita o processo anterior.
   - Nome do usuário: **Engenheiro2**.
   - Atribua ao grupo **"EngenheiroDeDados"**.

3. **Criar Usuário 3 (Cientista de Dados):**
   - Repita o processo anterior.
   - Nome do usuário: **Cientista1**.
   - Atribua ao grupo **"CientistaDeDados"**.

4. **Criar Usuário 4 (Lambda Executor):**
   - Repita o processo anterior.
   - Nome do usuário: **LambdaExecutor1**.
   - **Atribuir ao Grupo:**
     - Selecione o grupo **"LambdaExecutors"**.

#### **3. Anexar Usuários aos Grupos e Roles**

1. **Verificar Grupos e Políticas:**
   - Navegue até a página de cada grupo (EngenheiroDeDados, CientistaDeDados, LambdaExecutors) e verifique se as políticas apropriadas foram anexadas.

2. **Anexar Roles (para Lambda):**
   - Para o usuário **LambdaExecutor1**, vá em **"Roles"** no menu do IAM.
   - Crie uma nova role chamada **LambdaExecutionRole**.
   - Anexe a política **"AmazonDynamoDBFullAccess"**.
   - Atribua a role ao usuário **LambdaExecutor1**.

#### **4. Revisão e Testes**

1. **Verificar Acessos:**
   - Faça login como cada usuário criado para verificar se as permissões e acessos aos serviços AWS estão funcionando conforme o esperado.
   - Teste a criação e o gerenciamento de recursos em S3, EC2, VPC para engenheiros de dados, Athena e Glue para cientistas de dados, e execução de funções Lambda para o executor de Lambda.

2. **Documentação e Tags:**
   - Considere adicionar tags aos usuários e grupos para facilitar a organização e a gestão dentro da AWS.

---

### **11. Melhores Práticas de IAM**

- **Princípio do Menor Privilégio**: Conceder apenas as permissões necessárias.
- **Senhas Fortes e MFA**: Utilizar autenticação multifator em todas as contas críticas.
- **Monitoramento e Auditoria**: Usar ferramentas como AWS CloudTrail para monitorar atividades.

---

### **12. Acesso Programático na AWS**

**O que é Acesso Programático?**  
Acesso programático permite que você interaja com os serviços da AWS usando a AWS CLI, SDKs (como boto3 para Python), e ferramentas de automação. É ideal para cenários onde você precisa integrar seus aplicativos ou scripts com a AWS para gerenciar e operar recursos na nuvem.

### **Cenário: Construindo um Projeto Python que Precisa de Acesso a um Projeto Específico**

Se você está construindo um projeto Python que precisa de acesso a recursos específicos na AWS, como S3, DynamoDB, ou EC2, você deve criar um usuário IAM com permissões programáticas específicas para o projeto. Aqui está como fazer isso:

---

### **13. Passo a Passo para Criar Acesso Programático para um Projeto Específico**

1. **Acessar o Console IAM:**
   - Faça login no console da AWS.
   - Navegue até o serviço **IAM (Identity and Access Management)**.

2. **Criar um Novo Usuário:**
   - No painel do IAM, selecione **"Usuários"** no menu lateral.
   - Clique em **"Adicionar Usuário"**.
   - Nome do usuário: Escolha

 um nome que reflita o propósito do projeto, como **"ProjetoPythonUser"**.

3. **Selecionar o Tipo de Acesso:**
   - Marque **"Acesso programático"**. Isso gerará um **Access Key ID** e **Secret Access Key**, que você usará para configurar as credenciais no seu código Python.

4. **Configurar Permissões:**
   - **Criar uma Política Personalizada** (opcional):
     - Selecione **"Anexar Políticas Diretamente"** e procure pelas políticas gerenciadas que você precisa, ou crie uma política personalizada que conceda apenas as permissões necessárias para o projeto específico.
     - Exemplo de serviços que você pode precisar:
       - **AmazonS3FullAccess** para interagir com buckets S3.
       - **AmazonDynamoDBFullAccess** para trabalhar com tabelas DynamoDB.
       - **AmazonEC2FullAccess** se seu projeto gerencia instâncias EC2.
   - **Anexar Políticas ao Usuário**: Selecione as políticas necessárias para o usuário conforme o escopo do projeto.

5. **Revisar e Criar o Usuário:**
   - Revise as configurações e clique em **"Criar usuário"**.
   - **Download do CSV**: Baixe o arquivo CSV que contém o **Access Key ID** e o **Secret Access Key**. Estas credenciais serão usadas para configurar o acesso programático no seu projeto Python.

6. **Configurar o Projeto Python:**
   - No seu ambiente de desenvolvimento, configure as credenciais AWS usando o SDK boto3 ou outro SDK apropriado.
   - Exemplo em Python usando boto3:
     ```python
     import boto3

     # Configurando o acesso com as credenciais do IAM
     session = boto3.Session(
         aws_access_key_id='your-access-key-id',
         aws_secret_access_key='your-secret-access-key',
         region_name='your-region'  # Ex: 'us-east-1'
     )

     # Exemplo de uso do S3
     s3 = session.resource('s3')
     for bucket in s3.buckets.all():
         print(bucket.name)
     ```

### **14. Recomendações**

- **Princípio do Menor Privilégio**: Sempre conceda apenas as permissões necessárias para o projeto específico. Se o projeto só precisa acessar o S3, evite adicionar permissões desnecessárias para outros serviços.
- **Segurança das Credenciais**: Armazene as credenciais de forma segura e evite commitá-las em sistemas de controle de versão, como Git. Use serviços como AWS Secrets Manager ou variáveis de ambiente para gerenciar credenciais de maneira segura.

### **15. Exemplo de Uso de Role no IAM: EC2 com Acesso ao S3**

**Cenário Real**: Suponha que você esteja configurando uma instância EC2 na AWS que precisa acessar um bucket S3 para armazenar ou recuperar dados. Em vez de configurar credenciais de acesso diretamente na instância (o que pode ser inseguro), você pode usar uma **IAM Role** para conceder à instância EC2 as permissões necessárias para interagir com o S3. Isso é uma prática recomendada para aumentar a segurança e simplificar o gerenciamento de permissões.

### **Passo a Passo para Criar uma Role para EC2 com Acesso ao S3**

1. **Criar uma IAM Role**

   1.1. **Acessar o Console IAM**:
   - Faça login no console da AWS e navegue até o serviço **IAM (Identity and Access Management)**.
   - No menu lateral, selecione **"Roles"** e clique em **"Create Role"**.

   1.2. **Escolher o Tipo de Trusted Entity**:
   - Na tela de criação da role, selecione **"AWS Service"** como o tipo de trusted entity.
   - Em seguida, selecione **EC2** como o serviço que usará esta role.

   1.3. **Anexar Políticas de Permissão**:
   - Na etapa de anexar políticas, procure pela política gerenciada **AmazonS3FullAccess** ou **AmazonS3ReadOnlyAccess**, dependendo das necessidades do seu projeto.
   - Selecione a política e clique em **"Next"**.

   1.4. **Configurar Nome e Tags**:
   - Dê um nome descritivo para a role, como **EC2-S3-Access-Role**.
   - (Opcional) Adicione tags para facilitar a gestão e identificação da role.

   1.5. **Revisar e Criar a Role**:
   - Revise as configurações e clique em **"Create Role"**.

2. **Atribuir a Role à Instância EC2**

   2.1. **Criar ou Selecionar uma Instância EC2**:
   - Navegue até o serviço **EC2** no console da AWS.
   - Ao criar uma nova instância, na etapa de configuração, procure a seção **IAM role** e selecione a role **EC2-S3-Access-Role** criada anteriormente.

   2.2. **Anexar a Role a uma Instância Existente**:
   - Se você já possui uma instância EC2 em execução, vá para o painel de **Instâncias** no EC2.
   - Selecione a instância desejada, clique em **Actions** > **Security** > **Modify IAM Role**.
   - Selecione a role **EC2-S3-Access-Role** e clique em **Update IAM Role**.

3. **Testar o Acesso do EC2 ao S3**

   3.1. **Acessar a Instância EC2**:
   - Conecte-se à instância EC2 via SSH.

   3.2. **Verificar o Acesso ao S3**:
   - Usando a AWS CLI, execute um comando simples para listar os buckets S3:
     ```bash
     aws s3 ls
     ```
   - Se a role foi atribuída corretamente, você verá a lista de buckets no S3.

   3.3. **Exemplo de Uso no Código**:
   - Se o seu aplicativo na EC2 precisa interagir com o S3 programaticamente, você pode usar o SDK da AWS (por exemplo, boto3 para Python) sem precisar gerenciar manualmente as credenciais:
     ```python
     import boto3

     s3 = boto3.client('s3')
     response = s3.list_buckets()
     for bucket in response['Buckets']:
         print(f'Bucket Name: {bucket["Name"]}')
     ```

### **Por Que Usar uma IAM Role em EC2?**

- **Segurança**: Evita o uso de credenciais embutidas na instância, que podem ser comprometidas.
- **Gerenciamento Simplificado**: As permissões são centralizadas na role e podem ser ajustadas sem necessidade de alterar a configuração da instância.
- **Automação e Escalabilidade**: Ao usar uma role, o acesso a recursos AWS pode ser facilmente gerenciado em grandes ambientes de forma consistente e segura.

Esse é um cenário muito comum em projetos na nuvem, onde a segurança e a facilidade de gerenciamento são cruciais para o sucesso e a manutenção da infraestrutura.

Exatamente! Essa é uma das principais vantagens de usar uma **IAM Role** para uma instância EC2.

### **Vantagens da IAM Role em uma Instância EC2**

1. **Eliminação da Necessidade de Credenciais Embutidas**: 
   - Quando você atribui uma IAM Role a uma instância EC2, o código ou os scripts que você executa nessa instância podem acessar diretamente os recursos da AWS (como S3, DynamoDB, etc.) sem precisar incluir manualmente as **Access Key** e **Secret Access Key** no código. Isso reduz o risco de exposição dessas credenciais, que poderiam ser comprometidas se fossem incluídas diretamente no código.

2. **Segurança Aprimorada**:
   - Ao evitar o uso de credenciais embutidas, você minimiza o risco de que essas credenciais sejam expostas ou mal utilizadas. A IAM Role é gerenciada pela AWS, e as permissões podem ser ajustadas centralmente sem necessidade de atualizar o código na instância.

3. **Gerenciamento Simples e Centralizado**:
   - Com IAM Roles, as permissões para acessar diferentes serviços AWS são definidas de maneira centralizada. Isso facilita a gestão das permissões à medida que o ambiente cresce, sem necessidade de modificar cada instância individualmente.

4. **Rotação Automática de Credenciais**:
   - A AWS automaticamente gerencia e rotaciona as credenciais temporárias associadas a uma IAM Role. Isso significa que as credenciais são continuamente atualizadas sem intervenção manual, garantindo que o acesso continue seguro.

### **Como Funciona na Prática?**

Sim, se você subir um código Python na instância EC2 que tem uma IAM Role configurada com acesso ao S3, por exemplo, você **não precisará especificar as chaves de acesso**. O código simplesmente usará as permissões da IAM Role atribuída à instância para acessar os recursos. Aqui está um exemplo simples:

```python
import boto3

# Não é necessário especificar aws_access_key_id ou aws_secret_access_key
s3 = boto3.client('s3')

# Listando os buckets no S3
response = s3.list_buckets()
for bucket in response['Buckets']:
    print(f'Bucket Name: {bucket["Name"]}')
```

Neste exemplo, o código Python roda na instância EC2 e usa automaticamente as permissões da IAM Role para interagir com o S3, sem que você precise se preocupar com as credenciais. Isso torna o processo mais seguro, eficiente e fácil de gerenciar.

================================================
File: /Bootcamp - Cloud para dados/Aula_05/README.md
================================================
# **Bootcamp Cloud: Aula 05: VPC na AWS**

**Objetivo**: Nesta aula, vamos explorar o conceito de VPC (Virtual Private Cloud) na AWS, entender seus componentes principais, como sub-redes públicas e privadas, gateways de internet, endpoints de VPC, e aprender a configurar e utilizar esses elementos para criar uma infraestrutura de rede segura e eficiente na nuvem.

### **1. O Que é VPC?**

- **Definição**: A VPC (Virtual Private Cloud) é um serviço da AWS que permite criar uma rede privada virtual dentro da nuvem. Com a VPC, você tem controle total sobre o seu ambiente de rede, incluindo a escolha de seu próprio intervalo de endereços IP, a criação de sub-redes e a configuração de tabelas de roteamento e gateways de internet.
  
- **Benefícios**:
  - **Isolamento de Rede**: Mantém seus recursos isolados de outros clientes da AWS.
  - **Controle de Segurança**: Permite definir regras de segurança detalhadas para proteger seus recursos.
  - **Flexibilidade de Configuração**: Oferece a possibilidade de personalizar a configuração de sua rede, como endereçamento IP, gateways, e roteamento.

### **2. Componentes Principais de uma VPC**

#### **Sub-redes (Subnets)**

**O que são Sub-redes?**
- As sub-redes são divisões dentro de uma VPC que permitem segmentar e organizar recursos. Cada sub-rede é associada a uma parte do intervalo de endereços IP da VPC e reside em uma única Zona de Disponibilidade (AZ).

**Tipos de Sub-redes:**
- **Sub-redes Públicas**: São sub-redes conectadas a um Gateway de Internet (Internet Gateway), permitindo que recursos dentro delas tenham acesso direto à internet. Ideal para recursos que precisam ser acessíveis externamente, como servidores web.
- **Sub-redes Privadas**: São sub-redes que não possuem acesso direto à internet. Elas são usadas para recursos que devem ser isolados do acesso público, como bancos de dados. O acesso à internet a partir dessas sub-redes é feito por meio de um NAT Gateway, que roteia o tráfego de saída de forma segura.

#### **Sub-redes Públicas (Public Subnets)**

- **Definição**: Sub-redes públicas são configuradas para que os recursos nelas alocados possam se comunicar diretamente com a internet. Isso é feito associando a sub-rede a um Gateway de Internet (IGW).
  
- **Uso Comum**:
  - **Servidores Web**: Servidores que precisam estar acessíveis ao público, como servidores web, geralmente são colocados em sub-redes públicas. Isso permite que eles recebam tráfego de entrada diretamente da internet.
  - **Recursos que Necessitam de Acesso Direto à Internet**: Recursos como proxies, gateways, ou load balancers que devem receber ou enviar dados diretamente pela internet também são colocados em sub-redes públicas.

- **Regras de Segurança**:
  - **Security Groups**: Devem ser configurados para permitir tráfego apenas nas portas e protocolos necessários (por exemplo, HTTP na porta 80 ou HTTPS na porta 443).

#### **Sub-redes Privadas (Private Subnets)**

- **Definição**: Sub-redes privadas são aquelas que não têm acesso direto à internet. Elas são usadas para hospedar recursos que devem ser protegidos de acessos externos diretos, como bancos de dados ou servidores de backend. O tráfego de saída para a internet a partir dessas sub-redes é feito usando um NAT Gateway.

- **Uso Comum**:
  - **Bancos de Dados e Servidores de Aplicação**: Servidores que armazenam dados críticos, como bancos de dados, ou que executam lógica de aplicação sensível, são colocados em sub-redes privadas para proteger contra acessos diretos da internet.
  - **Recursos Backend**: Qualquer recurso que não precisa de acesso direto da internet, mas pode precisar se conectar a outros serviços da AWS ou realizar atualizações de software pela internet.

- **Acesso à Internet através de NAT**:
  - **NAT Gateway**: Para permitir que recursos em sub-redes privadas acessem a internet de forma segura (por exemplo, para baixar atualizações ou acessar APIs), utilizamos um NAT Gateway. O NAT roteia o tráfego de saída das instâncias privadas para a internet, sem expor esses recursos diretamente.

Um **NAT Gateway** (Network Address Translation Gateway) é um serviço na AWS que permite que instâncias em uma sub-rede privada façam solicitações de saída para a internet (como downloads ou atualizações de software) sem estarem diretamente acessíveis de fora da VPC. Vamos entender melhor o papel do NAT Gateway e a vantagem de usá-lo, mesmo com o custo associado.

### **O Que é um NAT Gateway?**
- **NAT Gateway** é um recurso gerenciado da AWS que permite que instâncias em sub-redes privadas tenham acesso de saída à internet (como para downloads ou atualizações), enquanto impede conexões de entrada não solicitadas vindas da internet.
- Funciona traduzindo endereços IP privados (da sub-rede privada) em um endereço IP público (associado ao NAT Gateway) para todo o tráfego de saída, e vice-versa para o tráfego de resposta.

### **Vantagem de Usar um NAT Gateway em uma Sub-rede Privada**

1. **Segurança Aprimorada:**
   - **Controle de Acesso:** Quando você coloca uma instância em uma sub-rede privada e usa um NAT Gateway, a instância não é acessível diretamente pela internet. Isso significa que ninguém pode iniciar uma conexão diretamente com a instância, reduzindo a superfície de ataque.
   - **Isolamento da Rede:** Instâncias em sub-redes privadas são protegidas por não serem visíveis diretamente na internet. O NAT Gateway permite que apenas tráfego de saída seja permitido, não o de entrada não solicitado.

2. **Limitação de Acesso Direto:**
   - **Acesso Controlado:** As instâncias privadas que precisam de conectividade de saída (por exemplo, para acessar atualizações de software, bibliotecas, ou APIs externas) ainda podem fazê-lo sem ficarem diretamente expostas à internet. Isso oferece um equilíbrio entre segurança e funcionalidade.
   - **Prevenção de Ataques:** Como as instâncias em sub-redes privadas não têm IPs públicos, elas não são diretamente atacáveis por scans de portas, ataques de DDoS, ou tentativas de invasão que podem ser direcionadas a IPs públicos.

3. **Compliance e Conformidade:**
   - **Requisitos de Segurança:** Algumas regulamentações de segurança e conformidade (como PCI-DSS, HIPAA, etc.) exigem que servidores de dados críticos (como bancos de dados) sejam isolados e não tenham exposição direta à internet. Usar sub-redes privadas com NAT Gateway cumpre esses requisitos.

### **Por que Não Colocar a Sub-rede Direto como Pública?**

Se você coloca suas instâncias diretamente em uma **sub-rede pública**:
- **Exposição Direta à Internet:** Cada instância em uma sub-rede pública pode receber tráfego de entrada direto da internet. Mesmo que você configure regras de firewall e segurança (Security Groups e NACLs), a instância ainda estará visível e pode ser alvo de ataques.
- **Necessidade de Endereços IP Públicos:** Cada instância precisaria de um IP público, aumentando o custo e o risco de segurança.
- **Superfície de Ataque Maior:** Com a exposição direta, a chance de ataques, como scans de portas ou ataques de DDoS, aumenta significativamente.

Vamos esclarecer os conceitos de **entrada (inbound)** e **saída (outbound)** no contexto de redes e comunicação na internet, bem como entender como isso se aplica a uma infraestrutura na nuvem, como a AWS.

### **Diferença entre Entrada (Inbound) e Saída (Outbound)**

1. **Tráfego de Entrada (Inbound Traffic):**
   - **Definição:** Tráfego de entrada refere-se a todas as conexões e dados que vêm de fora da rede e estão entrando em seus recursos, como instâncias de servidores ou dispositivos.
   - **Exemplos:**
     - Um usuário acessando um site hospedado em um servidor web na AWS.
     - Requisições de API enviadas para um serviço hospedado na sua VPC.
     - Conexões SSH de um administrador tentando acessar remotamente um servidor.

2. **Tráfego de Saída (Outbound Traffic):**
   - **Definição:** Tráfego de saída refere-se a todas as conexões e dados que estão saindo de seus recursos e indo para fora da rede, como para a internet ou outras redes.
   - **Exemplos:**
     - Um servidor em sua sub-rede privada baixando atualizações de software da internet.
     - Envio de dados para um serviço externo, como uma API de terceiros.
     - Conexões de um banco de dados na sua VPC para outro banco de dados em uma rede externa.

### **Como Funciona o NAT Gateway no Contexto de Saída e Entrada?**

Um **NAT Gateway** permite que o tráfego de saída (outbound) de instâncias em uma sub-rede privada alcance a internet sem permitir o tráfego de entrada (inbound) direto da internet para essas instâncias. Vamos ver como isso se aplica a ambos os tipos de tráfego:

- **Tráfego de Saída (Outbound) com NAT Gateway:**
  - **Exemplo de Funcionamento:**
    - Instâncias em uma sub-rede privada que não têm endereços IP públicos enviam solicitações para a internet (como atualizações de software ou chamadas para APIs).
    - O NAT Gateway, localizado em uma sub-rede pública, recebe essas solicitações, traduz os endereços IP privados para o IP público do NAT Gateway e encaminha o tráfego para a internet.
    - Quando a resposta chega, o NAT Gateway traduz de volta para os endereços IP privados e entrega a resposta às instâncias na sub-rede privada.

- **Tráfego de Entrada (Inbound) com NAT Gateway:**
  - **Exemplo de Restrição:**
    - Um NAT Gateway **não** permite conexões de entrada iniciadas da internet para as instâncias na sub-rede privada. Ele bloqueia automaticamente qualquer tentativa de conexão iniciada externamente, garantindo que apenas o tráfego de saída e suas respectivas respostas sejam permitidos.

### **Por que a Distinção é Importante?**

- **Segurança:**
  - Separar o tráfego de entrada e saída é uma prática importante para segurança. Tráfego de entrada direto da internet representa um risco de segurança, pois expõe seus recursos a ataques externos, como tentativas de invasão, DDoS, etc.
  - Tráfego de saída é necessário para muitas operações, como atualizações de software e chamadas de API. No entanto, ele não deve expor diretamente os recursos a ataques de entrada.

- **Uso de NAT Gateway:**
  - Usar um **NAT Gateway** permite que recursos em sub-redes privadas tenham conectividade de saída segura com a internet, sem expô-los a tráfego de entrada desnecessário. Isso protege seus recursos enquanto ainda permite que realizem operações necessárias.

### **Outbound e Inbound: Exemplos Comuns na AWS**

1. **Instância EC2 Pública:**
   - **Inbound:** Pode receber conexões de entrada diretamente da internet (por exemplo, um servidor web recebendo requisições HTTP).
   - **Outbound:** Pode enviar tráfego para a internet (por exemplo, fazer solicitações de saída a uma API externa).

2. **Instância EC2 Privada com NAT Gateway:**
   - **Inbound:** Não pode receber conexões de entrada diretamente da internet (somente resposta a requisições de saída previamente iniciadas).
   - **Outbound:** Pode enviar tráfego de saída para a internet através do NAT Gateway (por exemplo, baixar pacotes de atualização).

### **Conclusão: Diferença entre Entrada e Saída**

- **Entrada (Inbound):** Conexões iniciadas de fora da rede em direção aos seus recursos.
- **Saída (Outbound):** Conexões iniciadas pelos seus recursos em direção a fora da rede.
- **NAT Gateway:** Facilita conexões de saída para recursos em sub-redes privadas, mantendo-os protegidos de conexões de entrada diretas, garantindo um ambiente mais seguro e controlado.

### **Quando Usar um NAT Gateway?**

Use um **NAT Gateway** quando:
- Você precisa que instâncias em sub-redes privadas tenham acesso de saída à internet, mas sem serem diretamente acessíveis de fora da VPC.
- A segurança é uma prioridade e você quer minimizar a exposição dos seus recursos à internet.
- Você quer aproveitar a solução gerenciada da AWS, que oferece alta disponibilidade e manutenção automática.

### **Diferença entre NAT Gateway e VPC Endpoint**

Antes de detalhar onde usar um **NAT Gateway** em vez de um **VPC Endpoint**, vamos esclarecer o que cada um desses recursos faz:

- **NAT Gateway**: Permite que instâncias em sub-redes privadas tenham acesso de saída à internet sem permitir conexões de entrada da internet. Ideal para casos em que os recursos na sub-rede privada precisam se comunicar com serviços fora da VPC (como APIs de terceiros ou atualizações de software).

- **VPC Endpoint**: Permite que sua VPC se conecte diretamente a determinados serviços da AWS (como S3 ou DynamoDB) sem passar pela internet pública. Ideal para quando você deseja acessar serviços da AWS de forma segura e sem exposição à internet.

### **Quando Usar NAT Gateway ao Invés de VPC Endpoint?**

Use o **NAT Gateway** em vez de um **VPC Endpoint** quando:

1. **Conectar a Recursos Fora da AWS:**
   - Se seus recursos em sub-redes privadas precisam se comunicar com serviços ou recursos fora da AWS, como APIs de terceiros, sites para atualizações de software, ou servidores externos.
   - **Exemplo:** Sua instância EC2 em uma sub-rede privada precisa baixar pacotes de software ou atualizações diretamente da internet. Nesse caso, um NAT Gateway permitirá que essa instância tenha acesso de saída sem estar diretamente exposta à internet para conexões de entrada.

2. **Acesso Genérico à Internet:**
   - Quando você precisa de acesso geral à internet para vários propósitos e não está limitado a serviços específicos da AWS.
   - **Exemplo:** Um servidor de banco de dados em uma sub-rede privada que precisa se conectar a um repositório de pacotes para baixar atualizações ou enviar logs para um serviço de monitoramento externo.

3. **Acessar Serviços Não Suportados por VPC Endpoints:**
   - VPC Endpoints atualmente suportam apenas um subconjunto de serviços da AWS (por exemplo, S3, DynamoDB). Se você precisa acessar serviços ou destinos que não são cobertos por VPC Endpoints, você precisará usar um NAT Gateway.
   - **Exemplo:** Conectar-se a um serviço da AWS que não possui um VPC Endpoint específico (como algumas APIs menos comuns ou serviços regionais específicos).

### **Quando Usar VPC Endpoint ao Invés de NAT Gateway?**

Use um **VPC Endpoint** em vez de um **NAT Gateway** quando:

1. **Conectar-se a Serviços da AWS de Forma Segura:**
   - Quando você deseja que sua VPC se conecte diretamente a serviços da AWS (como S3 ou DynamoDB) sem passar pela internet pública. Isso proporciona mais segurança e pode reduzir a latência.
   - **Exemplo:** Um aplicativo em uma instância EC2 em uma sub-rede privada precisa acessar um bucket S3 para armazenar ou recuperar arquivos de dados. Um VPC Endpoint para S3 permitirá essa comunicação sem passar pela internet.

2. **Reduzir Custos:**
   - VPC Endpoints geralmente são mais baratos do que usar um NAT Gateway porque não há cobrança por dados de saída através de um endpoint, enquanto o NAT Gateway cobra por dados transferidos.
   - **Exemplo:** Transferir grandes volumes de dados entre sua VPC e o S3. Um VPC Endpoint para S3 pode reduzir significativamente os custos de transferência de dados em comparação ao uso de um NAT Gateway.

3. **Compliance e Conformidade:**
   - Se suas políticas de segurança exigem que todo o tráfego permaneça dentro da rede da AWS sem passar pela internet pública, usar VPC Endpoints é ideal.
   - **Exemplo:** Organizações que precisam estar em conformidade com regulamentações como PCI-DSS, HIPAA, ou outras normas que exigem controle estrito de onde os dados trafegam.

### **Resumo da Escolha: NAT Gateway vs. VPC Endpoint**

| **Cenário**                                         | **Usar NAT Gateway**                                 | **Usar VPC Endpoint**                                |
|-----------------------------------------------------|------------------------------------------------------|------------------------------------------------------|
| **Conectar a recursos fora da AWS**                 | Sim                                                  | Não                                                  |
| **Acesso geral à internet (saída)**                 | Sim                                                  | Não                                                  |
| **Acessar serviços da AWS sem expor à internet**    | Não                                                  | Sim                                                  |
| **Reduzir custos de transferência de dados**        | Não (pode ser caro para grandes volumes)             | Sim                                                  |
| **Conformidade com regulamentações de segurança**   | Não (passa pela internet pública)                    | Sim (tráfego permanece dentro da rede da AWS)        |
| **Acessar serviços da AWS que não possuem Endpoint**| Sim                                                  | Não                                                  |

### **Conclusão**

- **Use NAT Gateway** quando você precisar que recursos em uma sub-rede privada tenham acesso de saída à internet para fins gerais ou para se comunicar com recursos fora da AWS.
- **Use VPC Endpoints** quando você deseja que seus recursos em sub-redes privadas acessem serviços da AWS de maneira segura e econômica, sem expô-los à internet pública.

### **Conclusão**

Embora o NAT Gateway tenha um custo, ele é essencial para garantir que as instâncias em sub-redes privadas possam se comunicar com a internet sem comprometer a segurança. Ele oferece uma solução balanceada entre a necessidade de conectividade de saída e a necessidade de segurança, proteção e conformidade com normas e regulamentações.

#### **Internet Gateway (IGW)**

- **Definição**: Um Internet Gateway (IGW) é um componente que conecta a VPC à internet. Ele permite que recursos em sub-redes públicas possam se comunicar com a internet.

- **Uso**:
  - **Acesso de Entrada e Saída**: O IGW permite que o tráfego entre a internet e os recursos da VPC flua livremente, garantindo que servidores em sub-redes públicas possam receber e enviar tráfego.
  - **Associado a Sub-redes Públicas**: Uma sub-rede é considerada pública quando associada a um Internet Gateway através de uma tabela de roteamento que direciona o tráfego para a internet.

#### **NAT Gateway**

- **Definição**: Um NAT (Network Address Translation) Gateway permite que instâncias em sub-redes privadas acessem a internet para tráfego de saída, sem permitir conexões de entrada vindas da internet.

- **Uso**:
  - **Tráfego de Saída Seguro**: Permite que recursos em sub-redes privadas, como servidores de aplicação ou bancos de dados, façam solicitações de saída para a internet (por exemplo, para atualizações de software), sem expor esses recursos a acessos externos.
  - **Alocação de IP Elástico**: O NAT Gateway é associado a um Elastic IP para gerenciar o tráfego de saída.

#### **VPC Endpoints**

- **Definição**: Um VPC Endpoint permite conectar sua VPC a serviços AWS diretamente, sem passar pela internet pública. Isso fornece uma maneira segura e eficiente de acessar serviços da AWS como S3 ou DynamoDB dentro da VPC.

- **Tipos de VPC Endpoints**:
  - **Gateway Endpoints**: Usados para serviços baseados em S3 e DynamoDB.
  - **Interface Endpoints**: Usados para conectar serviços AWS mais amplamente (como SQS, SNS) diretamente dentro de sua VPC via interfaces de rede privadas (ENIs).

- **Benefícios**:
  - **Segurança**: Todo o tráfego permanece dentro da rede AWS, sem transitar pela internet pública.
  - **Redução de Latência e Custos**: Ao evitar o tráfego pela internet, você reduz a latência e evita custos associados ao uso de largura de banda pública.

### **3. Passo a Passo: Criando uma VPC na AWS**

**Cenário**: Vamos criar uma VPC com uma sub-rede pública e uma privada, configurar um gateway de internet, um NAT Gateway, VPC Endpoints, e aplicar as regras de segurança necessárias.

#### **1. Criar a VPC**

1. **Acessar o Console VPC**:
   - Faça login no console da AWS e navegue até o serviço **VPC (Virtual Private Cloud)**.
   - No painel do VPC, clique em **"Your VPCs"** e depois em **"Create VPC"**.

2. **Configurar a VPC**:
   - **Nome**: Dê um nome à sua VPC, por exemplo, "MinhaVPC".
   - **CIDR Block**: Defina um bloco CIDR, como "10.0.0.0/16", que representa o intervalo de endereços IP que sua VPC utilizará.
   - **Tenancy**: Escolha entre "default" (uso compartilhado) ou "dedicated" (hardware dedicado).

3. **Criar a VPC**:
   - Clique em **"Create VPC"** para finalizar a criação da VPC.

#### **2. Criar Sub-redes**

1. **Criar Sub-rede Pública**:
   - No painel VPC, selecione **"Subnets"** e clique em **"Create Subnet"**.
   - **Nome**: Dê um nome, como "MinhaSubnetPublica".
   - **VPC**: Selecione a VPC criada anteriormente ("MinhaVPC").
   - **CIDR Block**: Defina um bloco CIDR, como "10.0.1.0/24".
   - **Availability Zone**: Escolha uma zona de disponibilidade.

2. **Criar Sub-rede Privada**:
   - Repita o processo para criar uma sub-rede privada.
   - **Nome**: "MinhaSubnetPrivada".
   - **CIDR Block**: "10.0.2.0/24".
   - Esta sub-rede não terá acesso direto à internet.

#### **3. Configurar um Gateway de Internet**

1. **Criar o Gateway de Internet**:
   - No painel VPC, clique em **"Internet Gateways"** e depois em **"Create Internet Gateway"**.
   - **Nome**: "MeuInternetGateway".
   - Clique em **"Create"**.

2. **Associar o Gateway de Internet à VPC**:
   - Selecione o gateway de internet recém-criado.
   - Clique em **"Actions"** e selecione **"Attach to VPC"**.
  

 - Escolha "MinhaVPC" e clique em **"Attach"**.

#### **4. Configurar um NAT Gateway para a Sub-rede Privada**

1. **Criar um NAT Gateway**:
   - No painel VPC, clique em **"NAT Gateways"** e depois em **"Create NAT Gateway"**.
   - **Nome**: "MeuNATGateway".
   - **Sub-rede**: Escolha a "MinhaSubnetPublica" para que o NAT Gateway possa ter acesso ao Internet Gateway.
   - **Elastic IP Allocation**: Clique em "Allocate Elastic IP" para associar um IP Elástico ao NAT Gateway.

2. **Associar o NAT Gateway à Tabela de Roteamento da Sub-rede Privada**:
   - Vá até **"Route Tables"** e selecione a tabela associada à "MinhaSubnetPrivada".
   - Adicione uma rota para o NAT Gateway para tráfego de saída para a internet.

#### **5. Configurar Tabelas de Roteamento**

1. **Criar uma Tabela de Roteamento para Sub-rede Pública**:
   - No painel VPC, clique em **"Route Tables"** e depois em **"Create Route Table"**.
   - **Nome**: "TabelaPublica".
   - **VPC**: Selecione "MinhaVPC".
   - Clique em **"Create"**.

2. **Adicionar uma Rota para o Gateway de Internet**:
   - Selecione a "TabelaPublica" criada.
   - Clique em **"Routes"** > **"Edit routes"** > **"Add route"**.
   - **Destination**: "0.0.0.0/0" (rota padrão para toda a internet).
   - **Target**: Selecione "MeuInternetGateway".
   - Clique em **"Save changes"**.

3. **Associar a Tabela de Roteamento à Sub-rede Pública**:
   - Clique em **"Subnet associations"** > **"Edit subnet associations"**.
   - Selecione "MinhaSubnetPublica".
   - Clique em **"Save associations"**.

#### **6. Configurar VPC Endpoints**

1. **Criar um VPC Endpoint para o S3**:
   - No painel VPC, clique em **"Endpoints"** e depois em **"Create Endpoint"**.
   - **Nome**: "S3-Endpoint".
   - **Service**: Escolha "com.amazonaws.region.s3".
   - **VPC**: Selecione "MinhaVPC".
   - **Rota**: Selecione as sub-redes desejadas (por exemplo, "MinhaSubnetPrivada") e clique em **"Create Endpoint"**.

### **4. Testando e Verificando a Configuração da VPC**

1. **Lançar Instâncias EC2 em Sub-redes Públicas e Privadas**:
   - No painel EC2, crie uma instância em "MinhaSubnetPublica" e outra em "MinhaSubnetPrivada".
   - Verifique se a instância pública pode acessar a internet e a privada não.

2. **Testar Conexões entre as Instâncias**:
   - Conecte-se à instância na sub-rede pública usando SSH.
   - Da instância pública, tente pingar a instância na sub-rede privada para verificar a conectividade interna.

3. **Testar VPC Endpoints**:
   - Verifique se a instância na sub-rede privada pode acessar o S3 diretamente usando o VPC Endpoint, sem precisar passar pela internet.

### **5. Melhores Práticas ao Configurar VPCs**

- **Segregação de Ambientes**: Use diferentes VPCs para ambientes de desenvolvimento, teste e produção.
- **Monitoramento e Logs**: Habilite o AWS VPC Flow Logs para monitorar e registrar o tráfego de rede.
- **Princípio do Menor Privilégio**: Configure security groups e NACLs para permitir apenas o tráfego necessário.
- **Uso de VPC Endpoints**: Utilize VPC Endpoints para reduzir o tráfego pela internet pública e aumentar a segurança ao acessar serviços AWS.

### **6. Projeto: Criando uma Rede Segura na AWS com VPC**

**Objetivo do Projeto**: Construir uma infraestrutura de rede usando a VPC, garantindo que os recursos críticos estejam isolados e protegidos, enquanto mantém o acesso adequado aos recursos públicos.

**Passo a Passo Resumido:**

1. **Criação de VPC e Sub-redes**:
   - Configure a VPC e divida-a em sub-redes públicas e privadas.
2. **Implementação de Gateways e Tabelas de Roteamento**:
   - Configure o acesso à internet para sub-redes públicas e o uso de NAT para sub-redes privadas.
3. **Aplicação de Regras de Segurança**:
   - Use security groups, NACLs e VPC Endpoints para proteger recursos e otimizar o tráfego.
4. **Teste e Verificação**:
   - Teste o acesso entre instâncias e à internet para validar a configuração.

Essa aula cobre todos os componentes essenciais de uma VPC e fornece uma base sólida para a construção de redes seguras e bem configuradas na AWS.

================================================
File: /Bootcamp - Cloud para dados/Aula_06/README.md
================================================
### **Desafio de Negócio**

**Dever de casa **

https://blog.intigriti.com/hacking-tools/hacking-misconfigured-aws-s3-buckets-a-complete-guide

**Desafio**: **Proteção e Processamento Seguro de Dados Sensíveis em Ambientes Regulatórios**

![image](1_C2EJ15gGamMmbWGAt7I0Rg.png)

Em muitas indústrias, como a financeira, de saúde ou de tecnologia, as empresas lidam com grandes volumes de dados altamente sensíveis que devem ser protegidos rigorosamente para cumprir regulamentações como GDPR, HIPAA, ou normas internas de segurança. Um desafio comum é garantir que esses dados sejam processados de forma segura, sem exposição a ameaças externas, ao mesmo tempo em que permitem o acesso controlado para manutenção e monitoramento.

### **Cenário**

Uma empresa de serviços financeiros precisa processar dados de transações bancárias que são armazenados em um ambiente de armazenamento seguro (S3). Esses dados são altamente sensíveis e sujeitos a rigorosas regulamentações de proteção de dados. A empresa enfrenta o desafio de processar esses dados de forma eficiente e segura, garantindo que nenhum dado seja exposto à internet durante o processamento.

### **Motivação para Montar a Arquitetura**

1. **Segurança e Conformidade**: A principal motivação é proteger os dados sensíveis e cumprir as exigências regulatórias. Ao isolar o processamento de dados em uma subrede privada, a arquitetura garante que os dados não sejam expostos à internet. A conexão ao bucket S3 é feita exclusivamente através de um VPC Endpoint, mantendo o tráfego de dados dentro da infraestrutura segura da AWS.

2. **Acesso Controlado e Seguro**: Para manutenção e monitoramento, é necessário acesso à instância que processa os dados. No entanto, para evitar riscos, o acesso direto à instância privada não é permitido. Em vez disso, uma instância pública (Bastion Host) na subrede pública atua como um ponto de entrada seguro. Isso permite que apenas usuários autorizados, através de conexões seguras, possam acessar a instância privada, minimizando os riscos de ataques externos.

3. **Gerenciamento Eficiente de Recursos**: A arquitetura permite separar responsabilidades, com a instância privada focada exclusivamente no processamento seguro dos dados, enquanto a instância pública gerencia o acesso e monitoramento. Isso facilita a manutenção, escalabilidade e otimização de recursos, garantindo que cada componente esteja configurado para desempenhar sua função da forma mais eficiente possível.

### **Benefício para o Negócio**

Implementar essa arquitetura permite à empresa processar dados sensíveis em conformidade com regulamentações rigorosas, ao mesmo tempo em que assegura que os dados permanecem seguros e acessíveis apenas por usuários autorizados. Isso não só reduz o risco de violações de dados, como também garante que a empresa esteja em total conformidade com normas de proteção de dados, evitando penalidades e mantendo a confiança dos clientes e parceiros.

### **Estimativa de Custo para Implementação da Arquitetura**

Os custos de implementação de uma arquitetura como a descrita podem variar dependendo de vários fatores, como o tamanho e o uso dos recursos, a região da AWS em que o projeto é implementado, e o nível de utilização dos serviços. Abaixo, forneço uma estimativa aproximada baseada em preços de mercado comuns para os principais componentes da arquitetura.

#### **Componentes e Custos Estimados**

1. **VPC (Virtual Private Cloud)**
   - **Custo:** Geralmente, a criação e o uso de uma VPC são gratuitos. No entanto, os componentes dentro da VPC, como subredes e roteamento, podem gerar custos dependendo do tráfego.

2. **Instância EC2 Pública (Bastion Host)**
   - **Tipo de Instância:** t3.medium (2 vCPUs, 4 GB RAM)
   - **Custo:** Aproximadamente $0,0416 por hora.
   - **Uso Mensal (720 horas):** $29,95

3. **Instância EC2 Privada (Processamento)**
   - **Tipo de Instância:** m5.large (2 vCPUs, 8 GB RAM)
   - **Custo:** Aproximadamente $0,096 por hora.
   - **Uso Mensal (720 horas):** $69,12

4. **S3 Storage**
   - **Custo por GB armazenado:** $0,023 por GB.
   - **Transferência de dados entre buckets:** Sem custos dentro da mesma região.
   - **Estimativa de 500 GB de dados:** $11,50 por mês.

5. **VPC Endpoint para S3**
   - **Custo:** $0,01 por hora por endpoint.
   - **Uso Mensal (720 horas):** $7,20

6. **Tráfego de Dados (Internet Gateway)**
   - **Custo de saída de dados para a internet:** $0,09 por GB.
   - **Estimativa para 100 GB de tráfego de saída:** $9,00

7. **CloudWatch Logs e Métricas**
   - **Métricas Customizadas:** $0,30 por métrica por mês.
   - **Logs (5 GB por mês):** $0,50 por GB.
   - **Estimativa para logs e métricas:** $5,00 por mês.

8. **IAM (Roles e Usuários)**
   - **Custo:** A criação e gerenciamento de IAM Roles e Usuários não têm custos diretos associados.

#### **Custo Total Estimado Mensal**

| **Componente**                | **Custo Mensal** |
|--------------------------------|------------------|
| Instância EC2 Pública (t3.medium)  | $29,95           |
| Instância EC2 Privada (m5.large)   | $69,12           |
| Armazenamento S3 (500 GB)      | $11,50           |
| VPC Endpoint para S3           | $7,20            |
| Tráfego de Dados (100 GB)      | $9,00            |
| CloudWatch (Logs e Métricas)   | $5,00            |
| **Total Estimado**             | **$131,77**      |

### **Considerações Finais**

- **Custos Variáveis:** O custo final pode variar dependendo do uso efetivo dos recursos, como o tráfego de dados e o armazenamento adicional no S3.
- **Reduções Potenciais:** Usar instâncias spot ou reservar instâncias EC2 para contratos de longo prazo pode reduzir significativamente os custos.
- **Testes e Desenvolvimento:** Durante a fase de desenvolvimento e teste, você pode utilizar instâncias menores e reduzir a quantidade de dados processados para minimizar os custos.

Essas estimativas fornecem uma visão geral dos custos mensais associados a manter essa arquitetura AWS em produção. Ajustes específicos podem ser feitos com base nas necessidades e no uso real do projeto.

O custo de desenvolvimento de um projeto como esse pode variar amplamente dependendo de vários fatores, como a complexidade do projeto, a experiência e localização dos desenvolvedores, o tempo necessário para implementação, e se você contratará uma equipe interna, freelancers ou uma empresa especializada.

### **Fatores a Considerar no Cálculo do Custo de Desenvolvimento**

1. **Tamanho e Complexidade do Projeto**:
   - Este projeto envolve a configuração de uma infraestrutura AWS, desenvolvimento de scripts de automação e monitoramento, além de garantir segurança e conformidade com práticas recomendadas.

2. **Equipe Necessária**:
   - **Arquiteto de Soluções AWS**: Responsável por desenhar a arquitetura e configurar os serviços AWS.
   - **Engenheiro de DevOps**: Para implementar a infraestrutura como código (IaC) usando ferramentas como Terraform ou CloudFormation.
   - **Desenvolvedor Backend (Python, etc.)**: Para desenvolver scripts de automação e o dashboard.
   - **Especialista em Segurança**: Para configurar corretamente as políticas de IAM e garantir que a infraestrutura seja segura.
   - **Gerente de Projeto**: Para coordenar o desenvolvimento e garantir que o projeto seja entregue dentro do prazo e orçamento.

3. **Tempo de Desenvolvimento**:
   - **Fase de Planejamento e Design**: 1-2 semanas
   - **Configuração da Infraestrutura AWS**: 2-3 semanas
   - **Desenvolvimento de Scripts e Dashboard**: 3-4 semanas
   - **Testes e Ajustes**: 1-2 semanas
   - **Total Estimado**: 7-11 semanas

### **Estimativa de Custo de Desenvolvimento**

1. **Arquiteto de Soluções AWS**:
   - **Taxa por Hora**: $100 - $200
   - **Horas Estimadas**: 40 - 80 horas
   - **Custo Total**: $4,000 - $16,000

2. **Engenheiro de DevOps**:
   - **Taxa por Hora**: $80 - $150
   - **Horas Estimadas**: 60 - 100 horas
   - **Custo Total**: $4,800 - $15,000

3. **Desenvolvedor Backend**:
   - **Taxa por Hora**: $60 - $120
   - **Horas Estimadas**: 80 - 120 horas
   - **Custo Total**: $4,800 - $14,400

4. **Especialista em Segurança**:
   - **Taxa por Hora**: $100 - $200
   - **Horas Estimadas**: 20 - 40 horas
   - **Custo Total**: $2,000 - $8,000

5. **Gerente de Projeto**:
   - **Taxa por Hora**: $60 - $100
   - **Horas Estimadas**: 40 - 60 horas
   - **Custo Total**: $2,400 - $6,000

### **Custo Total Estimado de Desenvolvimento**

| **Função**                       | **Custo Estimado** |
|----------------------------------|--------------------|
| Arquiteto de Soluções AWS        | $4,000 - $16,000   |
| Engenheiro de DevOps             | $4,800 - $15,000   |
| Desenvolvedor Backend            | $4,800 - $14,400   |
| Especialista em Segurança        | $2,000 - $8,000    |
| Gerente de Projeto               | $2,400 - $6,000    |
| **Custo Total Estimado**         | **$18,000 - $59,400** |

### **Considerações Adicionais**

- **Sobrecarga e Contingências**: Geralmente, é recomendado adicionar 10-20% ao orçamento total para cobrir eventuais imprevistos ou mudanças no escopo do projeto.
- **Opções de Redução de Custo**: Se o orçamento for um problema, pode-se considerar freelancers ou empresas de desenvolvimento em regiões com taxas mais baixas. Alternativamente, parte do trabalho pode ser automatizada ou simplificada, reduzindo o tempo de desenvolvimento.

### **Custo Total Estimado (Desenvolvimento + Operação)**

- **Desenvolvimento**: $18,000 - $59,400
- **Operação Mensal**: $131,77 (como estimado anteriormente)
- **Primeiro Mês (Desenvolvimento + Operação)**: $18,131.77 - $59,531.77

Essa estimativa deve fornecer uma visão abrangente dos custos envolvidos na criação e operação dessa arquitetura AWS, permitindo um planejamento financeiro mais preciso.

### **Requisitos do Projeto: Arquitetura AWS para Acesso a S3 a Partir de EC2 em Rede Privada**

#### **1. Objetivo do Projeto**
O objetivo deste projeto é configurar uma arquitetura AWS onde uma instância EC2 privada acessa buckets S3 em uma rede privada usando um Gateway Endpoint. Para facilitar a gestão e o monitoramento, também será criada uma instância EC2 pública, que atuará como um bastion host, permitindo o acesso seguro à instância privada. A arquitetura será implementada dentro de uma VPC, com subredes públicas e privadas devidamente configuradas para garantir segurança e eficiência.

#### **2. Requisitos do Projeto**

**2.1. Infraestrutura de Rede (VPC)**
- **VPC**: Criar uma Virtual Private Cloud (VPC) que incluirá subredes públicas e privadas.
- **Subrede Pública**:
  - Deve conter uma instância EC2 pública que atuará como um bastion host, permitindo o acesso SSH à instância privada na subrede privada.
  - Deve ser conectada a um Internet Gateway para permitir o tráfego de entrada e saída da internet.
- **Subrede Privada**:
  - Deve conter uma instância EC2 privada, responsável por acessar e processar dados nos buckets S3.
  - Não deve ter acesso direto à internet, garantindo que todo o tráfego externo seja gerenciado através da instância pública.
- **Gateway Endpoint**:
  - Criar um Gateway Endpoint para S3, associado à tabela de rotas da subrede privada, permitindo que a instância EC2 privada acesse os buckets S3 sem passar pela internet.

**2.2. EC2 Instances**
- **Instância EC2 Pública (Bastion Host)**:
  - Deve ser configurada na subrede pública para permitir o acesso SSH à instância EC2 privada.
  - Esta instância terá acesso à internet para downloads de pacotes e outras operações de manutenção.
- **Instância EC2 Privada**:
  - Localizada na subrede privada, será responsável por processar os dados nos buckets S3.
  - Deve ser configurada para acessar os buckets S3 via o Gateway Endpoint.
  - O acesso SSH à instância privada deve ser feito exclusivamente através do bastion host, garantindo que a instância permaneça isolada da internet.

**2.3. S3 Buckets**
- **Buckets S3**:
  - Criar e configurar os buckets S3 necessários para o projeto.
  - Garantir que o acesso a esses buckets seja feito exclusivamente via o Gateway Endpoint a partir da instância privada, mantendo os dados seguros dentro da VPC.

**2.4. IAM Roles e Políticas**
- **IAM Role para Instância EC2 Privada**:
  - Criar uma IAM Role que conceda à instância EC2 privada as permissões necessárias para acessar os buckets S3 via o Gateway Endpoint.
  - Esta role deve ser restrita apenas às ações necessárias, seguindo o princípio do menor privilégio.
- **IAM User com Permissões Restritas**:
  - Criar um usuário IAM com permissões específicas para gerenciar os recursos do projeto (EC2, VPC, S3).
  - Este usuário deve ter permissões limitadas ao escopo do projeto e não deve ser capaz de criar outros usuários IAM ou modificar configurações globais de segurança.

#### **3. Escopo do Projeto**
O escopo do projeto inclui a configuração de uma VPC com subredes públicas e privadas, a criação de uma instância EC2 pública (bastion host) e uma instância EC2 privada, a configuração de um Gateway Endpoint para S3, e a implementação de políticas de segurança que garantam acesso seguro e restrito aos recursos AWS envolvidos.

**Incluído no Escopo**:
- Configuração da VPC com subredes públicas e privadas.
- Criação e configuração de uma instância EC2 pública (bastion host) e uma instância EC2 privada.
- Configuração de um Gateway Endpoint para S3.
- Criação e configuração de buckets S3 com políticas de acesso apropriadas.
- Configuração de IAM Roles e políticas para acesso seguro e restrito aos recursos.

**Excluído do Escopo**:
- Gerenciamento de outras infraestruturas não relacionadas ao projeto.
- Configuração de segurança e políticas IAM globais não especificadas no escopo do projeto.

#### **4. Entregáveis do Projeto**

**4.1. Documentação de Configuração**
- Instruções detalhadas sobre como criar e configurar cada componente da arquitetura (VPC, subredes, EC2, IAM Role, Gateway Endpoint, S3 buckets, etc.).
- Especificações das políticas IAM personalizadas utilizadas no projeto.

**4.2. Instâncias EC2 Configuradas**
- Instância EC2 pública (bastion host) e instância EC2 privada operacionais e configuradas corretamente.

**4.3. Buckets S3 Configurados**
- Buckets S3 configurados com as políticas de acesso corretas, garantido que o acesso seja feito via Gateway Endpoint.

**4.4. Usuário IAM com Permissões Restritas**
- Criação do usuário IAM com a política personalizada, garantindo que ele possa gerenciar os recursos deste projeto de forma segura e controlada.

**4.5. VPC Configurada**
- VPC configurada com subredes públicas e privadas, e um Gateway Endpoint para S3 configurado corretamente.

Com base nesse documento, o usuário deve ser capaz de criar e configurar toda a infraestrutura necessária para o projeto de transferência segura de dados entre buckets S3 na AWS utilizando uma instância EC2 privada e um bastion host.

Aqui está o tutorial revisado, incluindo todos os passos mencionados, para configurar um VPC Endpoint na AWS e acessar o S3 de forma segura:

Aqui está a versão revisada do tutorial com sugestões para nomes de recursos mais adequados para um ambiente corporativo:

---

**Tutorial Completo: Configuração de VPC Endpoint na AWS**

**0) Criar um Usuário IAM com Permissão PowerUser**  
Antes de iniciar, crie um usuário IAM com a permissão PowerUser para gerenciar todos os recursos na AWS de forma segura. Nomeie o usuário de forma clara, como `project-admin-user`.

**1) Criar uma VPC**  
No console da AWS, vá para "VPC".  
Clique em "Create VPC".  
Escolha a opção "VPC Only".  
Nomeie a VPC como `corp-vpc` e defina o range CIDR como `10.0.0.0/16`.  
Clique em "Create VPC".

**2) Criar Subnets**  
No painel da VPC, vá para "Subnets".  
Clique em "Create Subnet" e selecione a VPC criada (`corp-vpc`).  
Crie duas sub-redes:  
- **Subrede Pública**: Nome: `public-subnet-az1`, Zona de Disponibilidade: `eu-west-1a`, CIDR: `10.0.1.0/24`.  
- **Subrede Privada**: Nome: `private-subnet-az1`, Zona de Disponibilidade: `eu-west-1a`, CIDR: `10.0.2.0/24`.

**3) Criar um Internet Gateway**  
No painel da VPC, vá para "Internet Gateways".  
Clique em "Create Internet Gateway" e nomeie como `corp-igw`.  
Selecione o Internet Gateway criado e clique em "Attach to VPC". Selecione a VPC (`corp-vpc`).

**4) Criar Tabelas de Rotas**  
Vá para "Route Tables" e clique em "Create Route Table".  
Crie duas tabelas de rotas:  
- **Tabela de Rota Pública**: Nome: `public-route-table`, associada à VPC `corp-vpc`.  
- **Tabela de Rota Privada**: Nome: `private-route-table`, associada à VPC `corp-vpc`.

**5) Associar a Tabela de Rota Pública à Subrede Pública**  
Selecione a tabela de rota pública (`public-route-table`).  
Vá para "Subnet Associations" e associe-a à `public-subnet-az1`.

**6) Editar Rotas da Tabela Pública para o Internet Gateway**  
Na tabela de rota pública, vá para "Routes" e clique em "Edit Routes".  
Adicione uma rota com destino `0.0.0.0/0` e selecione o Internet Gateway `corp-igw`.

**7) Associar a Tabela de Rota Privada à Subrede Privada**  
Selecione a tabela de rota privada (`private-route-table`).  
Vá para "Subnet Associations" e associe-a à `private-subnet-az1`.

**8) Configurar Instância EC2 Pública**  
Vá para "EC2" e clique em "Launch Instance".  
Nomeie como `public-ec2-instance`, selecione Amazon Linux e o tipo `t2.micro`.  
Escolha a sub-rede pública (`public-subnet-az1`) e habilite o IP público.  
Crie um par de chaves (`corp-key-pair`) para SSH e salve.  
Edite as configurações de segurança para liberar o acesso SSH.  
Acesse a instância pública via terminal com o comando SSH usando o arquivo `.pem`.

**9) Configurar Instância EC2 Privada**  
Clique em "Launch Instance" novamente.  
Nomeie como `private-ec2-instance`, selecione Amazon Linux e o tipo `t2.micro`.  
Escolha a sub-rede privada (`private-subnet-az1`) e desabilite o IP público.  
Use a mesma chave privada gerada anteriormente (`corp-key-pair`).  
Edite as configurações de segurança para permitir o acesso SSH da instância pública.  
Na instância pública, use o comando `vi corp-key.pem` para copiar a chave privada e acessar a instância privada via SSH.

**10) Criar um Bucket S3**  
No console AWS, vá para "S3" e crie um bucket, nomeando-o como `corp-data-bucket`.  
Configure as credenciais AWS na instância privada com:  
`aws configure`  
Execute o comando `aws s3 ls` para listar os buckets.

**11) Criar um VPC Endpoint**  
No painel da VPC, vá para "Endpoints".  
Clique em "Create Endpoint" e escolha o tipo Gateway.  
Selecione o serviço `com.amazonaws.eu-west-1.s3` e escolha a VPC (`corp-vpc`).  
Associe o endpoint à tabela de rota privada (`private-route-table`).

**12) Associar o VPC Endpoint à Subrede Privada**  
Garanta que o VPC Endpoint está associado corretamente com a sub-rede privada (`private-subnet-az1`).

**13) Testar Operações no S3**  
Na instância privada, teste a conectividade com o S3:  
Para inserir um arquivo:  
`aws s3 cp arquivo.txt s3://corp-data-bucket/`  
Para deletar um arquivo:  
`aws s3 rm s3://corp-data-bucket/arquivo.txt`

**Conclusão**  
Seguindo estes passos, você configurou um ambiente seguro na AWS onde as instâncias em sub-rede privada acessam o S3 sem sair do ambiente da AWS, utilizando o VPC Endpoint. 

### Diagrama em Mermaid

Aqui está o diagrama Mermaid da solução de configuração da VPC com VPC Endpoint para acesso ao S3:

```mermaid
graph TD
    A[Usuário IAM com Permissão PowerUser] --> B[VPC - corp-vpc]
    B --> C[Subrede Pública - public-subnet-az1]
    B --> D[Subrede Privada - private-subnet-az1]
    B --> E[Internet Gateway - corp-igw]
    C --> F[Tabela de Rota Pública - public-route-table]
    D --> G[Tabela de Rota Privada - private-route-table]
    F --> E
    H[EC2 Instância Pública - public-ec2-instance] --> C
    I[EC2 Instância Privada - private-ec2-instance] --> D
    J[VPC Endpoint - S3 Gateway] --> G
    G --> D
    I --> |Acesso SSH| H
    K[S3 Bucket - corp-data-bucket] --> J
    I --> |Acesso ao S3| K
```

### Checklist de Desenvolvimento

#### Pré-Configuração
- [ ] Criar um usuário IAM com permissão PowerUser (`project-admin-user`).

#### Configuração da VPC e Subnets
- [ ] Criar uma VPC (`corp-vpc`) com o range CIDR `10.0.0.0/16`.
- [ ] Criar Subrede Pública (`public-subnet-az1`) com CIDR `10.0.1.0/24`.
- [ ] Criar Subrede Privada (`private-subnet-az1`) com CIDR `10.0.2.0/24`.

#### Configuração de Internet Gateway e Rotas
- [ ] Criar um Internet Gateway (`corp-igw`) e associá-lo à VPC (`corp-vpc`).
- [ ] Criar Tabela de Rota Pública (`public-route-table`) e associar à sub-rede pública (`public-subnet-az1`).
- [ ] Criar Tabela de Rota Privada (`private-route-table`) e associar à sub-rede privada (`private-subnet-az1`).
- [ ] Editar rotas da Tabela de Rota Pública para direcionar o tráfego `0.0.0.0/0` para o Internet Gateway (`corp-igw`).

#### Configuração das Instâncias EC2
- [ ] Configurar EC2 Instância Pública (`public-ec2-instance`) na sub-rede pública com IP público e criar um par de chaves (`corp-key-pair`).
- [ ] Configurar EC2 Instância Privada (`private-ec2-instance`) na sub-rede privada sem IP público, usando o mesmo par de chaves.

#### Configuração do S3 e VPC Endpoint
- [ ] Criar um bucket S3 (`corp-data-bucket`).
- [ ] Criar um VPC Endpoint do tipo Gateway para o S3 (`com.amazonaws.eu-west-1.s3`) e associar à tabela de rota privada (`private-route-table`).
- [ ] Testar a conectividade da instância privada com o S3, incluindo operações de copiar e remover arquivos.

#### Teste e Validação
- [ ] Verificar acesso SSH entre a instância pública e privada.
- [ ] Confirmar que a instância privada consegue acessar o S3 através do VPC Endpoint.

Esse checklist cobre todos os passos do desenvolvimento e validação da configuração descrita, assegurando que o ambiente esteja seguro e funcional conforme as especificações.

================================================
File: /Bootcamp - Cloud para dados/Aula_07/README.md
================================================
# **Bootcamp Cloud: Aula 07: Gerenciando Bancos de Dados na Amazon com RDS**

**Objetivo**: Nesta aula, vamos explorar o Amazon RDS (Relational Database Service), entender suas principais funcionalidades, tipos de bancos de dados suportados, e aprender a configurar, gerenciar e otimizar bancos de dados na AWS.

### **1. O Que é o Amazon RDS?**

- **Definição**: O Amazon RDS é um serviço gerenciado de banco de dados relacional na nuvem que facilita a configuração, operação e escalabilidade de bancos de dados relacionais. Ele oferece suporte para diversos mecanismos de banco de dados, como MySQL, PostgreSQL, MariaDB, Oracle, SQL Server e Amazon Aurora.

- **Benefícios**:
  - **Automação de Tarefas**: Automatiza tarefas como backups, atualizações de software, e recuperação de falhas.
  - **Escalabilidade**: Permite ajustar a capacidade de acordo com a demanda de maneira rápida e fácil, sem precisar de grandes investimentos em hardware.
  - **Segurança**: Suporte para encriptação de dados em repouso e em trânsito, além de controles de acesso granulares.
  - **Alta Disponibilidade**: Oferece replicação multi-AZ (Availability Zones) para garantir alta disponibilidade e durabilidade.

### **2. Principais Funcionalidades do Amazon RDS**

#### **Multi-AZ Deployment**

- **O que é?**: Uma configuração que permite implantar instâncias de banco de dados em múltiplas zonas de disponibilidade, proporcionando alta disponibilidade e tolerância a falhas.
- **Benefícios**:
  - **Failover Automático**: Se a instância primária falhar, o RDS promove automaticamente uma réplica para minimizar o tempo de inatividade.
  - **Melhor Desempenho de Leitura e Escrita**: Separação de leituras e escritas entre instâncias primárias e réplicas.

#### **Read Replicas**

- **O que são?**: Réplicas de leitura permitem replicar dados de uma instância RDS para melhorar a performance de leitura.
- **Uso Comum**:
  - **Escalabilidade de Leitura**: Descarrega o tráfego de leitura da instância primária.
  - **Disaster Recovery**: Pode ser usada para recuperar dados rapidamente em caso de falhas.

#### **Backup e Restore Automático**

- **Backup Automático**: Realiza backups automáticos dos dados e dos logs de transações.
- **Snapshots Manuais**: Permitem capturar o estado do banco de dados a qualquer momento.
- **Restore Point-in-Time**: Restaurar o banco de dados para um ponto específico no tempo com precisão.

#### **Segurança no Amazon RDS**

- **Encryption**: Criptografa dados em repouso e em trânsito, utilizando chaves gerenciadas pelo AWS KMS (Key Management Service).
- **Controle de Acesso**: Usando VPC Security Groups, IAM Roles e políticas para restringir o acesso ao banco de dados.
- **Monitoramento**: CloudWatch e Event Subscriptions para monitorar métricas e receber alertas sobre o status do banco de dados.

### **3. Configurando o Amazon RDS: Passo a Passo**

#### **1. Criando uma Instância de Banco de Dados no RDS**

1. **Acessar o Console do RDS**:
   - Faça login no console da AWS e navegue até o serviço **RDS (Relational Database Service)**.
   - Clique em **"Create Database"** para iniciar o processo de configuração.

2. **Escolher o Motor de Banco de Dados**:
   - Selecione o mecanismo de banco de dados desejado (MySQL, PostgreSQL, etc.).
   - Escolha a versão do mecanismo com base nas necessidades de compatibilidade e recursos.

3. **Configuração da Instância**:
   - **Tipo de Instância**: Escolha o tipo de instância que atende à sua carga de trabalho (db.t3.micro para testes ou db.m5.large para produção).
   - **Multi-AZ Deployment**: Ative se desejar alta disponibilidade.

4. **Configuração de Armazenamento**:
   - **Tipo de Armazenamento**: Escolha entre armazenamento SSD padrão, provisionado ou magnético.
   - **Auto Scaling de Armazenamento**: Ative para permitir que o RDS ajuste automaticamente o armazenamento conforme necessário.

5. **Configuração de Conectividade**:
   - **VPC**: Selecione a VPC onde a instância será criada.
   - **Sub-redes**: Escolha as sub-redes dentro da VPC (pública ou privada).
   - **Grupos de Segurança**: Configure Security Groups para controlar o acesso de entrada e saída.

6. **Configuração de Autenticação e Backup**:
   - **Autenticação IAM**: Opcional, para usar credenciais IAM para conectar ao banco de dados.
   - **Backup Automático**: Configure a retenção de backups automáticos e os horários de backup.

7. **Criação da Instância**:
   - Clique em **"Create Database"** para finalizar a criação da instância.

#### **2. Gerenciando Backups e Restaurações**

- **Configurar Backups Automáticos**:
  - Defina o período de retenção dos backups e o horário preferido para realização.
- **Snapshots Manuais**:
  - Faça snapshots manuais antes de realizar mudanças críticas no banco de dados.
- **Restauração**:
  - Utilize a funcionalidade de **Restore to Point in Time** para restaurar o banco a um estado anterior.

#### **3. Monitoramento e Escalabilidade**

- **CloudWatch Metrics**:
  - Monitore o uso de CPU, memória, IOPS, e outras métricas críticas através do CloudWatch.
- **Auto Scaling**:
  - Configure para ajustar automaticamente a capacidade da instância com base na demanda.

### **4. Melhores Práticas ao Utilizar o Amazon RDS**

- **Isolamento de Sub-redes Privadas**: Sempre que possível, implante bancos de dados em sub-redes privadas para evitar a exposição direta à internet.
- **Regras de Segurança e Autenticação**: Use Security Groups restritivos e a autenticação IAM para aumentar a segurança.
- **Multi-AZ e Read Replicas**: Utilize Multi-AZ para alta disponibilidade e Read Replicas para escalar leituras.
- **Testes de Backup e Restauração**: Realize testes periódicos de restauração para garantir que seus backups funcionam conforme o esperado.
- **Monitoramento e Alertas**: Configure alertas para monitorar o desempenho e a integridade do banco de dados.

### **5. Projeto Prático: Criando e Gerenciando um Banco de Dados com Amazon RDS**

**Objetivo do Projeto**: Criar uma instância RDS para um ambiente de produção, configurando backups, segurança e escalabilidade.

**Passo a Passo Resumido**:

1. **Criar a Instância RDS**:
   - Escolha o tipo de banco de dados e configure conforme as necessidades da aplicação.
2. **Configurar Multi-AZ e Read Replicas**:
   - Implemente uma configuração Multi-AZ para alta disponibilidade e uma Read Replica para melhorar o desempenho de leitura.
3. **Aplicar Regras de Segurança**:
   - Use Security Groups para restringir o acesso ao banco de dados, garantindo que apenas instâncias autorizadas possam conectar-se.
4. **Monitorar e Ajustar a Instância**:
   - Utilize o CloudWatch para monitorar métricas de desempenho e ajuste a instância conforme necessário.
5. **Testar Backup e Restauração**:
   - Realize testes para garantir que o processo de backup e restauração está funcionando corretamente.

Essa aula oferece uma visão abrangente sobre o gerenciamento de bancos de dados na AWS com o Amazon RDS, capacitando você a configurar, proteger e escalar seus bancos de dados na nuvem.

### **6. Criando uma VPC do Zero e Configurando o RDS em uma Rede Pública**

Para configurar o Amazon RDS em uma rede pública, vamos criar uma VPC do zero, configurar suas sub-redes, habilitar DNS, e então criar uma instância de banco de dados RDS. Essa configuração é especialmente útil para fins de desenvolvimento ou teste, onde o acesso público ao banco de dados é necessário. 

#### **Passo a Passo para Criar uma VPC com RDS**

**1. Criando a VPC e Sub-rede Pública**

1. **Acessar o Console VPC**:
   - Acesse o console da AWS e navegue até o serviço **VPC**.
   - Clique em **"Create VPC"**.

2. **Configurar a VPC**:
   - **Nome**: Dê um nome à sua VPC, por exemplo, "MinhaVPC-RDS".
   - **CIDR Block**: Defina um bloco CIDR, como "10.0.0.0/16", para o intervalo de endereços IP.
   - **Habilitar DNS**: Marque as opções para habilitar DNS hostnames e DNS resolution para garantir que as instâncias dentro da VPC possam resolver nomes de DNS.
   - Clique em **"Create VPC"** para finalizar a criação.

3. **Criar uma Sub-rede Pública**:
   - No painel VPC, selecione **"Subnets"** e clique em **"Create Subnet"**.
   - **Nome**: "MinhaSubnetPublica".
   - **VPC**: Selecione a VPC recém-criada ("MinhaVPC-RDS").
   - **CIDR Block**: Defina um bloco CIDR, como "10.0.1.0/24".
   - **Availability Zone**: Escolha uma zona de disponibilidade para a sub-rede.
   - Clique em **"Create Subnet"**.

4. **Configurar o Gateway de Internet**:
   - No painel VPC, clique em **"Internet Gateways"** e depois em **"Create Internet Gateway"**.
   - **Nome**: "MeuInternetGateway".
   - Clique em **"Create"**, depois associe o gateway à VPC ("MinhaVPC-RDS") clicando em **"Attach to VPC"**.

5. **Configurar a Tabela de Roteamento**:
   - Vá para **"Route Tables"** e selecione a tabela associada à sua VPC.
   - **Adicionar Rota**: Clique em **"Edit Routes"**, adicione uma rota para "0.0.0.0/0" e selecione o Internet Gateway criado anteriormente.
   - **Associar à Sub-rede Pública**: Vá para **"Subnet Associations"** e associe a tabela à "MinhaSubnetPublica".

**2. Criando o RDS na Rede Pública**

1. **Acessar o Console do RDS**:
   - Navegue para o serviço **RDS** no console da AWS e clique em **"Create Database"**.

2. **Escolher o Motor de Banco de Dados**:
   - Selecione o mecanismo **MySQL** ou outro que desejar.
   - Escolha a versão mais recente que suporte o **Free Tier**.

3. **Configurações da Instância**:
   - **Tipo de Instância**: Escolha **t4g.micro** (Free Tier).
   - **Multi-AZ Deployment**: Desative para um ambiente de teste ou desenvolvimento.
   - **Master Username**: Defina o usuário mestre, por exemplo, "admin".
   - **Master Password**: Crie uma senha segura para o administrador.

4. **Configuração de Conectividade**:
   - **VPC**: Selecione "MinhaVPC-RDS".
   - **Sub-rede Pública**: Escolha a sub-rede pública criada anteriormente.
   - **Public Access**: Ative para permitir que o RDS seja acessado publicamente (para ambientes de teste).
   - **Security Group**: Configure para permitir acesso ao banco (por exemplo, na porta 3306 para MySQL).

5. **Configuração de Backup**:
   - **Backup Retention Period**: Defina o período de retenção de backups automáticos, que pode variar de 1 a 35 dias.
   - **Backup Window**: Escolha um horário de manutenção para os backups automáticos; isso deve ser durante um período de menor carga.

6. **Criar o Banco de Dados**:
   - Revise as configurações e clique em **"Create Database"**. O processo de criação pode levar alguns minutos.

**3. Detalhes de Criação e Configuração do Amazon RDS**

- **Tipos de Armazenamento**:
  - **General Purpose SSD (gp2)**: Para a maioria dos usos comuns, com desempenho balanceado.
  - **Provisioned IOPS (io1)**: Para cargas de trabalho de I/O intensivo, como grandes aplicações OLTP.
  - **Magnetic Storage**: Armazenamento mais econômico, mas menos performático.

- **Opções de Backup**:
  - **Backup Automático**: Realiza backups completos diariamente e backups incrementais dos logs de transações.
  - **Snapshots Manuais**: Criação manual de snapshots para capturar o estado atual do banco de dados.
  - **Point-in-Time Restore**: Permite restaurar o banco de dados para um ponto específico no tempo.

- **Segurança**:
  - **IAM Authentication**: Autenticação de usuários via IAM, para controle mais granular.
  - **VPC Security Groups**: Controle de acesso baseado em IPs e portas, para proteger o banco de dados.

- **Janela de Manutenção**:
  - **Configuração de Manutenção**: Escolha um período de manutenção para realizar updates de software que afetam a instância, garantindo o mínimo de interrupções possíveis.

### **Conclusão**

Essa abordagem de criar uma VPC pública para configurar um RDS permite que você gerencie e acesse seu banco de dados com facilidade, mantendo um nível básico de segurança e controle. Para ambientes de produção, recomenda-se uma configuração mais restritiva, como o uso de sub-redes privadas e a desativação do acesso público direto ao RDS.

Entendido! Vamos configurar o WordPress na instância EC2 usando o AWS Systems Manager Session Manager (AWS Connect) para acesso, sem a necessidade de chaves PEM. Segue o passo a passo completo:

### **Passo a Passo Completo: Instalando o WordPress em uma Instância EC2 Usando Amazon RDS**

#### **1. Preparando o Ambiente na AWS**

**1.1 Criar a VPC e Configurar Rede**

1. **Acessar o Console VPC**:
   - No console da AWS, navegue até **VPC** e clique em **"Create VPC"**.

2. **Configurar a VPC**:
   - **Nome**: "MinhaVPC-WordPress".
   - **CIDR Block**: "10.0.0.0/16".
   - Habilitar DNS hostnames e DNS resolution.
   - Clique em **"Create VPC"**.

3. **Criar Sub-rede Pública**:
   - No painel VPC, selecione **"Subnets"** e clique em **"Create Subnet"**.
   - **Nome**: "MinhaSubnetPublica".
   - **VPC**: Selecione "MinhaVPC-WordPress".
   - **CIDR Block**: "10.0.1.0/24".
   - Escolha uma zona de disponibilidade (ex: us-east-1a).
   - Clique em **"Create Subnet"**.

4. **Configurar o Gateway de Internet**:
   - No painel VPC, clique em **"Internet Gateways"** e em **"Create Internet Gateway"**.
   - **Nome**: "MeuInternetGateway".
   - Clique em **"Create"**, depois associe o gateway à VPC criada clicando em **"Attach to VPC"**.

5. **Configurar a Tabela de Roteamento**:
   - Vá para **"Route Tables"**, selecione a tabela associada à VPC.
   - Clique em **"Edit Routes"** > **"Add route"**, adicione "0.0.0.0/0" e selecione o Internet Gateway.
   - Associe a tabela à sub-rede pública criada.

#### **2. Criar a Instância do Banco de Dados com Amazon RDS**

1. **Acessar o Console do RDS**:
   - Navegue até **RDS** e clique em **"Create Database"**.

2. **Configurar o Banco de Dados**:
   - **Engine**: Selecione **MySQL**.
   - **Version**: Escolha uma versão compatível com WordPress.
   - **Template**: Escolha **Free Tier** se aplicável.
   - **DB Instance Class**: Selecione **db.t4g.micro** (Free Tier).
   - **Multi-AZ**: Desative para ambientes de teste.

3. **Configurações de Identificação e Autenticação**:
   - **DB Instance Identifier**: Nome da instância, ex: "WordPressDB".
   - **Master Username**: "admin".
   - **Master Password**: Defina uma senha segura.

4. **Configurações de Conectividade**:
   - **VPC**: Selecione "MinhaVPC-WordPress".
   - **Subnets**: Escolha a sub-rede pública.
   - **Public Access**: Ative para permitir conexão da instância EC2.
   - **Security Group**: Crie ou selecione um grupo que permita acesso na porta 3306 (MySQL).

5. **Backup e Manutenção**:
   - Configure backups automáticos e defina a janela de manutenção para horários de baixa demanda.

6. **Criar o Banco de Dados**:
   - Revise as configurações e clique em **"Create Database"**.

================================================
File: /Bootcamp - Cloud para dados/Aula_08/README.md
================================================
# Bootcamp Cloud: Aula 08 - Integração entre EC2 e RDS para Processamento de Requisições de API e Armazenamento de Dados

**Objetivo**: Nesta aula, vamos aprender a configurar uma instância EC2 para rodar uma aplicação de API utilizando Docker e integrar essa aplicação com um banco de dados Amazon RDS. Vamos explorar as vantagens dessa arquitetura, as melhores práticas de segurança, e como configurar os recursos necessários na AWS.

### **1. Introdução à Integração EC2 e RDS**

A combinação de instâncias EC2 com o Amazon RDS é amplamente utilizada para construir aplicações robustas que precisam processar requisições de APIs e armazenar dados em um banco de dados relacional gerenciado. EC2 oferece flexibilidade e controle sobre a execução do código da aplicação, enquanto o RDS cuida da gestão do banco de dados, garantindo alta disponibilidade, backups automatizados e segurança.

### **2. Benefícios da Integração entre EC2 e RDS**

- **Escalabilidade e Flexibilidade**: A instância EC2 permite escalabilidade horizontal da aplicação, enquanto o RDS escala automaticamente o banco de dados de acordo com a demanda.
- **Gerenciamento Simplificado**: O RDS cuida de atualizações, backups, e manutenção do banco de dados, liberando tempo para focar no desenvolvimento da aplicação.
- **Segurança Avançada**: Com o uso de Security Groups, podemos restringir o acesso ao banco de dados, garantindo que apenas a instância EC2 tenha permissão para se conectar.
- **Alto Desempenho**: O uso combinado de EC2 e RDS permite que aplicações sejam otimizadas para lidar com grandes volumes de requisições e operações de banco de dados.

### **3. Passo a Passo para Configuração da Instância EC2**

#### **3.1. Configurando a Instância EC2**

1. **Acessar o Console EC2**:
   - No console da AWS, navegue até **EC2** e clique em **"Launch Instance"**.

2. **Escolher a AMI (Amazon Machine Image)**:
   - Selecione uma AMI com Ubuntu Server (última versão LTS).

3. **Escolher o Tipo de Instância**:
   - Escolha um tipo de instância que atenda às necessidades do projeto (ex.: t2.micro para desenvolvimento ou t3.medium para produção).

4. **Configuração de Rede**:
   - Selecione a VPC e Sub-rede desejadas.
   - Configure o Security Group permitindo acesso necessário, como SSH na porta 22 para administração e outras portas conforme a aplicação.

5. **Configuração de Armazenamento**:
   - Configure o armazenamento de acordo com o necessário para a aplicação (ex.: 30GB SSD).

6. **Launch**:
   - Revise as configurações e clique em **"Launch"**. Escolha um par de chaves para acesso SSH ou crie um novo.

#### **3.2. Configurando a Aplicação na Instância EC2**

1. **Acessar a Instância via SSH**:
   - Conecte-se à instância EC2 usando o terminal:
     ```bash
     ssh -i "caminho/para/sua-chave.pem" ubuntu@seu-endereco-ec2
     ```

2. **Atualizar Pacotes e Instalar Docker e Git**:
   - Execute os seguintes comandos:
     ```bash
     sudo su
     sudo apt-get update
     sudo apt install -y git docker.io docker-compose
     ```

3. **Clonar o Repositório do Projeto**:
   - Clone o repositório do projeto:
     ```bash
     git clone https://github.com/lvgalvao/api-scheduler-python-rds.git /app
     ```

4. **Construir e Executar o Contêiner Docker**:
   - Navegue para o diretório do projeto e construa a imagem Docker:
     ```bash
     cd /app
     sudo docker build -t api-schedule-app .
     ```
   - Execute o contêiner com as variáveis de ambiente para integração com o RDS:
     ```bash
     sudo docker run -d \
     --name api-schedule-app-container \
     -e DB_HOST=<endereco-rds> \
     -e DB_USER=<usuario> \
     -e DB_PASS=<senha> \
     -e DB_NAME=<nome-do-banco> \
     api-schedule-app
     ```

### **4. Configuração do Banco de Dados Amazon RDS**

#### **4.1. Criando uma Instância RDS**

1. **Acessar o Console do RDS**:
   - Navegue até o serviço **RDS** no console da AWS e clique em **"Create Database"**.

2. **Escolher o Mecanismo do Banco de Dados**:
   - Selecione o banco de dados desejado (ex.: PostgreSQL, MySQL).
   - Escolha a versão de acordo com os requisitos do projeto.

3. **Configurações da Instância**:
   - **DB Instance Class**: Escolha uma classe de instância compatível com a carga esperada (ex.: db.t3.micro para teste).
   - **Multi-AZ Deployment**: Ative se precisar de alta disponibilidade.

4. **Configurações de Conectividade**:
   - **VPC**: Selecione a mesma VPC da instância EC2.
   - **Sub-redes**: Escolha sub-redes privadas para o banco de dados.
   - **Public Access**: Desative para um ambiente de produção, habilite apenas para desenvolvimento com restrições de segurança.

5. **Configurações de Autenticação e Backup**:
   - Configure backups automáticos e defina as políticas de retenção e manutenção.

6. **Criação da Instância**:
   - Clique em **"Create Database"** para finalizar.

### **5. Criando e Configurando Security Groups**

#### **5.1. Criando um Security Group para a Instância EC2**

1. **Acessar o Console EC2**:
   - Navegue até **Security Groups** e clique em **"Create Security Group"**.

2. **Configurar o Security Group**:
   - **Nome**: Nomeie seu grupo (ex.: SG-EC2-API).
   - **Descrição**: Descreva a função (ex.: Security Group para instância EC2 da API).
   - **Regras de Entrada**:
     - **SSH (Porta 22)**: Permita o acesso somente do seu IP (para acesso seguro).
     - **HTTP/HTTPS**: Habilite conforme necessário para a aplicação.

#### **5.2. Criando um Security Group para o Banco de Dados RDS**

1. **Acessar o Console RDS**:
   - No painel do RDS, acesse **Security Groups** e clique em **"Create Security Group"**.

2. **Configurar o Security Group do RDS**:
   - **Nome**: Nomeie seu grupo (ex.: SG-RDS-Database).
   - **Regras de Entrada**:
     - **Banco de Dados (Porta 5432 para PostgreSQL, ou outra conforme o banco)**.
     - **Origem**: Especifique o Security Group da instância EC2 (ex.: SG-EC2-API) para garantir que apenas a instância EC2 possa se conectar ao banco.

### **6. Vantagens e Oportunidades da Integração EC2 + RDS para Engenharia de Dados**

1. **Desempenho e Escalabilidade**: A combinação EC2 e RDS permite processar grandes volumes de dados e escalar conforme a demanda, ideal para pipelines de dados e processamento intensivo.

2. **Facilidade de Manutenção**: Com o RDS gerenciado, as tarefas complexas de manutenção do banco de dados, como backups e patches, são automatizadas, liberando os engenheiros para focar em otimizações de processamento.

3. **Segurança e Controle de Acesso**: A configuração de Security Groups restritivos garante que o tráfego de dados entre a aplicação e o banco seja seguro, minimizando riscos de acesso não autorizado.

4. **Implementação Ágil de Projetos de Dados**: Essa arquitetura facilita a rápida implementação de APIs que coletam, processam e armazenam dados, otimizando fluxos de trabalho de ETL (Extração, Transformação e Carga) com acesso seguro ao banco de dados.

5. **Monitoramento e Otimização**: Utilizando ferramentas da AWS como CloudWatch, é possível monitorar métricas críticas de desempenho tanto da instância EC2 quanto do RDS, ajustando conforme necessário para garantir alta performance.

### **Conclusão**

Nesta aula, aprendemos como configurar e integrar uma instância EC2 com o Amazon RDS, criando uma arquitetura robusta para aplicações que processam requisições de APIs e armazenam dados de forma eficiente e segura. Essa integração é um pilar essencial para qualquer projeto de engenharia de dados, permitindo que você construa soluções escaláveis e de alto desempenho na nuvem.

================================================
File: /Bootcamp - Cloud para dados/Aula_08/Dockerfile
================================================
# Usar uma imagem base do Python
FROM python:3.12

# Definir o diretório de trabalho
WORKDIR /app

# Copiar o arquivo requirements.txt para o container
COPY requirements.txt .

# Instalar as dependências listadas no requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copiar todo o conteúdo do projeto para o diretório de trabalho do container
COPY . .

# Comando para rodar o script fetch.py localizado na pasta src
CMD ["python", "src/fetch.py"]


================================================
File: /Bootcamp - Cloud para dados/Aula_08/requirements.txt
================================================
annotated-types==0.7.0
certifi==2024.8.30
charset-normalizer==3.3.2
idna==3.8
psycopg2-binary==2.9.9
pydantic==2.8.2
pydantic_core==2.20.1
python-dotenv==1.0.1
requests==2.32.3
schedule==1.2.2
typing_extensions==4.12.2
urllib3==2.2.2


================================================
File: /Bootcamp - Cloud para dados/Aula_08/src/fetch.py
================================================
from requests import Session
from requests.exceptions import ConnectionError, Timeout, TooManyRedirects
from dotenv import load_dotenv
import schedule
import json
import os
import time
import psycopg2
from psycopg2 import sql

# Carregar variáveis do arquivo .env
load_dotenv()

# URL da API de Produção para obter a última cotação do Bitcoin
url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'

# Parâmetros da requisição para obter a cotação do Bitcoin
parameters = {
    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo
    'convert': 'USD'  # Convertendo a cotação para USD
}

# Headers com a chave da API obtida do arquivo .env
headers = {
    'Accepts': 'application/json',
    'X-CMC_PRO_API_KEY': os.getenv('CMC_API_KEY'),  # Obtendo a chave do .env
}

# Criar uma sessão
session = Session()
session.headers.update(headers)

# Configuração do banco de dados RDS
DB_HOST = os.getenv("DB_HOST")
DB_NAME = os.getenv("DB_NAME")
DB_USER = os.getenv("DB_USER")
DB_PASS = os.getenv("DB_PASS")

# Função para criar a tabela caso ainda não exista
def criar_tabela():
    try:
        conn = psycopg2.connect(
            host=DB_HOST,
            database=DB_NAME,
            user=DB_USER,
            password=DB_PASS
        )
        cursor = conn.cursor()
        
        # Criação da tabela
        create_table_query = '''
        CREATE TABLE IF NOT EXISTS bitcoin_quotes (
            id SERIAL PRIMARY KEY,
            price NUMERIC,
            volume_24h NUMERIC,
            market_cap NUMERIC,
            last_updated TIMESTAMP
        );
        '''
        cursor.execute(create_table_query)
        conn.commit()
        cursor.close()
        conn.close()
        print("Tabela criada ou já existente.")
    except Exception as e:
        print(f"Erro ao criar a tabela: {e}")

# Função para salvar os dados no banco de dados
def salvar_no_rds(usd_quote):
    try:
        conn = psycopg2.connect(
            host=DB_HOST,
            database=DB_NAME,
            user=DB_USER,
            password=DB_PASS
        )
        cursor = conn.cursor()
        
        # Inserção dos dados na tabela
        insert_query = sql.SQL(
            "INSERT INTO bitcoin_quotes (price, volume_24h, market_cap, last_updated) VALUES (%s, %s, %s, %s)"
        )
        cursor.execute(insert_query, (
            usd_quote['price'],
            usd_quote['volume_24h'],
            usd_quote['market_cap'],
            usd_quote['last_updated']
        ))
        conn.commit()
        cursor.close()
        conn.close()
        print("Dados salvos com sucesso!")
    except Exception as e:
        print(f"Erro ao salvar dados no RDS: {e}")

# Função que faz a requisição à API e salva a última cotação do Bitcoin
def consultar_cotacao_bitcoin():
    try:
        response = session.get(url, params=parameters)
        data = json.loads(response.text)
        
        # Verificar se os dados do Bitcoin estão presentes na resposta
        if 'data' in data and 'BTC' in data['data']:
            bitcoin_data = data['data']['BTC']
            usd_quote = bitcoin_data['quote']['USD']
            
            # Salvar os dados no banco de dados
            salvar_no_rds(usd_quote)
        else:
            print("Erro ao obter a cotação do Bitcoin:", data['status'].get('error_message', 'Erro desconhecido'))

    except (ConnectionError, Timeout, TooManyRedirects) as e:
        print(f"Erro na requisição: {e}")

# Criar a tabela no banco de dados
criar_tabela()

# Agendar a função para rodar a cada 15 segundos
schedule.every(15).seconds.do(consultar_cotacao_bitcoin)

# Loop principal para manter o agendamento ativo
if __name__ == "__main__":
    print("Iniciando o agendamento para consultar a API a cada 15 segundos...")
    while True:
        schedule.run_pending()
        time.sleep(1)


================================================
File: /Bootcamp - Cloud para dados/Aula_08/src/fetch_1.py
================================================
import urllib3
import json
import os

# Carregar variáveis de ambiente
url = os.getenv('API_URL')  # URL da API obtida das variáveis de ambiente
api_key = os.getenv('CMC_API_KEY')  # Chave da API obtida das variáveis de ambiente

# Parâmetros da requisição para obter a cotação do Bitcoin
parameters = {
 'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo
 'convert': 'USD'  # Convertendo a cotação para USD
}

# Headers com a chave da API
headers = {
 'Accept': 'application/json',
 'X-CMC_PRO_API_KEY': api_key  # Obtendo a chave da API das variáveis de ambiente
}

# Criar um PoolManager para gerenciar conexões
http = urllib3.PoolManager()

# Função Lambda
def lambda_handler(event, context):
 try:
     # Converte os parâmetros para o formato de query string
     query_string = '&'.join([f'{key}={value}' for key, value in parameters.items()])
     full_url = f"{url}?{query_string}"
     
     # Fazendo o request GET para a API
     response = http.request('GET', full_url, headers=headers)
     data = json.loads(response.data.decode('utf-8'))
     
     # Verificar se os dados do Bitcoin estão presentes na resposta
     if 'data' in data and 'BTC' in data['data']:
         bitcoin_data = data['data']['BTC']
         usd_quote = bitcoin_data['quote']['USD']
         
         # Log da resposta
         print(f"Cotação do Bitcoin obtida: {usd_quote}")
     else:
         print("Erro ao obter a cotação do Bitcoin:", data.get('status', {}).get('error_message', 'Erro desconhecido'))

 except urllib3.exceptions.HTTPError as e:
     print(f"Erro na requisição: {e}")


================================================
File: /Bootcamp - Cloud para dados/Aula_08/src/fetch_2.py
================================================
from requests import Session
from requests.exceptions import ConnectionError, Timeout, TooManyRedirects
from dotenv import load_dotenv
from pydantic import BaseModel, ValidationError, Field
import json
import os

# Carregar variáveis do arquivo .env
load_dotenv()

# URL da API de Produção para obter a última cotação do Bitcoin
url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'

# Parâmetros da requisição para obter a cotação do Bitcoin
parameters = {
    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo
    'convert': 'USD'  # Convertendo a cotação para USD
}

# Headers com a chave da API obtida do arquivo .env
headers = {
    'Accepts': 'application/json',
    'X-CMC_PRO_API_KEY': os.getenv('CMC_API_KEY'),  # Obtendo a chave do .env
}

# Criar uma sessão
session = Session()
session.headers.update(headers)

# Modelo Pydantic para o Parsing da Resposta
class QuoteModel(BaseModel):
    price: float
    volume_24h: float = Field(alias='volume_24h')
    market_cap: float = Field(alias='market_cap')
    last_updated: str = Field(alias='last_updated')

class BitcoinDataModel(BaseModel):
    symbol: str
    quote: dict

    def get_usd_quote(self) -> QuoteModel:
        return QuoteModel(**self.quote['USD'])

class ApiResponseModel(BaseModel):
    data: dict
    status: dict

    def get_bitcoin_data(self) -> BitcoinDataModel:
        return BitcoinDataModel(**self.data['BTC'])

# Função que faz a requisição à API e processa os dados usando Pydantic
def consultar_cotacao_bitcoin():
    try:
        response = session.get(url, params=parameters)
        data = json.loads(response.text)
        
        # Parsing da resposta usando Pydantic
        api_response = ApiResponseModel(**data)
        bitcoin_data = api_response.get_bitcoin_data()
        quote = bitcoin_data.get_usd_quote()

        # Imprimir os dados da cotação
        print(f"Última cotação do Bitcoin: ${quote.price:.2f} USD")
        print(f"Volume 24h: ${quote.volume_24h:.2f} USD")
        print(f"Market Cap: ${quote.market_cap:.2f} USD")
        print(f"Última atualização: {quote.last_updated}")

    except (ConnectionError, Timeout, TooManyRedirects) as e:
        print(f"Erro na requisição: {e}")
    except ValidationError as e:
        print(f"Erro ao validar a resposta da API: {e}")

# Executa a função para consultar a cotação do Bitcoin
consultar_cotacao_bitcoin()


================================================
File: /Bootcamp - Cloud para dados/Aula_08/src/fetch_3.py
================================================
from requests import Session
from requests.exceptions import ConnectionError, Timeout, TooManyRedirects
from dotenv import load_dotenv
import schedule
import json
import os
import time

# Carregar variáveis do arquivo .env
load_dotenv()

# URL da API de Produção para obter a última cotação do Bitcoin
url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'

# Parâmetros da requisição para obter a cotação do Bitcoin
parameters = {
    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo
    'convert': 'USD'  # Convertendo a cotação para USD
}

# Headers com a chave da API obtida do arquivo .env
headers = {
    'Accepts': 'application/json',
    'X-CMC_PRO_API_KEY': os.getenv('CMC_API_KEY'),  # Obtendo a chave do .env
}

# Criar uma sessão
session = Session()
session.headers.update(headers)

# Função que faz a requisição à API e imprime a última cotação do Bitcoin
def consultar_cotacao_bitcoin():
    try:
        response = session.get(url, params=parameters)
        data = json.loads(response.text)
        
        # Verificar se os dados do Bitcoin estão presentes na resposta
        if 'data' in data and 'BTC' in data['data']:
            bitcoin_data = data['data']['BTC']
            usd_quote = bitcoin_data['quote']['USD']
            
            # Imprimir os dados da cotação
            print(f"Última cotação do Bitcoin: ${usd_quote['price']:.2f} USD")
            print(f"Volume 24h: ${usd_quote['volume_24h']:.2f} USD")
            print(f"Market Cap: ${usd_quote['market_cap']:.2f} USD")
            print(f"Última atualização: {usd_quote['last_updated']}")
        else:
            print("Erro ao obter a cotação do Bitcoin:", data['status'].get('error_message', 'Erro desconhecido'))

    except (ConnectionError, Timeout, TooManyRedirects) as e:
        print(f"Erro na requisição: {e}")

# Agendar a função para rodar a cada 15 segundos
schedule.every(15).seconds.do(consultar_cotacao_bitcoin)

# Loop principal para manter o agendamento ativo
if __name__ == "__main__":
    print("Iniciando o agendamento para consultar a API a cada 15 segundos...")
    while True:
        schedule.run_pending()
        time.sleep(1)


================================================
File: /Bootcamp - Cloud para dados/Aula_08/src/fetch_4.py
================================================
from requests import Session
from requests.exceptions import ConnectionError, Timeout, TooManyRedirects
from dotenv import load_dotenv
from pydantic import BaseModel, ValidationError, Field
import json
import os

# Carregar variáveis do arquivo .env
load_dotenv()

# URL da API de Produção para obter a última cotação do Bitcoin
url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'

# Parâmetros da requisição para obter a cotação do Bitcoin
parameters = {
    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo
    'convert': 'USD'  # Convertendo a cotação para USD
}

# Headers com a chave da API obtida do arquivo .env
headers = {
    'Accepts': 'application/json',
    'X-CMC_PRO_API_KEY': os.getenv('CMC_API_KEY'),  # Obtendo a chave do .env
}

# Criar uma sessão
session = Session()
session.headers.update(headers)

# Modelo Pydantic para o Parsing da Resposta
class QuoteModel(BaseModel):
    price: float
    volume_24h: float = Field(alias='volume_24h')
    market_cap: float = Field(alias='market_cap')
    last_updated: str = Field(alias='last_updated')

class BitcoinDataModel(BaseModel):
    symbol: str
    quote: dict

    def get_usd_quote(self) -> QuoteModel:
        return QuoteModel(**self.quote['USD'])

class ApiResponseModel(BaseModel):
    data: dict
    status: dict

    def get_bitcoin_data(self) -> BitcoinDataModel:
        return BitcoinDataModel(**self.data['BTC'])

# Função que faz a requisição à API e processa os dados usando Pydantic
def consultar_cotacao_bitcoin():
    try:
        response = session.get(url, params=parameters)
        data = json.loads(response.text)
        
        # Parsing da resposta usando Pydantic
        api_response = ApiResponseModel(**data)
        bitcoin_data = api_response.get_bitcoin_data()
        quote = bitcoin_data.get_usd_quote()

        # Imprimir os dados da cotação
        print(f"Última cotação do Bitcoin: ${quote.price:.2f} USD")
        print(f"Volume 24h: ${quote.volume_24h:.2f} USD")
        print(f"Market Cap: ${quote.market_cap:.2f} USD")
        print(f"Última atualização: {quote.last_updated}")

    except (ConnectionError, Timeout, TooManyRedirects) as e:
        print(f"Erro na requisição: {e}")
    except ValidationError as e:
        print(f"Erro ao validar a resposta da API: {e}")

# Executa a função para consultar a cotação do Bitcoin
consultar_cotacao_bitcoin()


================================================
File: /Bootcamp - Cloud para dados/Aula_09/README.md
================================================
### Aula 09: AWS Lambda e Eventos na AWS

**Objetivo**: Nesta aula, exploraremos o AWS Lambda e seu papel dentro de arquiteturas serverless na AWS. Vamos entender as diferenças entre o AWS Lambda e o EC2, discutir suas vantagens e desafios, e aprender como eventos podem ser aproveitados para criar sistemas escaláveis e eficientes.

### 3. **Arquitetura Serverless com AWS Lambda**

Este diagrama mostra uma arquitetura típica serverless, onde múltiplos serviços da AWS interagem com o Lambda para automatizar processos.

```mermaid
graph TB
    S3[S3] --> |Upload de Arquivo| Lambda1[AWS Lambda]
    APIGateway[API Gateway] --> |Requisição HTTP| Lambda2[AWS Lambda]
    DynamoDB[DynamoDB Streams] --> |Alteração na Tabela| Lambda3[AWS Lambda]
    CloudWatch[CloudWatch Events] --> |Alerta| Lambda4[AWS Lambda]
    SQS[SQS] --> |Mensagem na Fila| Lambda5[AWS Lambda]

    Lambda1 --> Process1[Processa Arquivo]
    Lambda2 --> Process2[API Backend]
    Lambda3 --> Process3[Sincroniza Dados]
    Lambda4 --> Process4[Automatiza Resposta]
    Lambda5 --> Process5[Processa Mensagem]
```

### **1. Introdução ao AWS Lambda**

AWS Lambda é um serviço de computação serverless que executa código sem que você precise gerenciar servidores. Ele é ideal para processos pontuais que respondem automaticamente a eventos, como uploads no S3, alterações em bancos de dados, e requisições HTTP através do API Gateway. Embora o termo "serverless" sugira a ausência de servidores, na prática, isso significa que o código é executado em servidores gerenciados pela AWS, não por você.

### **2. Diferença entre AWS Lambda e EC2**

#### **2.1. AWS EC2 (Elastic Compute Cloud)**
- **Definição**: Serviço de computação que permite criar e gerenciar instâncias de servidores virtuais na nuvem, oferecendo controle total sobre o ambiente.
- **Vantagens**:
  - **Controle Completo**: Customização do ambiente de software e hardware.
  - **Ambientes Persistentes**: Ideal para aplicações que exigem disponibilidade contínua.
  - **Escalabilidade Flexível**: Permite ajustes manuais ou automáticos de recursos.
- **Desafios**:
  - **Gerenciamento**: Requer manutenção contínua e configuração detalhada.
  - **Custo**: Pagamento contínuo pelo tempo de execução, independente do uso.
  - **Complexidade de Configuração**: Requer atenção aos detalhes de rede, segurança e capacidade.

#### **2.2. AWS Lambda**
- **Definição**: Serviço que executa código em resposta a eventos sem necessidade de gerenciar servidores, cobrando apenas pelo tempo de execução.
- **Vantagens**:
  - **Serverless**: Reduz a complexidade operacional.
  - **Escalabilidade Automática**: Ajusta automaticamente com base na carga de eventos.
  - **Custo-Eficiência**: Ideal para cargas intermitentes com pagamento por uso.
- **Desafios**:
  - **Limitações de Execução**: Tempo máximo de 15 minutos, com restrições de memória e armazenamento.
  - **Cold Starts**: Pequenos atrasos iniciais quando funções são ativadas após inatividade.
  - **Configuração de Permissões**: Requer configurações cuidadosas para garantir segurança.

### **3. Principais Casos de Uso do AWS Lambda**
- **Processamento de Arquivos**: Automatiza tarefas como redimensionamento de imagens e análise de dados em S3.
- **ETL em Tempo Real**: Transformação de dados em tempo real a partir de streams de dados.
- **APIs Serverless**: Gerenciamento de APIs com API Gateway e Lambda.
- **Eventos de IoT**: Resposta a dados de dispositivos IoT.
- **Automação de Infraestrutura**: Tarefas automatizadas como limpeza de recursos e monitoramento.

### **4. Eventos na AWS e Integração com Lambda**
Lambda pode ser acionado por diversos eventos na AWS, permitindo respostas dinâmicas a mudanças nos serviços.
- **Eventos S3**: Código executado ao upload de arquivos.
- **Eventos DynamoDB Streams**: Funções disparadas por alterações em tabelas.
- **Eventos API Gateway**: Requisições HTTP acionam funções Lambda.
- **Eventos CloudWatch**: Ações baseadas em alertas e agendamentos.
- **Eventos SQS**: Mensagens processadas de filas SQS.

### **5. Configurando uma Função AWS Lambda**
1. **Criar Função no Console AWS Lambda**: Selecione “Author from scratch” e configure nome, runtime e permissões.
2. **Escrever Código**: Escreva o código da função no editor integrado, definindo a lógica do lambda handler.
3. **Adicionar Trigger**: Configure o evento que irá acionar a função, como upload no S3 ou requisição HTTP.
4. **Testar Função**: Utilize o console para testes com eventos simulados e ajuste o código conforme necessário.

### **6. Comparação de Cenários de Uso: AWS Lambda vs. EC2**

| **Critério**           | **AWS EC2**                                  | **AWS Lambda**                              |
|------------------------|----------------------------------------------|---------------------------------------------|
| **Controle**           | Total sobre ambiente e recursos             | Limitado ao código da função                |
| **Persistência**       | Executa continuamente                       | Executa sob demanda, de forma intermitente  |
| **Escalabilidade**     | Manual ou automática                         | Automática com base em eventos              |
| **Gerenciamento**      | Requer configuração e manutenção            | Automação pela AWS                          |
| **Custo**              | Contínuo enquanto ativo                     | Paga apenas pelo uso                        |
| **Tempo de Resposta**  | Latência menor em execução contínua         | Cold starts podem aumentar a latência       |

### **7. Desafios ao Usar AWS Lambda e EC2**
- **AWS Lambda**: 
  - Limitações de tempo e recursos, cold starts e desafios de debug.
- **AWS EC2**: 
  - Requer gerenciamento contínuo, configuração de escalabilidade e custos constantes.

### **8. Motivação para Escolher AWS Lambda**
- **Economia**: Pagamento por uso sem custos fixos.
- **Escalabilidade**: Ajustes automáticos com base na demanda.
- **Foco no Código**: Sem preocupações com infraestrutura.

### **Conclusão**
AWS Lambda oferece uma abordagem serverless que simplifica a execução de código na nuvem, focando em escalabilidade e economia de custos. Comparado ao EC2, o Lambda reduz a complexidade de gerenciamento, mas exige um entendimento claro de suas limitações. A escolha entre Lambda e EC2 deve considerar o caso de uso específico, pesando controle, custos e necessidades operacionais.

Vou criar alguns diagramas usando Mermaid para ilustrar os conceitos apresentados na aula sobre AWS Lambda e EC2. Esses diagramas ajudarão a visualizar como o AWS Lambda funciona, suas integrações com eventos e uma comparação com o EC2.

### **Exemplo: Teste Meu Primeiro Lambda**

Este exemplo de função Lambda vai simplesmente retornar uma mensagem de teste quando invocada. Isso ajudará você a entender o básico de como criar, configurar e testar uma função Lambda.

#### **Passo a Passo para Criar a Função Lambda**

1. **Acesse o Console do AWS Lambda:**
   - Vá para o [AWS Management Console](https://aws.amazon.com/console/) e selecione **Lambda** no menu de serviços.

2. **Criar a Função Lambda:**
   - Clique em **Create Function**.
   - Escolha **Author from scratch**.
   - Configure os seguintes detalhes:
     - **Function name**: `TesteMeuPrimeiroLambda`.
     - **Runtime**: Selecione `Python 3.9` ou outra versão que você prefira.
     - **Permissions**: Selecione **Create a new role with basic Lambda permissions** para permitir que a função grave logs no CloudWatch.

3. **Adicionar o Código da Função:**

   Após criar a função, adicione o seguinte código no editor do console do Lambda:

   ```python
   def lambda_handler(event, context):
       # Função básica que retorna uma mensagem de teste
       return {
           'statusCode': 200,
           'body': 'Olá! Este é o meu primeiro teste com AWS Lambda.'
       }
   ```

4. **Testar a Função Lambda:**

   - Clique em **Deploy** para salvar o código.
   - Clique em **Test** para criar um evento de teste:
     - Dê um nome ao evento de teste, como `EventoTeste`.
     - Use o payload padrão ou um JSON simples, como:
       ```json
       {
         "mensagem": "Teste de invocação"
       }
       ```
   - Clique em **Test** novamente para executar a função.

   Você verá o resultado da execução na parte inferior da página, mostrando algo semelhante a:

   ```json
   {
     "statusCode": 200,
     "body": "Olá! Este é o meu primeiro teste com AWS Lambda."
   }
   ```

### **Explicação do Código**

- **`lambda_handler(event, context)`**: É a função principal que o AWS Lambda executa. Ela recebe dois parâmetros:
  - **`event`**: Contém os dados que você envia quando aciona a função, como um JSON com informações específicas.
  - **`context`**: Inclui informações de contexto sobre a execução da função, como o tempo de execução restante.

- **Retorno da Função**:
  - **`statusCode`**: Código HTTP 200 indicando sucesso.
  - **`body`**: Uma mensagem de resposta simples que é retornada para quem acionou a função.

### **Vantagens deste Exemplo Simples:**

- **Sem Dependências Externas**: Nenhuma instalação de bibliotecas é necessária, o que simplifica o deploy.
- **Facilidade de Configuração**: Com apenas alguns cliques e um pequeno trecho de código, você pode experimentar o AWS Lambda.
- **Ideal para Primeiros Testes**: Um bom ponto de partida para entender o funcionamento básico do AWS Lambda e como ele responde a eventos.

Este exemplo proporciona uma introdução ao uso do AWS Lambda de forma simples, sem complicações adicionais, sendo ideal para iniciantes que desejam entender como começar com funções serverless na AWS.

Vamos ajustar o exemplo para que a função Lambda receba uma mensagem através de um API Gateway, salve essa mensagem em um arquivo JSON e armazene o arquivo em um bucket S3. Esse exemplo será mais prático e útil para demonstrar o fluxo completo de integração entre o Lambda, o API Gateway e o Amazon S3.

================================================
File: /Bootcamp - Cloud para dados/Aula_10/README.md
================================================
# Aula 10: Gestão de Custos na AWS

**Objetivo**: Vocês já sentiram que seus projetos na AWS estão sempre te assustando no final do mês com faturas inesperadas? Com recursos espalhados por todos os lados sem nenhum controle? Os custos estão subindo e a organização parece um desafio insuperável? Hoje, na nossa Aula 10 - Gestão de Custos na AWS, isso vai mudar. Nesta aula, vamos refatorar nossa conta, organizar os recursos e implementar práticas eficientes de gestão de custos. Nosso objetivo é organizar a casa e assumir o controle financeiro dos nossos projetos.

Hoje vamos desvendar os segredos da gestão eficiente de custos na AWS. Vamos aprender a estabelecer uma **política de tags focada em governança de custos**, utilizar grupos de recursos, aplicar tags inteligentes e explorar ferramentas como o AWS Cost Explorer e AWS Budgets, que nos darão o controle total sobre nossos projetos. Imagine transformar dez projetos caóticos em um ambiente organizado e econômico. Está na hora de assumir o controle, otimizar recursos e impulsionar nossos resultados. Preparados para essa jornada rumo à eficiência e à economia? Então, vamos começar hoje às 12h!

### **Política de Tags Focada em Governança de Custos**

Antes de iniciarmos com as ferramentas, é crucial estabelecermos uma política de tags consistente e obrigatória para todos os recursos AWS. As tags são fundamentais para a governança de custos, pois permitem a categorização e atribuição de custos aos respectivos responsáveis e projetos.

**5 Tags Obrigatórias Focadas em Governança de Custos:**

1. **CostCenter (Centro de Custo)**:
   - **Descrição**: Identifica o centro de custo ou departamento responsável pelas despesas associadas ao recurso.
   - **Exemplo**: `CostCenter: CC1001`

2. **Project (Projeto)**:
   - **Descrição**: Nome do projeto ao qual o recurso pertence.
   - **Exemplo**: `Project: ProjetoX`

3. **Owner (Responsável)**:
   - **Descrição**: Pessoa ou equipe responsável pelo recurso.
   - **Exemplo**: `Owner: JoãoSilva`

4. **Environment (Ambiente)**:
   - **Descrição**: Indica o ambiente de utilização do recurso.
   - **Exemplo**: `Environment: Production` ou `Environment: Development`

5. **Application (Aplicação)**:
   - **Descrição**: Nome da aplicação ou serviço que utiliza o recurso.
   - **Exemplo**: `Application: AppMobile`

**Importância das Tags na Governança de Custos:**

- **Atribuição de Custos**: Permite que os custos sejam atribuídos corretamente aos centros de custo, projetos e responsáveis.
- **Responsabilidade**: Identifica claramente quem é responsável por cada recurso.
- **Otimização**: Facilita a identificação de recursos subutilizados ou desnecessários.
- **Relatórios Personalizados**: Possibilita a geração de relatórios detalhados por tags no AWS Cost Explorer e outros serviços.

### **Projetos da Aula 10**

1. **Estabelecendo e Aplicando a Política de Tags**

   **Objetivo**: Criar uma política de tags consistente e aplicá-la a todos os recursos AWS, garantindo que as 5 tags obrigatórias estejam presentes em cada recurso.

   **Passo a Passo**:

   1. **Definir a Política de Tags**:
      - Documente as tags obrigatórias e suas respectivas chaves e valores esperados.
      - Distribua a política para todas as equipes envolvidas.

   2. **Acessar o Console AWS**:
      - Faça login no AWS Management Console.

   3. **Identificar Recursos Sem as Tags Obrigatórias**:
      - Navegue até o **Resource Groups & Tag Editor**.
      - Clique em **Tag Editor**.
      - Selecione as regiões e tipos de recursos relevantes.
      - Clique em **Find resources**.

   4. **Filtrar Recursos Sem as Tags Obrigatórias**:
      - Utilize filtros para identificar recursos que não possuem as tags obrigatórias.

   5. **Adicionar as Tags aos Recursos**:
      - Selecione os recursos sem as tags.
      - Clique em **Add tags to selected resources**.
      - Adicione as 5 tags obrigatórias com os valores apropriados.
      - Clique em **Save changes**.

   6. **Automatizar a Conformidade de Tags**:
      - Utilize o **AWS Config** para criar regras que verificam a conformidade das tags.
      - Configure alertas para recursos que não atendam à política de tags.

   **Benefícios**:

   - **Consistência**: Todos os recursos seguem o mesmo padrão de tagueamento.
   - **Governança**: Melhora o controle e a gestão dos recursos.
   - **Facilidade na Gestão de Custos**: As tags permitem atribuir custos corretamente.

   ```mermaid
   graph LR
       Policy[Política de Tags] --> Teams[Equipes]
       Teams --> ApplyTags[Aplicam Tags nos Recursos]
       ApplyTags --> AWSResources[Recursos AWS]
   ```

2. **Organização de Recursos com Tags**

   **Objetivo**: Utilizar as tags aplicadas para organizar e gerenciar os recursos de forma eficaz.

   **Passo a Passo**:

   1. **Verificar Tags nos Recursos**:
      - Confirme que todos os recursos possuem as 5 tags obrigatórias.

   2. **Navegar pelos Recursos Usando Tags**:
      - Utilize o **Tag Editor** para pesquisar recursos por tags.
      - Filtre recursos por `Project`, `CostCenter`, `Owner`, etc.

   3. **Analisar Recursos por Responsável**:
      - Filtre recursos pela tag `Owner` para ver quais recursos estão sob responsabilidade de cada pessoa ou equipe.

   4. **Gerenciar Recursos por Ambiente**:
      - Use a tag `Environment` para distinguir recursos de produção, desenvolvimento, etc.

   **Benefícios**:

   - **Eficiência**: Localização rápida de recursos específicos.
   - **Responsabilização**: Facilita o acompanhamento de quem é responsável por cada recurso.
   - **Otimização**: Identificação de recursos redundantes ou subutilizados.

   ```mermaid
   graph LR
       Tags[Tags] --> Organize[Organização de Recursos]
       Organize --> EfficientManagement[Gestão Eficiente]
   ```

3. **Criando Grupos de Recursos Baseados em Tags**

   **Objetivo**: Criar grupos de recursos utilizando as tags para facilitar o gerenciamento coletivo.

   **Passo a Passo**:

   1. **Acessar o AWS Resource Groups**:
      - No Console AWS, procure por **Resource Groups**.

   2. **Criar um Novo Grupo de Recursos**:
      - Clique em **Create a resource group**.
      - Escolha **Tag-based group**.

   3. **Definir Critérios do Grupo**:
      - Configure o grupo usando as tags obrigatórias.
      - Por exemplo, para agrupar recursos de um projeto:
        - **Key**: `Project`
        - **Value**: `ProjetoX`

   4. **Nomear e Salvar o Grupo**:
      - Dê um nome ao grupo, como `Grupo_ProjetoX`.
      - Clique em **Create group**.

   5. **Utilizar Grupos para Gestão**:
      - Navegue pelos grupos para gerenciar recursos coletivamente.
      - Realize operações em lote quando aplicável.

   **Benefícios**:

   - **Visibilidade**: Visualização consolidada dos recursos por projeto, centro de custo, etc.
   - **Gestão Simplificada**: Ações coletivas nos recursos do grupo.
   - **Facilita a Governança**: Monitoramento e controle aprimorados.

   ```mermaid
   graph LR
       Tags[Tags] --> ResourceGroups[Grupos de Recursos]
       ResourceGroups --> Management[Gestão Simplificada]
   ```

4. **Utilizando o AWS Cost Explorer com Tags**

   **Objetivo**: Analisar os custos detalhadamente utilizando as tags aplicadas nos recursos.

   **Passo a Passo**:

   1. **Acessar o AWS Cost Management**:
      - No Console AWS, vá para **Cost Management**.
      - Selecione **Cost Explorer**.

   2. **Ativar o Cost Explorer (se ainda não estiver ativo)**:
      - Clique em **Enable Cost Explorer**.

   3. **Configurar o Cost Explorer para Exibir Tags**:
      - Vá para **Cost Explorer Settings**.
      - Em **Cost Allocation Tags**, ative as tags que deseja utilizar nos relatórios (as 5 tags obrigatórias).

   4. **Explorar os Custos por Tags**:
      - Crie relatórios filtrando ou agrupando por tags como `CostCenter`, `Project`, `Owner`, etc.
      - Visualize os custos associados a cada centro de custo, projeto ou responsável.

   5. **Identificar Tendências e Oportunidades de Otimização**:
      - Analise os gráficos e dados para identificar áreas de alto gasto.
      - Use as informações para tomar decisões de otimização.

   **Benefícios**:

   - **Transparência**: Visibilidade clara de onde os custos estão sendo gerados.
   - **Atribuição de Custos Precisa**: Alocação correta de despesas aos centros de custo e projetos.
   - **Decisões Informadas**: Dados detalhados para suportar estratégias de otimização.

   ```mermaid
   graph LR
       CostExplorer[Cost Explorer] --> Analysis[Análise de Custos por Tags]
       Analysis --> Optimization[Otimização]
   ```

5. **Configurando AWS Budgets com Base nas Tags**

   **Objetivo**: Estabelecer orçamentos para monitorar os gastos e receber alertas quando os custos excederem limites definidos, utilizando as tags para especificar o escopo.

   **Passo a Passo**:

   1. **Acessar o AWS Budgets**:
      - No **Cost Management**, selecione **Budgets**.

   2. **Criar um Novo Orçamento**:
      - Clique em **Create budget**.

   3. **Selecionar Tipo de Orçamento**:
      - Escolha **Cost Budget**.

   4. **Definir Detalhes do Orçamento**:
      - Nome do orçamento: `Orçamento_CentroDeCusto_CC1001`.
      - Período: Mensal.
      - Valor do orçamento: Defina o valor máximo para o centro de custo, por exemplo, `$5,000`.

   5. **Aplicar Filtros com Tags**:
      - Em **Filters**, adicione a tag:
        - **Key**: `CostCenter`
        - **Value**: `CC1001`

   6. **Configurar Alertas**:
      - Defina quando deseja ser notificado:
        - Quando atingir 80% do orçamento.
        - Quando ultrapassar 100% do orçamento.
      - Adicione os emails dos responsáveis (utilizando a tag `Owner` para referência).

   7. **Revisar e Criar**:
      - Revise as configurações e clique em **Create budget**.

   8. **Repetir para Outros Centros de Custo ou Projetos**:
      - Crie orçamentos para cada centro de custo ou projeto conforme necessário.

   **Benefícios**:

   - **Controle Financeiro**: Monitoramento proativo dos gastos por centro de custo ou projeto.
   - **Alertas Personalizados**: Notificações para os responsáveis quando os limites são atingidos.
   - **Planejamento Financeiro Eficaz**: Apoio na elaboração e cumprimento de orçamentos.

   ```mermaid
   graph LR
       Budgets[AWS Budgets] --> Monitoring[Monitoramento de Custos por Tags]
       Monitoring --> Alerts[Alertas Personalizados]
   ```

6. **Implementando AWS Cost Anomaly Detection com Tags**

   **Objetivo**: Configurar o AWS Cost Anomaly Detection para identificar automaticamente anomalias nos gastos específicos a centros de custo, projetos ou responsáveis.

   **Passo a Passo**:

   1. **Acessar o Cost Anomaly Detection**:
      - No **Cost Management**, selecione **Anomaly Detection**.

   2. **Criar um Monitor de Anomalias**:
      - Clique em **Create monitor**.

   3. **Definir o Escopo do Monitor com Tags**:
      - Escolha **Single Account** ou **Linked Accounts** se houver.
      - Em **Monitor dimensions**, selecione **Tag**:
        - **Key**: `CostCenter`
        - **Value**: `CC1001`

   4. **Configurar Alertas**:
      - Defina o **Anomaly threshold** para determinar a sensibilidade.
      - Adicione os emails dos responsáveis (`Owner`) para notificações.

   5. **Revisar e Criar**:
      - Revise as configurações e clique em **Create monitor**.

   **Benefícios**:

   - **Detecção Personalizada**: Análise de anomalias em áreas específicas.
   - **Resposta Rápida**: Notificações imediatas para os responsáveis.
   - **Proteção Financeira**: Prevenção de gastos inesperados.

   ```mermaid
   graph LR
       AnomalyDetection[Anomaly Detection] --> IdentifyAnomalies[Identifica Anomalias por Tags]
       IdentifyAnomalies --> Notifications[Notificações para Responsáveis]
   ```

### **Ferramentas Adicionais**

- **AWS Config**:
  - Configure regras para garantir que todos os recursos tenham as tags obrigatórias.
  - Receba alertas ou visualize não conformidades quando recursos não atenderem à política.

- **AWS Trusted Advisor**:
  - Utilize as verificações de otimização de custos, segurança e desempenho.
  - Acesse através do Console AWS em **Trusted Advisor**.

- **AWS Cost and Usage Reports**:
  - Obtenha relatórios detalhados que incluem as tags para análise avançada.
  - Configure no **Cost Management** em **Cost and Usage Reports**.

### **Conclusão da Aula 10**

Nesta aula, enfatizamos a importância de uma política de tags bem definida e consistente como base para a governança eficaz de custos na AWS. Aprendemos a aplicar as 5 tags obrigatórias focadas em governança de custos, permitindo uma gestão detalhada e precisa dos recursos. Utilizamos essas tags em conjunto com ferramentas poderosas como o AWS Cost Explorer, AWS Budgets e AWS Cost Anomaly Detection para obter visibilidade, controle e otimização dos gastos.

Ao estabelecer e aderir a uma política de tags robusta, capacitamos nossas equipes a serem responsáveis pelos recursos que gerenciam, promovendo uma cultura de responsabilidade e eficiência. Com essas práticas implementadas, estamos melhor posicionados para transformar um ambiente desorganizado em uma infraestrutura otimizada e financeiramente saudável, garantindo o sucesso contínuo dos nossos projetos.

---

**Próximos Passos**:

- **Revisar a Política de Tags Regularmente**: Certifique-se de que a política continua relevante e atualizada com as necessidades da organização.
- **Treinamento Contínuo**: Promova workshops e sessões de treinamento para as equipes sobre a importância da tagueação correta.
- **Automatização**: Explore ferramentas e scripts para automatizar a aplicação de tags e a conformidade com a política estabelecida.

Com essas ações, garantiremos uma gestão de custos eficaz e uma governança sólida, permitindo que nos concentremos no que realmente importa: impulsionar nossos projetos para o sucesso.

### **Checklist para Evitar Surpresas no Final do Mês**

Para garantir que os custos na AWS permaneçam sob controle e evitar despesas inesperadas, siga este checklist regularmente:

1. **Revisar Orçamentos no AWS Budgets**:
   - Verifique os orçamentos configurados para cada centro de custo, projeto e responsável.
   - Certifique-se de que os alertas estão configurados corretamente e os contatos estão atualizados.

2. **Monitorar Gastos Diariamente**:
   - Utilize o **AWS Cost Explorer** para acompanhar os gastos diários ou semanais.
   - Configure relatórios personalizados para serem enviados por e-mail regularmente.

3. **Analisar Anomalias com o AWS Cost Anomaly Detection**:
   - Revise notificações de anomalias de custo.
   - Investigue imediatamente quaisquer gastos inesperados ou incomuns.

4. **Garantir a Conformidade das Tags**:
   - Use o **AWS Config** para verificar se todos os recursos possuem as **5 tags obrigatórias**.
   - Corrija recursos que não estejam em conformidade com a política de tags.

5. **Desligar Recursos Não Utilizados**:
   - Identifique recursos ociosos ou subutilizados, como instâncias EC2 não utilizadas, volumes EBS não anexados, etc.
   - Desligue ou exclua esses recursos para reduzir custos desnecessários.

6. **Revisar Reservas e Savings Plans**:
   - Avalie se **Reserved Instances** ou **Savings Plans** podem ser utilizados para recursos de uso contínuo.
   - Ajuste ou renove conforme necessário para maximizar economias.

7. **Comunicar-se com as Equipes Responsáveis**:
   - Mantenha as equipes informadas sobre o status dos gastos e orçamentos.
   - Promova uma cultura de responsabilidade financeira e uso consciente dos recursos.

8. **Atualizar a Política de Tags e Governança**:
   - Revise a política de tags periodicamente para garantir que ela atenda às necessidades atuais da organização.
   - Treine novas equipes ou membros sobre a importância e a aplicação correta das tags.

9. **Automatizar Alertas e Ações**:
   - Considere a configuração de alertas adicionais via **Amazon CloudWatch** para monitorar métricas específicas.
   - Use scripts ou **AWS Lambda** para automatizar ações corretivas quando certos limites forem atingidos.

10. **Auditar Configurações de Segurança e Compliance**:
    - Verifique se não há recursos expostos que possam gerar custos devido a uso indevido.
    - Utilize o **AWS Trusted Advisor** e o **AWS Security Hub** para identificar vulnerabilidades.

11. **Documentar e Revisar Processos**:
    - Mantenha a documentação atualizada sobre procedimentos de gestão de custos e governança.
    - Realize revisões regulares dos processos para identificar melhorias potenciais.

12. **Planejar para Eventos Futuros**:
    - Considere lançamentos de novos projetos, campanhas ou eventos sazonais que possam aumentar o uso de recursos.
    - Ajuste orçamentos e recursos antecipadamente para acomodar aumentos previstos.

13. **Utilizar Relatórios Detalhados**:
    - Configure o **AWS Cost and Usage Report** para obter dados detalhados de uso e custo.
    - Analise esses relatórios para insights aprofundados e para identificar áreas de otimização.

14. **Verificar Limites de Serviço**:
    - Monitore os limites de serviço (quotas) para evitar interrupções inesperadas ou custos adicionais.
    - Solicite aumentos de limite antecipadamente, se necessário.

15. **Implementar Políticas de Encerramento Automático**:
    - Configure políticas para encerrar automaticamente recursos não utilizados após um período definido.
    - Utilize tags como `ExpirationDate` ou `AutoDelete` para auxiliar nesse processo.

Seguindo este checklist regularmente, você estará melhor preparado para evitar surpresas no final do mês e manter os custos da AWS sob controle. A gestão proativa e contínua é essencial para uma operação financeira saudável e para o sucesso dos seus projetos na nuvem.

1) Verificar Frankfurt para ver quanto vai reduzir os custos! (removemos RDS e EC2)

2) Paris deletamos TUDO, lambda, ec2, rds e vpc



================================================
File: /Bootcamp - Cloud para dados/Aula_11/README.md
================================================
# Aula 11: Projetos Práticos com AWS Lambda

**Objetivo**: Nesta aula, realizaremos uma série de projetos práticos utilizando AWS Lambda. Vamos explorar como configurar funções Lambda para serem acionadas por eventos temporais e específicos, realizar requests HTTP e integrar com o Amazon RDS e com o AmazonS3 para criar soluções serverless eficientes.

## **Projetos da Aula 11**

### 1. **Configuração de Timer de 10 em 10 Minutos com AWS Lambda**

   **Objetivo**: Demonstrar como agendar uma função Lambda para ser executada a cada 10 minutos usando o Amazon CloudWatch Events (ou EventBridge).

   **Passo a Passo**:
   1. Acesse o AWS Management Console e selecione **Lambda**.
   2. Crie uma nova função Lambda com o nome `TimerFunction`.
   3. Vá para **CloudWatch Events** e crie uma nova regra com uma expressão cron `cron(0/10 * * * ? *)` para disparar a cada 10 minutos.
   4. Vincule essa regra à função `TimerFunction`.
   5. Teste para confirmar que a função está sendo acionada conforme esperado.

   ```python
   def lambda_handler(event, context):
       print("Função executada a cada 10 minutos.")
       return {
           'statusCode': 200,
           'body': 'Execução bem-sucedida.'
       }
   ```

   ```mermaid
   graph LR
       CW[CloudWatch Events] -->|Trigger a cada 10 minutos| Lambda1[AWS Lambda TimerFunction]
       Lambda1 --> Process1[Executa a Função]
   ```

### 2. **Configuração de Funções Lambda para Horários Específicos**

**Objetivo**: Ensinar como configurar a execução de uma função Lambda em horários específicos, como às 9h, 12h, e 18h diariamente.

**Passo a Passo**:
1. No AWS Management Console, crie uma função Lambda chamada `SpecificTimeFunction`.
2. Acesse **EventBridge (antigo CloudWatch Events)**, que é o serviço da AWS responsável por gerenciar eventos e agendamentos.
3. Crie uma regra com uma expressão cron, por exemplo, `cron(0 9,12,18 * * ? *)`, que dispara a função nos horários específicos: 9h, 12h e 18h.
4. Vincule essa regra à função Lambda `SpecificTimeFunction` como destino.
5. Teste a configuração simulando o disparo da função nos horários especificados.

```python
def lambda_handler(event, context):
    print("Função executada nos horários específicos: 9h, 12h e 18h.")
    return {
        'statusCode': 200,
        'body': 'Execução bem-sucedida nos horários específicos.'
    }
```

```mermaid
graph LR
    EB[Amazon EventBridge] -->|Trigger às 9h, 12h, 18h| Lambda2[AWS Lambda SpecificTimeFunction]
    Lambda2 --> Process2[Executa a Função]
```

### **Amazon EventBridge (antigo CloudWatch Events)**

Amazon EventBridge é um serviço que facilita a criação de arquiteturas baseadas em eventos, permitindo que os serviços da AWS e aplicativos personalizados respondam a eventos de forma rápida e escalável. EventBridge é amplamente utilizado para agendamentos e automatização de fluxos de trabalho, incluindo a execução de funções Lambda em horários específicos.

#### **Possibilidades com EventBridge (CloudWatch Scheduler)**

EventBridge oferece uma série de funcionalidades e flexibilidade na configuração de eventos e cronogramas:

1. **Cronogramas (Schedules)**:
   - Permite a criação de agendamentos para acionar funções Lambda, tarefas ECS, Step Functions, e outros destinos com base em uma expressão cron ou rate.
   - Pode ser utilizado para tarefas repetitivas como limpeza de logs, sincronização de dados, e execução de processos diários.

2. **Criar um Cronograma no EventBridge**:
   - **Etapa 1: Especificar Detalhes do Cronograma**:
     - Defina um nome para o cronograma e escolha entre uma expressão cron ou rate para determinar a frequência de execução.
     - Por exemplo, a expressão `cron(0 9,12,18 * * ? *)` agenda a execução para às 9h, 12h e 18h todos os dias.

   - **Etapa 2: Selecionar Destinos**:
     - Escolha o destino da regra de evento, como AWS Lambda, Step Functions, SQS, SNS, entre outros.
     - Nesse caso, o destino será a função `SpecificTimeFunction`.

   - **Etapa 3: Configurações**:
     - Configure permissões para que o EventBridge possa invocar o destino selecionado.
     - Inclua outras configurações opcionais, como filtros de eventos e políticas de retry em caso de falhas.

   - **Etapa 4: Revisar e Criar uma Programação**:
     - Revise as configurações definidas e clique em “Create” para finalizar a criação da regra.

3. **Exemplos de Uso do EventBridge**:
   - **Automação de Processos**: Execução de scripts de manutenção de banco de dados, backup de arquivos, ou limpeza de logs em horários pré-definidos.
   - **Alertas e Monitoramento**: Envio de alertas baseados em agendamentos ou eventos ocorridos em outros serviços AWS.
   - **Integração de Sistemas**: Facilita a comunicação entre sistemas, disparando eventos com base em horários ou outras condições.

4. **Benefícios do EventBridge**:
   - **Escalabilidade e Confiabilidade**: Gerencia eventos de forma eficiente e escalável, garantindo que os agendamentos ocorram conforme especificado.
   - **Redução de Complexidade**: Elimina a necessidade de gerenciar servidores para agendamentos, simplificando a arquitetura.
   - **Facilidade de Integração**: Integra facilmente com múltiplos serviços AWS e aplicações personalizadas.

Amazon EventBridge é uma ferramenta poderosa para agendamento e gerenciamento de eventos em sistemas distribuídos, proporcionando uma solução flexível para automatizar processos com Lambda e outros serviços na AWS. A utilização de cronogramas e regras facilita o desenvolvimento de soluções eficientes e escaláveis, reduzindo a carga operacional e permitindo que você foque em lógica de negócios ao invés de infraestrutura.

Se precisar de mais detalhes sobre EventBridge ou outros aspectos dos exemplos, me avise!

### 3. **Criação de Funções Lambda para Realizar Requests HTTP**

   **Objetivo**: Demonstrar como usar AWS Lambda para fazer requests HTTP utilizando o módulo `urllib3`, nativo do Python.

   **Passo a Passo**:
   1. Crie uma nova função Lambda chamada `HTTPRequestFunction`.
   2. Utilize o módulo `urllib3` para realizar um GET request para uma API pública e processar a resposta.
   3. Teste a função e visualize os logs para garantir que o request foi realizado corretamente.

   ```python
import urllib3
import json
import os

# Carregar variáveis de ambiente
url = os.getenv('API_URL')  # URL da API obtida das variáveis de ambiente
api_key = os.getenv('CMC_API_KEY')  # Chave da API obtida das variáveis de ambiente

# Parâmetros da requisição para obter a cotação do Bitcoin
parameters = {
    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo
    'convert': 'USD'  # Convertendo a cotação para USD
}

# Headers com a chave da API
headers = {
    'Accept': 'application/json',
    'X-CMC_PRO_API_KEY': api_key  # Obtendo a chave da API das variáveis de ambiente
}

# Criar um PoolManager para gerenciar conexões
http = urllib3.PoolManager()

# Função Lambda
def lambda_handler(event, context):
    try:
        # Converte os parâmetros para o formato de query string
        query_string = '&'.join([f'{key}={value}' for key, value in parameters.items()])
        full_url = f"{url}?{query_string}"
        
        # Fazendo o request GET para a API
        response = http.request('GET', full_url, headers=headers)
        data = json.loads(response.data.decode('utf-8'))
        
        # Verificar se os dados do Bitcoin estão presentes na resposta
        if 'data' in data and 'BTC' in data['data']:
            bitcoin_data = data['data']['BTC']
            usd_quote = bitcoin_data['quote']['USD']
            
            # Log da resposta
            print(f"Cotação do Bitcoin obtida: {usd_quote}")
        else:
            print("Erro ao obter a cotação do Bitcoin:", data.get('status', {}).get('error_message', 'Erro desconhecido'))

    except urllib3.exceptions.HTTPError as e:
        print(f"Erro na requisição: {e}")

   ```

   ```mermaid
   graph LR
       Lambda3[AWS Lambda HTTPRequestFunction] -->|Faz Request HTTP| API[API Externa]
       API --> Process3[Processa Resposta]
   ```

Para criar um cron job que execute uma AWS Lambda a cada minuto, você pode usar o Amazon CloudWatch Events (agora parte do Amazon EventBridge) para configurar um agendamento. A seguir, explico como criar um evento cron que dispara sua Lambda a cada minuto:

### Passo a passo para configurar um cron job de 1 minuto para AWS Lambda:

1. **Acesse o Amazon EventBridge:**
   - Vá para o Console da AWS.
   - Pesquise por "EventBridge" (ou "CloudWatch Events" dependendo da interface).

2. **Crie uma nova regra:**
   - Clique em "Create rule".

3. **Configure a regra:**
   - **Name:** Dê um nome para a regra, como `lambda-every-minute`.
   - **Description:** Adicione uma descrição opcional para explicar o que a regra faz.
   - **Define pattern:** Escolha "Event Source" como "Event pattern" ou "Schedule".
   - Selecione **Schedule** e escolha **Cron expression**.

4. **Defina a expressão cron:**
   - Use a expressão cron para executar a cada minuto:
     ```
     cron(0/1 * * * ? *)
     ```
   - Explicação da expressão:
     - `0/1`: Começa no segundo zero e repete a cada 1 minuto.
     - `*`: Qualquer hora.
     - `*`: Qualquer dia do mês.
     - `*`: Qualquer mês.
     - `?`: Qualquer dia da semana.

5. **Defina o destino da regra:**
   - Escolha "Lambda function".
   - Selecione a função Lambda que deseja disparar a cada minuto.

6. **Configurações adicionais:**
   - Se precisar, adicione um "Input Transformer" para modificar os dados enviados para a Lambda, ou ajuste permissões conforme necessário.

7. **Permissões:**
   - O EventBridge precisará de permissões para invocar sua função Lambda. Se necessário, o AWS adicionará as permissões automaticamente quando você configurar o destino.

8. **Criar a regra:**
   - Clique em "Create" para finalizar a configuração.

### Nota Importante:

- **Limites de execução:** Executar uma Lambda a cada minuto pode impactar os limites de execução simultânea e a fatura da AWS. Monitore o uso para garantir que o cron não exceda os limites da sua conta.
- **Lambda Timeout:** Certifique-se de que sua Lambda está configurada com um timeout adequado para o processamento, especialmente se a execução em 1 minuto for crítica.

Se precisar de mais alguma coisa, me avise!

### 4. **Criação de Funções Lambda para Realizar Requests HTTP com `requests`**

   **Objetivo**: Mostrar como instalar o módulo `requests` e utilizá-lo para fazer um GET request para uma API.

   **Passo a Passo**:
   1. No AWS Lambda, crie uma função chamada `GetRequestFunction`.
   2. Adicione a biblioteca `requests` ao ambiente Lambda (você pode empacotar o `requests` com o código ou usar um Lambda Layer).
   3. Utilize `requests` para realizar um GET request a uma API pública.
   4. Entre no WSL
   5. cd ..
   6. mkdir python
   7. cd python
   8. pip3 install requests -t .
   9. cd ..
   10. zip -r python.zip python

```python
import requests
import json
import os

# Carregar variáveis de ambiente
url = os.getenv('API_URL')  # URL da API obtida das variáveis de ambiente
api_key = os.getenv('CMC_API_KEY')  # Chave da API obtida das variáveis de ambiente

# Parâmetros da requisição para obter a cotação do Bitcoin
parameters = {
    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo
    'convert': 'USD'  # Convertendo a cotação para USD
}

# Headers com a chave da API
headers = {
    'Accept': 'application/json',
    'X-CMC_PRO_API_KEY': api_key  # Obtendo a chave da API das variáveis de ambiente
}

# Função Lambda
def lambda_handler(event, context):
    try:
        # Criar uma sessão para gerenciar as requisições
        with requests.Session() as session:
            # Configurar os headers da sessão
            session.headers.update(headers)
            
            # Fazer o request GET para a API com parâmetros
            response = session.get(url, params=parameters)
            response.raise_for_status()  # Levanta um erro se o status code for 4xx ou 5xx
            
            data = response.json()  # Carregar a resposta JSON
            
            # Verificar se os dados do Bitcoin estão presentes na resposta
            if 'data' in data and 'BTC' in data['data']:
                bitcoin_data = data['data']['BTC']
                usd_quote = bitcoin_data['quote']['USD']
                
                # Log da resposta
                print(f"Cotação do Bitcoin obtida: {usd_quote}")
            else:
                print("Erro ao obter a cotação do Bitcoin:", data.get('status', {}).get('error_message', 'Erro desconhecido'))

    except requests.exceptions.RequestException as e:
        print(f"Erro na requisição: {e}")

```

```mermaid
graph LR
    Lambda4[AWS Lambda GetRequestFunction] -->|Faz Request GET| API[API Externa]
    API --> Process4[Processa Resposta]
```

### **Conclusão da Aula 11**

Nesta aula prática, exploramos cinco cenários diferentes de uso do AWS Lambda, destacando sua flexibilidade e integração com outros serviços da AWS. As demonstrações forneceram uma base sólida sobre como configurar funções Lambda para operar com eventos temporais, realizar requisições HTTP e conectar-se a bancos de dados RDS, reforçando o papel do Lambda como uma ferramenta essencial para arquiteturas modernas e escaláveis.

================================================
File: /Bootcamp - Cloud para dados/Aula_11/exemplo.py
================================================
def lambda_handler(event, context):
    print("Função executada a cada 10 minutos.")
    return {
        'statusCode': 200,
        'body': 'Execução bem-sucedida.'
    }



================================================
File: /Bootcamp - Cloud para dados/Aula_11/exemplo_02.py
================================================
import urllib3
import json
import os

# Carregar variáveis de ambiente
url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'
api_key = ''  # Chave da API obtida das variáveis de ambiente

# Parâmetros da requisição para obter a cotação do Bitcoin
parameters = {
 'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo
 'convert': 'USD'  # Convertendo a cotação para USD
}

# Headers com a chave da API
headers = {
 'Accept': 'application/json',
 'X-CMC_PRO_API_KEY': api_key  # Obtendo a chave da API das variáveis de ambiente
}

# Criar um PoolManager para gerenciar conexões
http = urllib3.PoolManager()

# Função Lambda
def lambda_handler(event, context):
 try:
     # Converte os parâmetros para o formato de query string
     query_string = '&'.join([f'{key}={value}' for key, value in parameters.items()])
     full_url = f"{url}?{query_string}"
     
     # Fazendo o request GET para a API
     response = http.request('GET', full_url, headers=headers)
     data = json.loads(response.data.decode('utf-8'))
     
     # Verificar se os dados do Bitcoin estão presentes na resposta
     if 'data' in data and 'BTC' in data['data']:
         bitcoin_data = data['data']['BTC']
         usd_quote = bitcoin_data['quote']['USD']
         
         # Log da resposta
         print(f"Cotação do Bitcoin obtida: {usd_quote}")
     else:
         print("Erro ao obter a cotação do Bitcoin:", data.get('status', {}).get('error_message', 'Erro desconhecido'))

 except urllib3.exceptions.HTTPError as e:
     print(f"Erro na requisição: {e}")


================================================
File: /Bootcamp - Cloud para dados/Aula_12/README.md
================================================
# Aula 12: Introdução ao AWS CLI

**Objetivo**: Nesta aula, vamos aprender sobre o AWS Command Line Interface (CLI), explorando como instalar a ferramenta, configurá-la e utilizá-la para gerenciar serviços da AWS diretamente do terminal. Vamos também discutir o AWS CloudShell, uma alternativa prática que permite usar o AWS CLI sem precisar instalar nada. O objetivo é que você saia desta aula sabendo como interagir com a AWS de forma eficiente e automatizada.

```mermaid
graph TD
    A(AWS) -->|Autenticação| B(IAM)
    B --> C[AWS Management Console]
    B --> D[AWS CLI]
    B --> E[AWS CloudShell]
    B --> F[Terraform]
    B --> G[Boto3]
    C --> H(AWS APIs)
    D --> H(AWS APIs)
    E --> H(AWS APIs)
    F --> H(AWS APIs)
    G --> H(AWS APIs)
    H --> A
```

## **Tópicos da Aula 12**

### 1. **Introdução ao AWS CLI e AWS CloudShell**

   **Objetivo**: Entender o que é o AWS CLI e conhecer o AWS CloudShell como uma alternativa para começar a utilizar o CLI sem instalação local.

   **O que é AWS CLI**:
   - O AWS Command Line Interface (CLI) é uma ferramenta unificada para gerenciar serviços da AWS diretamente do terminal. Ele permite a criação, gerenciamento e automação de recursos na AWS de forma programática.
   - Com o AWS CLI, é possível interagir com todos os serviços da AWS, como S3, EC2, RDS, Lambda, entre outros, por meio de comandos simples.

   **Vantagens do AWS CLI**:
   - **Automação**: Facilita a automação de tarefas como criação de recursos, backups e monitoramento.
   - **Integração**: Trabalha bem com ferramentas de DevOps, pipelines de CI/CD, e scripts shell.
   - **Eficiência**: Reduz a necessidade de navegar pelo console da AWS para tarefas repetitivas.

   **O que é AWS CloudShell**:
   - O AWS CloudShell é uma ferramenta que permite usar o AWS CLI diretamente no navegador, sem precisar instalar nada no seu computador.
   - Ele oferece um ambiente shell baseado em navegador com o AWS CLI pré-instalado e configurado com suas credenciais de usuário.
   - Isso é particularmente útil quando você precisa executar comandos rápidos ou testar scripts em diferentes máquinas sem se preocupar com configurações locais.

   **Como usar o AWS CloudShell**:
   1. Acesse o **AWS Management Console** e procure por "CloudShell".
   2. Clique em **CloudShell** no menu superior. Isso abrirá um terminal diretamente no navegador, já autenticado com suas credenciais AWS.
   3. Use o AWS CLI no CloudShell como se estivesse no seu terminal local:
   ```bash
   aws s3 ls
   ```

   **Vantagens do CloudShell**:
   - Acesso rápido ao AWS CLI sem instalação.
   - Ambiente seguro e pré-configurado.
   - Capacidade de armazenar até 1 GB de dados no ambiente CloudShell.
   - Ideal para quem está começando ou para quem trabalha com múltiplos dispositivos.

### 2. **Instalação do AWS CLI no Windows**

   **Objetivo**: Demonstrar o passo a passo para instalar o AWS CLI em máquinas com o sistema operacional Windows.

   **Passo a Passo**:
   1. **Baixar o AWS CLI**:
      - Acesse a [página oficial de download do AWS CLI](https://aws.amazon.com/cli/) e selecione a versão para Windows.
   
   2. **Instalar o AWS CLI**:
      - Após o download, execute o arquivo `.msi` e siga as instruções do instalador.
   
   3. **Verificar a instalação**:
      - Abra o Prompt de Comando (cmd) ou PowerShell e digite o seguinte comando para verificar se a instalação foi concluída com sucesso:
      ```bash
      aws --version
      ```
      - A saída deve ser algo como `aws-cli/2.x.x Python/3.x.x`.

   4. **Configuração do AWS CLI**:
      - Para configurar o AWS CLI com suas credenciais, você pode usar o comando `aws configure`. Isso cria um perfil padrão que será usado para autenticação:
      ```bash
      aws configure
      ```
      - Insira as seguintes informações:
        - **AWS Access Key ID**: Sua chave de acesso.
        - **AWS Secret Access Key**: Sua chave secreta.
        - **Default region name**: A região padrão, como `us-east-1`.
        - **Default output format**: O formato de saída, como `json`.

### 3. **Configuração de Perfis com AWS CLI**

   **Objetivo**: Aprender a configurar múltiplos perfis para gerenciar diferentes contas ou ambientes de trabalho na AWS.

   **O que são perfis no AWS CLI?**
   - Perfis permitem que você configure diferentes credenciais e configurações regionais para várias contas ou projetos AWS, sem sobrescrever suas credenciais padrão.
   - Você pode alternar facilmente entre perfis para acessar diferentes ambientes (produção, desenvolvimento, etc.).

   **Como configurar perfis no AWS CLI**:
   1. Use o comando `aws configure --profile` para criar um novo perfil:
   ```bash
   aws configure --profile nome-do-perfil
   ```
   2. Você será solicitado a inserir suas credenciais da mesma forma que no comando `aws configure`:
      - **AWS Access Key ID**: Insira a chave de acesso para esse perfil específico.
      - **AWS Secret Access Key**: Insira a chave secreta associada.
      - **Default region name**: A região para este perfil.
      - **Default output format**: O formato de saída desejado.

   **Alternar entre perfis**:
   - Para usar um perfil específico em um comando, você pode adicionar o parâmetro `--profile`:
   ```bash
   aws s3 ls --profile nome-do-perfil
   ```
   - Para definir um perfil como o padrão para a sessão, defina a variável de ambiente `AWS_PROFILE`:
   ```bash
   export AWS_PROFILE=nome-do-perfil
   ```

   **Exemplo de uso de múltiplos perfis**:
   - Pode ser útil se você estiver trabalhando com diferentes ambientes, como `desenvolvimento`, `staging` e `produção`. Cada perfil pode ter suas próprias credenciais e configuração regional:
   ```bash
   aws ec2 describe-instances --profile producao
   aws ec2 describe-instances --profile desenvolvimento
   ```

### 3.1 Exercício

Aqui está o passo a passo detalhado para criar dois perfis no AWS CLI e alternar entre eles:

### 1. **Configuração de Perfis com AWS CLI**

   **Objetivo**: Criar dois perfis de CLI para diferentes contas ou ambientes (por exemplo, desenvolvimento e produção) e alternar entre eles.

#### **Passo 1: Criar o primeiro perfil (Desenvolvimento)**

1. Abra o terminal ou o Prompt de Comando (cmd) no Windows.
2. Execute o comando `aws configure --profile` para configurar o primeiro perfil, que será para **desenvolvimento**:
   ```bash
   aws configure --profile desenvolvimento
   ```
3. O AWS CLI solicitará as seguintes informações:
   - **AWS Access Key ID**: Insira a chave de acesso da conta de desenvolvimento.
   - **AWS Secret Access Key**: Insira a chave secreta correspondente.
   - **Default region name**: Especifique a região, como `us-east-1`.
   - **Default output format**: Escolha o formato de saída desejado, como `json` ou `text`.

#### **Passo 2: Criar o segundo perfil (Produção)**

1. Novamente, no terminal ou Prompt de Comando, execute o comando para configurar o segundo perfil, que será para **produção**:
   ```bash
   aws configure --profile producao
   ```
2. O AWS CLI solicitará as mesmas informações:
   - **AWS Access Key ID**: Insira a chave de acesso da conta de produção.
   - **AWS Secret Access Key**: Insira a chave secreta correspondente.
   - **Default region name**: Especifique a região, como `us-west-2`.
   - **Default output format**: Escolha o formato de saída, como `json`.

### 2. **Verificar a Configuração dos Perfis**

Para verificar se os perfis foram configurados corretamente, você pode listar os perfis que estão salvos no arquivo de configuração:

```bash
cat ~/.aws/credentials
```

Ou no Windows:
```bash
type %USERPROFILE%\.aws\credentials
```

O arquivo `credentials` deve mostrar algo assim:

```plaintext
[desenvolvimento]
aws_access_key_id = YOUR_ACCESS_KEY_ID_DEV
aws_secret_access_key = YOUR_SECRET_ACCESS_KEY_DEV

[producao]
aws_access_key_id = YOUR_ACCESS_KEY_ID_PROD
aws_secret_access_key = YOUR_SECRET_ACCESS_KEY_PROD
```

### 3. **Como Alternar Entre Perfis**

#### **Método 1: Usar o parâmetro `--profile` em cada comando**

Sempre que quiser executar um comando usando um perfil específico, adicione o parâmetro `--profile`:

- Para listar buckets S3 no ambiente de **desenvolvimento**:
  ```bash
  aws s3 ls --profile desenvolvimento
  ```

- Para listar buckets S3 no ambiente de **produção**:
  ```bash
  aws s3 ls --profile producao
  ```

#### **Método 2: Definir o perfil padrão para a sessão usando a variável de ambiente `AWS_PROFILE`**

Você pode configurar o perfil padrão para a sessão atual definindo a variável de ambiente `AWS_PROFILE`.

- Para definir o perfil **desenvolvimento** como padrão:
  - No Linux/macOS:
    ```bash
    export AWS_PROFILE=desenvolvimento
    ```
  - No Windows (PowerShell):
    ```bash
    $env:AWS_PROFILE="desenvolvimento"
    ```
  - No Windows (CMD):
    ```bash
    set AWS_PROFILE=desenvolvimento
    ```

- Para definir o perfil **produção** como padrão:
  - No Linux/macOS:
    ```bash
    export AWS_PROFILE=producao
    ```
  - No Windows (PowerShell):
    ```bash
    $env:AWS_PROFILE="producao"
    ```
  - No Windows (CMD):
    ```bash
    set AWS_PROFILE=producao
    ```

Com o perfil definido como variável de ambiente, todos os comandos subsequentes usarão esse perfil por padrão. Por exemplo:

```bash
aws s3 ls
```

### 4. **Alternando Perfis em Scripts**

Se estiver escrevendo scripts, você pode incluir o parâmetro `--profile` diretamente nos comandos, ou definir o perfil desejado no início do script:

```bash
#!/bin/bash

# Definindo o perfil de produção
export AWS_PROFILE=producao

# Listar instâncias EC2 no ambiente de produção
aws ec2 describe-instances
```

### 5. **Conclusão**

Agora você sabe como configurar múltiplos perfis no AWS CLI e alternar entre eles. Isso facilita muito o gerenciamento de diferentes ambientes ou contas AWS, permitindo que você mantenha as credenciais organizadas e use o perfil correto para cada tarefa.

### 4. **Comandos Básicos no AWS CLI**

   **Objetivo**: Introduzir os principais comandos utilizados no AWS CLI e explicar sua sintaxe básica.

   **Comandos Comuns**:
   - **Listar buckets no S3**:
     ```bash
     aws s3 ls
     ```
   - **Criar um bucket S3**:
     ```bash
     aws s3 mb s3://nome-do-seu-bucket
     ```
   - **Descrever instâncias EC2**:
     ```bash
     aws ec2 describe-instances
     ```

   **Estrutura dos comandos do AWS CLI**:
   ```bash
   aws [serviço] [operação] [opções]
   ```

Aqui está o passo a passo para criar um arquivo JSON usando o comando `touch`, e em seguida, exportá-lo para o bucket S3 que você criou:

### Passo a Passo

#### 1. **Criar o bucket S3**
Se você ainda não criou o bucket no S3, utilize o comando abaixo para criá-lo:

```bash
aws s3 mb s3://nome-do-seu-bucket
```

#### 2. **Criar o arquivo JSON**
Agora vamos criar um arquivo JSON localmente.

1. No terminal, use o comando `touch` para criar um arquivo vazio chamado `dados.json`:
   ```bash
   touch dados.json
   ```

2. Em seguida, abra o arquivo com um editor de texto (como `nano`, `vim`, ou outro de sua escolha) e adicione alguns dados JSON de exemplo:

   ```bash
   nano dados.json
   ```

   Adicione o seguinte conteúdo:

   ```json
   {
     "nome": "Luciano",
     "idade": 34,
     "profissao": "Engenheiro de Dados"
   }
   ```

   Salve o arquivo e saia do editor.

#### 3. **Enviar o arquivo JSON para o bucket S3**

Agora, use o comando `aws s3 cp` para copiar o arquivo JSON que você criou para o bucket S3:

```bash
aws s3 cp dados.json s3://nome-do-seu-bucket/dados.json
```

Esse comando copiará o arquivo `dados.json` para o bucket `nome-do-seu-bucket`.

#### 4. **Verificar o upload no S3**
Você pode listar o conteúdo do bucket para verificar se o arquivo foi enviado corretamente:

```bash
aws s3 ls s3://nome-do-seu-bucket/
```

Se tudo estiver certo, você verá o arquivo `dados.json` listado no bucket.

#### 3.2 Principais comandos do S3 no AWS CLI

Aqui está uma lista completa dos comandos do AWS CLI para interagir com o **Amazon S3**:

### Comandos Principais do S3 no AWS CLI

1. **`mb`** (Make Bucket): Cria um novo bucket no Amazon S3.
   ```bash
   aws s3 mb s3://nome-do-seu-bucket
   ```

2. **`rb`** (Remove Bucket): Remove (deleta) um bucket vazio no S3.
   ```bash
   aws s3 rb s3://nome-do-seu-bucket
   ```

3. **`cp`** (Copy): Copia arquivos ou objetos entre o sistema de arquivos local e o S3, ou entre dois buckets S3.
   ```bash
   aws s3 cp origem destino
   ```

4. **`mv`** (Move): Move arquivos ou objetos entre o sistema de arquivos local e o S3, ou entre dois buckets S3.
   ```bash
   aws s3 mv origem destino
   ```

5. **`ls`** (List): Lista todos os buckets ou lista os arquivos em um bucket S3.
   - Para listar todos os buckets:
     ```bash
     aws s3 ls
     ```
   - Para listar os arquivos em um bucket:
     ```bash
     aws s3 ls s3://nome-do-seu-bucket/
     ```

6. **`rm`** (Remove): Remove arquivos ou objetos do bucket no S3.
   ```bash
   aws s3 rm s3://nome-do-seu-bucket/arquivo.txt
   ```

7. **`sync`** (Synchronize): Sincroniza o conteúdo entre um diretório local e um bucket S3, ou entre dois buckets S3.
   - Sincronizar um diretório local com um bucket S3:
     ```bash
     aws s3 sync ./diretorio-local s3://nome-do-seu-bucket
     ```
   - Sincronizar um bucket S3 com outro:
     ```bash
     aws s3 sync s3://bucket-origem s3://bucket-destino
     ```

8. **`website`**: Configura um bucket S3 para hospedar um site estático.
   - Para definir um bucket como um site:
     ```bash
     aws s3 website s3://nome-do-seu-bucket/ --index-document index.html --error-document error.html
     ```

9. **`presign`**: Gera uma URL pré-assinada para um objeto no S3, permitindo que o objeto seja acessado temporariamente.
   ```bash
   aws s3 presign s3://nome-do-seu-bucket/arquivo.txt
   ```

### Comandos Avançados

1. **`control-list` (ACL)**: Define ou recupera permissões de acesso ao bucket.
   - Para obter a lista de permissões (ACL) de um bucket:
     ```bash
     aws s3api get-bucket-acl --bucket nome-do-seu-bucket
     ```

2. **`head-object`**: Recupera metadados de um objeto sem precisar baixá-lo.
   ```bash
   aws s3api head-object --bucket nome-do-seu-bucket --key caminho-do-arquivo
   ```

3. **`put-object`**: Carrega um arquivo para um bucket S3 (similar ao `cp`).
   ```bash
   aws s3api put-object --bucket nome-do-seu-bucket --key caminho-do-arquivo --body arquivo.txt
   ```

4. **`get-object`**: Baixa um arquivo de um bucket S3 (similar ao `cp`).
   ```bash
   aws s3api get-object --bucket nome-do-seu-bucket --key caminho-do-arquivo arquivo.txt
   ```

5. **`list-objects`**: Lista todos os objetos dentro de um bucket S3 (similar ao `ls`, mas com mais detalhes).
   ```bash
   aws s3api list-objects --bucket nome-do-seu-bucket
   ```

### Outros Comandos Úteis

1. **`put-bucket-encryption`**: Ativa a criptografia no lado do servidor (Server-Side Encryption) para um bucket.
   ```bash
   aws s3api put-bucket-encryption --bucket nome-do-seu-bucket --server-side-encryption-configuration file://encryption.json
   ```

2. **`put-bucket-policy`**: Define uma política de acesso para o bucket.
   ```bash
   aws s3api put-bucket-policy --bucket nome-do-seu-bucket --policy file://policy.json
   ```

3. **`delete-bucket-policy`**: Remove uma política de acesso de um bucket.
   ```bash
   aws s3api delete-bucket-policy --bucket nome-do-seu-bucket
   ```

4. **`put-bucket-lifecycle-configuration`**: Define as regras de ciclo de vida para o bucket (ex.: mover para o Glacier, expirar objetos antigos).
   ```bash
   aws s3api put-bucket-lifecycle-configuration --bucket nome-do-seu-bucket --lifecycle-configuration file://lifecycle.json
   ```

---

### Resumo dos Comandos:

- **`mb`**: Criar bucket.
- **`rb`**: Remover bucket.
- **`cp`**: Copiar arquivos.
- **`mv`**: Mover arquivos.
- **`ls`**: Listar buckets ou arquivos.
- **`rm`**: Remover arquivos.
- **`sync`**: Sincronizar diretórios/buckets.
- **`website`**: Configurar bucket para hospedagem de sites.
- **`presign`**: Gerar URL pré-assinada.
- **`put-object`** e **`get-object`**: Enviar/baixar arquivos para/do bucket.

Esses são os comandos principais e avançados do AWS CLI para o serviço Amazon S3, abrangendo desde a criação e remoção de buckets até a configuração de permissões e políticas de acesso.

Aqui está um fluxo completo que inclui os comandos para **criar**, **verificar** e **deletar** uma instância RDS no AWS, com detalhes e formatação amigável.

### 1. **Criar a Instância RDS**

Use o seguinte comando para criar uma instância MySQL com a classe `db.t4g.micro` e a versão `8.0.35`:

```bash
aws rds create-db-instance \
--db-instance-identifier mydbinstance \
--db-instance-class db.t4g.micro \
--engine mysql \
--engine-version 8.0.35 \
--master-username admin \
--master-user-password password123 \
--allocated-storage 20
```

### 2. **Verificar o Status da Instância RDS**

Após iniciar o processo de criação, você pode verificar o status da instância com o comando abaixo:

```bash
aws rds describe-db-instances --db-instance-identifier mydbinstance
```

Para uma saída mais organizada e fácil de entender, você pode formatar a resposta em forma de tabela, exibindo o **identificador**, o **status** e o **endpoint** da instância:

```bash
aws rds describe-db-instances \
--db-instance-identifier mydbinstance \
--query 'DBInstances[*].[DBInstanceIdentifier,DBInstanceStatus,Endpoint.Address]' \
--output table
```

A saída será semelhante a esta quando a instância estiver disponível:

```plaintext
-------------------------------
| DBInstanceIdentifier | DBInstanceStatus | Endpoint.Address      |
-------------------------------
| mydbinstance          | available        | mydbinstance.xxxxxx.us-east-1.rds.amazonaws.com |
-------------------------------
```

### 3. **Script para Monitorar o Status da Instância RDS**

Você pode monitorar continuamente o status da instância com este script, que verifica o status a cada 30 segundos e exibe uma mensagem quando a instância estiver disponível:

```bash
while true; do
    STATUS=$(aws rds describe-db-instances --db-instance-identifier mydbinstance --query 'DBInstances[0].DBInstanceStatus' --output text)
    echo "Status: $STATUS"
    if [ "$STATUS" == "available" ]; then
        echo "A instância está disponível!"
        break
    fi
    sleep 30
done
```

### 4. **Deletar a Instância RDS**

Depois que você terminar de usar a instância, é importante deletá-la para evitar custos adicionais. O comando abaixo exclui a instância RDS:

```bash
aws rds delete-db-instance \
--db-instance-identifier mydbinstance \
--skip-final-snapshot
```

- **`--skip-final-snapshot`**: O parâmetro `--skip-final-snapshot` instrui o AWS a **não criar** um snapshot final antes de excluir a instância. Se você quiser criar um snapshot para backup antes de deletar, remova essa opção e adicione `--final-db-snapshot-identifier` seguido do nome do snapshot.

### 5. **Verificar o Status de Deleção**

Você pode verificar se a instância foi marcada para deleção ou já foi excluída usando o seguinte comando:

```bash
aws rds describe-db-instances --db-instance-identifier mydbinstance
```

Se a instância estiver em processo de deleção, o status será algo como `deleting`. Uma vez excluída, o comando retornará um erro informando que a instância não existe mais.

### Resumo Completo dos Comandos:

1. **Criar a Instância**:
   ```bash
   aws rds create-db-instance \
   --db-instance-identifier mydbinstance \
   --db-instance-class db.t4g.micro \
   --engine mysql \
   --engine-version 8.0.35 \
   --master-username admin \
   --master-user-password password123 \
   --allocated-storage 20
   ```

2. **Verificar o Status**:
   ```bash
   aws rds describe-db-instances \
   --db-instance-identifier mydbinstance \
   --query 'DBInstances[*].[DBInstanceIdentifier,DBInstanceStatus,Endpoint.Address]' \
   --output table
   ```

3. **Deletar a Instância**:
   ```bash
   aws rds delete-db-instance \
   --db-instance-identifier mydbinstance \
   --skip-final-snapshot
   ```

Esse fluxo completo permite que você crie, verifique o status e depois delete a instância RDS conforme necessário.

### 6. **Automatizando Tarefas com AWS CLI**

   **Objetivo**: Demonstrar como usar scripts para automatizar processos na AWS com o AWS CLI.

   **Exemplo**: Crie um arquivo Bash (.sh) contendo o script que você quer executar. Suponha que o arquivo se chame backup_rds.sh.

   ```bash
   #!/bin/bash
DB_INSTANCE_IDENTIFIER=mydbinstance
BACKUP_NAME=db-backup-$(date +%F)

if aws rds create-db-snapshot --db-instance-identifier $DB_INSTANCE_IDENTIFIER --db-snapshot-identifier $BACKUP_NAME --profile producao; then
    echo "Backup criado com sucesso: $BACKUP_NAME"
else
    echo "Erro ao criar o backup"
fi
   ```


### 7. **Conclusão da Aula 12**

   **Resumo**:
   - Exploramos o AWS CLI e o AWS CloudShell como duas formas de interagir com a AWS sem depender do console.
   - Aprendemos a instalar e configurar o AWS CLI no Windows, e vimos como utilizar perfis para gerenciar múltiplas contas.
   - Demonstramos como criar um banco de dados RDS e automatizar tarefas com scripts.

   **Tarefa de Casa**:
   - Experimente configurar múltiplos perfis no AWS CLI e realizar operações em diferentes contas ou ambientes.
   - Crie um script para automatizar a criação e exclusão de buckets S3.

---

**Material de Apoio**:
- [Documentação oficial do AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html)

================================================
File: /Bootcamp - Cloud para dados/Aula_13/README.md
================================================
# Aula 13: Introdução ao Amazon SQS para Engenharia de Dados

**Objetivo**: Nesta aula, vamos explorar o Amazon Simple Queue Service (SQS), sua aplicação na engenharia de dados e como integrá-lo com outros serviços AWS para garantir escalabilidade e desacoplamento de processos. Ao final da aula, você será capaz de criar filas SQS, enviar e receber mensagens de forma programática, e entender como utilizar SQS em pipelines de dados.

![imagem](./pics/foto1.png)

Mais informções no site

## **Tópicos da Aula 13**

### 1. **O que é o Amazon SQS?**

   **Objetivo**: Compreender o funcionamento e os conceitos básicos do Amazon SQS.

   **O que é Amazon SQS?**
   - O Amazon Simple Queue Service (SQS) é um serviço de fila de mensagens completamente gerenciado pela AWS, projetado para ajudar no desacoplamento de componentes em aplicações distribuídas e pipelines de dados.
   - Com o SQS, você pode enviar, armazenar e receber mensagens entre diferentes partes de uma aplicação de maneira confiável e assíncrona.

   **Vantagens do Amazon SQS**:
   - **Escalabilidade**: Suporta grandes volumes de mensagens sem se preocupar com a infraestrutura subjacente.
   - **Desacoplamento**: Permite a separação dos componentes de uma aplicação, o que melhora a modularidade e manutenção.
   - **Durabilidade**: As mensagens são armazenadas de forma durável até que sejam processadas, com backups automáticos.

### 2. **Tipos de Filas no SQS**

   **Objetivo**: Entender os dois principais tipos de filas disponíveis no SQS: Standard e FIFO.

   - **Standard Queue**: 
     - Oferece entrega em "melhor esforço", garantindo alta taxa de transferência, mas sem garantia de ordem exata na entrega das mensagens.
     - **Caso de uso**: Processos que podem tolerar duplicação ocasional de mensagens e não precisam que as mensagens sejam processadas exatamente na ordem de envio.

   - **FIFO Queue**:
     - Oferece entrega exatamente uma vez e preserva a ordem das mensagens. Ideal para aplicações em que a ordem exata é crítica.
     - **Caso de uso**: Processos onde a ordem e entrega única das mensagens são fundamentais, como processamento de transações.

### 3. **Criando uma Fila SQS**

   **Objetivo**: Demonstrar o processo de criação de uma fila SQS no AWS Management Console e via AWS CLI.

   **No Console AWS**:
   1. Acesse o **AWS Management Console** e procure por **SQS**.
   2. Clique em **Criar Fila**.
   3. Escolha o tipo de fila (Standard ou FIFO) e defina as configurações, como tempo de retenção de mensagens e permissões de acesso.

Aqui está uma descrição detalhada das opções disponíveis ao criar uma fila Amazon SQS no AWS Management Console:

#### **1. Tipo de Fila (Type)**

   - **Standard**:
     - **Descrição**: Garante a entrega de cada mensagem ao menos uma vez, mas a ordem exata de entrega não é garantida.
     - **Recursos**:
       - **Entrega pelo menos uma vez**: A mensagem pode ser entregue mais de uma vez.
       - **Melhor esforço na ordenação**: A ordem de entrega das mensagens pode não ser preservada.
     - **Caso de Uso**: Ideal para sistemas onde a ordem exata de entrega não é crítica, como processamento em massa e comunicação entre microserviços.

   - **FIFO (First-in, First-out)**:
     - **Descrição**: Garante a entrega das mensagens na ordem em que foram enviadas e exatamente uma vez.
     - **Recursos**:
       - **Entrega exatamente uma vez**: As mensagens não serão duplicadas.
       - **Ordenação garantida**: As mensagens são processadas na ordem de envio.
     - **Caso de Uso**: Adequado para sistemas onde a ordem das mensagens é crucial, como processamento de transações financeiras.

#### **2. Nome da Fila (Name)**

   - O nome da fila é sensível a maiúsculas e minúsculas e pode ter até 80 caracteres.
   - Permitidos: Caracteres alfanuméricos, hífens (`-`), e underscores (`_`).
   - Fila FIFO deve terminar com `.fifo` no nome.

#### **3. Configurações da Fila (Configuration)**

   - **Visibility Timeout** (Timeout de Visibilidade):
     - **Descrição**: O tempo em segundos durante o qual uma mensagem ficará invisível para outros consumidores após ser lida.
     - **Padrão**: 30 segundos.
     - **Intervalo**: Entre 0 segundos e 12 horas.
     - **Caso de Uso**: Configurar para evitar que mensagens em processamento sejam visíveis para outros consumidores antes que o processo seja concluído.

   - **Message Retention Period** (Período de Retenção de Mensagens):
     - **Descrição**: O tempo durante o qual uma mensagem será armazenada na fila antes de ser excluída.
     - **Padrão**: 4 dias.
     - **Intervalo**: Entre 1 minuto e 14 dias.
     - **Caso de Uso**: Útil para gerenciar o tempo de vida das mensagens e evitar sobrecarga de armazenamento.

   - **Delivery Delay** (Atraso de Entrega):
     - **Descrição**: Define um atraso em segundos antes que uma mensagem seja entregue aos consumidores.
     - **Padrão**: 0 segundos.
     - **Intervalo**: Entre 0 segundos e 15 minutos.
     - **Caso de Uso**: Usado para controlar o tempo em que as mensagens ficam disponíveis para consumo.

   - **Maximum Message Size** (Tamanho Máximo da Mensagem):
     - **Descrição**: O tamanho máximo permitido para uma mensagem.
     - **Padrão**: 256 KB.
     - **Intervalo**: Entre 1 KB e 256 KB.
     - **Caso de Uso**: Definir o limite de tamanho para mensagens grandes ou pequenas, conforme a necessidade do sistema.

   - **Receive Message Wait Time** (Tempo de Espera para Receber Mensagens):
     - **Descrição**: O tempo máximo que o Amazon SQS aguardará por novas mensagens antes de retornar uma resposta vazia a um consumidor.
     - **Padrão**: 0 segundos.
     - **Intervalo**: Entre 0 e 20 segundos.
     - **Caso de Uso**: Usado para implementar o **long polling**, que reduz o custo de requisições e melhora a eficiência da fila.

#### **4. Criptografia (Encryption)**

   - **Criptografia Server-side (SSE)**:
     - **Opções**:
       - **Desativada**: A fila não usará criptografia em repouso.
       - **Ativada**: A fila usará criptografia para mensagens armazenadas.
     - **Tipos de Chave**:
       - **SSE-SQS (Chave SQS)**: Uma chave de criptografia gerenciada pela Amazon SQS.
       - **SSE-KMS (Chave AWS KMS)**: Uma chave gerenciada pelo AWS Key Management Service (KMS), que oferece mais controle sobre o gerenciamento da chave.
     - **Caso de Uso**: Para aumentar a segurança de dados em repouso, como em filas que processam informações sensíveis.

#### **5. Políticas de Acesso (Access Policy)**

   - **Método de Configuração**:
     - **Básico**: Use critérios simples para definir uma política de acesso.
     - **Avançado**: Use um objeto JSON para definir uma política de acesso mais detalhada.

   - **Quem pode enviar mensagens para a fila**:
     - **Apenas o proprietário**: Apenas o proprietário da fila pode enviar mensagens.
     - **AWS Accounts, IAM users and roles especificados**: Especifique IDs de contas da AWS, usuários e roles do IAM que podem enviar mensagens.

   - **Quem pode receber mensagens da fila**:
     - **Apenas o proprietário**: Apenas o proprietário da fila pode receber mensagens.
     - **AWS Accounts, IAM users and roles especificados**: Especifique IDs de contas da AWS, usuários e roles do IAM que podem receber mensagens.

   - **JSON (Política de Acesso)**:
     - É possível definir diretamente uma política em formato JSON, fornecendo um controle mais granular sobre quem pode realizar ações específicas na fila.

#### **6. Política de Redrive Allow (Redrive Allow Policy)**

   - **Descrição**: Determina quais filas de origem podem usar essa fila como fila de "dead-letter" (mensagens não entregues).
   - **Opções**:
     - **Desabilitada**: A fila não será usada como uma fila "dead-letter".
     - **Habilitada**: Permite que essa fila seja usada como fila "dead-letter" por outras filas de origem.

#### **7. Fila Dead-letter (Dead-letter Queue)**

   - **Descrição**: Configura essa fila para receber mensagens não entregáveis de outras filas. Se uma mensagem falhar repetidamente, ela é redirecionada para uma fila "dead-letter" para análise posterior.
   - **Opções**:
     - **Desabilitada**: A fila não atuará como uma fila "dead-letter".
     - **Habilitada**: A fila pode ser usada para receber mensagens de filas que falharam em processar mensagens após várias tentativas.

#### **8. Tags (Etiquetas - Tags)**

   - **Descrição**: Etiquetas são rótulos associados a um recurso AWS para ajudar a organizar, filtrar e rastrear os custos de recursos.
   - **Configuração**:
     - Adicionar até 50 tags no formato chave-valor.
     - **Exemplo**:
       - **Key**: `Projeto`
       - **Value**: `ProcessamentoDeDados`

### 4. **Envio e Recebimento de Mensagens com o CLI**

Verifique suas listas ativas

```bash
aws sqs list-queues
```

Para evitar a necessidade de adicionar o `--profile jornadadedados` em cada comando, você pode definir o perfil padrão da sessão atual utilizando o comando `export`. Esse comando irá configurar o perfil `AWS_PROFILE`, e todos os comandos subsequentes utilizarão automaticamente esse perfil.

### Configurando o perfil padrão com `export`:

```bash
export AWS_PROFILE=jornadadedados
```

Agora, o `AWS CLI` usará o perfil `jornadadedados` por padrão para todos os comandos, sem precisar especificá-lo novamente. Após configurar o perfil com o comando `export`, você pode executar seus comandos como normalmente, sem o parâmetro `--profile`.

Vamos explicar em detalhes o passo a passo dos comandos utilizados e a saída recebida, além de configurar o ambiente para não precisar especificar o perfil a cada comando.

#### Como verificar se o perfil foi configurado corretamente:

Você pode testar a configuração do perfil padrão verificando se o AWS CLI usa o perfil configurado ao executar qualquer comando:

```bash
aws sqs list-queues
```

Se o comando listar as filas sem exigir a região ou o perfil, significa que a configuração foi aplicada corretamente.

### Criando uma Fila SQS

**Objetivo**: Demonstrar como criar uma fila SQS no AWS Management Console e via AWS CLI.

#### No Console AWS:

1. Acesse o **AWS Management Console** e, na barra de busca, procure por **SQS**.
2. Clique em **Criar Fila**.
3. Escolha o tipo de fila que melhor atende às suas necessidades:
   - **Standard**: Oferece alta taxa de transferência e não garante a ordem exata de entrega das mensagens.
   - **FIFO**: Garante a entrega em ordem e a entrega única de cada mensagem.
4. Defina as configurações da fila, como:
   - Tempo de retenção de mensagens
   - Visibilidade das mensagens para consumidores
   - Tamanho máximo das mensagens
   - Permissões de acesso (quem pode enviar e receber mensagens da fila)
5. Clique em **Criar Fila** para finalizar a criação.

#### Usando AWS CLI:

- Para criar uma fila **Standard**, execute o seguinte comando no terminal:
   ```bash
   aws sqs create-queue --queue-name minha-fila-standard
   ```

- Para criar uma fila **FIFO**, adicione o atributo `FifoQueue=true` ao comando:
   ```bash
   aws sqs create-queue --queue-name minha-fila.fifo --attributes FifoQueue=true
   ```

---

### Envio e Recebimento de Mensagens

**Objetivo**: Demonstrar como enviar, receber e remover mensagens de uma fila SQS de forma programática usando AWS CLI.

#### Enviando Mensagens:

Para enviar uma mensagem para a fila, utilize o comando abaixo, especificando a URL da fila e o corpo da mensagem:

```bash
aws sqs send-message --queue-url URL_DA_FILA --message-body "Mensagem para a fila"
```

#### Recebendo Mensagens:

Para receber mensagens de uma fila SQS, use o comando:

```bash
aws sqs receive-message --queue-url URL_DA_FILA
```

Esse comando retornará as mensagens que estão na fila, junto com informações como o `ReceiptHandle`, necessário para deletar a mensagem após o processamento.

#### Explicação da Saída:

- **MessageId**: O identificador único da mensagem que foi recebida (mesmo ID da mensagem enviada anteriormente).
- **ReceiptHandle**: Um identificador temporário que será usado para remover a mensagem da fila após o processamento. Esse handle deve ser utilizado na operação de exclusão de mensagem.
- **MD5OfBody**: Um hash MD5 da mensagem recebida para garantir a integridade dos dados.
- **Body**: O conteúdo da mensagem, que é "Mensagem para a fila".

#### Removendo Mensagens:

Após processar a mensagem, ela deve ser removida da fila. Para isso, use o `ReceiptHandle` da mensagem recebida:

```bash
aws sqs delete-message --queue-url URL_DA_FILA --receipt-handle HANDLE_DA_MENSAGEM
```

Esses passos cobrem o ciclo completo de envio, recebimento e remoção de mensagens em uma fila SQS usando o AWS CLI.

### 5. **Integrando o SQS com código Python**

### 6. **Controle de Concorrência e Limite de Taxa**

   **Objetivo**: Explicar como gerenciar a concorrência no processamento de mensagens e configurar limites para evitar sobrecarga.

   - **Controle de Concorrência**: Defina o número máximo de mensagens que um worker pode processar simultaneamente.
   - **Visibilidade de Mensagens**: Use o parâmetro `Visibility Timeout` para garantir que uma mensagem seja invisível para outros consumidores enquanto está sendo processada.

   - **Exemplo**:
     ```bash
     aws sqs change-message-visibility --queue-url URL_DA_FILA --receipt-handle HANDLE_DA_MENSAGEM --visibility-timeout 30
     ```

### 7. **Boas Práticas com SQS na Engenharia de Dados**

   **Objetivo**: Discutir as melhores práticas ao usar o SQS em projetos de engenharia de dados.

   - **Monitoramento**: Utilize o Amazon CloudWatch para monitorar o número de mensagens não processadas e ajustar a escala dos consumidores.
   - **Erros e Retries**: Implemente Dead Letter Queues (DLQs) para capturar mensagens que falharam repetidamente no processamento.
   - **Segurança**: Configure permissões no IAM para restringir quem pode enviar e receber mensagens da fila.

### 8. **Exercício**

   **Objetivo**: Colocar em prática o que foi aprendido nesta aula.

   - **Tarefa**: Crie uma fila SQS, envie e receba mensagens utilizando a AWS CLI. Em seguida, integre o SQS com uma função AWS Lambda para processar as mensagens automaticamente.

### 9. **Conclusão da Aula 13**

   **Resumo**:
   - Exploramos o Amazon SQS e como ele pode ser utilizado para construir pipelines de dados escaláveis e desacoplados.
   - Aprendemos a criar filas, enviar e receber mensagens, e integrar o SQS com outros serviços AWS.
   - Discutimos as melhores práticas para implementar o SQS em projetos de engenharia de dados.

   **Tarefa de Casa**:
   - Configure uma Dead Letter Queue para gerenciar mensagens com falhas e monitore o uso do SQS com CloudWatch.

--- 

**Material de Apoio**:
- [Documentação oficial do Amazon SQS](https://docs.aws.amazon.com/sqs/)

================================================
File: /Bootcamp - Cloud para dados/Aula_13/enviando.py
================================================
import boto3

# Criando o cliente SQS
sqs = boto3.client('sqs')

# URL da fila SQS
queue_url = 'https://sqs.us-east-1.amazonaws.com/148761673709/minha-fila-standard'

# Enviando uma mensagem para a fila
response = sqs.send_message(
    QueueUrl=queue_url,
    MessageBody='Mensagem de exemplo para minha-fila-standard'
)

# Exibindo o ID da mensagem enviada
print(f"Mensagem enviada com sucesso! ID da mensagem: {response['MessageId']}")


================================================
File: /Bootcamp - Cloud para dados/Aula_13/recebendo.py
================================================
import boto3

# Criando o cliente SQS
sqs = boto3.client('sqs')

# URL da fila SQS
queue_url = 'https://sqs.us-east-1.amazonaws.com/148761673709/minha-fila-standard'

# Recebendo mensagens da fila
response = sqs.receive_message(
    QueueUrl=queue_url,
    MaxNumberOfMessages=10,  # Número máximo de mensagens a receber
    WaitTimeSeconds=10       # Tempo máximo de espera (em segundos)
)

# Verificando se há mensagens recebidas
if 'Messages' in response:
    for message in response['Messages']:
        print(f"Mensagem recebida: {message['Body']}")
        
        # Excluindo a mensagem da fila após o processamento
        sqs.delete_message(
            QueueUrl=queue_url,
            ReceiptHandle=message['ReceiptHandle']
        )
        print(f"Mensagem excluída: ID {message['MessageId']}")
else:
    print("Nenhuma mensagem disponível na fila.")


================================================
File: /Bootcamp - Cloud para dados/Aula_14/README.md
================================================
# Aula 14: Amazon SNS (Simple Notification Service)

## **Objetivo da Aula**
O foco desta aula é mostrar como engenheiros e analistas de dados podem utilizar o **Amazon SNS** para implementar sistemas de notificação e publicação/assinatura (Pub/Sub) em suas pipelines de dados e outras aplicações distribuídas. Vamos explorar os tópicos, as assinaturas, a configuração de tópicos no console da AWS, e as diferenças principais entre **SNS e SQS**.

## **Conteúdo**
1. O que é o Amazon SNS?
2. Diferença entre SNS e SQS.
3. Demonstração: Criando um tópico no Console AWS.
4. Como assinar um tópico (subscribers).
5. Pub/Sub (Publicação/Assinatura) e casos de uso.
6. Boas práticas e pontos importantes.

## **O que é o Amazon SNS?**
O **Amazon SNS** (Simple Notification Service) é um serviço de mensageria gerenciado que facilita a criação de sistemas baseados no modelo **Pub/Sub** (publicação/assinatura). Ele é utilizado para enviar notificações ou mensagens de eventos importantes para diferentes serviços ou usuários.

**Por que engenheiros e analistas de dados utilizariam o SNS?**
- **Notificações em tempo real**: Alertas sobre eventos importantes em pipelines de dados.
- **Escalabilidade**: SNS pode lidar com grandes volumes de mensagens e entrega quase em tempo real.
- **Integração entre serviços**: SNS permite conectar diversos serviços AWS como Lambda, SQS, e até endpoints HTTP para automatizar fluxos de trabalho de dados.

### **Tópicos e Assinantes**
- **Tópico**: Um canal onde as mensagens são publicadas. Os assinantes vinculados a esse tópico recebem as mensagens.
- **Assinantes (Subscribers)**: Os destinos que recebem as mensagens publicadas. Isso pode incluir serviços como Lambda, SQS, HTTP/S, e-mail, SMS, entre outros.

---

## **Comparação SNS vs SQS**

| **Características**              | **Amazon SNS**                                    | **Amazon SQS**                           |
|-----------------------------------|---------------------------------------------------|------------------------------------------|
| **Modelo**                        | Pub/Sub                                           | Fila                                     |
| **Entrega de mensagens**          | Entrega para múltiplos assinantes simultaneamente    | Entrega individual para consumidores   |
| **Tipo de comunicação**           | Múltiplos assinantes (fan-out)                    | Ponto-a-ponto                            |
| **Latência**                      | Baixa latência (entrega quase em tempo real)      | Processamento assíncrono, maior latência |
| **Método de entrega**             | HTTP/S, Lambda, SQS, SMS, e-mail                  | Apenas consumidores de filas             |
| **Uso típico**                    | Notificações em tempo real                        | Processamento em segundo plano           |
| **Mensagens persistidas?**        | Não                                               | Sim, até 14 dias                         |
| **Garantia de ordem**             | Não no modelo padrão (apenas em FIFO)             | FIFO (quando configurado)                |

### **Mermaid Diagram para SNS e SQS**:
Aqui está uma representação visual para você entender melhor como SNS e SQS funcionam juntos em um sistema Pub/Sub:

```mermaid
graph TD
    A[Publicador] -->|Publica| B[Tópico SNS]
    B --> C[Lambda]
    B --> D[SQS]
    B --> E[Email]
    B --> F[SMS]
    B --> G[HTTP/S]
    C --> H[Processa evento]
    D --> I[Processa via fila]
    E --> J[Notificação por Email]
    F --> K[Notificação por SMS]
    G --> L[Notificação HTTP/S]
```

### **Modelo Pub/Sub e Casos de Uso**
No modelo **Pub/Sub**, o SNS permite que múltiplos assinantes sejam notificados ao mesmo tempo quando uma mensagem é publicada em um tópico. Esse padrão é especialmente útil para criar notificações em larga escala e orquestrar serviços automatizados.

**Exemplos de uso de SNS:**
- **Monitoramento de pipelines de dados**: Notificar equipes quando um job de ETL falhar ou for concluído.
- **Alertas em tempo real**: Notificações para usuários de sistemas quando determinados eventos ocorrerem, como grandes volumes de dados sendo processados ou dados chegando em um banco de dados.
- **Integração de sistemas**: Usar SNS para conectar diferentes serviços em uma arquitetura de microserviços.

---
### **Demonstração: Criando um Tópico no Console AWS**

### **Fluxo da Arquitetura (Mermaid)**

```mermaid
sequenceDiagram
    participant Publisher
    participant SNS_Topic
    participant Lambda_Python
    participant Lambda_JavaScript

    Publisher->>SNS_Topic: Envia Mensagem {"nome": "Luciano", "aula": "aula13"}
    SNS_Topic->>Lambda_Python: Aciona Lambda Python
    SNS_Topic->>Lambda_JavaScript: Aciona Lambda JavaScript
    Lambda_Python->>SNS_Topic: Processa Mensagem
    Lambda_JavaScript->>SNS_Topic: Processa Mensagem
```

Neste diagrama, uma mensagem é enviada para o tópico SNS, que então aciona duas funções Lambda: uma escrita em Python e outra em JavaScript, para processar a mesma mensagem de diferentes formas.

---

### **1. Criar um Tópico SNS**
No **AWS Management Console**:
1. Acesse o **Amazon SNS** e clique em **Create Topic**.
2. Escolha o tipo de tópico:
   - **FIFO (First-In-First-Out)**: Garante a ordem das mensagens.
   - **Standard**: Permite alta taxa de mensagens, mas a ordem de entrega não é garantida.
   
   Para este exemplo, vamos usar o tipo **Standard**.
   
3. Defina o nome do tópico, por exemplo: `MeuTopicoJornadadeDados`.
4. Complete as demais configurações (como criptografia, políticas de acesso, etc.) e clique em **Create Topic**.

---

### **2. Criar as Funções Lambda**

#### **Função Lambda em Python**

1. Acesse o **AWS Lambda** e clique em **Create Function**.
2. Escolha **Author from Scratch**.
3. Dê um nome à sua função, por exemplo: `ProcessaMensagemSNSPython`.
4. Selecione **Python 3.9** como a linguagem de runtime.
5. Role até as permissões e configure permissões básicas que permitam à função Lambda ser acionada pelo SNS.
6. Clique em **Create Function**.

#### **Função Lambda em JavaScript**

1. Repita o processo acima, mas desta vez nomeie a função como `ProcessaMensagemSNSJavaScript`.
2. Selecione **Node.js 14.x** como a linguagem de runtime.
3. Configure as permissões como no exemplo anterior.

---

### **3. Escrevendo o Código Lambda**

#### **Código Python**

Este código extrai a mensagem enviada pelo SNS, faz o parsing para JSON e imprime os dados recebidos.

```python
import json

def lambda_handler(event, context):
    # Extrair a mensagem do evento SNS
    message = event['Records'][0]['Sns']['Message']
    
    # Parsear a mensagem para JSON
    parsed_message = json.loads(message)
    
    # Extrair informações da mensagem
    nome = parsed_message.get('nome')
    aula = parsed_message.get('aula')
    
    # Imprimir as informações
    print(f"Nome: {nome}, Aula: {aula}")
    
    # Retornar a resposta
    return {
        'statusCode': 200,
        'body': json.dumps({'nome': nome, 'aula': aula})
    }
```

#### **Código JavaScript**

A função Lambda em JavaScript também extrai e processa a mensagem SNS recebida.

```javascript
export const handler = async (event) => {
    // Extrair a mensagem do evento SNS
    const message = event.Records[0].Sns.Message;
    
    // Parsear a mensagem para JSON
    let parsedMessage;
    try {
        parsedMessage = JSON.parse(message);
    } catch (error) {
        console.error("Erro ao parsear a mensagem:", error);
        return {
            statusCode: 500,
            body: JSON.stringify({ error: "Erro ao processar a mensagem" })
        };
    }
    
    // Extrair informações da mensagem
    const nome = parsedMessage.nome;
    const aula = parsedMessage.aula;
    
    // Imprimir as informações
    console.log(`Nome: ${nome}, Aula: ${aula}`);
    
    // Retornar a resposta
    return {
        statusCode: 200,
        body: JSON.stringify({ nome: nome, aula: aula })
    };
};
```

---

### **4. Configurando o SNS como Trigger para os Lambdas**

1. Volte para o **Console SNS** e selecione o tópico que você criou.
2. Clique em **Create Subscription**.
3. Escolha **AWS Lambda** como o protocolo.
4. Selecione a função Lambda Python (`ProcessaMensagemSNSPython`) e crie a assinatura.
5. Repita o processo para a função Lambda JavaScript (`ProcessaMensagemSNSJavaScript`).

Agora, as duas funções Lambda serão acionadas automaticamente sempre que uma mensagem for publicada no tópico SNS.

---

### **5. Publicando Mensagens no SNS**

#### **Exemplo de Mensagem SNS Válida**
A mensagem enviada para o SNS deve estar formatada como JSON:

```json
{
  "nome": "Luciano",
  "aula": "aula13"
}
```

#### **Publicação Usando Python (com boto3)**

Aqui está um exemplo de código Python para publicar uma mensagem no SNS:

```python
import boto3
import json

# Criar o cliente SNS
sns_client = boto3.client('sns', region_name='us-east-2')

# Criar a mensagem JSON
message = {
    "nome": "Luciano",
    "aula": "aula13"
}

# Publicar a mensagem no tópico SNS
response = sns_client.publish(
    TopicArn='arn:aws:sns:us-east-2:148761673709:MeuTopico',
    Message=json.dumps(message)
)

print(response)
```

#### **Publicação Usando AWS CLI**
Se preferir usar a CLI da AWS, publique uma mensagem no SNS com o seguinte comando:

```bash
aws sns publish \
    --topic-arn arn:aws:sns:us-east-2:148761673709:MeuTopico \
    --message '{"nome": "Luciano", "aula": "aua13"}' \
    --region us-east-2
```

#### **Publicação via Console AWS**
1. No Console AWS, navegue até o tópico SNS que você criou.
2. Clique em **Publish Message**.
3. No campo **Message Body**, insira a mensagem JSON como mostrado acima.
4. Clique em **Publish** para enviar a mensagem.

---

### **6. Verificando a Mensagem no Lambda**

1. Acesse o **AWS CloudWatch**.
2. Vá até os logs da função Lambda.
3. Verifique o log da execução e veja se as informações da mensagem foram processadas corretamente.
   
Para cada mensagem enviada ao tópico SNS, ambas as funções Lambda (Python e JavaScript) serão acionadas, processando os dados e registrando-os nos logs.

### **Conclusão**

Este exemplo demonstra como integrar o **SNS** com **Lambda** para processar mensagens em tempo real. Essa arquitetura é poderosa para pipelines de dados, alertas e orquestração de serviços, fornecendo escalabilidade e desacoplamento dos componentes.

Aqui está um exemplo de uma ETL simples que transforma um dicionário em um arquivo CSV e, no final, envia uma mensagem para o Amazon SNS indicando que a ETL foi bem-sucedida:

### **Código Python para ETL com notificação SNS**

```python
import csv
import boto3
import json

# Função para converter o dicionário em CSV
def dicionario_para_csv(dicionario, nome_arquivo):
    # Abrir arquivo CSV para escrita
    with open(nome_arquivo, mode='w', newline='') as file:
        writer = csv.DictWriter(file, fieldnames=dicionario[0].keys())
        writer.writeheader()
        writer.writerows(dicionario)
    print(f"Arquivo CSV '{nome_arquivo}' gerado com sucesso!")

# Função para enviar notificação SNS
def enviar_notificacao_sns(topico_arn, mensagem):
    sns_client = boto3.client('sns', region_name='us-east-2')
    response = sns_client.publish(
        TopicArn=topico_arn,
        Message=json.dumps(mensagem)
    )
    print("Notificação enviada:", response)

# ETL principal
def executar_etl():
    # Dicionário de exemplo
    dados = [
        {"nome": "Luciano", "idade": 34, "profissao": "Engenheiro de Dados"},
        {"nome": "Maria", "idade": 29, "profissao": "Cientista de Dados"},
        {"nome": "José", "idade": 45, "profissao": "Analista de Dados"}
    ]
    
    # Nome do arquivo CSV a ser gerado
    nome_arquivo_csv = 'dados.csv'
    
    # Executar transformação (dicionário para CSV)
    dicionario_para_csv(dados, nome_arquivo_csv)
    
    # Enviar notificação para o SNS
    mensagem_sns = {
        "status": "Sucesso",
        "descricao": "A ETL foi executada com sucesso e o arquivo CSV foi gerado."
    }
    
    # ARN do tópico SNS (substitua pelo seu ARN de SNS)
    topico_arn = 'arn:aws:sns:us-east-2:148761673709:MeuTopico'
    
    # Enviar a notificação
    enviar_notificacao_sns(topico_arn, mensagem_sns)

# Executar a ETL
executar_etl()
```

### **Explicação**:
1. **Dicionário para CSV**: A função `dicionario_para_csv` pega o dicionário de dados e o transforma em um arquivo CSV. 
   - Neste exemplo, os dados consistem em uma lista de dicionários, onde cada dicionário representa uma linha no arquivo CSV.
2. **Notificação SNS**: A função `enviar_notificacao_sns` usa o cliente boto3 para enviar uma mensagem JSON para um tópico SNS, notificando que a ETL foi bem-sucedida.
3. **ETL Principal**: A função `executar_etl` executa a lógica de transformação dos dados em CSV e, em seguida, envia uma notificação ao SNS.

### **Mensagem de Notificação SNS**:
A mensagem SNS enviada ao final indica que a ETL foi concluída com sucesso:

```json
{
    "status": "Sucesso",
    "descricao": "A ETL foi executada com sucesso e o arquivo CSV foi gerado."
}
```

### **Output esperado**:
- Um arquivo CSV chamado `dados.csv` será gerado.
- A notificação será enviada ao SNS informando que a ETL foi concluída com sucesso.

Este exemplo pode ser usado para automações de ETL em pipelines de dados e pode ser integrado com outros serviços na AWS para notificações e monitoramento.

================================================
File: /Bootcamp - Cloud para dados/Aula_15/README.md
================================================
# Aula 15: Construção de uma Página de Sorteio com AWS

## **Objetivo da Aula**

Nesta aula, vamos construir uma página de sorteio automática usando AWS. O usuário poderá inserir o nome do sorteio, o número mínimo e o número máximo, e nossa aplicação, utilizando AWS Lambda, retornará um número aleatório dentro desse intervalo. Vamos dividir o projeto em duas etapas principais, utilizando o AWS Amplify para hospedar a página e AWS Lambda para a lógica de geração do número.

## **Conteúdo**
1. Estruturação da aplicação.
2. Criando a página web com Amplify (Etapa 1 e Etapa 2).
3. Configuração da lógica de sorteio com Lambda.
4. Integração com API Gateway.
5. Persistindo dados com DynamoDB (opcional).
6. Controle de permissões com IAM.
7. Conclusão e boas práticas.

---

## **1. Estruturação da Aplicação**

Vamos criar uma aplicação simples, onde os usuários poderão realizar sorteios online. A página web será criada em duas etapas: a primeira será bem básica, e na segunda, vamos melhorar a interação e adicionar a funcionalidade de sorteio.

Os dados que o usuário fornecerá serão:
- Nome do sorteio
- Número mínimo
- Número máximo

Com essas informações, uma função Lambda será acionada para gerar um número aleatório entre os números fornecidos.

---

## **2. Criando a Página Web com Amplify**

### **O que é o Amplify?**
O AWS Amplify é uma ferramenta que facilita a criação e hospedagem de páginas web. Vamos utilizá-lo para hospedar nossa página de sorteio.

### **Etapa 1: Página Simples**

Nesta primeira etapa, vamos criar uma página estática simples com HTML, para garantir que a estrutura está funcionando corretamente.

#### **Passo a Passo: Criando a página HTML simples**

1. Crie um arquivo `index.html` com o seguinte conteúdo:

   ```html
   <!DOCTYPE html>
   <html>
   <head>
       <title>Sorteio Automático</title>
   </head>
   <body>
       <h1>Essa é nossa página de sorteio</h1>
       <p>Agora os nossos sorteios vão ficar automáticos!</p>
   </body>
   </html>
   ```

2. **Empacotamento e Upload para o Amplify:**
   - Comprima o arquivo `index.html` em um arquivo `.zip`.
   - No console da AWS, vá até **AWS Amplify** e crie um novo aplicativo.
   - Selecione a opção de **"Host a web app"**.
   - Faça o upload do arquivo `.zip` e implante o aplicativo.

Agora você tem uma página web simples com a estrutura inicial do seu projeto de sorteio.

---

## **3. Configurando a Lógica de Sorteio com Lambda**

### **O que é o Lambda?**
O AWS Lambda permite executar código em resposta a eventos. Vamos usá-lo para gerar um número aleatório entre o número mínimo e o máximo fornecido pelo usuário.

#### **Passo a Passo: Criando a Função Lambda**

1. No console da AWS, acesse **AWS Lambda** e crie uma nova função chamada `SorteioFunction`.
2. Selecione **Python 3.9** como a linguagem de execução.
3. Substitua o código padrão pelo seguinte:

```python
import json
import random

def lambda_handler(event, context):
    nome_sorteio = event['nomeSorteio']
    num_min = int(event['numMin'])
    num_max = int(event['numMax'])
    numero_sorteado = random.randint(num_min, num_max)
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'nomeSorteio': nome_sorteio,
            'numeroSorteado': numero_sorteado
        })
    }
```

Aqui está um body em JSON para testar essa função Lambda:

```json
{
    "nomeSorteio": "Sorteio Aula 15",
    "numMin": 1,
    "numMax": 15
}
```

Este body passará o nome do sorteio como "Sorteio Aula 15" e sorteará um número aleatório entre 1 e 15.

4. **Salve** e **implante** a função.

Essa função receberá os números mínimo e máximo e retornará um número aleatório entre eles.

---

## **4. Integração com API Gateway**

### **O que é o API Gateway?**
O AWS API Gateway nos permite criar uma API REST para invocar a função Lambda a partir da página web.

#### **Passo a Passo: Configurando o API Gateway**

1. Acesse o **API Gateway** no console da AWS e crie uma nova API do tipo **REST API**.
2. Crie um método **POST** vinculado à função Lambda (`SorteioFunction`).
3. Copie a URL de invocação da API, que será usada no próximo passo para conectar o frontend ao backend.

---

## **5. Persistindo Dados no DynamoDB**

Agora que já temos a página de sorteio e a função Lambda configurada para realizar o sorteio, o próximo passo é armazenar os dados do sorteio no DynamoDB. Vamos criar uma tabela DynamoDB onde vamos salvar as seguintes informações:
- Nome do sorteio
- Número mínimo
- Número máximo
- Número sorteado

### **Passo a Passo: Criando a Tabela DynamoDB**

1. No console da AWS, vá até o serviço **DynamoDB** e clique em **Create Table**.
2. Defina o nome da tabela como `Sorteios`.
3. A chave de partição será `SorteioID` (tipo String).
4. Conclua a criação da tabela.

Agora, a tabela `Sorteios` está pronta para receber os dados dos sorteios.

---

## **6. Atualizando a Função Lambda para Persistir os Dados no DynamoDB**

Além de realizar o sorteio, agora a função Lambda vai salvar no DynamoDB o nome do sorteio, o intervalo de números (mínimo e máximo) e o número sorteado. Vamos atualizar a função Lambda para incluir essa lógica.

### **Passo a Passo: Atualizando a Função Lambda**

Atualize o código da sua função Lambda para persistir os dados no DynamoDB:

```python
import json
import random
import boto3
from time import gmtime, strftime

# Criar o objeto DynamoDB usando o SDK boto3
dynamodb = boto3.resource('dynamodb')
# Selecionar a tabela que criamos
table = dynamodb.Table('Sorteios')

# Função Lambda que realiza o sorteio e persiste os dados
def lambda_handler(event, context):
    nome_sorteio = event['nomeSorteio']
    num_min = int(event['numMin'])
    num_max = int(event['numMax'])
    numero_sorteado = random.randint(num_min, num_max)
    
    # Salvar os dados no DynamoDB
    response = table.put_item(
        Item={
            'SorteioID': strftime("%Y%m%d%H%M%S", gmtime()),  # Gerar um ID único baseado no horário
            'NomeSorteio': nome_sorteio,
            'NumeroMin': num_min,
            'NumeroMax': num_max,
            'NumeroSorteado': numero_sorteado,
            'DataSorteio': strftime("%a, %d %b %Y %H:%M:%S +0000", gmtime())  # Data e hora do sorteio
        }
    )
    
    # Retornar o resultado do sorteio
    return {
        'statusCode': 200,
        'body': json.dumps({
            'nomeSorteio': nome_sorteio,
            'numeroSorteado': numero_sorteado
        })
    }
```

### **Explicação do Código:**
- **boto3**: Usamos o boto3 para interagir com o DynamoDB.
- **strftime**: Geramos um ID único para cada sorteio baseado na data e hora em que ele foi realizado.
- **put_item**: Salvamos os dados do sorteio (nome, números mínimo e máximo, número sorteado, e a data do sorteio) na tabela DynamoDB.

### **Atualizando o Frontend (Formulário HTML)**

### Aula Completa: Criando um Sistema de Sorteio com AWS e Frontend Integrado

Nesta aula, vamos implementar uma aplicação completa de sorteio, integrando frontend, AWS Lambda, DynamoDB e API Gateway. O usuário poderá inserir o nome do sorteio, o número mínimo e o número máximo. A função Lambda, acionada via API Gateway, retornará um número aleatório dentro desse intervalo e salvará os detalhes no DynamoDB.

---

### **7. Frontend: Página de Sorteio**

Vamos começar criando o frontend da nossa aplicação. Ele terá um formulário que captura os dados do sorteio e chama a API para realizar o sorteio.

#### **Código HTML do Frontend:**

Aqui está o código HTML para a página de sorteio. Vamos aplicar um design simples com tons de azul.

```html
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Sorteio Automático</title>
    <!-- Estilização da página -->
    <style>
    h1 {
        color: #FFFFFF;
        font-family: system-ui;
        margin-left: 20px;
    }
    body {
        background-color: #003366;
    }
    label {
        color: #66CCFF;
        font-family: system-ui;
        font-size: 20px;
        margin-left: 20px;
        margin-top: 20px;
    }
    button {
        background-color: #66CCFF;
        border-color: #66CCFF;
        color: #FFFFFF;
        font-family: system-ui;
        font-size: 20px;
        font-weight: bold;
        margin-left: 30px;
        margin-top: 20px;
        width: 140px;
    }
    input {
        color: #003366;
        font-family: system-ui;
        font-size: 20px;
        margin-left: 10px;
        margin-top: 20px;
        width: 100px;
    }
    </style>
    <script>
        // Função para chamar a API
        var realizarSorteio = (nomeSorteio, numMin, numMax) => {
            var myHeaders = new Headers();
            myHeaders.append("Content-Type", "application/json");

            var raw = JSON.stringify({
                "nomeSorteio": nomeSorteio,
                "numMin": numMin,
                "numMax": numMax
            });

            var requestOptions = {
                method: 'POST',
                headers: myHeaders,
                body: raw,
                redirect: 'follow'
            };

            fetch("YOUR_API_GATEWAY_ENDPOINT", requestOptions)
            .then(response => response.text())
            .then(result => alert("Número sorteado: " + JSON.parse(result).numeroSorteado))
            .catch(error => console.log('error', error));
        }
    </script>
</head>
<body>
    <h1>Bem-vindo ao Sorteio Automático!</h1>
    <form>
        <label>Nome do sorteio:</label>
        <input type="text" id="nomeSorteio"><br>
        <label>Número Mínimo:</label>
        <input type="number" id="numMin"><br>
        <label>Número Máximo:</label>
        <input type="number" id="numMax"><br>
        <!-- Botão que chama a função realizarSorteio -->
        <button type="button" onclick="realizarSorteio(
            document.getElementById('nomeSorteio').value,
            document.getElementById('numMin').value,
            document.getElementById('numMax').value
        )">Realizar Sorteio</button>
    </form>
</body>
</html>
```

================================================
File: /Bootcamp - Cloud para dados/Aula_15/Policy.txt
================================================
{
"Version": "2012-10-17",
"Statement": [
    {
        "Sid": "VisualEditor0",
        "Effect": "Allow",
        "Action": [
            "dynamodb:PutItem",
            "dynamodb:DeleteItem",
            "dynamodb:GetItem",
            "dynamodb:Scan",
            "dynamodb:Query",
            "dynamodb:UpdateItem"
        ],
        "Resource": "arn:aws:dynamodb:us-east-1:148761673709:table/SorteioJornada"
    }
    ]
}

================================================
File: /Bootcamp - Cloud para dados/Aula_15/index.html
================================================
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Sorteio Automático</title>
    <!-- Estilização da página -->
    <style>
    h1 {
        color: #FFFFFF;
        font-family: system-ui;
        margin-left: 20px;
    }
    body {
        background-color: #003366;
    }
    label {
        color: #66CCFF;
        font-family: system-ui;
        font-size: 20px;
        margin-left: 20px;
        margin-top: 20px;
    }
    button {
        background-color: #66CCFF;
        border-color: #66CCFF;
        color: #FFFFFF;
        font-family: system-ui;
        font-size: 20px;
        font-weight: bold;
        margin-left: 30px;
        margin-top: 20px;
        width: 140px;
    }
    input {
        color: #003366;
        font-family: system-ui;
        font-size: 20px;
        margin-left: 10px;
        margin-top: 20px;
        width: 100px;
    }
    </style>
    <script>
        // Função para chamar a API
        var realizarSorteio = (nomeSorteio, numMin, numMax) => {
            var myHeaders = new Headers();
            myHeaders.append("Content-Type", "application/json");

            var raw = JSON.stringify({
                "nomeSorteio": nomeSorteio,
                "numMin": numMin,
                "numMax": numMax
            });

            var requestOptions = {
                method: 'POST',
                headers: myHeaders,
                body: raw,
                redirect: 'follow'
            };

            fetch("https://5bciix4nk2.execute-api.us-east-2.amazonaws.com/devc", requestOptions)
            .then(response => response.text())
            .then(result => alert("Número sorteado: " + JSON.parse(result).numeroSorteado))
            .catch(error => console.log('error', error));
        }
    </script>
</head>
<body>
    <h1>Bem-vindo ao Sorteio Automático!</h1>
    <form>
        <label>Nome do sorteio:</label>
        <input type="text" id="nomeSorteio"><br>
        <label>Número Mínimo:</label>
        <input type="number" id="numMin"><br>
        <label>Número Máximo:</label>
        <input type="number" id="numMax"><br>
        <!-- Botão que chama a função realizarSorteio -->
        <button type="button" onclick="realizarSorteio(
            document.getElementById('nomeSorteio').value,
            document.getElementById('numMin').value,
            document.getElementById('numMax').value
        )">Realizar Sorteio</button>
    </form>
</body>
</html>


================================================
File: /Bootcamp - Cloud para dados/Aula_16_17/README.md
================================================
# **Bootcamp Cloud - Aula 16: Introdução ao Azure para Dados**  

---

### **Objetivo:**  
Explorar o ambiente do Azure, abordando a criação de conta, uso do **Azure Blob Storage** (equivalente ao S3), **máquinas virtuais (VMs)** para processamento de dados e o **Azure Active Directory (AAD)**, que substitui o IAM para gerenciamento de identidade e controle de acesso.

---

## Criação de Conta no Azure  
1. Acesse [portal.azure.com](https://portal.azure.com) e selecione **Create a Free Account**.  
2. Preencha as informações básicas: nome completo, telefone, e-mail e senha.  
3. Adicione as informações de **cartão de crédito** (somente para verificação; não haverá cobranças iniciais).  
4. Escolha o plano gratuito:  
   - 750 horas de uma VM B1S por mês (Ubuntu ou Windows).  
   - 5 GB gratuitos no Blob Storage.  
   - Banco de dados SQL com 250 GB gratuitos.  
5. No **Cost Management**, configure alertas para controlar custos e crie um orçamento mensal de teste.

---

## Assinaturas no Azure  
Uma **Assinatura do Azure** é uma unidade lógica para gerenciar o acesso, controle de custos e recursos. Permite organizar e isolar recursos como máquinas virtuais, bancos de dados e serviços.  

### Funções e Finalidades  
- **Gestão de Recursos e Acesso:** Define permissões específicas para equipes e projetos.  
- **Faturamento e Controle de Custos:** Cada assinatura tem seu próprio relatório de faturamento.  
- **Isolamento e Limites:** Permite separar ambientes de produção, desenvolvimento e teste.  
- **Governança:** As assinaturas aplicam políticas e limites via Grupos de Gerenciamento e AAD.

### Tipos de Assinaturas  
- **Plano Gratuito:** Oferece crédito inicial e limites gratuitos por 12 meses.  
- **Pay-As-You-Go:** Paga-se apenas pelo uso.  
- **Enterprise Agreement (EA):** Contrato com desconto por volume para grandes empresas.  
- **Cloud Solution Provider (CSP):** Parceiros que revendem serviços Azure.

---

## Equivalente na AWS  
Na AWS, o conceito mais próximo de uma assinatura do Azure é uma **Conta AWS**.  

- **AWS Organizations:** Permite consolidar e gerenciar várias contas AWS sob uma organização.  
- **Controle de Custos:** Cada conta tem seus próprios alertas e relatórios de custo.  

### Comparação entre Assinatura do Azure e Conta AWS  
| Azure (Assinatura)             | AWS (Conta AWS)                       |
|---------------------------------|----------------------------------------|
| Separa recursos e define limites | Isola ambientes e projetos            |
| Gerenciada por AAD              | Gerenciada por IAM e AWS Organizations |
| Faturamento por assinatura      | Faturamento por conta                 |

---

## Projeto 1. Acessando variáveis de ambiente no Azure

```mermaid
sequenceDiagram
    participant User as Usuário (Local)
    participant AzureAD as Azure AD
    participant AppReg as App Registration
    participant KeyVault as Azure Key Vault
    participant IAMUser as IAM do Usuário
    participant IAMService as IAM do Serviço

    User->>AzureAD: 1. Solicita Token de Acesso
    AzureAD-->>User: 2. Retorna Token

    User->>AppReg: 3. Envia Credenciais e Token
    AppReg-->>User: 4. Validação e Permissão

    User->>KeyVault: 5. Solicita Segredo (com Token)
    KeyVault-->>IAMUser: 6. Verifica Permissões do Usuário
    KeyVault-->>IAMService: 7. Verifica Permissões do Serviço

    KeyVault-->>User: 8. Retorna Valor do Segredo

    User->>User: 9. Exibe Segredo no Terminal
```

---

### **Azure Key Vault: O que é e para que serve?**

### **Objetivo:**
O **Azure Key Vault** é um serviço de nuvem da Microsoft projetado para gerenciar segredos, chaves de criptografia e certificados de maneira segura. Seu principal objetivo é fornecer uma forma centralizada e protegida de armazenar e acessar informações sensíveis, como senhas, tokens e chaves criptográficas, minimizando riscos de segurança.

### **Funcionalidades do Azure Key Vault:**

1. **Armazenamento Seguro de Segredos:**
   - Gerencia senhas, strings de conexão, tokens de API e outras informações confidenciais.
   - Fornece acesso seguro aos segredos por meio de autenticação robusta e controle de permissões via **Azure AD**.

2. **Gerenciamento de Chaves de Criptografia:**
   - Armazena e protege chaves usadas para criptografia de dados em repouso e em trânsito.
   - Oferece suporte para criptografia assimétrica e simétrica, podendo ser integrado a outros serviços do Azure, como SQL Database.

3. **Gestão de Certificados:**
   - Automatiza o processo de renovação e gerenciamento de certificados SSL/TLS.
   - Permite criar e manter certificados seguros que podem ser utilizados por serviços na nuvem ou locais.

4. **Controle de Acesso Granular:**
   - Usa **IAM (Identity and Access Management)** para controlar quais aplicações e usuários podem acessar os segredos e chaves.
   - Mantém logs de auditoria de todas as interações com o Key Vault.

### **Benefícios:**
- **Centralização:** Consolida a gestão de segredos e chaves, evitando que sejam armazenados em diferentes locais de forma não segura.
- **Automação:** Facilita a renovação automática de certificados e permite que aplicações acessem segredos sem intervenção manual.
- **Conformidade:** Ajuda a cumprir regulamentações de segurança e privacidade, mantendo as informações críticas em um ambiente protegido.
- **Alta Disponibilidade:** O serviço é distribuído por regiões do Azure, garantindo que segredos e chaves estejam sempre acessíveis.

---

### **Comparação com AWS Secrets Manager:**

| **Azure Key Vault**                    | **AWS Secrets Manager**                |
|----------------------------------------|----------------------------------------|
| Gerencia segredos, chaves e certificados | Gerencia segredos e credenciais |
| Integra-se com serviços Azure, como SQL Database | Integra-se com serviços AWS, como RDS |
| Oferece criptografia usando HSM (Hardware Security Modules) | Oferece criptografia e rotação automática de segredos |
| Controle de acesso via Azure AD        | Controle de acesso via IAM |
| Preços por transação e armazenamento   | Preços por segredos armazenados |

---

### **Nosso primeiro Erro**

Importância do IAM
O uso do IAM (Identity and Access Management) é essencial para criar e gerenciar um Key Vault. Ele permite configurar controle de acesso baseado em papéis (RBAC), garantindo que apenas usuários ou aplicações autorizadas possam acessar ou modificar segredos e chaves.

IAM no Azure é o serviço responsável por gerenciar identidades e permissões, assegurando que cada recurso ou aplicação tenha apenas os acessos necessários. Ele garante uma abordagem de segurança baseada em princípio de menor privilégio.

## **View Exemple**

### **"View Example" no Portal Azure Key Vault**

A opção **"View Example"** (ou "Ver Exemplo") na interface do **Key Vault** facilita o entendimento e uso do serviço. Esta funcionalidade:

1. **Demonstração de Exemplos Práticos:**  
   - Mostra exemplos de como armazenar e acessar chaves e segredos.
   - Fornece código de amostra, muitas vezes em **Python, C#, ou PowerShell**, para acessar os segredos via SDK.

2. **Ajuda na Automação:**  
   - Exemplos incluem snippets para automação com **Azure CLI** ou scripts para aplicações.
   - Acelera o processo de integração com outras aplicações e pipelines de dados.

3. **Facilita a Configuração Inicial:**  
   - Apresenta instruções claras de como criar e acessar segredos, chaves e certificados.
   - Demonstra como configurar corretamente o acesso via **IAM** ou atribuir permissões específicas.

A funcionalidade **"View Example"** é uma poderosa ferramenta educacional e prática no portal Azure. Ela facilita a adoção do Azure Key Vault ao fornecer exemplos de código claros, economizando tempo e simplificando o processo de integração. Com essa abordagem orientada a exemplos, é possível configurar o acesso seguro e garantir a conformidade com as boas práticas de segurança na nuvem.

---

## **Azure Active Directory (Azure AD)**

### **Visão Geral**

O **Azure Active Directory (Azure AD)** é o serviço de gerenciamento de identidade e acesso baseado na nuvem da Microsoft. Ele permite que as organizações administrem identidades e controlem o acesso a recursos em nuvem e on-premises, além de aplicativos SaaS. 

---

### **Objetivos do Azure AD**

1. **Autenticação e Autorização**  
   - Verifica identidades para permitir ou negar o acesso a recursos e aplicativos.
   - Suporta métodos como senha, MFA (autenticação multi-fator) e biometria.

2. **Gerenciamento de Usuários e Grupos**  
   - Permite criar usuários e organizar grupos para definir permissões e acessos.
   - Facilita a ativação e desativação de contas de usuários.

3. **Integração com Aplicativos SaaS**  
   - Oferece autenticação única (SSO) para diversos aplicativos, como Salesforce e Google Workspace.
   - Melhora a experiência do usuário eliminando a necessidade de múltiplas senhas.

4. **Políticas de Acesso e Segurança**  
   - Controla o acesso com base na localização e tipo de dispositivo.
   - Oferece políticas de acesso condicional para aumentar a segurança.

5. **Autenticação para APIs e Aplicativos Customizados**  
   - Desenvolvedores podem integrar o Azure AD para autenticação segura em suas APIs e aplicações.

---

### **Passo a Passo: Configuração no Azure AD**

#### **1. Acessar o Azure AD no Portal**
- Vá para o [portal do Azure](https://portal.azure.com).
- No menu, clique em **Azure Active Directory**.

#### **2. Registrar um Aplicativo no Azure AD**
- Clique em **App Registrations** > **New Registration**.
- Dê um nome ao aplicativo e escolha a conta ou organização que terá acesso.
- Clique em **Register** para finalizar.

#### **3. Criar um Client Secret**
- Na aplicação registrada, vá para **Certificates & Secrets**.
- Clique em **New Client Secret**.
- Adicione uma descrição (opcional) e defina a validade do segredo (6, 12 ou 24 meses).
- Ao clicar em **Add**, o segredo será gerado. **Copie o valor agora** – ele não será exibido novamente.

#### **4. Configurar IAM para o Serviço**
- Volte para o **Azure Key Vault** ou outro serviço relevante.
- Vá para **Access Control (IAM)** > **Role Assignments**.
- Adicione um novo papel, como **Key Vault Secrets User** ou **Reader**.
- Em **Select Members**, selecione o **App Registration** que você criou.

---

### **Relação entre Azure AD e AWS IAM**

| **Azure AD**                                | **AWS IAM**                                  |
|---------------------------------------------|----------------------------------------------|
| Gerencia identidade de usuários e autenticação. | Gerencia permissões de recursos na AWS. |
| Oferece SSO e políticas de acesso condicional. | Configura políticas detalhadas para acesso a recursos. |
| Integrado com Office 365 e aplicativos SaaS. | Integrado com serviços AWS, como EC2 e S3. |
| Suporta políticas baseadas em usuário e dispositivo. | Suporta permissões detalhadas por função e políticas. |

---

### **Conclusão**

O **Azure AD** é essencial para a autenticação e controle de identidades em ambientes Microsoft e aplicativos SaaS. Ele complementa o **IAM da AWS**, que é mais voltado para gerenciar permissões de recursos. Integrar o Azure AD com a AWS pode proporcionar uma experiência de autenticação unificada e simplificar a gestão de identidades e acessos.

## Projeto 2. Consumindo arquivos no Blob Storage

```mermaid
sequenceDiagram
    participant User as Usuário (Local)
    participant AzureAD as Azure AD
    participant AppReg as App Registration
    participant BlobStorage as Azure Blob Storage
    participant IAMUser as IAM do Usuário
    participant IAMService as IAM do Serviço

    User->>AzureAD: 1. Solicita Token de Acesso
    AzureAD-->>User: 2. Retorna Token

    User->>AppReg: 3. Envia Credenciais e Token
    AppReg-->>User: 4. Validação e Permissão

    User->>BlobStorage: 5. Solicita Listagem de Arquivos (com Token)
    BlobStorage-->>IAMUser: 6. Verifica Permissões do Usuário
    BlobStorage-->>IAMService: 7. Verifica Permissões do Serviço

    BlobStorage-->>User: 8. Retorna Lista de Arquivos

    User->>BlobStorage: 9. Baixa Arquivo Específico
    BlobStorage-->>User: 10. Envia Arquivo

    User->>User: 11. Exibe ou Processa Arquivo Localmente
```

### **Projeto: Acessar e Fazer Upload de Arquivos no Azure Blob Storage com Python SDK**

Este passo a passo mostrará como criar uma **Storage Account no Azure**, configurar permissões usando **IAM**, e listar arquivos dentro de um **container Blob** utilizando o **Python SDK**.

---

## **Passo 1: Criar uma Storage Account no Azure**

1. Acesse o [Azure Portal](https://portal.azure.com/).
2. No menu de navegação, clique em **Storage accounts** e depois em **Create**.
3. **Configuração básica**:
   - **Subscription**: Escolha a assinatura correta.
   - **Resource group**: Selecione o grupo criado anteriormente.
   - **Storage account name**: Escolha um nome único (por exemplo, `armazenamentoexemplo`).
   - **Region**: Escolha a mesma região onde seus recursos estão hospedados (ex.: East US).
   - **Performance**: Standard.
   - **Replication**: LRS (Locally Redundant Storage) para este exemplo.
4. Clique em **Review + Create** e, em seguida, **Create**.

---

## **Passo 2: Criar um Container no Blob Storage**

1. Dentro da **Storage Account** recém-criada, vá para **Containers** no menu lateral.
2. Clique em **+ Container**.
3. **Nome do Container**: Por exemplo, `meus-arquivos`.
4. **Tipo de acesso público**: Deixe como **Private** (somente acesso autenticado).
5. Clique em **Create**.

---

## **Passo 3: Obter a URL de Conexão do Storage**

1. Na **Storage Account**, vá para **Access keys** no menu lateral.
2. Copie o **Connection string**. Ele será usado no código para conectar-se ao Blob Storage.

---

## **Passo 4: Dar Acesso no IAM ao Aplicativo Criado Anteriormente**

1. Acesse o **Azure Portal** e vá para **Storage accounts** > **sua Storage Account**.
2. No menu lateral, clique em **Access control (IAM)**.
3. Clique em **Add role assignment**.
4. **Função**: Selecione **Blob Data Contributor**.
5. **Membro**: Selecione o **App Registration** criado anteriormente no Azure Active Directory.
6. Clique em **Review + Assign**.

---

## **Passo 5: Código Python para Listar Arquivos no Blob Storage**

Crie um arquivo Python chamado `list_blob_files.py` e adicione o seguinte código:

### **Código Python**

```python
from azure.identity import ClientSecretCredential
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv
import os

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Variáveis de ambiente necessárias
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
storage_account_url = os.environ["AZURE_STORAGE_URL"]  # Ex: https://<nome_da_storage>.blob.core.windows.net/
container_name = "meus-arquivos"  # Nome do container criado

# Configura as credenciais usando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id,
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Conectar ao Blob Storage
blob_service_client = BlobServiceClient(
    account_url=storage_account_url,
    credential=credentials
)

# Acessa o container
container_client = blob_service_client.get_container_client(container_name)

# Lista todos os arquivos dentro do container
print(f"Listando arquivos no container '{container_name}':")
for blob in container_client.list_blobs():
    print(f" - {blob.name}")
```

---

## **Passo 6: Configurar o Arquivo `.env`**

Crie um arquivo chamado `.env` na mesma pasta do código e adicione as variáveis necessárias:

```env
AZURE_CLIENT_ID=seu_client_id
AZURE_TENANT_ID=seu_tenant_id
AZURE_CLIENT_SECRET=seu_client_secret
AZURE_STORAGE_URL=https://<nome_da_storage>.blob.core.windows.net/
```

---

## **Passo 7: Instalar as Dependências**

No terminal, execute o seguinte comando para instalar as bibliotecas necessárias:

```bash
pip install azure-identity azure-storage-blob python-dotenv
```

---

## **Passo 8: Executar o Código**

No terminal, execute o script:

```bash
python list_blob_files.py
```

---

## **Resultado Esperado**

Ao executar o código, você verá a lista de todos os arquivos presentes no container:

```
Listando arquivos no container 'meus-arquivos':
 - exemplo1.txt
 - relatorio2024.csv
 - imagem.png
```

---

## **Conclusão**

Com este projeto, você aprendeu a:

- **Criar uma Storage Account** e **container Blob** no Azure.
- **Configurar permissões IAM** para o aplicativo.
- **Listar arquivos** armazenados no container Blob usando **Python SDK**.

Este processo é fundamental para manipular dados na nuvem com segurança e eficiência, garantindo o acesso controlado por meio de credenciais e políticas de IAM.

## Projeto 3. Streamlit para inserir dados no Blob Storage

### **Código com Streamlit: Inserir Arquivos no Blob Storage**

Este exemplo utiliza o **Streamlit** para criar uma interface gráfica que permite ao usuário fazer upload de arquivos para o **Azure Blob Storage**.

---

#### **Instalar as Dependências**

Antes de começar, certifique-se de instalar as bibliotecas necessárias:

```bash
pip install streamlit azure-identity azure-storage-blob python-dotenv
```

---

#### **Código Python (app.py)**

```python
import streamlit as st
from azure.identity import ClientSecretCredential
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv
import os

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Variáveis de ambiente necessárias
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
storage_account_url = os.environ["AZURE_STORAGE_URL"]
container_name = "meucontainer"

# Configura credenciais usando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id,
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Conectar ao Blob Storage
blob_service_client = BlobServiceClient(
    account_url=storage_account_url,
    credential=credentials
)

# Acessa o container
container_client = blob_service_client.get_container_client(container_name)

# Função para upload de arquivo
def upload_file(file):
    try:
        blob_client = container_client.get_blob_client(file.name)
        blob_client.upload_blob(file, overwrite=True)
        st.success(f"Arquivo '{file.name}' enviado com sucesso!")
    except Exception as e:
        st.error(f"Erro ao enviar arquivo: {str(e)}")

# Função para listar arquivos no container
def listar_arquivos():
    try:
        blobs = container_client.list_blobs()
        return [blob.name for blob in blobs]
    except Exception as e:
        st.error(f"Erro ao listar arquivos: {str(e)}")
        return []

# Interface do Streamlit
st.title("Upload para Azure Blob Storage")

uploaded_file = st.file_uploader("Escolha um arquivo para enviar", type=["csv", "txt", "png", "jpg", "pdf"])

if uploaded_file is not None:
    if st.button("Enviar"):
        upload_file(uploaded_file)

st.subheader("Arquivos no Container")
arquivos = listar_arquivos()
if arquivos:
    for arquivo in arquivos:
        st.write(f"- {arquivo}")
else:
    st.write("Nenhum arquivo encontrado.")
```

---

#### **Configuração do Arquivo `.env`**

Crie um arquivo chamado `.env` e adicione as variáveis de ambiente:

```env
AZURE_CLIENT_ID=seu_client_id
AZURE_TENANT_ID=seu_tenant_id
AZURE_CLIENT_SECRET=seu_client_secret
AZURE_STORAGE_URL=https://<nome_da_storage>.blob.core.windows.net/
```

---

#### **Como Executar o Projeto**

1. **Inicie o Streamlit** com o seguinte comando:

   ```bash
   streamlit run app.py
   ```

2. **Acesse a interface** no navegador através do link fornecido no terminal (por exemplo, `http://localhost:8501`).

---

#### **O que este Código Faz?**

- **Upload de Arquivos**: O usuário pode selecionar um arquivo e enviá-lo para o **Blob Storage** clicando em "Enviar".
- **Listagem de Arquivos**: Todos os arquivos presentes no container são listados abaixo da interface.
- **Tratamento de Erros**: Mensagens de erro e sucesso são exibidas para garantir uma melhor experiência do usuário.

---

#### **Conclusão**

Com este projeto, você pode enviar e gerenciar arquivos diretamente no **Azure Blob Storage** através de uma interface simples e intuitiva criada com **Streamlit**.

---
## Projeto 4. Máquinas Virtuais (VMs) no Azure  
VMs no Azure permitem criar máquinas para processamento de dados e desenvolvimento de aplicações.  

### Configuração de uma VM  
1. No Azure Portal, vá em **Virtual Machine** > **Create**.  
2. Escolha a região e imagem (ex.: Ubuntu Server 20.04 LTS).  
3. Configure o tamanho (ex.: B1S).  
4. Escolha entre SSH ou senha como método de login.

### Configuração de Rede e Segurança  
1. Crie uma VNet e uma sub-rede para isolar a comunicação.  
2. Configure o NSG para liberar a porta 22 (SSH).  
3. Defina o IP como estático.

---

## Deploy da Aplicação no Azure VM  

Para rodar uma aplicação Streamlit em uma **VM do Azure** utilizando HTTP (porta 80), siga os passos abaixo.

---

### Configuração da VM no Azure  

1. No **Azure Portal**, vá em **Virtual Machines** > **Create** > **Azure Virtual Machine**.  
2. Preencha os detalhes básicos:  
   - Nome: `streamlit-vm`  
   - Região: **East US**  
   - Imagem: **Ubuntu Server 20.04 LTS**  
   - Tamanho: **B1S** (ou outro disponível no plano gratuito).  
3. Em **Opções de Autenticação**, selecione **Chave SSH** e gere um novo par de chaves ou utilize um existente.  
4. Configure a **Rede**:  
   - Crie uma nova **Virtual Network (VNet)** e uma **sub-rede**.  
   - Configure um **Network Security Group (NSG)** e adicione uma **regra de entrada** para liberar a **porta 80 (HTTP)**.  
   - Defina o IP público como **estático**.  
5. Clique em **Review + Create** e, após a validação, selecione **Create**.  

---

### Acesso à VM via SSH  

Após a criação, conecte-se à VM utilizando o terminal:  

```bash
ssh -i /caminho/para/chave.pem azureuser@<ip-publico-da-vm>
```

---

### Instalação do Docker e Git na VM  

1. Atualize os pacotes do sistema:  
   ```bash
   sudo apt update && sudo apt upgrade -y
   ```  
2. Instale o Docker:  
   ```bash
   sudo apt install docker.io -y
   sudo systemctl start docker
   sudo systemctl enable docker
   ```  
3. Adicione o usuário ao grupo Docker para evitar o uso do `sudo`:  
   ```bash
   sudo usermod -aG docker $USER
   ```  
4. Desconecte-se e reconecte-se para aplicar as permissões:  
   ```bash
   exit
   ssh -i /caminho/para/chave.pem azureuser@<ip-publico-da-vm>
   ```

---

### Configuração da Aplicação e Dockerfile  

1. Clone o repositório com o código da aplicação:  
   ```bash
   git clone https://github.com/lvgalvao/hello-world-streamlit.git
   cd hello-world-streamlit
   ```  

2. Crie o **Dockerfile** se ainda não existir:  
   ```dockerfile
   FROM python:3.9-slim
   WORKDIR /app
   COPY requirements.txt .
   COPY app.py .
   RUN pip install --no-cache-dir -r requirements.txt
   EXPOSE 80
   CMD ["streamlit", "run", "app.py", "--server.port=80", "--server.address=0.0.0.0"]
   ```

---

### Construção e Execução da Imagem Docker  

1. **Construa a imagem Docker:**  
   ```bash
   docker build -t streamlit-app .
   ```  
2. **Rode o container na porta 80:**  
   ```bash
   docker run -d -p 80:80 streamlit-app
   ```  

---

### Acessando a Aplicação  

Abra o navegador e acesse a aplicação através do IP público da sua VM:  

```
http://<ip-publico-da-vm>
```

---

### Solução de Problemas  

1. **Erro de permissão com Docker:**  
   Se receber um erro de permissão, tente:  
   ```bash
   sudo docker run -d -p 80:80 streamlit-app
   ```  

2. **Problemas de conexão via HTTP:**  
   - Verifique se a porta 80 está aberta no **NSG**.  
   - Confirme que o container está rodando:  
     ```bash
     docker ps
     ```

---

### Parar o Container (Opcional)  

1. Liste os containers em execução:  
   ```bash
   docker ps
   ```  
2. Pare o container usando o ID:  
   ```bash
   docker stop <container_id>
   ```

---

Agora você tem sua aplicação **Streamlit "Hello World"** rodando em uma **VM do Azure** utilizando HTTP na porta 80, pronta para ser acessada e utilizada.

---

## Azure Active Directory (AAD)  
O AAD é o sistema de identidade e controle de acesso do Azure, semelhante ao IAM da AWS.

### Criando um Usuário e Atribuindo Funções  
1. No portal, vá em **Azure Active Directory** > **Usuários** > **Novo usuário**.  
2. Configure nome, função (Contributor) e senha inicial.  
3. No Access Control (IAM), adicione o usuário como Contributor na VM.

---

## Automatizando Uploads para Azure Blob Storage  
Exemplo de código em Python:
```python
from azure.storage.blob import BlobServiceClient

connect_str = "sua-connection-string"
service_client = BlobServiceClient.from_connection_string(connect_str)
container_client = service_client.get_container_client("dados-clientes")
container_client.create_container()

with open("exemplo.csv", "rb") as data:
    container_client.upload_blob(data)

print("Upload concluído!")
```

---

## Comparação: Azure vs AWS  
| Serviço                 | Azure                      | AWS                   |
|-------------------------|----------------------------|-----------------------|
| Armazenamento           | Azure Blob Storage         | Amazon S3             |
| Máquinas Virtuais       | Azure Virtual Machines     | EC2                   |
| Controle de Acesso      | Azure Active Directory     | IAM                   |
| Banco de Dados          | Azure SQL Database         | Amazon RDS            |
| Rede Virtual            | Virtual Network (VNet)     | VPC                   |
| Automação               | Azure Functions            | AWS Lambda            |

---

## Boas Práticas no Azure  
- **Automação com CLI:** Use Azure CLI para tarefas repetitivas.  
- **Tags:** Aplique tags para organizar e controlar custos.  
- **Backup:** Use replicação GRS para resiliência.  
- **Orçamento:** Configure alertas no Cost Management.

---

## Conclusão  
Esta aula apresentou conceitos fundamentais do Azure e sua comparação com a AWS, mostrando como configurar contas, VMs, Storage Accounts e identidade. Na próxima aula, exploraremos pipelines de dados no Azure com Data Factory e SQL Database.

Sim, a **VNet (Virtual Network)** no Azure é equivalente à **VPC (Virtual Private Cloud)** na AWS, embora cada uma tenha suas particularidades. Ambas são usadas para criar redes isoladas onde você pode executar seus recursos, como máquinas virtuais e bancos de dados, garantindo segurança e controle de tráfego.

### **Comparação entre VNet e VPC**

| **Aspecto**                     | **VNet (Azure)**                           | **VPC (AWS)**                          |
|----------------------------------|--------------------------------------------|---------------------------------------|
| **Propósito**                   | Rede virtual para isolar recursos no Azure | Rede virtual para isolar recursos na AWS |
| **Isolamento**                   | Totalmente isolada de outras VNets          | Totalmente isolada de outras VPCs    |
| **Sub-redes**                    | Suporta múltiplas sub-redes dentro da VNet | Suporta múltiplas sub-redes na VPC   |
| **Controle de Tráfego**          | NSG (Network Security Group) para regras de segurança | Security Groups e NACLs (Network ACLs) |
| **Conectividade entre Redes**   | VNet Peering                               | VPC Peering                          |
| **Gateway VPN**                  | VPN Gateway para conectar on-premises ou outras VNets | VPN Gateway para conectar on-premises ou outras VPCs |
| **CIDR Blocks**                  | Define o intervalo IP privado com CIDR     | Define o intervalo IP privado com CIDR |
| **DNS**                         | Azure-provided DNS ou customizado          | AWS-provided DNS ou customizado      |
| **Gateway NAT**                  | NAT Gateway para acesso à internet         | NAT Gateway ou NAT Instance          |
| **Firewall**                    | Azure Firewall                            | AWS Network Firewall                 |

### **Principais Semelhanças**
- **Isolamento:** Ambas permitem criar redes isoladas para manter a segurança e controle dos recursos.
- **Sub-redes:** Em ambas, você pode definir várias sub-redes para segmentar diferentes tipos de recursos.
- **Conectividade:** Tanto a VNet quanto a VPC suportam **peering**, permitindo comunicação entre redes.

### **Diferenças Notáveis**
1. **Controle de Tráfego:** 
   - Azure usa **NSGs (Network Security Groups)** para definir regras de tráfego para sub-redes e VMs.
   - AWS utiliza **Security Groups** e **Network ACLs**.

2. **Peering Global:**
   - O Azure permite **VNet Peering** global entre regiões.
   - A AWS também suporta **VPC Peering** entre contas e regiões, mas com algumas limitações dependendo do tipo de configuração.

3. **Firewall:**
   - O Azure oferece o **Azure Firewall** como um serviço integrado para proteção avançada.
   - A AWS possui o **AWS Network Firewall** e outras ferramentas como **WAF** para proteção.

### **Exemplo de Uso em Engenharia de Dados**
- No Azure, você pode criar uma **VNet** e definir sub-redes específicas para hospedar uma **máquina virtual** que processa dados.
- Essa VM pode acessar dados armazenados em uma **Storage Account** (como Blob Storage) pela VNet, garantindo que o tráfego não passe pela internet pública.

Essa arquitetura é ideal para pipelines de dados seguros e de alta performance, como processamento de arquivos CSV para Parquet, conforme o projeto que você está desenvolvendo.

### **Projeto 5. Criação de Banco de Dados SQL e Inserção de Dados com Streamlit**

Necessário baixar o driver de sqlserver

https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?redirectedfrom=MSDN&view=sql-server-ver16

#### **1. Criando o Banco de Dados SQL no Azure**

1. Acesse o portal Azure ([portal.azure.com](https://portal.azure.com)).
2. Navegue para **SQL Databases** e clique em **Create**.
3. **Configurações Básicas**:
   - Nome do Banco: `meubancodedados`.
   - Servidor: Crie um novo servidor ou selecione um existente.
   - Grupo de Recursos: Selecione ou crie um novo grupo, por exemplo, `myResourceGroup`.
   - Região: Selecione **Brazil South** ou outra região próxima.
   - Elastic Pool: Escolha **Não** (se não for usar um pool).
4. **Compute + Storage**:
   - Escolha **General Purpose** com 2 vCores.
   - Defina 32 GB de armazenamento e redundância local ou geo-redundante.
5. Clique em **Review + Create** e depois em **Create**.

#### **2. Configurando o Acesso (IAM)**

1. Vá até o banco de dados criado e clique em **Access Control (IAM)**.
2. Adicione uma função **Contributor** ou **Data Reader** ao serviço usado no **App Registration** que foi criado anteriormente.
3. Copie a **string de conexão** do banco para utilizá-la no código.

#### **3. Configurando o Projeto em Python com Streamlit**

```sql
CREATE TABLE pessoas (
    id INT IDENTITY(1,1) PRIMARY KEY,  -- Coluna com auto incremento
    nome VARCHAR(100) NOT NULL,         -- Nome com limite de 100 caracteres
    idade INT NOT NULL                  -- Idade como número inteiro
);
```

1. Instale as dependências no terminal:
   ```bash
   pip install streamlit pyodbc python-dotenv
   ```
2. Crie um arquivo **`.env`** com as variáveis:
   ```
   AZURE_DB_SERVER=<seu-servidor>.database.windows.net
   AZURE_DB_NAME=meubancodedados
   AZURE_DB_USER=<seu-usuario>
   AZURE_DB_PASSWORD=<sua-senha>
   ```
3. Crie o seguinte **código Python** no arquivo `app.py`:

   ```python
   import streamlit as st
   import pyodbc
   from dotenv import load_dotenv
   import os

   # Carregar variáveis de ambiente
   load_dotenv()

   # Conectar ao banco de dados
   server = os.getenv("AZURE_DB_SERVER")
   database = os.getenv("AZURE_DB_NAME")
   username = os.getenv("AZURE_DB_USER")
   password = os.getenv("AZURE_DB_PASSWORD")
   
   connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'
   conn = pyodbc.connect(connection_string)
   cursor = conn.cursor()

   # Interface Streamlit
   st.title("Inserção de Dados no Banco SQL")

   nome = st.text_input("Nome")
   idade = st.number_input("Idade", min_value=0, max_value=120)

   if st.button("Inserir Dados"):
       cursor.execute(f"INSERT INTO pessoas (nome, idade) VALUES (?, ?)", (nome, idade))
       conn.commit()
       st.success("Dados inseridos com sucesso!")

   cursor.close()
   conn.close()
   ```

4. Execute o **Streamlit**:
   ```bash
   streamlit run app.py
   ```

---

### **Mermaid: Inserção de Dados com Streamlit e Azure SQL Database**

```mermaid
sequenceDiagram
    participant User as Usuário (Interface Streamlit)
    participant Streamlit as Streamlit App
    participant Env as .env (Variáveis de Ambiente)
    participant SQLDB as Banco de Dados SQL (Azure)
    participant IAM as IAM (Controle de Acesso)

    User->>Streamlit: 1. Preenche nome e idade na UI
    User->>Streamlit: 2. Clica no botão "Inserir Dados"
    
    Streamlit->>Env: 3. Carrega variáveis de ambiente (.env)
    Env-->>Streamlit: 4. Retorna credenciais do banco de dados
    
    Streamlit->>IAM: 5. Solicita acesso ao banco SQL
    IAM-->>Streamlit: 6. Concede acesso (verificação do IAM)
    
    Streamlit->>SQLDB: 7. Conecta ao banco de dados
    SQLDB-->>Streamlit: 8. Conexão estabelecida
    
    Streamlit->>SQLDB: 9. Insere dados na tabela 'pessoas'
    SQLDB-->>Streamlit: 10. Confirmação de inserção

    Streamlit->>User: 11. Exibe mensagem de sucesso
```

---

### **Conclusão**

Com este projeto, você agora possui uma aplicação em **Streamlit** conectada a um banco de dados **SQL no Azure**. A interface permite a inserção de dados e a configuração utiliza boas práticas, como o uso de **variáveis de ambiente** e **controle de acesso IAM**.

================================================
File: /Bootcamp - Cloud para dados/Aula_16_17/Dockerfile
================================================
# Imagem base oficial do Python
FROM python:3.9-slim

# Define o diretório de trabalho dentro do container
WORKDIR /app

# Copia o arquivo de requisitos e o código para o container
COPY requirements.txt .
COPY app.py .

# Instala o Streamlit e outras dependências
RUN pip install --no-cache-dir -r requirements.txt

# Define a porta padrão que o Streamlit usará
EXPOSE 80

# Comando para rodar a aplicação Streamlit
CMD ["streamlit", "run", "app.py", "--server.port=80", "--server.enableCORS=false"]


================================================
File: /Bootcamp - Cloud para dados/Aula_16_17/README-resourcegroup.md
================================================
### **Resource Group no Azure**

O **Resource Group** no Azure é uma unidade lógica para **organizar, gerenciar e agrupar recursos** relacionados, como VMs, bancos de dados, contas de armazenamento e redes virtuais. Ele facilita o gerenciamento de infraestrutura em nuvem, garantindo que todos os recursos de um projeto ou serviço específico estejam concentrados em um único local.

---

### **Características Principais do Resource Group**

1. **Organização e Gestão Centralizada:**
   - Todos os recursos de uma aplicação ou projeto ficam organizados em um único grupo.
   - Exemplo: Uma aplicação pode ter VMs, bancos de dados e uma VNet no mesmo Resource Group.

2. **Controle de Acesso e Permissões:**
   - Permissões podem ser atribuídas ao Resource Group inteiro usando **Azure Role-Based Access Control (RBAC)**.
   - Assim, equipes específicas podem ter acesso apenas aos recursos dentro do grupo correspondente.

3. **Monitoramento e Gestão de Custos:**
   - Cada Resource Group pode ter seu custo monitorado separadamente.
   - Ajuda a visualizar e controlar o orçamento de projetos específicos.

4. **Aplicação de Tags:**
   - Tags podem ser aplicadas para facilitar a **organização e busca** de recursos com base em critérios como projeto, cliente ou departamento.

---

### **Casos de Uso do Resource Group**

- **Isolamento de Ambientes:**  
  Organize ambientes de **produção, desenvolvimento e teste** em Resource Groups separados.

- **Projetos Multi-Equipe:**  
  Múltiplos times trabalhando em diferentes serviços podem criar grupos específicos para gerenciar seus recursos sem interferir entre si.

- **Automação:**  
  Scripts de **Infraestrutura como Código (IaC)**, como ARM Templates e Terraform, utilizam Resource Groups para provisionar e gerenciar recursos.

---

### **Benefícios de Usar Resource Groups**

- **Escalabilidade e Flexibilidade:**  
  Recursos podem ser adicionados e removidos rapidamente dentro de um grupo.

- **Governança Simplificada:**  
  Atribuir políticas de segurança e controle de acesso a grupos inteiros facilita a governança.

- **Gerenciamento Facilitado:**  
  Operações como **backup**, **exportação** ou **deleção** podem ser feitas no nível do Resource Group, afetando todos os recursos contidos nele.

---

### **Como Criar um Resource Group no Azure**

1. **Acessar o Portal do Azure:**  
   - [https://portal.azure.com](https://portal.azure.com)

2. **Ir para Resource Groups:**  
   - No menu lateral, selecione **Resource Groups** e clique em **Create**.

3. **Preencher Informações:**
   - **Nome:** Defina um nome significativo (ex.: `RG-ProjetoX`).
   - **Região:** Escolha a região onde os recursos serão provisionados (ex.: **East US**).
   - **Tags:** (Opcional) Aplique tags para facilitar o controle de custos e organização.

4. **Criar o Resource Group:**  
   - Clique em **Review + Create** e em seguida **Create**.

---

### **Conclusão**
O **Resource Group** é essencial para o gerenciamento eficaz dos recursos no Azure. Ele não apenas organiza recursos, mas também simplifica a **administração**, **segurança** e **monitoramento** de projetos e serviços em nuvem.

### **Resource Group no Azure e o Equivalente na AWS**

Na **AWS**, o equivalente direto ao **Resource Group** do Azure é o **AWS Resource Groups**. Ambos servem para organizar e gerenciar recursos relacionados, mas existem algumas diferenças na implementação e uso.

---

### **Comparação: Resource Group (Azure) vs. AWS Resource Groups**

| **Aspecto**               | **Resource Group (Azure)**                     | **Resource Groups (AWS)**                    |
|---------------------------|-------------------------------------------------|---------------------------------------------|
| **Propósito**              | Organizar recursos relacionados por projeto ou serviço. | Agrupar e gerenciar recursos por projeto, ambiente ou finalidade. |
| **Controle de Acesso**     | Gerenciado via **Azure Role-Based Access Control (RBAC)** aplicado ao grupo inteiro. | Controle de acesso configurado por **IAM Policies** e **Tags**. |
| **Monitoramento de Custos**| Monitoramento de custos do grupo no **Cost Management**. | Visualização de custos no **AWS Cost Explorer** por tags ou grupos. |
| **Organização**            | Agrupa múltiplos recursos em um único container lógico. | Agrupa recursos por meio de **tags** aplicadas. |
| **Tags**                   | Tags são opcionais e ajudam na categorização. | Tags são fundamentais e podem ser usadas para definir Resource Groups dinamicamente. |

---

### **Funcionamento do AWS Resource Groups**

1. **Organização e Tags:**
   - Os recursos na AWS não são "alocados fisicamente" dentro de um grupo, mas organizados e filtrados usando **tags**.
   - Exemplo: Todos os recursos (EC2, RDS, S3) de um projeto podem ter a tag `Projeto: X`.

2. **Gerenciamento Centralizado:**
   - O AWS Resource Groups permite visualizar, gerenciar e executar operações em massa para todos os recursos associados a um projeto ou serviço específico.

3. **Controle de Acesso:**  
   - O controle é feito via **IAM Policies**, que podem conceder permissões com base em tags aplicadas aos recursos.

4. **Monitoramento e Custos:**
   - Utilizando o **AWS Cost Explorer** e **AWS Budgets**, é possível monitorar o custo de recursos organizados por tags ou grupos.

---

### **Quando Usar AWS Resource Groups?**

- **Ambientes Multi-Projetos:**  
  Organize recursos por projetos usando tags como `Projeto: Marketing` ou `Projeto: Financeiro`.

- **Isolamento de Ambientes:**  
  Defina ambientes de **produção**, **desenvolvimento** e **teste** utilizando tags para cada ambiente.

- **Automação:**  
  Scripts como Terraform e CloudFormation usam tags para organizar e automatizar a criação e gerenciamento de recursos.

---

### **Como Criar um Resource Group na AWS**

1. **Acessar o Console AWS:**  
   - [https://aws.amazon.com/console/](https://aws.amazon.com/console/)

2. **Ir para Resource Groups:**  
   - No menu superior, clique em **Resource Groups** > **Create Group**.

3. **Definir Critérios do Grupo:**
   - Nomeie o grupo e defina **filtros de tags**. Por exemplo: `Projeto: X`.

4. **Revisar e Criar:**  
   - Revise as configurações e clique em **Create Group**.

---

### **Conclusão**
Embora **Azure Resource Group** e **AWS Resource Groups** compartilhem a função de organizar e gerenciar recursos relacionados, a AWS depende fortemente de **tags** para criar grupos dinâmicos, enquanto o Azure usa uma abordagem mais direta com grupos lógicos. Ambos simplificam o gerenciamento e são essenciais para controle de custos e governança na nuvem.

================================================
File: /Bootcamp - Cloud para dados/Aula_16_17/app.py
================================================
import streamlit as st

# Título do aplicativo
st.title("Hello, World! com Streamlit")

# Exibindo uma mensagem de boas-vindas
st.write("Este é um exemplo simples de uma aplicação Streamlit em Docker.")

# Adicionando um botão de interação
if st.button("Clique aqui"):
    st.success("Você clicou no botão!")


================================================
File: /Bootcamp - Cloud para dados/Aula_16_17/projeto_01.py
================================================
from azure.identity import ClientSecretCredential  # Importa a classe para autenticação usando Client Secret
from azure.keyvault.secrets import SecretClient  # Importa a classe para manipular segredos no Key Vault
from dotenv import load_dotenv  # Importa a função para carregar variáveis de ambiente de um arquivo .env
import os  # Importa o módulo para acessar variáveis de ambiente

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém as variáveis de ambiente necessárias para a autenticação
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
vault_url = os.environ["AZURE_VAULT_URL"]

# Nome do segredo a ser acessado no Key Vault
secret_name = "ExemploKey"

# Cria uma credencial para autenticação no Azure utilizando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id, 
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Cria o cliente do Key Vault para acessar os segredos
secret_client = SecretClient(vault_url=vault_url, credential=credentials)

# Recupera o valor do segredo a partir do Key Vault
secret = secret_client.get_secret(secret_name)

# Exibe o valor do segredo no terminal
print("O valor do segredo é: " + secret.value)


================================================
File: /Bootcamp - Cloud para dados/Aula_16_17/projeto_02.py
================================================
from azure.identity import ClientSecretCredential
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv
import os

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Variáveis de ambiente necessárias
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
storage_account_url = os.environ["AZURE_STORAGE_URL"]  # Ex: https://<nome_da_storage>.blob.core.windows.net/
container_name = "meucontainer"  # Nome do container criado

# Configura as credenciais usando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id,
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Conectar ao Blob Storage
blob_service_client = BlobServiceClient(
    account_url=storage_account_url,
    credential=credentials
)

# Acessa o container
container_client = blob_service_client.get_container_client(container_name)

# Lista todos os arquivos dentro do container
print(f"Listando arquivos no container '{container_name}':")
for blob in container_client.list_blobs():
    print(f" - {blob.name}")


================================================
File: /Bootcamp - Cloud para dados/Aula_16_17/projeto_03.py
================================================
import streamlit as st
from azure.identity import ClientSecretCredential
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv
import os

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Variáveis de ambiente necessárias
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
storage_account_url = os.environ["AZURE_STORAGE_URL"]
container_name = "meucontainer"

# Configura credenciais usando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id,
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Conectar ao Blob Storage
blob_service_client = BlobServiceClient(
    account_url=storage_account_url,
    credential=credentials
)

# Acessa o container
container_client = blob_service_client.get_container_client(container_name)

# Função para upload de arquivo
def upload_file(file):
    try:
        blob_client = container_client.get_blob_client(file.name)
        blob_client.upload_blob(file, overwrite=True)
        st.success(f"Arquivo '{file.name}' enviado com sucesso!")
    except Exception as e:
        st.error(f"Erro ao enviar arquivo: {str(e)}")

# Função para listar arquivos no container
def listar_arquivos():
    try:
        blobs = container_client.list_blobs()
        return [blob.name for blob in blobs]
    except Exception as e:
        st.error(f"Erro ao listar arquivos: {str(e)}")
        return []

# Interface do Streamlit
st.title("Upload para Azure Blob Storage")

uploaded_file = st.file_uploader("Escolha um arquivo para enviar", type=["csv", "txt", "png", "jpg", "pdf"])

if uploaded_file is not None:
    if st.button("Enviar"):
        upload_file(uploaded_file)

st.subheader("Arquivos no Container")
arquivos = listar_arquivos()
if arquivos:
    for arquivo in arquivos:
        st.write(f"- {arquivo}")
else:
    st.write("Nenhum arquivo encontrado.")


================================================
File: /Bootcamp - Cloud para dados/Aula_16_17/projeto_05.py
================================================
import streamlit as st
import pyodbc
import os
from dotenv import load_dotenv

# Carregar variáveis de ambiente
load_dotenv()

# Configuração do banco de dados
server = os.getenv("DB_SERVER")
database = os.getenv("DB_NAME")
username = os.getenv("DB_USERNAME")
password = os.getenv("DB_PASSWORD")
connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'

# Função para conectar ao banco
def connect_to_database():
    try:
        conn = pyodbc.connect(connection_string)
        return conn
    except Exception as e:
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        return None

# Função para inserir dados
def insert_data(nome, idade):
    conn = connect_to_database()
    if conn:
        cursor = conn.cursor()
        try:
            cursor.execute("INSERT INTO pessoas (nome, idade) VALUES (?, ?)", (nome, idade))
            conn.commit()
            st.success(f"Dados inseridos: {nome}, {idade} anos")
        except Exception as e:
            st.error(f"Erro ao inserir dados: {e}")
        finally:
            conn.close()

# Interface Streamlit
st.title("Inserção de Dados no Banco de Dados SQL")

nome = st.text_input("Nome:")
idade = st.number_input("Idade:", min_value=0, step=1)

if st.button("Inserir Dados"):
    if nome and idade:
        insert_data(nome, idade)
    else:
        st.warning("Preencha todos os campos.")


================================================
File: /Bootcamp - Cloud para dados/Aula_16_17/requirements.txt
================================================
streamlit
python-dotenv
azurealtair==5.4.1
attrs==24.2.0
azure-core==1.31.0
azure-identity==1.19.0
azure-keyvault==4.2.0
azure-keyvault-certificates==4.9.0
azure-keyvault-keys==4.10.0
azure-keyvault-secrets==4.9.0
azure-storage-blob==12.23.1
blinker==1.8.2
cachetools==5.5.0
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
click==8.1.7
colorama==0.4.6
cryptography==43.0.3
gitdb==4.0.11
GitPython==3.1.43
idna==3.10
isodate==0.7.2
Jinja2==3.1.4
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
markdown-it-py==3.0.0
MarkupSafe==3.0.2
mdurl==0.1.2
msal==1.31.0
msal-extensions==1.2.0
narwhals==1.11.1
numpy==2.1.2
packaging==24.1
pandas==2.2.3
pillow==10.4.0
portalocker==2.10.1
protobuf==5.28.3
pyarrow==18.0.0
pycparser==2.22
pydeck==0.9.1
Pygments==2.18.0
PyJWT==2.9.0
pyodbc==5.2.0
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytz==2024.2
pywin32==308
referencing==0.35.1
requests==2.32.3
rich==13.9.3
rpds-py==0.20.0
six==1.16.0
smmap==5.0.1
streamlit==1.39.0
tenacity==9.0.0
toml==0.10.2
tornado==6.4.1
typing_extensions==4.12.2
tzdata==2024.2
urllib3==2.2.3
watchdog==5.0.3


================================================
File: /Bootcamp - Cloud para dados/Aula_18_19/README.md
================================================
# **Bootcamp Cloud - Aula 16: Introdução ao Azure para Dados**  

---

### **Objetivo:**  
Explorar o ambiente do Azure, abordando a criação de conta, uso do **Azure Blob Storage** (equivalente ao S3), **máquinas virtuais (VMs)** para processamento de dados e o **Azure Active Directory (AAD)**, que substitui o IAM para gerenciamento de identidade e controle de acesso.

---

## Criação de Conta no Azure  
1. Acesse [portal.azure.com](https://portal.azure.com) e selecione **Create a Free Account**.  
2. Preencha as informações básicas: nome completo, telefone, e-mail e senha.  
3. Adicione as informações de **cartão de crédito** (somente para verificação; não haverá cobranças iniciais).  
4. Escolha o plano gratuito:  
   - 750 horas de uma VM B1S por mês (Ubuntu ou Windows).  
   - 5 GB gratuitos no Blob Storage.  
   - Banco de dados SQL com 250 GB gratuitos.  
5. No **Cost Management**, configure alertas para controlar custos e crie um orçamento mensal de teste.

---

## Assinaturas no Azure  
Uma **Assinatura do Azure** é uma unidade lógica para gerenciar o acesso, controle de custos e recursos. Permite organizar e isolar recursos como máquinas virtuais, bancos de dados e serviços.  

### Funções e Finalidades  
- **Gestão de Recursos e Acesso:** Define permissões específicas para equipes e projetos.  
- **Faturamento e Controle de Custos:** Cada assinatura tem seu próprio relatório de faturamento.  
- **Isolamento e Limites:** Permite separar ambientes de produção, desenvolvimento e teste.  
- **Governança:** As assinaturas aplicam políticas e limites via Grupos de Gerenciamento e AAD.

### Tipos de Assinaturas  
- **Plano Gratuito:** Oferece crédito inicial e limites gratuitos por 12 meses.  
- **Pay-As-You-Go:** Paga-se apenas pelo uso.  
- **Enterprise Agreement (EA):** Contrato com desconto por volume para grandes empresas.  
- **Cloud Solution Provider (CSP):** Parceiros que revendem serviços Azure.

---

## Equivalente na AWS  
Na AWS, o conceito mais próximo de uma assinatura do Azure é uma **Conta AWS**.  

- **AWS Organizations:** Permite consolidar e gerenciar várias contas AWS sob uma organização.  
- **Controle de Custos:** Cada conta tem seus próprios alertas e relatórios de custo.  

### Comparação entre Assinatura do Azure e Conta AWS  
| Azure (Assinatura)             | AWS (Conta AWS)                       |
|---------------------------------|----------------------------------------|
| Separa recursos e define limites | Isola ambientes e projetos            |
| Gerenciada por AAD              | Gerenciada por IAM e AWS Organizations |
| Faturamento por assinatura      | Faturamento por conta                 |

---

## Projeto 1. Acessando variáveis de ambiente no Azure

```mermaid
sequenceDiagram
    participant User as Usuário (Local)
    participant AzureAD as Azure AD
    participant AppReg as App Registration
    participant KeyVault as Azure Key Vault
    participant IAMUser as IAM do Usuário
    participant IAMService as IAM do Serviço

    User->>AzureAD: 1. Solicita Token de Acesso
    AzureAD-->>User: 2. Retorna Token

    User->>AppReg: 3. Envia Credenciais e Token
    AppReg-->>User: 4. Validação e Permissão

    User->>KeyVault: 5. Solicita Segredo (com Token)
    KeyVault-->>IAMUser: 6. Verifica Permissões do Usuário
    KeyVault-->>IAMService: 7. Verifica Permissões do Serviço

    KeyVault-->>User: 8. Retorna Valor do Segredo

    User->>User: 9. Exibe Segredo no Terminal
```

---

### **Azure Key Vault: O que é e para que serve?**

### **Objetivo:**
O **Azure Key Vault** é um serviço de nuvem da Microsoft projetado para gerenciar segredos, chaves de criptografia e certificados de maneira segura. Seu principal objetivo é fornecer uma forma centralizada e protegida de armazenar e acessar informações sensíveis, como senhas, tokens e chaves criptográficas, minimizando riscos de segurança.

### **Funcionalidades do Azure Key Vault:**

1. **Armazenamento Seguro de Segredos:**
   - Gerencia senhas, strings de conexão, tokens de API e outras informações confidenciais.
   - Fornece acesso seguro aos segredos por meio de autenticação robusta e controle de permissões via **Azure AD**.

2. **Gerenciamento de Chaves de Criptografia:**
   - Armazena e protege chaves usadas para criptografia de dados em repouso e em trânsito.
   - Oferece suporte para criptografia assimétrica e simétrica, podendo ser integrado a outros serviços do Azure, como SQL Database.

3. **Gestão de Certificados:**
   - Automatiza o processo de renovação e gerenciamento de certificados SSL/TLS.
   - Permite criar e manter certificados seguros que podem ser utilizados por serviços na nuvem ou locais.

4. **Controle de Acesso Granular:**
   - Usa **IAM (Identity and Access Management)** para controlar quais aplicações e usuários podem acessar os segredos e chaves.
   - Mantém logs de auditoria de todas as interações com o Key Vault.

### **Benefícios:**
- **Centralização:** Consolida a gestão de segredos e chaves, evitando que sejam armazenados em diferentes locais de forma não segura.
- **Automação:** Facilita a renovação automática de certificados e permite que aplicações acessem segredos sem intervenção manual.
- **Conformidade:** Ajuda a cumprir regulamentações de segurança e privacidade, mantendo as informações críticas em um ambiente protegido.
- **Alta Disponibilidade:** O serviço é distribuído por regiões do Azure, garantindo que segredos e chaves estejam sempre acessíveis.

---

### **Comparação com AWS Secrets Manager:**

| **Azure Key Vault**                    | **AWS Secrets Manager**                |
|----------------------------------------|----------------------------------------|
| Gerencia segredos, chaves e certificados | Gerencia segredos e credenciais |
| Integra-se com serviços Azure, como SQL Database | Integra-se com serviços AWS, como RDS |
| Oferece criptografia usando HSM (Hardware Security Modules) | Oferece criptografia e rotação automática de segredos |
| Controle de acesso via Azure AD        | Controle de acesso via IAM |
| Preços por transação e armazenamento   | Preços por segredos armazenados |

---

### **Nosso primeiro Erro**

Importância do IAM
O uso do IAM (Identity and Access Management) é essencial para criar e gerenciar um Key Vault. Ele permite configurar controle de acesso baseado em papéis (RBAC), garantindo que apenas usuários ou aplicações autorizadas possam acessar ou modificar segredos e chaves.

IAM no Azure é o serviço responsável por gerenciar identidades e permissões, assegurando que cada recurso ou aplicação tenha apenas os acessos necessários. Ele garante uma abordagem de segurança baseada em princípio de menor privilégio.

## **View Exemple**

### **"View Example" no Portal Azure Key Vault**

A opção **"View Example"** (ou "Ver Exemplo") na interface do **Key Vault** facilita o entendimento e uso do serviço. Esta funcionalidade:

1. **Demonstração de Exemplos Práticos:**  
   - Mostra exemplos de como armazenar e acessar chaves e segredos.
   - Fornece código de amostra, muitas vezes em **Python, C#, ou PowerShell**, para acessar os segredos via SDK.

2. **Ajuda na Automação:**  
   - Exemplos incluem snippets para automação com **Azure CLI** ou scripts para aplicações.
   - Acelera o processo de integração com outras aplicações e pipelines de dados.

3. **Facilita a Configuração Inicial:**  
   - Apresenta instruções claras de como criar e acessar segredos, chaves e certificados.
   - Demonstra como configurar corretamente o acesso via **IAM** ou atribuir permissões específicas.

A funcionalidade **"View Example"** é uma poderosa ferramenta educacional e prática no portal Azure. Ela facilita a adoção do Azure Key Vault ao fornecer exemplos de código claros, economizando tempo e simplificando o processo de integração. Com essa abordagem orientada a exemplos, é possível configurar o acesso seguro e garantir a conformidade com as boas práticas de segurança na nuvem.

---

## **Azure Active Directory (Azure AD)**

### **Visão Geral**

O **Azure Active Directory (Azure AD)** é o serviço de gerenciamento de identidade e acesso baseado na nuvem da Microsoft. Ele permite que as organizações administrem identidades e controlem o acesso a recursos em nuvem e on-premises, além de aplicativos SaaS. 

---

### **Objetivos do Azure AD**

1. **Autenticação e Autorização**  
   - Verifica identidades para permitir ou negar o acesso a recursos e aplicativos.
   - Suporta métodos como senha, MFA (autenticação multi-fator) e biometria.

2. **Gerenciamento de Usuários e Grupos**  
   - Permite criar usuários e organizar grupos para definir permissões e acessos.
   - Facilita a ativação e desativação de contas de usuários.

3. **Integração com Aplicativos SaaS**  
   - Oferece autenticação única (SSO) para diversos aplicativos, como Salesforce e Google Workspace.
   - Melhora a experiência do usuário eliminando a necessidade de múltiplas senhas.

4. **Políticas de Acesso e Segurança**  
   - Controla o acesso com base na localização e tipo de dispositivo.
   - Oferece políticas de acesso condicional para aumentar a segurança.

5. **Autenticação para APIs e Aplicativos Customizados**  
   - Desenvolvedores podem integrar o Azure AD para autenticação segura em suas APIs e aplicações.

---

### **Passo a Passo: Configuração no Azure AD**

#### **1. Acessar o Azure AD no Portal**
- Vá para o [portal do Azure](https://portal.azure.com).
- No menu, clique em **Azure Active Directory**.

#### **2. Registrar um Aplicativo no Azure AD**
- Clique em **App Registrations** > **New Registration**.
- Dê um nome ao aplicativo e escolha a conta ou organização que terá acesso.
- Clique em **Register** para finalizar.

#### **3. Criar um Client Secret**
- Na aplicação registrada, vá para **Certificates & Secrets**.
- Clique em **New Client Secret**.
- Adicione uma descrição (opcional) e defina a validade do segredo (6, 12 ou 24 meses).
- Ao clicar em **Add**, o segredo será gerado. **Copie o valor agora** – ele não será exibido novamente.

#### **4. Configurar IAM para o Serviço**
- Volte para o **Azure Key Vault** ou outro serviço relevante.
- Vá para **Access Control (IAM)** > **Role Assignments**.
- Adicione um novo papel, como **Key Vault Secrets User** ou **Reader**.
- Em **Select Members**, selecione o **App Registration** que você criou.

---

### **Relação entre Azure AD e AWS IAM**

| **Azure AD**                                | **AWS IAM**                                  |
|---------------------------------------------|----------------------------------------------|
| Gerencia identidade de usuários e autenticação. | Gerencia permissões de recursos na AWS. |
| Oferece SSO e políticas de acesso condicional. | Configura políticas detalhadas para acesso a recursos. |
| Integrado com Office 365 e aplicativos SaaS. | Integrado com serviços AWS, como EC2 e S3. |
| Suporta políticas baseadas em usuário e dispositivo. | Suporta permissões detalhadas por função e políticas. |

---

### **Conclusão**

O **Azure AD** é essencial para a autenticação e controle de identidades em ambientes Microsoft e aplicativos SaaS. Ele complementa o **IAM da AWS**, que é mais voltado para gerenciar permissões de recursos. Integrar o Azure AD com a AWS pode proporcionar uma experiência de autenticação unificada e simplificar a gestão de identidades e acessos.

## Projeto 2. Consumindo arquivos no Blob Storage

```mermaid
sequenceDiagram
    participant User as Usuário (Local)
    participant AzureAD as Azure AD
    participant AppReg as App Registration
    participant BlobStorage as Azure Blob Storage
    participant IAMUser as IAM do Usuário
    participant IAMService as IAM do Serviço

    User->>AzureAD: 1. Solicita Token de Acesso
    AzureAD-->>User: 2. Retorna Token

    User->>AppReg: 3. Envia Credenciais e Token
    AppReg-->>User: 4. Validação e Permissão

    User->>BlobStorage: 5. Solicita Listagem de Arquivos (com Token)
    BlobStorage-->>IAMUser: 6. Verifica Permissões do Usuário
    BlobStorage-->>IAMService: 7. Verifica Permissões do Serviço

    BlobStorage-->>User: 8. Retorna Lista de Arquivos

    User->>BlobStorage: 9. Baixa Arquivo Específico
    BlobStorage-->>User: 10. Envia Arquivo

    User->>User: 11. Exibe ou Processa Arquivo Localmente
```

### **Projeto: Acessar e Fazer Upload de Arquivos no Azure Blob Storage com Python SDK**

Este passo a passo mostrará como criar uma **Storage Account no Azure**, configurar permissões usando **IAM**, e listar arquivos dentro de um **container Blob** utilizando o **Python SDK**.

---

## **Passo 1: Criar uma Storage Account no Azure**

1. Acesse o [Azure Portal](https://portal.azure.com/).
2. No menu de navegação, clique em **Storage accounts** e depois em **Create**.
3. **Configuração básica**:
   - **Subscription**: Escolha a assinatura correta.
   - **Resource group**: Selecione o grupo criado anteriormente.
   - **Storage account name**: Escolha um nome único (por exemplo, `armazenamentoexemplo`).
   - **Region**: Escolha a mesma região onde seus recursos estão hospedados (ex.: East US).
   - **Performance**: Standard.
   - **Replication**: LRS (Locally Redundant Storage) para este exemplo.
4. Clique em **Review + Create** e, em seguida, **Create**.

---

## **Passo 2: Criar um Container no Blob Storage**

1. Dentro da **Storage Account** recém-criada, vá para **Containers** no menu lateral.
2. Clique em **+ Container**.
3. **Nome do Container**: Por exemplo, `meus-arquivos`.
4. **Tipo de acesso público**: Deixe como **Private** (somente acesso autenticado).
5. Clique em **Create**.

---

## **Passo 3: Obter a URL de Conexão do Storage**

1. Na **Storage Account**, vá para **Access keys** no menu lateral.
2. Copie o **Connection string**. Ele será usado no código para conectar-se ao Blob Storage.

---

## **Passo 4: Dar Acesso no IAM ao Aplicativo Criado Anteriormente**

1. Acesse o **Azure Portal** e vá para **Storage accounts** > **sua Storage Account**.
2. No menu lateral, clique em **Access control (IAM)**.
3. Clique em **Add role assignment**.
4. **Função**: Selecione **Blob Data Contributor**.
5. **Membro**: Selecione o **App Registration** criado anteriormente no Azure Active Directory.
6. Clique em **Review + Assign**.

---

## **Passo 5: Código Python para Listar Arquivos no Blob Storage**

Crie um arquivo Python chamado `list_blob_files.py` e adicione o seguinte código:

### **Código Python**

```python
from azure.identity import ClientSecretCredential
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv
import os

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Variáveis de ambiente necessárias
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
storage_account_url = os.environ["AZURE_STORAGE_URL"]  # Ex: https://<nome_da_storage>.blob.core.windows.net/
container_name = "meus-arquivos"  # Nome do container criado

# Configura as credenciais usando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id,
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Conectar ao Blob Storage
blob_service_client = BlobServiceClient(
    account_url=storage_account_url,
    credential=credentials
)

# Acessa o container
container_client = blob_service_client.get_container_client(container_name)

# Lista todos os arquivos dentro do container
print(f"Listando arquivos no container '{container_name}':")
for blob in container_client.list_blobs():
    print(f" - {blob.name}")
```

---

## **Passo 6: Configurar o Arquivo `.env`**

Crie um arquivo chamado `.env` na mesma pasta do código e adicione as variáveis necessárias:

```env
AZURE_CLIENT_ID=seu_client_id
AZURE_TENANT_ID=seu_tenant_id
AZURE_CLIENT_SECRET=seu_client_secret
AZURE_STORAGE_URL=https://<nome_da_storage>.blob.core.windows.net/
```

---

## **Passo 7: Instalar as Dependências**

No terminal, execute o seguinte comando para instalar as bibliotecas necessárias:

```bash
pip install azure-identity azure-storage-blob python-dotenv
```

---

## **Passo 8: Executar o Código**

No terminal, execute o script:

```bash
python list_blob_files.py
```

---

## **Resultado Esperado**

Ao executar o código, você verá a lista de todos os arquivos presentes no container:

```
Listando arquivos no container 'meus-arquivos':
 - exemplo1.txt
 - relatorio2024.csv
 - imagem.png
```

---

## **Conclusão**

Com este projeto, você aprendeu a:

- **Criar uma Storage Account** e **container Blob** no Azure.
- **Configurar permissões IAM** para o aplicativo.
- **Listar arquivos** armazenados no container Blob usando **Python SDK**.

Este processo é fundamental para manipular dados na nuvem com segurança e eficiência, garantindo o acesso controlado por meio de credenciais e políticas de IAM.

## Projeto 3. Streamlit para inserir dados no Blob Storage

### **Código com Streamlit: Inserir Arquivos no Blob Storage**

Este exemplo utiliza o **Streamlit** para criar uma interface gráfica que permite ao usuário fazer upload de arquivos para o **Azure Blob Storage**.

---

#### **Instalar as Dependências**

Antes de começar, certifique-se de instalar as bibliotecas necessárias:

```bash
pip install streamlit azure-identity azure-storage-blob python-dotenv
```

---

#### **Código Python (app.py)**

```python
import streamlit as st
from azure.identity import ClientSecretCredential
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv
import os

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Variáveis de ambiente necessárias
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
storage_account_url = os.environ["AZURE_STORAGE_URL"]
container_name = "meucontainer"

# Configura credenciais usando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id,
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Conectar ao Blob Storage
blob_service_client = BlobServiceClient(
    account_url=storage_account_url,
    credential=credentials
)

# Acessa o container
container_client = blob_service_client.get_container_client(container_name)

# Função para upload de arquivo
def upload_file(file):
    try:
        blob_client = container_client.get_blob_client(file.name)
        blob_client.upload_blob(file, overwrite=True)
        st.success(f"Arquivo '{file.name}' enviado com sucesso!")
    except Exception as e:
        st.error(f"Erro ao enviar arquivo: {str(e)}")

# Função para listar arquivos no container
def listar_arquivos():
    try:
        blobs = container_client.list_blobs()
        return [blob.name for blob in blobs]
    except Exception as e:
        st.error(f"Erro ao listar arquivos: {str(e)}")
        return []

# Interface do Streamlit
st.title("Upload para Azure Blob Storage")

uploaded_file = st.file_uploader("Escolha um arquivo para enviar", type=["csv", "txt", "png", "jpg", "pdf"])

if uploaded_file is not None:
    if st.button("Enviar"):
        upload_file(uploaded_file)

st.subheader("Arquivos no Container")
arquivos = listar_arquivos()
if arquivos:
    for arquivo in arquivos:
        st.write(f"- {arquivo}")
else:
    st.write("Nenhum arquivo encontrado.")
```

---

#### **Configuração do Arquivo `.env`**

Crie um arquivo chamado `.env` e adicione as variáveis de ambiente:

```env
AZURE_CLIENT_ID=seu_client_id
AZURE_TENANT_ID=seu_tenant_id
AZURE_CLIENT_SECRET=seu_client_secret
AZURE_STORAGE_URL=https://<nome_da_storage>.blob.core.windows.net/
```

---

#### **Como Executar o Projeto**

1. **Inicie o Streamlit** com o seguinte comando:

   ```bash
   streamlit run app.py
   ```

2. **Acesse a interface** no navegador através do link fornecido no terminal (por exemplo, `http://localhost:8501`).

---

#### **O que este Código Faz?**

- **Upload de Arquivos**: O usuário pode selecionar um arquivo e enviá-lo para o **Blob Storage** clicando em "Enviar".
- **Listagem de Arquivos**: Todos os arquivos presentes no container são listados abaixo da interface.
- **Tratamento de Erros**: Mensagens de erro e sucesso são exibidas para garantir uma melhor experiência do usuário.

---

#### **Conclusão**

Com este projeto, você pode enviar e gerenciar arquivos diretamente no **Azure Blob Storage** através de uma interface simples e intuitiva criada com **Streamlit**.

---
## Projeto 4. Máquinas Virtuais (VMs) no Azure  
VMs no Azure permitem criar máquinas para processamento de dados e desenvolvimento de aplicações.  

### Configuração de uma VM  
1. No Azure Portal, vá em **Virtual Machine** > **Create**.  
2. Escolha a região e imagem (ex.: Ubuntu Server 20.04 LTS).  
3. Configure o tamanho (ex.: B1S).  
4. Escolha entre SSH ou senha como método de login.

### Configuração de Rede e Segurança  
1. Crie uma VNet e uma sub-rede para isolar a comunicação.  
2. Configure o NSG para liberar a porta 22 (SSH).  
3. Defina o IP como estático.

---

## Deploy da Aplicação no Azure VM  

Para rodar uma aplicação Streamlit em uma **VM do Azure** utilizando HTTP (porta 80), siga os passos abaixo.

---

### Configuração da VM no Azure  

1. No **Azure Portal**, vá em **Virtual Machines** > **Create** > **Azure Virtual Machine**.  
2. Preencha os detalhes básicos:  
   - Nome: `streamlit-vm`  
   - Região: **East US**  
   - Imagem: **Ubuntu Server 20.04 LTS**  
   - Tamanho: **B1S** (ou outro disponível no plano gratuito).  
3. Em **Opções de Autenticação**, selecione **Chave SSH** e gere um novo par de chaves ou utilize um existente.  
4. Configure a **Rede**:  
   - Crie uma nova **Virtual Network (VNet)** e uma **sub-rede**.  
   - Configure um **Network Security Group (NSG)** e adicione uma **regra de entrada** para liberar a **porta 80 (HTTP)**.  
   - Defina o IP público como **estático**.  
5. Clique em **Review + Create** e, após a validação, selecione **Create**.  

---

### Acesso à VM via SSH  

Após a criação, conecte-se à VM utilizando o terminal:  

```bash
ssh -i /caminho/para/chave.pem azureuser@<ip-publico-da-vm>
```

---

### Instalação do Docker e Git na VM  

1. Atualize os pacotes do sistema:  
   ```bash
   sudo apt update && sudo apt upgrade -y
   ```  
2. Instale o Docker:  
   ```bash
   sudo apt install docker.io -y
   sudo systemctl start docker
   sudo systemctl enable docker
   ```  
3. Adicione o usuário ao grupo Docker para evitar o uso do `sudo`:  
   ```bash
   sudo usermod -aG docker $USER
   ```  
4. Desconecte-se e reconecte-se para aplicar as permissões:  
   ```bash
   exit
   ssh -i /caminho/para/chave.pem azureuser@<ip-publico-da-vm>
   ```

---

### Configuração da Aplicação e Dockerfile  

1. Clone o repositório com o código da aplicação:  
   ```bash
   git clone https://github.com/lvgalvao/hello-world-streamlit.git
   cd hello-world-streamlit
   ```  

2. Crie o **Dockerfile** se ainda não existir:  
   ```dockerfile
   FROM python:3.9-slim
   WORKDIR /app
   COPY requirements.txt .
   COPY app.py .
   RUN pip install --no-cache-dir -r requirements.txt
   EXPOSE 80
   CMD ["streamlit", "run", "app.py", "--server.port=80", "--server.address=0.0.0.0"]
   ```

---

### Construção e Execução da Imagem Docker  

1. **Construa a imagem Docker:**  
   ```bash
   docker build -t streamlit-app .
   ```  
2. **Rode o container na porta 80:**  
   ```bash
   docker run -d -p 80:80 streamlit-app
   ```  

---

### Acessando a Aplicação  

Abra o navegador e acesse a aplicação através do IP público da sua VM:  

```
http://<ip-publico-da-vm>
```

---

### Solução de Problemas  

1. **Erro de permissão com Docker:**  
   Se receber um erro de permissão, tente:  
   ```bash
   sudo docker run -d -p 80:80 streamlit-app
   ```  

2. **Problemas de conexão via HTTP:**  
   - Verifique se a porta 80 está aberta no **NSG**.  
   - Confirme que o container está rodando:  
     ```bash
     docker ps
     ```

---

### Parar o Container (Opcional)  

1. Liste os containers em execução:  
   ```bash
   docker ps
   ```  
2. Pare o container usando o ID:  
   ```bash
   docker stop <container_id>
   ```

---

Agora você tem sua aplicação **Streamlit "Hello World"** rodando em uma **VM do Azure** utilizando HTTP na porta 80, pronta para ser acessada e utilizada.

---

## Azure Active Directory (AAD)  
O AAD é o sistema de identidade e controle de acesso do Azure, semelhante ao IAM da AWS.

### Criando um Usuário e Atribuindo Funções  
1. No portal, vá em **Azure Active Directory** > **Usuários** > **Novo usuário**.  
2. Configure nome, função (Contributor) e senha inicial.  
3. No Access Control (IAM), adicione o usuário como Contributor na VM.

---

## Automatizando Uploads para Azure Blob Storage  
Exemplo de código em Python:
```python
from azure.storage.blob import BlobServiceClient

connect_str = "sua-connection-string"
service_client = BlobServiceClient.from_connection_string(connect_str)
container_client = service_client.get_container_client("dados-clientes")
container_client.create_container()

with open("exemplo.csv", "rb") as data:
    container_client.upload_blob(data)

print("Upload concluído!")
```

---

## Comparação: Azure vs AWS  
| Serviço                 | Azure                      | AWS                   |
|-------------------------|----------------------------|-----------------------|
| Armazenamento           | Azure Blob Storage         | Amazon S3             |
| Máquinas Virtuais       | Azure Virtual Machines     | EC2                   |
| Controle de Acesso      | Azure Active Directory     | IAM                   |
| Banco de Dados          | Azure SQL Database         | Amazon RDS            |
| Rede Virtual            | Virtual Network (VNet)     | VPC                   |
| Automação               | Azure Functions            | AWS Lambda            |

---

## Boas Práticas no Azure  
- **Automação com CLI:** Use Azure CLI para tarefas repetitivas.  
- **Tags:** Aplique tags para organizar e controlar custos.  
- **Backup:** Use replicação GRS para resiliência.  
- **Orçamento:** Configure alertas no Cost Management.

---

## Conclusão  
Esta aula apresentou conceitos fundamentais do Azure e sua comparação com a AWS, mostrando como configurar contas, VMs, Storage Accounts e identidade. Na próxima aula, exploraremos pipelines de dados no Azure com Data Factory e SQL Database.

Sim, a **VNet (Virtual Network)** no Azure é equivalente à **VPC (Virtual Private Cloud)** na AWS, embora cada uma tenha suas particularidades. Ambas são usadas para criar redes isoladas onde você pode executar seus recursos, como máquinas virtuais e bancos de dados, garantindo segurança e controle de tráfego.

### **Comparação entre VNet e VPC**

| **Aspecto**                     | **VNet (Azure)**                           | **VPC (AWS)**                          |
|----------------------------------|--------------------------------------------|---------------------------------------|
| **Propósito**                   | Rede virtual para isolar recursos no Azure | Rede virtual para isolar recursos na AWS |
| **Isolamento**                   | Totalmente isolada de outras VNets          | Totalmente isolada de outras VPCs    |
| **Sub-redes**                    | Suporta múltiplas sub-redes dentro da VNet | Suporta múltiplas sub-redes na VPC   |
| **Controle de Tráfego**          | NSG (Network Security Group) para regras de segurança | Security Groups e NACLs (Network ACLs) |
| **Conectividade entre Redes**   | VNet Peering                               | VPC Peering                          |
| **Gateway VPN**                  | VPN Gateway para conectar on-premises ou outras VNets | VPN Gateway para conectar on-premises ou outras VPCs |
| **CIDR Blocks**                  | Define o intervalo IP privado com CIDR     | Define o intervalo IP privado com CIDR |
| **DNS**                         | Azure-provided DNS ou customizado          | AWS-provided DNS ou customizado      |
| **Gateway NAT**                  | NAT Gateway para acesso à internet         | NAT Gateway ou NAT Instance          |
| **Firewall**                    | Azure Firewall                            | AWS Network Firewall                 |

### **Principais Semelhanças**
- **Isolamento:** Ambas permitem criar redes isoladas para manter a segurança e controle dos recursos.
- **Sub-redes:** Em ambas, você pode definir várias sub-redes para segmentar diferentes tipos de recursos.
- **Conectividade:** Tanto a VNet quanto a VPC suportam **peering**, permitindo comunicação entre redes.

### **Diferenças Notáveis**
1. **Controle de Tráfego:** 
   - Azure usa **NSGs (Network Security Groups)** para definir regras de tráfego para sub-redes e VMs.
   - AWS utiliza **Security Groups** e **Network ACLs**.

2. **Peering Global:**
   - O Azure permite **VNet Peering** global entre regiões.
   - A AWS também suporta **VPC Peering** entre contas e regiões, mas com algumas limitações dependendo do tipo de configuração.

3. **Firewall:**
   - O Azure oferece o **Azure Firewall** como um serviço integrado para proteção avançada.
   - A AWS possui o **AWS Network Firewall** e outras ferramentas como **WAF** para proteção.

### **Exemplo de Uso em Engenharia de Dados**
- No Azure, você pode criar uma **VNet** e definir sub-redes específicas para hospedar uma **máquina virtual** que processa dados.
- Essa VM pode acessar dados armazenados em uma **Storage Account** (como Blob Storage) pela VNet, garantindo que o tráfego não passe pela internet pública.

Essa arquitetura é ideal para pipelines de dados seguros e de alta performance, como processamento de arquivos CSV para Parquet, conforme o projeto que você está desenvolvendo.

### **Projeto 5. Criação de Banco de Dados SQL e Inserção de Dados com Streamlit**

Necessário baixar o driver de sqlserver

https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?redirectedfrom=MSDN&view=sql-server-ver16

#### **1. Criando o Banco de Dados SQL no Azure**

1. Acesse o portal Azure ([portal.azure.com](https://portal.azure.com)).
2. Navegue para **SQL Databases** e clique em **Create**.
3. **Configurações Básicas**:
   - Nome do Banco: `meubancodedados`.
   - Servidor: Crie um novo servidor ou selecione um existente.
   - Grupo de Recursos: Selecione ou crie um novo grupo, por exemplo, `myResourceGroup`.
   - Região: Selecione **Brazil South** ou outra região próxima.
   - Elastic Pool: Escolha **Não** (se não for usar um pool).
4. **Compute + Storage**:
   - Escolha **General Purpose** com 2 vCores.
   - Defina 32 GB de armazenamento e redundância local ou geo-redundante.
5. Clique em **Review + Create** e depois em **Create**.

#### **2. Configurando o Acesso (IAM)**

1. Vá até o banco de dados criado e clique em **Access Control (IAM)**.
2. Adicione uma função **Contributor** ou **Data Reader** ao serviço usado no **App Registration** que foi criado anteriormente.
3. Copie a **string de conexão** do banco para utilizá-la no código.

#### **3. Configurando o Projeto em Python com Streamlit**

```sql
CREATE TABLE pessoas (
    id INT IDENTITY(1,1) PRIMARY KEY,  -- Coluna com auto incremento
    nome VARCHAR(100) NOT NULL,         -- Nome com limite de 100 caracteres
    idade INT NOT NULL                  -- Idade como número inteiro
);
```

1. Instale as dependências no terminal:
   ```bash
   pip install streamlit pyodbc python-dotenv
   ```
2. Crie um arquivo **`.env`** com as variáveis:
   ```
   AZURE_DB_SERVER=<seu-servidor>.database.windows.net
   AZURE_DB_NAME=meubancodedados
   AZURE_DB_USER=<seu-usuario>
   AZURE_DB_PASSWORD=<sua-senha>
   ```
3. Crie o seguinte **código Python** no arquivo `app.py`:

   ```python
   import streamlit as st
   import pyodbc
   from dotenv import load_dotenv
   import os

   # Carregar variáveis de ambiente
   load_dotenv()

   # Conectar ao banco de dados
   server = os.getenv("AZURE_DB_SERVER")
   database = os.getenv("AZURE_DB_NAME")
   username = os.getenv("AZURE_DB_USER")
   password = os.getenv("AZURE_DB_PASSWORD")
   
   connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'
   conn = pyodbc.connect(connection_string)
   cursor = conn.cursor()

   # Interface Streamlit
   st.title("Inserção de Dados no Banco SQL")

   nome = st.text_input("Nome")
   idade = st.number_input("Idade", min_value=0, max_value=120)

   if st.button("Inserir Dados"):
       cursor.execute(f"INSERT INTO pessoas (nome, idade) VALUES (?, ?)", (nome, idade))
       conn.commit()
       st.success("Dados inseridos com sucesso!")

   cursor.close()
   conn.close()
   ```

4. Execute o **Streamlit**:
   ```bash
   streamlit run app.py
   ```

---

### **Mermaid: Inserção de Dados com Streamlit e Azure SQL Database**

```mermaid
sequenceDiagram
    participant User as Usuário (Interface Streamlit)
    participant Streamlit as Streamlit App
    participant Env as .env (Variáveis de Ambiente)
    participant SQLDB as Banco de Dados SQL (Azure)
    participant IAM as IAM (Controle de Acesso)

    User->>Streamlit: 1. Preenche nome e idade na UI
    User->>Streamlit: 2. Clica no botão "Inserir Dados"
    
    Streamlit->>Env: 3. Carrega variáveis de ambiente (.env)
    Env-->>Streamlit: 4. Retorna credenciais do banco de dados
    
    Streamlit->>IAM: 5. Solicita acesso ao banco SQL
    IAM-->>Streamlit: 6. Concede acesso (verificação do IAM)
    
    Streamlit->>SQLDB: 7. Conecta ao banco de dados
    SQLDB-->>Streamlit: 8. Conexão estabelecida
    
    Streamlit->>SQLDB: 9. Insere dados na tabela 'pessoas'
    SQLDB-->>Streamlit: 10. Confirmação de inserção

    Streamlit->>User: 11. Exibe mensagem de sucesso
```

---

### **Conclusão**

Com este projeto, você agora possui uma aplicação em **Streamlit** conectada a um banco de dados **SQL no Azure**. A interface permite a inserção de dados e a configuração utiliza boas práticas, como o uso de **variáveis de ambiente** e **controle de acesso IAM**.

================================================
File: /Bootcamp - Cloud para dados/Aula_18_19/Dockerfile
================================================
# Imagem base oficial do Python
FROM python:3.9-slim

# Define o diretório de trabalho dentro do container
WORKDIR /app

# Copia o arquivo de requisitos e o código para o container
COPY requirements.txt .
COPY app.py .

# Instala o Streamlit e outras dependências
RUN pip install --no-cache-dir -r requirements.txt

# Define a porta padrão que o Streamlit usará
EXPOSE 80

# Comando para rodar a aplicação Streamlit
CMD ["streamlit", "run", "app.py", "--server.port=80", "--server.enableCORS=false"]


================================================
File: /Bootcamp - Cloud para dados/Aula_18_19/README-resourcegroup.md
================================================
### **Resource Group no Azure**

O **Resource Group** no Azure é uma unidade lógica para **organizar, gerenciar e agrupar recursos** relacionados, como VMs, bancos de dados, contas de armazenamento e redes virtuais. Ele facilita o gerenciamento de infraestrutura em nuvem, garantindo que todos os recursos de um projeto ou serviço específico estejam concentrados em um único local.

---

### **Características Principais do Resource Group**

1. **Organização e Gestão Centralizada:**
   - Todos os recursos de uma aplicação ou projeto ficam organizados em um único grupo.
   - Exemplo: Uma aplicação pode ter VMs, bancos de dados e uma VNet no mesmo Resource Group.

2. **Controle de Acesso e Permissões:**
   - Permissões podem ser atribuídas ao Resource Group inteiro usando **Azure Role-Based Access Control (RBAC)**.
   - Assim, equipes específicas podem ter acesso apenas aos recursos dentro do grupo correspondente.

3. **Monitoramento e Gestão de Custos:**
   - Cada Resource Group pode ter seu custo monitorado separadamente.
   - Ajuda a visualizar e controlar o orçamento de projetos específicos.

4. **Aplicação de Tags:**
   - Tags podem ser aplicadas para facilitar a **organização e busca** de recursos com base em critérios como projeto, cliente ou departamento.

---

### **Casos de Uso do Resource Group**

- **Isolamento de Ambientes:**  
  Organize ambientes de **produção, desenvolvimento e teste** em Resource Groups separados.

- **Projetos Multi-Equipe:**  
  Múltiplos times trabalhando em diferentes serviços podem criar grupos específicos para gerenciar seus recursos sem interferir entre si.

- **Automação:**  
  Scripts de **Infraestrutura como Código (IaC)**, como ARM Templates e Terraform, utilizam Resource Groups para provisionar e gerenciar recursos.

---

### **Benefícios de Usar Resource Groups**

- **Escalabilidade e Flexibilidade:**  
  Recursos podem ser adicionados e removidos rapidamente dentro de um grupo.

- **Governança Simplificada:**  
  Atribuir políticas de segurança e controle de acesso a grupos inteiros facilita a governança.

- **Gerenciamento Facilitado:**  
  Operações como **backup**, **exportação** ou **deleção** podem ser feitas no nível do Resource Group, afetando todos os recursos contidos nele.

---

### **Como Criar um Resource Group no Azure**

1. **Acessar o Portal do Azure:**  
   - [https://portal.azure.com](https://portal.azure.com)

2. **Ir para Resource Groups:**  
   - No menu lateral, selecione **Resource Groups** e clique em **Create**.

3. **Preencher Informações:**
   - **Nome:** Defina um nome significativo (ex.: `RG-ProjetoX`).
   - **Região:** Escolha a região onde os recursos serão provisionados (ex.: **East US**).
   - **Tags:** (Opcional) Aplique tags para facilitar o controle de custos e organização.

4. **Criar o Resource Group:**  
   - Clique em **Review + Create** e em seguida **Create**.

---

### **Conclusão**
O **Resource Group** é essencial para o gerenciamento eficaz dos recursos no Azure. Ele não apenas organiza recursos, mas também simplifica a **administração**, **segurança** e **monitoramento** de projetos e serviços em nuvem.

### **Resource Group no Azure e o Equivalente na AWS**

Na **AWS**, o equivalente direto ao **Resource Group** do Azure é o **AWS Resource Groups**. Ambos servem para organizar e gerenciar recursos relacionados, mas existem algumas diferenças na implementação e uso.

---

### **Comparação: Resource Group (Azure) vs. AWS Resource Groups**

| **Aspecto**               | **Resource Group (Azure)**                     | **Resource Groups (AWS)**                    |
|---------------------------|-------------------------------------------------|---------------------------------------------|
| **Propósito**              | Organizar recursos relacionados por projeto ou serviço. | Agrupar e gerenciar recursos por projeto, ambiente ou finalidade. |
| **Controle de Acesso**     | Gerenciado via **Azure Role-Based Access Control (RBAC)** aplicado ao grupo inteiro. | Controle de acesso configurado por **IAM Policies** e **Tags**. |
| **Monitoramento de Custos**| Monitoramento de custos do grupo no **Cost Management**. | Visualização de custos no **AWS Cost Explorer** por tags ou grupos. |
| **Organização**            | Agrupa múltiplos recursos em um único container lógico. | Agrupa recursos por meio de **tags** aplicadas. |
| **Tags**                   | Tags são opcionais e ajudam na categorização. | Tags são fundamentais e podem ser usadas para definir Resource Groups dinamicamente. |

---

### **Funcionamento do AWS Resource Groups**

1. **Organização e Tags:**
   - Os recursos na AWS não são "alocados fisicamente" dentro de um grupo, mas organizados e filtrados usando **tags**.
   - Exemplo: Todos os recursos (EC2, RDS, S3) de um projeto podem ter a tag `Projeto: X`.

2. **Gerenciamento Centralizado:**
   - O AWS Resource Groups permite visualizar, gerenciar e executar operações em massa para todos os recursos associados a um projeto ou serviço específico.

3. **Controle de Acesso:**  
   - O controle é feito via **IAM Policies**, que podem conceder permissões com base em tags aplicadas aos recursos.

4. **Monitoramento e Custos:**
   - Utilizando o **AWS Cost Explorer** e **AWS Budgets**, é possível monitorar o custo de recursos organizados por tags ou grupos.

---

### **Quando Usar AWS Resource Groups?**

- **Ambientes Multi-Projetos:**  
  Organize recursos por projetos usando tags como `Projeto: Marketing` ou `Projeto: Financeiro`.

- **Isolamento de Ambientes:**  
  Defina ambientes de **produção**, **desenvolvimento** e **teste** utilizando tags para cada ambiente.

- **Automação:**  
  Scripts como Terraform e CloudFormation usam tags para organizar e automatizar a criação e gerenciamento de recursos.

---

### **Como Criar um Resource Group na AWS**

1. **Acessar o Console AWS:**  
   - [https://aws.amazon.com/console/](https://aws.amazon.com/console/)

2. **Ir para Resource Groups:**  
   - No menu superior, clique em **Resource Groups** > **Create Group**.

3. **Definir Critérios do Grupo:**
   - Nomeie o grupo e defina **filtros de tags**. Por exemplo: `Projeto: X`.

4. **Revisar e Criar:**  
   - Revise as configurações e clique em **Create Group**.

---

### **Conclusão**
Embora **Azure Resource Group** e **AWS Resource Groups** compartilhem a função de organizar e gerenciar recursos relacionados, a AWS depende fortemente de **tags** para criar grupos dinâmicos, enquanto o Azure usa uma abordagem mais direta com grupos lógicos. Ambos simplificam o gerenciamento e são essenciais para controle de custos e governança na nuvem.

================================================
File: /Bootcamp - Cloud para dados/Aula_18_19/app.py
================================================
import streamlit as st

# Título do aplicativo
st.title("Hello, World! com Streamlit")

# Exibindo uma mensagem de boas-vindas
st.write("Este é um exemplo simples de uma aplicação Streamlit em Docker.")

# Adicionando um botão de interação
if st.button("Clique aqui"):
    st.success("Você clicou no botão!")


================================================
File: /Bootcamp - Cloud para dados/Aula_18_19/projeto_01.py
================================================
from azure.identity import ClientSecretCredential  # Importa a classe para autenticação usando Client Secret
from azure.keyvault.secrets import SecretClient  # Importa a classe para manipular segredos no Key Vault
from dotenv import load_dotenv  # Importa a função para carregar variáveis de ambiente de um arquivo .env
import os  # Importa o módulo para acessar variáveis de ambiente

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Obtém as variáveis de ambiente necessárias para a autenticação
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
vault_url = os.environ["AZURE_VAULT_URL"]

# Nome do segredo a ser acessado no Key Vault
secret_name = "ExemploKey"

# Cria uma credencial para autenticação no Azure utilizando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id, 
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Cria o cliente do Key Vault para acessar os segredos
secret_client = SecretClient(vault_url=vault_url, credential=credentials)

# Recupera o valor do segredo a partir do Key Vault
secret = secret_client.get_secret(secret_name)

# Exibe o valor do segredo no terminal
print("O valor do segredo é: " + secret.value)


================================================
File: /Bootcamp - Cloud para dados/Aula_18_19/projeto_02.py
================================================
from azure.identity import ClientSecretCredential
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv
import os

# Carrega as variáveis de ambiente do arquivo .env
load_dotenv()

# Variáveis de ambiente necessárias
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
storage_account_url = os.environ["AZURE_STORAGE_URL"]  # Ex: https://<nome_da_storage>.blob.core.windows.net/
container_name = "meucontainer"  # Nome do container criado

# Configura as credenciais usando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id,
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Conectar ao Blob Storage
blob_service_client = BlobServiceClient(
    account_url=storage_account_url,
    credential=credentials
)

# Acessa o container
container_client = blob_service_client.get_container_client(container_name)

# Lista todos os arquivos dentro do container
print(f"Listando arquivos no container '{container_name}':")
for blob in container_client.list_blobs():
    print(f" - {blob.name}")


================================================
File: /Bootcamp - Cloud para dados/Aula_18_19/projeto_03.py
================================================
import streamlit as st
from azure.identity import ClientSecretCredential
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv
import os

# Carregar variáveis de ambiente do arquivo .env
load_dotenv()

# Variáveis de ambiente necessárias
client_id = os.environ['AZURE_CLIENT_ID']
tenant_id = os.environ['AZURE_TENANT_ID']
client_secret = os.environ['AZURE_CLIENT_SECRET']
storage_account_url = os.environ["AZURE_STORAGE_URL"]
container_name = "meucontainer"

# Configura credenciais usando Client Secret
credentials = ClientSecretCredential(
    client_id=client_id,
    client_secret=client_secret,
    tenant_id=tenant_id
)

# Conectar ao Blob Storage
blob_service_client = BlobServiceClient(
    account_url=storage_account_url,
    credential=credentials
)

# Acessa o container
container_client = blob_service_client.get_container_client(container_name)

# Função para upload de arquivo
def upload_file(file):
    try:
        blob_client = container_client.get_blob_client(file.name)
        blob_client.upload_blob(file, overwrite=True)
        st.success(f"Arquivo '{file.name}' enviado com sucesso!")
    except Exception as e:
        st.error(f"Erro ao enviar arquivo: {str(e)}")

# Função para listar arquivos no container
def listar_arquivos():
    try:
        blobs = container_client.list_blobs()
        return [blob.name for blob in blobs]
    except Exception as e:
        st.error(f"Erro ao listar arquivos: {str(e)}")
        return []

# Interface do Streamlit
st.title("Upload para Azure Blob Storage")

uploaded_file = st.file_uploader("Escolha um arquivo para enviar", type=["csv", "txt", "png", "jpg", "pdf"])

if uploaded_file is not None:
    if st.button("Enviar"):
        upload_file(uploaded_file)

st.subheader("Arquivos no Container")
arquivos = listar_arquivos()
if arquivos:
    for arquivo in arquivos:
        st.write(f"- {arquivo}")
else:
    st.write("Nenhum arquivo encontrado.")


================================================
File: /Bootcamp - Cloud para dados/Aula_18_19/projeto_05.py
================================================
import streamlit as st
import pyodbc
import os
from dotenv import load_dotenv

# Carregar variáveis de ambiente
load_dotenv()

# Configuração do banco de dados
server = os.getenv("DB_SERVER")
database = os.getenv("DB_NAME")
username = os.getenv("DB_USERNAME")
password = os.getenv("DB_PASSWORD")
connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'

# Função para conectar ao banco
def connect_to_database():
    try:
        conn = pyodbc.connect(connection_string)
        return conn
    except Exception as e:
        st.error(f"Erro ao conectar ao banco de dados: {e}")
        return None

# Função para inserir dados
def insert_data(nome, idade):
    conn = connect_to_database()
    if conn:
        cursor = conn.cursor()
        try:
            cursor.execute("INSERT INTO pessoas (nome, idade) VALUES (?, ?)", (nome, idade))
            conn.commit()
            st.success(f"Dados inseridos: {nome}, {idade} anos")
        except Exception as e:
            st.error(f"Erro ao inserir dados: {e}")
        finally:
            conn.close()

# Interface Streamlit
st.title("Inserção de Dados no Banco de Dados SQL")

nome = st.text_input("Nome:")
idade = st.number_input("Idade:", min_value=0, step=1)

if st.button("Inserir Dados"):
    if nome and idade:
        insert_data(nome, idade)
    else:
        st.warning("Preencha todos os campos.")


================================================
File: /Bootcamp - Cloud para dados/Aula_18_19/requirements.txt
================================================
streamlit
python-dotenv
azurealtair==5.4.1
attrs==24.2.0
azure-core==1.31.0
azure-identity==1.19.0
azure-keyvault==4.2.0
azure-keyvault-certificates==4.9.0
azure-keyvault-keys==4.10.0
azure-keyvault-secrets==4.9.0
azure-storage-blob==12.23.1
blinker==1.8.2
cachetools==5.5.0
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
click==8.1.7
colorama==0.4.6
cryptography==43.0.3
gitdb==4.0.11
GitPython==3.1.43
idna==3.10
isodate==0.7.2
Jinja2==3.1.4
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
markdown-it-py==3.0.0
MarkupSafe==3.0.2
mdurl==0.1.2
msal==1.31.0
msal-extensions==1.2.0
narwhals==1.11.1
numpy==2.1.2
packaging==24.1
pandas==2.2.3
pillow==10.4.0
portalocker==2.10.1
protobuf==5.28.3
pyarrow==18.0.0
pycparser==2.22
pydeck==0.9.1
Pygments==2.18.0
PyJWT==2.9.0
pyodbc==5.2.0
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytz==2024.2
pywin32==308
referencing==0.35.1
requests==2.32.3
rich==13.9.3
rpds-py==0.20.0
six==1.16.0
smmap==5.0.1
streamlit==1.39.0
tenacity==9.0.0
toml==0.10.2
tornado==6.4.1
typing_extensions==4.12.2
tzdata==2024.2
urllib3==2.2.3
watchdog==5.0.3


================================================
File: /Bootcamp - Python com Laennder/README.md
================================================
## Calendário Bootcamp - Python do Zero

| Aula  | Workshop                                                                 | Horário |
|-------|--------------------------------------------------------------------------|---------|
| Aula 01 | Python do Zero                                    | 26/09/2024 - 19h    |
| Aula 02 | Git, Github e bibliotecas externas                | 03/10/2024 - 19h    |
| Aula 03 | ETL         | 10/10/2024 - 19h    |
| Aula 04 | API | 17/10/2024 - 19h    |
| Aula 05 | Estruturando um projeto do zero   | 24/10/2024 - 19h    |

================================================
File: /Bootcamp - Python com Laennder/Aula_02/readme.md
================================================
## Aula 02: Métodos, Funções, Pandas e Cálculos Estatísticos

### 1. Introdução aos Métodos e Funções

- **Objetivo**: Entender a diferença entre métodos e funções em Python e aprender a criar funções personalizadas.

#### O que são Métodos?

Métodos são ações associadas a um objeto em Python. Eles são definidos dentro de uma classe e operam sobre os dados do objeto. Por exemplo, o método `.upper()` transforma uma string em letras maiúsculas:

```python
texto = "hello"
print(texto.upper())  # Saída: "HELLO"
```

#### O que são Funções?

Funções são blocos de código independentes que realizam uma tarefa específica e podem retornar valores. Exemplo de uma função que soma dois números:

```python
def somar(a, b):
    return a + b

resultado = somar(3, 5)
print(f"O resultado da soma é: {resultado}")  # Saída: O resultado da soma é: 8
```

### 2. Função de Cast (Conversão de Tipos)

Casting é a conversão de um valor de um tipo para outro, como transformar uma `string` em `int` ao receber dados de um usuário via `input`:

```python
entrada = input("Digite sua idade: ")  # A entrada é uma string
idade = int(entrada)  # Converte a string para inteiro
print(f"Sua idade é: {idade}")
```

### 3. Criando Funções Personalizadas

Agora, vamos criar funções para calcular **média**, **variância** e **desvio padrão** de uma lista de números.

#### Função para Calcular a Média:

A média é o valor central de um conjunto de números:

```python
def calcular_media(lista):
    soma = 0
    for valor in lista:
        soma += valor
    media = soma / len(lista)
    return media
```

#### Função para Calcular a Variância:

A variância mede a dispersão dos valores em relação à média:

```python
def calcular_variancia(lista):
    media = calcular_media(lista)
    soma_diferencas = 0
    for valor in lista:
        diferenca = valor - media
        soma_diferencas += diferenca ** 2
    variancia = soma_diferencas / len(lista)
    return variancia
```

#### Função para Calcular o Desvio Padrão:

O desvio padrão é a raiz quadrada da variância, medindo a dispersão dos valores:

```python
def calcular_desvio_padrao(lista):
    variancia = calcular_variancia(lista)
    desvio_padrao = variancia ** 0.5
    return desvio_padrao
```

### 4. Aplicação Prática

Vamos aplicar essas funções a uma lista de idades:

```python
idades = [23, 29, 31, 25]

media = calcular_media(idades)
variancia = calcular_variancia(idades)
desvio_padrao = calcular_desvio_padrao(idades)

print(f"Média: {media}")
print(f"Variância: {variancia}")
print(f"Desvio Padrão: {desvio_padrao}")
```

---

## Simplificando com pandas

Agora que calculamos as estatísticas manualmente, podemos simplificar o processo usando o **pandas**, uma biblioteca poderosa para manipulação de dados. Vamos explorar como usar pandas para realizar esses cálculos automaticamente.

### 1. **Média** (`mean`)

A **média** é o valor central de um conjunto de dados.

```python
media = idades_series.mean()
print(f"Média: {media}")
```

### 2. **Mediana** (`median`)

A **mediana** é o valor central de um conjunto ordenado de dados.

```python
mediana = idades_series.median()
print(f"Mediana: {mediana}")
```

### 3. **Desvio Padrão** (`std`)

O **desvio padrão** mede o quão dispersos os valores estão em relação à média.

```python
desvio_padrao = idades_series.std()
print(f"Desvio Padrão: {desvio_padrao}")
```

### 4. **Variância** (`var`)

A **variância** é a medida da dispersão ao quadrado dos valores em relação à média.

```python
variancia = idades_series.var()
print(f"Variância: {variancia}")
```

### 5. **Moda** (`mode`)

A **moda** é o valor que ocorre com mais frequência.

```python
moda = idades_series.mode()[0]
print(f"Moda: {moda}")
```

### 6. **Quartis** (`quantile`)

Os **quartis** dividem os dados em quatro partes iguais.

```python
primeiro_quartil = idades_series.quantile(0.25)
terceiro_quartil = idades_series.quantile(0.75)
print(f"Primeiro Quartil: {primeiro_quartil}")
print(f"Terceiro Quartil: {terceiro_quartil}")
```

### 7. Outros Métodos Estatísticos

#### Mínimo e Máximo:

```python
minimo = idades_series.min()
maximo = idades_series.max()
print(f"Mínimo: {minimo}")
print(f"Máximo: {maximo}")
```

#### Soma e Contagem:

```python
soma = idades_series.sum()
contagem = idades_series.count()
print(f"Soma: {soma}")
print(f"Contagem: {contagem}")
```

---

## Explorando Outros Métodos e Atributos do pandas

### 1. Método `describe`

O método `describe` fornece um resumo estatístico da série.

```python
resumo = idades_series.describe()
print(resumo)
```

### 2. Atributo `shape`

O atributo `shape` retorna as dimensões da série.

```python
forma = idades_series.shape
print(f"Forma da Série: {forma}")
```

### 3. Índices e `iloc`

Cada elemento em uma série tem um índice associado, que pode ser acessado diretamente:

```python
# Acessando valor pelo índice
valor_no_indice_2 = idades_series[2]
print(f"Valor no índice 2: {valor_no_indice_2}")

# Usando iloc para acessar pela posição
valor_posicao_1 = idades_series.iloc[1]
print(f"Valor na posição 1: {valor_posicao_1}")
```

### 4. Nomeando Séries

Podemos dar um nome à série usando o atributo `name`:

```python
idades_series.name = "Idades dos Participantes"
print(idades_series)
```

---

## Conclusão: DataFrame é um Conjunto de Séries

Um **DataFrame** no pandas é uma coleção de **Séries**. Cada coluna de um DataFrame é uma Série, e as linhas compartilham o mesmo índice. Vamos criar um DataFrame a partir de Séries:

```python
serie_nomes = pd.Series(["Ana", "Bruno", "Carla", "David"])
serie_sobrenomes = pd.Series(["Silva", "Oliveira", "Santos", "Costa"])
serie_idades = pd.Series([23, 29, 31, 25])

df_pessoas = pd.DataFrame({
    "nome": serie_nomes,
    "sobrenome": serie_sobrenomes,
    "idade": serie_idades
})

print(df_pessoas)
```

### Acessando Séries de um DataFrame

Podemos acessar uma coluna específica (Série) de um DataFrame da seguinte forma:

```python
serie_nomes_do_df = df_pessoas["nome"]
print(serie_nomes_do_df)
```

### Aula: Manipulação de DataFrames com Pandas e Integração de Múltiplos Arquivos CSV

Nesta aula, vamos trabalhar com três arquivos CSV de **vendas** e um arquivo CSV de **clientes**, aplicando as operações mais comuns do pandas, como **leitura**, **join**, **agregação**, **filtragem** e **salvamento**. O foco será entender como integrar múltiplas fontes de dados e fazer análises.

#### Arquivos CSV:
- **clientes.csv**: Arquivo contendo a dimensão dos clientes, com 10 linhas e informações pessoais dos clientes.
- **vendas.csv**: Arquivo contendo 100 vendas.
- **vendas_2.csv**: Segundo arquivo de vendas com mais 100 vendas.
- **vendas_3.csv**: Terceiro arquivo de vendas com 100 vendas adicionais.

### Estrutura dos Arquivos

#### 1. **clientes.csv** (Dimensão)
Colunas:
- `cliente_id`: Chave primária.
- `nome`: Nome do cliente.
- `sobrenome`: Sobrenome do cliente.
- `cidade`: Cidade de residência.
- `idade`: Idade do cliente.
- `email`: Endereço de email.

#### 2. **vendas.csv**, **vendas_2.csv**, **vendas_3.csv** (Fato)
Colunas:
- `venda_id`: ID único da venda.
- `cliente_id`: Chave estrangeira para relacionar com a tabela de clientes.
- `produto`: Produto vendido.
- `quantidade`: Quantidade vendida.
- `valor_unitario`: Preço por unidade do produto.
- `data_venda`: Data da venda.

### Passo a Passo: Manipulação de DataFrames com Pandas

#### Passo 1: Leitura dos Arquivos CSV

Vamos começar lendo os três arquivos de vendas e o arquivo de clientes usando `pd.read_csv()`.

```python
import pandas as pd

# Leitura dos CSVs de clientes e vendas
df_clientes = pd.read_csv('clientes.csv')
df_vendas = pd.read_csv('vendas.csv')
df_vendas_2 = pd.read_csv('vendas_2.csv')
df_vendas_3 = pd.read_csv('vendas_3.csv')

# Exibindo as primeiras linhas de cada DataFrame
print(df_clientes.head())
print(df_vendas.head())
print(df_vendas_2.head())
print(df_vendas_3.head())
```

#### Passo 2: Unindo os Arquivos de Vendas

Agora, vamos combinar os três DataFrames de vendas em um único DataFrame usando `pd.concat()`. Isso vai consolidar todas as 300 vendas.

```python
# Unindo as três tabelas de vendas
df_todas_vendas = pd.concat([df_vendas, df_vendas_2, df_vendas_3])

# Exibindo o DataFrame combinado
print(df_todas_vendas.shape)  # Verificar o número total de vendas (deve ser 300)
print(df_todas_vendas.head())
```

#### Passo 3: Fazendo o Join entre Vendas e Clientes

Agora que temos um DataFrame com todas as vendas, vamos juntar (fazer o **join**) com a tabela de clientes usando a coluna `cliente_id`, que está presente em ambos os DataFrames.

```python
# Fazendo o join entre o DataFrame de vendas e o de clientes
df_completo = pd.merge(df_todas_vendas, df_clientes, on="cliente_id")

# Exibindo o DataFrame resultante
print(df_completo.head())
```

#### Passo 4: Análise Exploratória

Agora que temos o DataFrame completo, vamos fazer algumas análises:

- **Número total de vendas**:
```python
total_vendas = df_completo['venda_id'].count()
print(f"Total de vendas: {total_vendas}")
```

- **Valor total vendido**:
Vamos calcular o valor total das vendas (quantidade * valor_unitário):

```python
df_completo['valor_total'] = df_completo['quantidade'] * df_completo['valor_unitario']
valor_total = df_completo['valor_total'].sum()
print(f"Valor total vendido: {valor_total}")
```

- **Média do valor por venda**:
```python
media_venda = df_completo['valor_total'].mean()
print(f"Média por venda: {media_venda}")
```

#### Passo 5: Agrupamento e Resumo de Vendas

Vamos agrupar as vendas por cliente e calcular o total de vendas por cada cliente:

- **Total de vendas por cliente**:
```python
vendas_por_cliente = df_completo.groupby('nome')['valor_total'].sum()
print(vendas_por_cliente)
```

- **Número de vendas por produto**:
```python
vendas_por_produto = df_completo.groupby('produto')['venda_id'].count()
print(vendas_por_produto)
```

#### Passo 6: Filtragem dos Dados

Agora, vamos filtrar o DataFrame para analisar as vendas de um cliente específico ou as vendas realizadas em uma cidade específica.

- **Vendas de clientes da cidade de São Paulo**:
```python
vendas_sp = df_completo[df_completo['cidade'] == 'São Paulo']
print(vendas_sp)
```

- **Vendas de um cliente específico (por exemplo, 'Ana')**:
```python
vendas_ana = df_completo[df_completo['nome'] == 'Ana']
print(vendas_ana)
```

#### Passo 7: Ordenação das Vendas

Vamos ordenar as vendas pelo maior valor total de venda:

```python
vendas_ordenadas = df_completo.sort_values(by='valor_total', ascending=False)
print(vendas_ordenadas.head())
```

#### Passo 8: Salvando o DataFrame Resultante

Após todas as manipulações, podemos salvar o DataFrame resultante em um novo arquivo CSV para análise futura:

```python
df_completo.to_csv('vendas_completo.csv', index=False)
```

### Resumo das Operações Realizadas:

- **Leitura de CSVs**: Importação dos dados de clientes e vendas.
- **Concatenação**: Unindo múltiplos DataFrames de vendas em um único DataFrame.
- **Merge/Join**: Juntando os dados de clientes com as vendas através da coluna `cliente_id`.
- **Cálculos**: Realizando operações de soma, média e agregações sobre as vendas.
- **Agrupamento**: Agrupando vendas por cliente e por produto.
- **Filtragem**: Selecionando dados com base em critérios, como cidade ou cliente.
- **Ordenação**: Ordenando os dados para identificar as maiores vendas.
- **Salvamento**: Exportando o DataFrame resultante para um novo CSV.

Sim, o pandas permite salvar os dados diretamente em um banco de dados relacional usando a função **`to_sql()`**, que possibilita gravar os dados de um **DataFrame** em tabelas de um banco de dados. Para isso, você pode utilizar bibliotecas como **SQLAlchemy** para criar uma conexão com o banco de dados.

Aqui está um exemplo básico de como você pode salvar um DataFrame em um banco de dados usando pandas:

### 1. Instale as Dependências

Primeiro, você precisa instalar o **SQLAlchemy** e o conector específico para o banco de dados que você está usando (por exemplo, **psycopg2** para PostgreSQL, **PyMySQL** para MySQL, etc.).

```bash
pip install sqlalchemy
pip install psycopg2  # Para PostgreSQL
pip install pymysql   # Para MySQL
```

### 2. Código para Salvar o DataFrame em um Banco de Dados

Vamos usar o **SQLAlchemy** para criar uma conexão com o banco de dados e, em seguida, usar a função **`to_sql()`** do pandas para salvar os dados.

#### Exemplo: Salvando em um Banco de Dados PostgreSQL

```python
import pandas as pd
from sqlalchemy import create_engine

# Supondo que df_completo seja o DataFrame resultante que você quer salvar
# Configurar a conexão com o banco de dados
engine = create_engine('postgresql+psycopg2://username:password@localhost:5432/nome_do_banco')

# Salvando o DataFrame 'df_completo' na tabela 'vendas_completo' do banco de dados
df_completo.to_sql('vendas_completo', engine, if_exists='replace', index=False)

print("Dados salvos no banco de dados com sucesso!")
```

### Explicação:

- **`create_engine()`**: Cria a conexão com o banco de dados usando a string de conexão. Você precisa ajustar o `username`, `password`, `localhost`, `5432` (porta) e `nome_do_banco` para refletir suas credenciais e configurações do banco.
  
- **`to_sql()`**:
  - O primeiro argumento é o nome da tabela onde os dados serão inseridos (`vendas_completo`).
  - O segundo argumento é a conexão (`engine`).
  - `if_exists='replace'` indica que, se a tabela já existir, ela será substituída. Você pode usar `'append'` para adicionar dados a uma tabela já existente.
  - `index=False` evita que o índice do DataFrame seja salvo como uma coluna na tabela.

================================================
File: /Bootcamp - Python com Laennder/Aula_02/csv_to_parquet.py
================================================
import pandas as pd

# Leitura do arquivo CSV
df = pd.read_csv('vendas.csv')

# Salvando o DataFrame como arquivo Parquet
df.to_parquet('vendas.parquet')


================================================
File: /Bootcamp - Python com Laennder/Aula_02/parquet_to_csv.py
================================================
import pandas as pd

# Leitura do arquivo Parquet
df = pd.read_parquet('vendas.parquet')

# Salvando o DataFrame como arquivo CSV
df.to_csv('vendas.csv', index=False)


================================================
File: /Bootcamp - Python com Laennder/Aula_02/requirements.txt
================================================
cramjam==2.8.4
fastparquet==2024.5.0
fsspec==2024.9.0
greenlet==3.1.1
numpy==2.1.1
packaging==24.1
pandas==2.2.3
psycopg2==2.9.9
python-dateutil==2.9.0.post0
pytz==2024.2
six==1.16.0
SQLAlchemy==2.0.35
typing_extensions==4.12.2
tzdata==2024.2


================================================
File: /Bootcamp - Python com Laennder/Aula_02/exercicios/readme.md
================================================
### Exercício: Cálculo de Estatísticas com Python Puro e pandas

Neste exercício, você vai trabalhar com uma lista de 20 itens e calcular **média**, **desvio padrão**, **máximo** e **mínimo** tanto usando Python puro quanto usando **pandas**.

#### Lista de números:
```python
numeros = [23, 45, 67, 89, 12, 34, 56, 78, 90, 21, 43, 65, 87, 32, 54, 76, 98, 10, 30, 50]
```

### Passo 1: Cálculos com Python Puro

1. **Média**: Soma dos elementos dividida pelo número de elementos.
2. **Desvio Padrão**: Raiz quadrada da variância.
3. **Máximo**: Maior valor da lista.
4. **Mínimo**: Menor valor da lista.

#### Cálculos com Python Puro:

```python

numeros = [23, 45, 67, 89, 12, 34, 56, 78, 90, 21, 43, 65, 87, 32, 54, 76, 98, 10, 30, 50]

# Função para calcular a média
def calcular_media(lista):
    soma = 0
    for valor in lista:
        soma += valor
    media = soma / len(lista)
    return media

# Função para calcular a variância
def calcular_variancia(lista):
    media = calcular_media(lista)
    soma_diferencas = 0
    for valor in lista:
        diferenca = valor - media
        soma_diferencas += diferenca ** 2
    variancia = soma_diferencas / len(lista)
    return variancia

# Função para calcular o desvio padrão
def calcular_desvio_padrao(lista):
    variancia = calcular_variancia(lista)
    desvio_padrao = variancia ** 0.5
    return desvio_padrao

# Cálculos
media = calcular_media(numeros)
desvio_padrao = calcular_desvio_padrao(numeros)
maximo = max(numeros)
minimo = min(numeros)

# Exibindo os resultados
print(f"Média: {media}")
print(f"Desvio Padrão: {desvio_padrao}")
print(f"Máximo: {maximo}")
print(f"Mínimo: {minimo}")
```

### Saída Esperada:

```plaintext
Média: 52.95
Desvio Padrão: 26.715776024038615
Máximo: 98
Mínimo: 10
```

### Passo 2: Cálculos com pandas Series

Agora, vamos realizar os mesmos cálculos usando uma **Série** do pandas. A vantagem do pandas é que essas operações já são implementadas como métodos da Série, o que simplifica o código.

#### Cálculos com pandas:

```python
import pandas as pd

# Criando uma Série com os números
serie_numeros = pd.Series(numeros)

# Cálculos usando pandas
media_pandas = serie_numeros.mean()
desvio_padrao_pandas = serie_numeros.std()
maximo_pandas = serie_numeros.max()
minimo_pandas = serie_numeros.min()

# Exibindo os resultados
print(f"Média (pandas): {media_pandas}")
print(f"Desvio Padrão (pandas): {desvio_padrao_pandas}")
print(f"Máximo (pandas): {maximo_pandas}")
print(f"Mínimo (pandas): {minimo_pandas}")
```

### Saída Esperada:

```plaintext
Média (pandas): 52.95
Desvio Padrão (pandas): 27.209487879199906
Máximo (pandas): 98
Mínimo (pandas): 10
```

### Comparação dos Resultados

- **Média** e **Máximo** e **Mínimo** devem ser iguais em ambos os casos.
- O **desvio padrão** pode diferir ligeiramente, porque o pandas, por padrão, calcula o desvio padrão amostral (dividido por `n-1`), enquanto o cálculo manual faz o desvio padrão populacional (dividido por `n`). Você pode ajustar o pandas para calcular o desvio padrão populacional usando o argumento `ddof=0`:

```python
desvio_padrao_pandas = serie_numeros.std(ddof=0)
```

Este exercício demonstra a flexibilidade do pandas para realizar cálculos estatísticos rapidamente e a importância de entender como as fórmulas são aplicadas tanto manualmente quanto com bibliotecas especializadas.

================================================
File: /Bootcamp - Python com Laennder/Aula_05/README.md
================================================
# Projeto ETL com Python e Scrapy - Coleta de Dados do Mercado Livre

Este projeto demonstra como construir um pipeline ETL utilizando Python e Scrapy para coletar dados do Mercado Livre, processá-los e armazená-los em um banco de dados SQL.

## **Índice**

- [Projeto ETL com Python e Scrapy - Coleta de Dados do Mercado Livre](#projeto-etl-com-python-e-scrapy---coleta-de-dados-do-mercado-livre)
  - [**Índice**](#índice)
  - [**Pré-requisitos**](#pré-requisitos)
  - [**Instalação**](#instalação)
    - [**1. Instale o Python**](#1-instale-o-python)
    - [**2. Crie um Ambiente Virtual**](#2-crie-um-ambiente-virtual)
    - [**3. Instale o Scrapy**](#3-instale-o-scrapy)
  - [**Inicialização do Projeto Scrapy**](#inicialização-do-projeto-scrapy)
  - [**Criação do Spider para o Mercado Livre**](#criação-do-spider-para-o-mercado-livre)
    - [**Editar o Spider**](#editar-o-spider)
  - [**Execução do Spider**](#execução-do-spider)
  - [**Processamento e Armazenamento dos Dados**](#processamento-e-armazenamento-dos-dados)
    - [**1. Configurar o Pipeline**](#1-configurar-o-pipeline)
    - [**2. Ativar o Pipeline**](#2-ativar-o-pipeline)
  - [**Recursos Adicionais**](#recursos-adicionais)
  - [**Observações Finais**](#observações-finais)

---

## **Pré-requisitos**

- **Python 3.12+**
- **Pip** (gerenciador de pacotes do Python)
- **Bibliotecas Python:**
  - Scrapy
  - Pandas
  - Streaming

## **Instalação**

### **1. Instale o Python**

Verifique se o Python está instalado:

```bash
python --version
```

Se não estiver instalado, faça o download e instale a partir do [site oficial do Python](https://www.python.org/downloads/).

### **2. Crie um Ambiente Virtual**

É recomendado usar um ambiente virtual para isolar as dependências do projeto.

```bash
python -m venv venv
```

Ative o ambiente virtual:

- **Windows:**

  ```bash
  venv\Scripts\activate
  ```

- **Linux/MacOS:**

  ```bash
  source venv/bin/activate
  ```

### **3. Instale o Scrapy**

Com o ambiente virtual ativado, instale o Scrapy usando o pip:

```bash
pip install scrapy
```

Verifique se a instalação foi bem-sucedida:

```bash
scrapy --version
```

---

## **Inicialização do Projeto Scrapy**

Dentro do diretório do seu projeto, inicialize um novo projeto Scrapy:

```bash
scrapy startproject mercado_livre
```

Isso criará a seguinte estrutura de diretórios:

```
mercado_livre/
    scrapy.cfg
    mercado_livre/
        __init__.py
        items.py
        middlewares.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
```

---

## **Criação do Spider para o Mercado Livre**

Navegue até o diretório `mercado_livre/spiders`:

```bash
cd mercado_livre/mercado_livre/spiders
```

Crie um novo Spider chamado `mercado_spider`:

```bash
scrapy genspider mercado_spider mercadolivre.com.br
```

### **Editar o Spider**

Abra o arquivo `mercado_spider.py` e edite-o conforme necessário.

**Exemplo de código:**

```python
import scrapy

class MercadoSpider(scrapy.Spider):
    name = 'mercado_spider'
    allowed_domains = ['mercadolivre.com.br']
    start_urls = ['https://lista.mercadolivre.com.br/livros#D[A:livros]']

    def parse(self, response):
        for product in response.css('li.ui-search-layout__item'):
            yield {
                'titulo': product.css('h2.ui-search-item__title::text').get(),
                'preco': product.css('span.price-tag-fraction::text').get(),
                'link': product.css('a.ui-search-link::attr(href)').get(),
            }

        next_page = response.css('a.andes-pagination__link--next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
```

**Notas:**

- Certifique-se de que os seletores CSS correspondam à estrutura atual do site do Mercado Livre.
- A estrutura do site pode mudar, portanto, verifique os elementos usando o inspetor do navegador.

---

## **Execução do Spider**

Para executar o spider e salvar os dados coletados em um arquivo JSON:

```bash
scrapy crawl mercado_spider -o produtos.json
```

---

## **Processamento e Armazenamento dos Dados**

### **1. Configurar o Pipeline**

Abra o arquivo `pipelines.py` e edite-o para processar e armazenar os dados.

**Exemplo de código:**

```python
import sqlite3

class MercadoLivrePipeline:

    def open_spider(self, spider):
        self.connection = sqlite3.connect('mercado_livre.db')
        self.cursor = self.connection.cursor()
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS produtos (
                titulo TEXT,
                preco TEXT,
                link TEXT
            )
        ''')
        self.connection.commit()

    def close_spider(self, spider):
        self.connection.close()

    def process_item(self, item, spider):
        self.cursor.execute('''
            INSERT INTO produtos (titulo, preco, link) VALUES (?, ?, ?)
        ''', (
            item.get('titulo'),
            item.get('preco'),
            item.get('link')
        ))
        self.connection.commit()
        return item
```

### **2. Ativar o Pipeline**

No arquivo `settings.py`, ative o pipeline:

```python
ITEM_PIPELINES = {
    'mercado_livre.pipelines.MercadoLivrePipeline': 300,
}
```

---

## **Recursos Adicionais**

- [Documentação Oficial do Scrapy](https://docs.scrapy.org/en/latest/)
- [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)
- [Seletores CSS](https://www.w3schools.com/cssref/css_selectors.asp)
- [Uso de XPath no Scrapy](https://docs.scrapy.org/en/latest/topics/selectors.html#working-with-xpath)

---

## **Observações Finais**

- Certifique-se de respeitar os termos de uso do site do Mercado Livre.
- Evite sobrecarregar o servidor com muitas requisições simultâneas.
- Use `DOWNLOAD_DELAY` no `settings.py` para adicionar um atraso entre as requisições:

```python
DOWNLOAD_DELAY = 1  # atraso de 1 segundo
```

================================================
File: /Bootcamp - Python para dados/README.md
================================================
## Calendário Bootcamp - Python do zero

Um intensivo único para você iniciar com Python e ir até tópicos avançados (API por exemplo) para você resolver problemas reais

| Aula  | Workshop                                                                 | Horário |
|-------|--------------------------------------------------------------------------|---------|
| [Aula 01](./aula01) | Python, Git e VScode: Python do Zero                                    | 12am    |
| [Aula 02](./aula02) | TypeError, Type Check, Type Conversion, try-except e if                | 12am    |
| [Aula 03](./aula03) | Controle de Fluxo: DEBUG, IF, FOR, While, Listas e Dicionários         | 12am    |
| [Aula 04](./aula04) | Tipos complexos e Type Hint (Dicionários vs DataFrames Vs Tabelas Vs Excel) | 12am    |
| [Aula 05](./aula05) | Projeto 01: Leitura e Escrita de Arquivos, lendo 1 bilhão de linhas    | 12am    |
| [Aula 06](./aula06) | Exercício de revisão                                                    | 12am    |
| [Aula 07](./aula07) | Funções em Python e Estrutura de Dados - Parte 1                       | 12am    |
| [Aula 08](./aula08) | Funções em Python e Estrutura de Dados - Parte 2                       | 12am    |
| [Aula 09](./aula09) | Funções em Python e Estrutura de Dados - Parte 3                       | 12am    |
| Aula 10 | Aula de revisão                                                         | 12am    |
| [Aula 11](./aula11-15) | Introdução a POO                                                        | 19pm    |
| [Aula 12](./aula11-15) | Introdução às Classes em Python - Parte 01                             | 19pm    |
| [Aula 13](./aula11-15) | Introdução às Classes em Python - Parte 02                             | 19pm    |
| [Aula 14](./aula11-15) | Introdução às Classes em Python - Parte 03                             | 19pm    |
| [Aula 15](./aula11-15) | Introdução às Classes em Python - Parte 04                             | 19pm    |
| [Aula 16](./aula16) | Aula de revisão de programação orientada a objetos + SQLModel          | 12am    |
| [Aula 17](./aula17) | SQLAlchemy - Conjunto de ferramentas para manipular SQL em Python      | 12am    |
| [Aula 18](./aula18) | O que é uma API? Request, Pydantic e fazendo nosso CRUD                | 12am    |
| [Aula 19](./aula19) | O que é uma API? Criando nossa primeira API                            | 12am    |
| [Aula 20](./aula20) | Nosso Projeto de CRUD Backend + Frontend + Banco de Dados              | 12am    |


================================================
File: /Bootcamp - Python para dados/aula01/README.md
================================================
## Bootcamp Aula 01

Bem-vindo ao material da Aula 01, focado no setup inicial das ferramentas essenciais para a programação em Python: Python, Git & Github, e VSCode.

Se você está revisando ou não conseguiu acompanhar o primeiro vídeo no Youtube, este é o lugar certo para começar!

Assista ao vídeo no Youtube sobre o Setup de Python do Zero aqui: [Python para dados](https://youtu.be/-M4pMd2yQOM?si=HVY3EDLt6aApJRG5)

Além disso, vamos criar nossa primeira aplicação em Python, e estou muito empolgado para guiá-lo nessa jornada!

## Desafio do dia: Cálculo de Bônus com Entrada do Usuário

Escreva um programa em Python que solicita ao usuário para digitar seu nome, o valor do seu salário mensal e o valor do bônus que recebeu. O programa deve, então, imprimir uma mensagem saudando o usuário pelo nome e informando o valor do salário em comparação com o bônus recebido.

![imagem](pic.png)

![imagem](pic2.png)

## Agenda (1 hora)

1. **Introdução** (5 minutos)
2. **Instalação do Python** (5 minutos)
3. **Instalação do VSCode** (5 minutos)
4. **Configuração do Git e Github** (5 minutos)
5. **Cálculo do nosso primeiro KPI** (20 minutos)
6. **Dúvidas e desafio** (20 minutos)

### 1. Introdução 

Nesta aula, focaremos em preparar seu ambiente de desenvolvimento. Antes de começarmos a programar em Python, precisamos configurar as ferramentas necessárias para tornar nosso trabalho mais eficiente e organizado.

- Nossa plataforma
- Grupos de ajuda e de whatsapp
- Por que estudar Python?
- Python na engenharia de dados

### 2. Instalação do Python

Começaremos instalando o Python, a linguagem que utilizaremos neste curso.

* Baixe o instalador do Python no [site oficial](https://www.python.org/).
* Siga as instruções de instalação de acordo com seu sistema operacional.
* Após a instalação, abra o terminal e verifique se o Python foi instalado corretamente digitando `python --version`.

Ao digitar `python` no terminal, iniciaremos o interpretador Python.

Vamos praticar com alguns exemplos simples:

1. **Imprimir uma mensagem de boas-vindas:**
    
    ```python
    print("Bom dia turma do Bootcamp!")
    ```
    
2. **Realizar uma operação matemática simples (soma):**
    
    ```python
    print(3 + 5)
    ```

3) **Atribuir um valor a uma variável e imprimi-la:**

    ```python
    variavel = "Bom dia turma!"
    print(variavel)
    ```

## 3. Instalação do VSCode

Agora, vamos instalar o VSCode, um ambiente de desenvolvimento leve e altamente personalizável.

* Baixe o instalador do VSCode no site oficial.
* Siga as instruções de instalação de acordo com seu sistema operacional.
* Após a instalação, abra o VSCode e familiarize-se com a interface.

## 4. Instalação do Git

Por fim, vamos configurar o Git para versionamento de código.

* Baixe o instalador do Git no site oficial.
* Siga as instruções de instalação de acordo com seu sistema operacional.
* Após a instalação, abra o terminal e configure seu nome de usuário e email no Git utilizando os comandos:
    
    ```arduino
    git config --global user.name "Seu Nome"
    git config --global user.email "seuemail@example.com"
    ```
    
## 5. Exercícios de `print()`, `input()`, variáveis e estrutura de dados

Parabéns! Agora que configuramos todas as ferramentas necessárias, vamos concluir nossa aula com um simples "Hello World" em Python.

* Abra o VSCode.
* Crie um novo arquivo Python.
* Crie um repositório no Github
* Crie nosso primeiro arquivo `main.py`

### Comandos

### 1) `print()`

Para usar o comando print basta digitar `print("Alguma coisa")`

Repare que ao redor do nosso texto, coloquei `"o que eu quero escrever"`

Assim, eu consigo avisar ao Python que o que eu quero imprimir é um texto, uma `string`.

Caso eu retire o parentese, irá dar errado.

```
print("Alguma coisa)
print(Alguma coisa")
print(Alguma coisa)
```

**Como lidar com erros?**

O primeiro passo quando você tem algum erro é buscar no Google uma solução para ele

Também é possíve somar operações

```python
print(3 + 5)
```

```python
print("Olá" + " " + "Turma")
```

#### 2) `input()`

O comando input() em Python é uma função incorporada usada para capturar dados de entrada do usuário. Quando esse comando é executado, o programa pausa sua execução e espera que o usuário digite algo no console (ou terminal) e pressione Enter. Os dados inseridos pelo usuário são então retornados pela função input() como uma string (texto). Isso permite que programas interativos recebam informações do usuário para diversos fins, como parâmetros de execução, dados para processamento, escolhas em menus interativos, entre outros.

**Exemplo:**

```python
input("Digite seu nome: ")
```

**Concatenando texto**

```python
print("Olá, " + input("Digite seu nome: ") + "!")
```

**Exercício 01**

Crie programa que o usuário digita o seu nome e retorna o número de caracteres

```bash
Digite o seu nome: Luciano
7
```

**Exercício 02**

Criar um programa onde o usuário digite dois valores e apareça a soma

```python
Digite um valor: 7
Digite outro valo: 10
17
```

#### Considerações Importantes

* **Tipo de Dados**: Por padrão, tudo o que é capturado pelo `input()` é tratado como uma `string`. Se você precisar trabalhar com outro tipo de dado (como inteiros ou floats), será necessário converter a entrada do usuário para o tipo desejado usando funções como `int()` ou `float()`.
    
    ```python
    idade = int(input("Digite sua idade: "))
    ```
    
* **Segurança**: Ao usar `input()` para receber dados do usuário, é importante considerar a validação desses dados, especialmente se eles forem usados em operações críticas ou transmitidos a outras partes do sistema.
    
* **Usabilidade**: O `prompt` deve ser claro e informativo para guiar o usuário sobre o que precisa ser inserido, melhorando a usabilidade e a experiência do usuário.
    
O comando `input()` é uma ferramenta fundamental para criar scripts e programas interativos em Python, permitindo a coleta de dados de entrada de uma maneira fácil e acessível.

#### 3) Declaração e Atribuição de Variáveis

Variáveis em Python são fundamentais para o desenvolvimento de programas, pois atuam como "recipientes" para armazenar dados que podem ser modificados ao longo da execução de um script. Ao contrário de algumas outras linguagens de programação, Python é dinamicamente tipado, o que significa que você não precisa declarar explicitamente o tipo de uma variável antes de usá-la. O tipo de uma variável é determinado automaticamente pelo Python no momento da atribuição de um valor.

**Declaração e Atribuição de Variáveis**

A atribuição de um valor a uma variável em Python é feita com o operador `=`. Por exemplo:

```python
numero = 10
mensagem = "Olá, mundo!"
```

No exemplo acima, `numero` é uma variável que armazena um inteiro (`10`), e `mensagem` é uma variável que armazena uma string (`"Olá, mundo!"`).

**Tipos de Dados**

Python suporta vários tipos de dados, incluindo, mas não se limitando a:

* Inteiros (`int`)
* Números de ponto flutuante (`float`)
* Strings (`str`)
* Listas (`list`)
* Tuplas (`tuple`)
* Dicionários (`dict`)
* Booleanos (`bool`)

A linguagem determina o tipo de dados de uma variável no momento da atribuição, o que permite grande flexibilidade, mas também exige atenção para evitar erros de tipo.

**Nomes de Variáveis**

Python tem algumas regras e convenções para nomes de variáveis:

* Os nomes podem conter letras, números e sublinhados (`_`), mas não podem começar com um número.
* Os nomes de variáveis são _case-sensitive_, o que significa que `variavel`, `Variavel`, e `VARIaVEL` são consideradas três variáveis diferentes.
* Existem algumas palavras reservadas que não podem ser usadas como nomes de variáveis, como `if`, `for`, `class`, entre outras.
* É recomendado seguir a convenção _snake_case_ para nomes de variáveis que consistem em mais de uma palavra, como `nome_usuario` ou `total_pedidos`.

**Dinamismo e Reatribuição**

Uma característica importante das variáveis em Python é a possibilidade de reatribuí-las a diferentes tipos de dados:

```python
x = 100        # x é um inteiro
x = "Python"   # Agora x é uma string
```

Isso demonstra a tipagem dinâmica do Python, mas também destaca a importância de gerenciar tipos de dados com cuidado para evitar confusão ou erros em programas mais complexos.

**Escopo de Variáveis**

O escopo de uma variável determina onde ela é acessível dentro do código. Variáveis definidas em um bloco principal são globalmente acessíveis, enquanto variáveis definidas dentro de funções são locais a essas funções, a menos que sejam explicitamente declaradas como `global`.

Entender variáveis e tipos de dados é essencial para programação em Python, pois permite manipular dados de maneira eficaz e criar programas dinâmicos e flexíveis. A capacidade de Python de inferir tipos de dados torna a linguagem acessível para iniciantes, ao mesmo tempo em que oferece poderosas funcionalidades para programadores experientes.

**Exercício 03: Refatore o exercício 02 atribuindo variáveis**

```python
print(len(input("Digite o seu nome: ")))
```

```
Qual é o seu nome? Luciano
7
```

## Questão: Cálculo de Bônus com Entrada do Usuário

Escreva um programa em Python que solicita ao usuário para digitar seu nome, o valor do seu salário mensal e o valor do bônus que recebeu. O programa deve, então, imprimir uma mensagem saudando o usuário pelo nome e informando o valor do salário em comparação com o bônus recebido.

![imagem](pic.png)

![imagem](pic2.png)

#### Instruções:

1. O programa deve começar solicitando ao usuário que insira seu nome.
2. Em seguida, o programa deve pedir ao usuário para inserir o valor do seu salário. Considere que este valor pode ser um número decimal.
3. Depois, o programa deve solicitar a porcentagem do bônus recebido pelo usuário, que também pode ser um número decimal.
4. O cálculo do KPI do bônus de 2024 é de `1000 + salario * bônus`
5. Finalmente, o programa deve imprimir uma mensagem no seguinte formato: "Olá [nome], o seu valor bônus foi de 5000".

#### Exemplo de Saída:

Se o usuário digitar "Luciano" como nome, "5000" como salário e "1.5" como bônus, o programa deve imprimir:

```bash
Olá Luciano, o seu bônus foi de 8500
```

6. Salve esse script python como `kpi.py`
7. Faça uma documentação simples de como executar esse programa, utilize o `README`
8. Salve no Git e no Github

Isso ajudará a consolidar seu conhecimento sobre o Git e a familiarizar-se com o processo de versionamento de código.

## Conclusão

Nesta aula, aprendemos a configurar nosso ambiente de desenvolvimento para começar a programar em Python. Com o Python, o VSCode, e o Git instalados e configurados, estamos prontos para mergulhar mais fundo no mundo da programação! Nos vemos na próxima aula!

================================================
File: /Bootcamp - Python para dados/aula01/kpi.py
================================================
# 1) Solicita ao usuário que digite seu nome

# 2) Solicita ao usuário que digite o valor do seu salário
# Converte a entrada para um número de ponto flutuante

# 3) Solicita ao usuário que digite o valor do bônus recebido
# Converte a entrada para um número de ponto flutuante

# 4) Calcule o valor do bônus final

# 5) Imprima cálculo do KPI para o usuário

# 6) Imprime a mensagem personalizada incluindo o nome do usuário, salário e bônus

# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?

================================================
File: /Bootcamp - Python para dados/aula01/aovivo/desafio.py
================================================
CONSTANTE_BONUS = 1000

# 1) Solicita ao usuário que digite seu nome
nome_usuario = input("Digite o seu nome: ")

# 2) Solicita ao usuário que digite o valor do seu salário
# Converte a entrada para um número de ponto flutuante
salario_usuario = float(input("Digite o seu salario: "))

# 3) Solicita ao usuário que digite o valor do bônus recebido
# Converte a entrada para um número de ponto flutuante
bonus_usuario = float(input("Digite o seu bonus: "))

# 4) Calcule o valor do bônus final

valor_do_bonus = CONSTANTE_BONUS + salario_usuario * bonus_usuario

# 5) Imprime a mensagem personalizada incluindo o nome do usuário e o valor do bonus
print(f"O usuario {nome_usuario} possui o bonus de {valor_do_bonus}")

# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?

================================================
File: /Bootcamp - Python para dados/aula01/aovivo/exemplo_01.py
================================================
# Crie programa que o usuário digita o seu nome e retorna o número de caracteres

print(len(input("Digite o seu nome: ")))

================================================
File: /Bootcamp - Python para dados/aula02/README.md
================================================
# Aula 02: TypeError, Type Check, Type Conversion, try-except e if

Bem-vindo à segunda aula do bootcamp! 

![imagem_01](./pics/1.png)

Hoje, vamos explorar mais a fundo um dos conceitos mais fundamentais da programação: variáveis. As variáveis são essenciais para armazenar e manipular dados em qualquer linguagem de programação, e em Python não é diferente. Nesta aula, vamos entender o que são variáveis, como declará-las, os tipos de dados simples suportados por Python e algumas boas práticas para nomeá-las.

Além disso, vamos mostrar como lidar e trabalhar com erros usando TypeError, Type Check, Type Conversion, try-except e if

## 1. Tipos primitivos

Variáveis são espaços de memória designados para armazenar dados que podem ser modificados durante a execução de um programa. Em Python, a declaração de variáveis é dinâmica, o que significa que o tipo de dado é inferido durante a atribuição.

**Exemplo em Python:**

Python suporta vários tipos de dados simples, tais como:

- **Inteiros (`int`)**: Representam números inteiros.
- **Ponto Flutuante (`float`)**: Representam números reais.
- **Strings (`str`)**: Representam sequências de caracteres.
- **Booleanos (`bool`)**: Representam valores verdadeiros (`True`) ou falsos (`False`).

![imagem_02](./pics/2.png)

#### 1. Inteiros (`int`)

* **Métodos e operações:**
    1. `+` (adição)
    2. `-` (subtração)
    3. `*` (multiplicação)
    4. `//` (divisão inteira)
    5. `%` (módulo - resto da divisão)

#### 2. Números de Ponto Flutuante (`float`)

* **Métodos e operações:**
    1. `+` (adição)
    2. `-` (subtração)
    3. `*` (multiplicação)
    4. `/` (divisão)
    5. `**` (potenciação)

#### 3. Strings (`str`)

* **Métodos e operações:**
    1. `.upper()` (converte para maiúsculas)
    2. `.lower()` (converte para minúsculas)
    3. `.strip()` (remove espaços em branco no início e no final)
    4. `.split(sep)` (divide a string em uma lista, utilizando `sep` como delimitador)
    5. `+` (concatenação de strings)

#### 4. Booleanos (`bool`)

* **Operações lógicas:**
    1. `and` (E lógico)
    2. `or` (OU lógico)
    3. `not` (NÃO lógico)
    4. `==` (igualdade)
    5. `!=` (diferença)

### Exercícios

#### Inteiros (`int`)

1. Escreva um programa que soma dois números inteiros inseridos pelo usuário.
2. Crie um programa que receba um número do usuário e calcule o resto da divisão desse número por 5.
3. Desenvolva um programa que multiplique dois números fornecidos pelo usuário e mostre o resultado.
4. Faça um programa que peça dois números inteiros e imprima a divisão inteira do primeiro pelo segundo.
5. Escreva um programa que calcule o quadrado de um número fornecido pelo usuário.

#### Números de Ponto Flutuante (`float`)

6. Escreva um programa que receba dois números flutuantes e realize sua adição.
7. Crie um programa que calcule a média de dois números flutuantes fornecidos pelo usuário.
8. Desenvolva um programa que calcule a potência de um número (base e expoente fornecidos pelo usuário).
9. Faça um programa que converta a temperatura de Celsius para Fahrenheit.
10. Escreva um programa que calcule a área de um círculo, recebendo o raio como entrada.

#### Strings (`str`)

11. Escreva um programa que receba uma string do usuário e a converta para maiúsculas.
12. Crie um programa que receba o nome completo do usuário e imprima o nome com todas as letras minúsculas.
13. Desenvolva um programa que peça ao usuário para inserir uma frase e, em seguida, imprima esta frase sem espaços em branco no início e no final.
14. Faça um programa que peça ao usuário para digitar uma data no formato "dd/mm/aaaa" e, em seguida, imprima o dia, o mês e o ano separadamente.
15. Escreva um programa que concatene duas strings fornecidas pelo usuário.

#### Booleanos (`bool`)

16. Escreva um programa que avalie duas expressões booleanas inseridas pelo usuário e retorne o resultado da operação AND entre elas.
17. Crie um programa que receba dois valores booleanos do usuário e retorne o resultado da operação OR.
18. Desenvolva um programa que peça ao usuário para inserir um valor booleano e, em seguida, inverta esse valor.
19. Faça um programa que compare se dois números fornecidos pelo usuário são iguais.
20. Escreva um programa que verifique se dois números fornecidos pelo usuário são diferentes.

### Exercícios Resolução

### Exercício 1: Soma de Dois Números Inteiros

```python
# num1 = int(input("Digite o primeiro número inteiro: "))
# num2 = int(input("Digite o segundo número inteiro: "))
num1 = 8  # Exemplo de entrada
num2 = 12  # Exemplo de entrada
resultado_soma = num1 + num2
print("A soma é:", resultado_soma)
```

### Exercício 2: Resto da Divisão por 5

```python
# num = int(input("Digite um número: "))
num = 18  # Exemplo de entrada
resultado_resto = num % 5
print("O resto da divisão por 5 é:", resultado_resto)
```

### Exercício 3: Multiplicação de Dois Números

```python
# num1 = int(input("Digite o primeiro número: "))
# num2 = int(input("Digite o segundo número: "))
num1 = 5  # Exemplo de entrada
num2 = 7  # Exemplo de entrada
resultado_multiplicacao = num1 * num2
print("O resultado da multiplicação é:", resultado_multiplicacao)
```

### Exercício 4: Divisão Inteira do Primeiro pelo Segundo Número

```python
# num1 = int(input("Digite o primeiro número inteiro: "))
# num2 = int(input("Digite o segundo número inteiro: "))
num1 = 20  # Exemplo de entrada
num2 = 3  # Exemplo de entrada
resultado_divisao_inteira = num1 // num2
print("O resultado da divisão inteira é:", resultado_divisao_inteira)
```

### Exercício 5: Quadrado de um Número

```python
# num = int(input("Digite um número: "))
num = 6  # Exemplo de entrada
resultado_quadrado = num ** 2
print("O quadrado do número é:", resultado_quadrado)
```

### Exercício 6: Adição de Dois Números Flutuantes

```python
# num1 = float(input("Digite o primeiro número flutuante: "))
# num2 = float(input("Digite o segundo número flutuante: "))
num1 = 2.5  # Exemplo de entrada
num2 = 4.5  # Exemplo de entrada
resultado_soma = num1 + num2
print("A soma é:", resultado_soma)
```

### Exercício 7: Média de Dois Números Flutuantes

```python
# num1 = float(input("Digite o primeiro número flutuante: "))
# num2 = float(input("Digite o segundo número flutuante: "))
num1 = 3.5  # Exemplo de entrada
num2 = 7.5  # Exemplo de entrada
media = (num1 + num2) / 2
print("A média é:", media)
```

### Exercício 8: Potência de um Número

```python
# base = float(input("Digite a base: "))
# expoente = float(input("Digite o expoente: "))
base = 2.0  # Exemplo de entrada
expoente = 3.0  # Exemplo de entrada
potencia = base ** expoente
print("O resultado da potência é:", potencia)
```

### Exercício 9: Conversão de Celsius para Fahrenheit

```python
# celsius = float(input("Digite a temperatura em Celsius: "))
celsius = 30.0  # Exemplo de entrada
fahrenheit = (celsius * 9/5) + 32
print(f"{celsius}°C é igual a {fahrenheit}°F")
```

### Exercício 10: Área de um Círculo

```python
# raio = float(input("Digite o raio do círculo: "))
raio = 5.0  # Exemplo de entrada
area = 3.14159 * raio ** 2
print("A área do círculo é:", area)
```

### Exercício 11: Converter String para Maiúsculas

```python
# texto = input("Digite um texto: ")
texto = "Olá, mundo!"  # Exemplo de entrada
texto_maiusculas = texto.upper()
print("Texto em maiúsculas:", texto_maiusculas)
```

### Exercício 12: Imprimir Nome Completo em Minúsculas

```python
# nome_completo = input("Digite seu nome completo: ")
nome_completo = "Fulano de Tal"  # Exemplo de entrada
nome_minusculas = nome_completo.lower()
print("Nome em minúsculas:", nome_minusculas)
```

### Exercício 13: Remover Espaços em Branco de uma Frase

```python
# frase = input("Digite uma frase: ")
frase = "  Olá, mundo!  "  # Exemplo de entrada
frase_sem_espacos = frase.strip()
print("Frase sem espaços no início e no final:", frase_sem_espacos)
```

### Exercício 14: Separar Dia, Mês e Ano de uma Data

```python
# data = input("Digite uma data no formato dd/mm/aaaa: ")
data = "01/01/2024"  # Exemplo de entrada
dia, mes, ano = data.split("/")
print("Dia:", dia)
print("Mês:", mes)
print("Ano:", ano)
```

### Exercício 15: Concatenar Duas Strings

```python
# parte1 = input("Digite a primeira parte do texto: ")
# parte2 = input("Digite a segunda parte do texto: ")
parte1 = "Olá,"  # Exemplo de entrada
parte2 = " mundo!"  # Exemplo de entrada
texto_concatenado = parte1 + parte2
print("Texto concatenado:", texto_concatenado)
```

#### Exercício 16. Operador `and` (E lógico)

```python
# Exemplo de entrada
valor1 = True
valor2 = False
resultado_and = valor1 and valor2
print("Resultado do AND lógico:", resultado_and)
```

#### Exercício 17. Operador `or` (OU lógico)

```python
# Exemplo de entrada
resultado_or = valor1 or valor2
print("Resultado do OR lógico:", resultado_or)
```

#### Exercício  18. Operador `not` (NÃO lógico)

```python
# Exemplo de entrada
resultado_not = not valor1
print("Resultado do NOT lógico:", resultado_not)
```

#### Exercício 19. Operador `==` (Igualdade)

```python
# Exemplo de entrada
num1 = 5
num2 = 5
resultado_igualdade = (num1 == num2)
print("Resultado da igualdade:", resultado_igualdade)
```

#### Exercício 20. Operador `!=` (Diferença)

```python
# Exemplo de entrada
resultado_diferenca = (num1 != num2)
print("Resultado da diferença:", resultado_diferenca)
```

# TypeError, Type Check e Type Conversion em Python

Python é uma linguagem de programação dinâmica, mas fortemente tipada, o que significa que não é necessário declarar tipos de variáveis explicitamente, mas o tipo de uma variável é determinado pelo valor que ela armazena. Isso introduz a necessidade de compreender como Python lida com diferentes tipos de dados, especialmente quando se trata de operações que envolvem múltiplos tipos. Vamos explorar três conceitos importantes: `TypeError`, verificação de tipo (`type check`), e conversão de tipo (`type conversion`).

## TypeError

Um `TypeError` ocorre em Python quando uma operação ou função é aplicada a um objeto de tipo inadequado. Python não sabe como aplicar a operação porque os tipos de dados não são compatíveis.

### Exemplo de TypeError

Um exemplo clássico é tentar utilizar a função `len()` com um inteiro, o que resulta em `TypeError`, pois `len()` espera um objeto iterável, como uma string, lista, ou tupla, não um inteiro.

```python
# Exemplo que causa TypeError
try:
    resultado = len(5)
except TypeError as e:
    print(e)  # Imprime a mensagem de erro
```

O código acima tenta obter o comprimento de um inteiro, o que não faz sentido, resultando na mensagem de erro: "object of type 'int' has no len()".

## Type Check

Verificação de tipo (`type check`) é o processo de verificar o tipo de uma variável. Isso pode ser útil para garantir que operações ou funções sejam aplicadas apenas a tipos de dados compatíveis, evitando erros em tempo de execução.

### Exemplo de Type Check

Para verificar o tipo de uma variável em Python, você pode usar a função `type()` ou `isinstance()`.

```python
numero = 10
if isinstance(numero, int):
    print("A variável é um inteiro.")
else:
    print("A variável não é um inteiro.")
```

Este código verifica se `numero` é uma instância de `int` e imprime uma mensagem apropriada.

## Type Conversion

Conversão de tipo (`type conversion`), também conhecida como casting, é o processo de converter o valor de uma variável de um tipo para outro. Python oferece várias funções integradas para realizar conversões explícitas de tipo, como `int()`, `float()`, `str()`, etc.

### Exemplo de Type Conversion

Se você quiser somar um inteiro e um número flutuante, pode ser necessário converter o inteiro para flutuante ou vice-versa para garantir que a operação de soma seja realizada corretamente.

```python
numero_inteiro = 5
numero_flutuante = 2.5
# Converte o inteiro para flutuante e realiza a soma
soma = float(numero_inteiro) + numero_flutuante
print(soma)  # Resultado: 7.5
```

### try-except

A estrutura `try-except` é usada para tratamento de exceções em Python. Uma exceção é um erro que ocorre durante a execução do programa e que, se não tratado, interrompe o fluxo normal do programa e termina sua execução. O tratamento de exceções permite que o programa lide com erros de maneira elegante, permitindo que continue a execução ou falhe de forma controlada.

* **try:** Este bloco é o primeiro na estrutura de tratamento de exceções. Python tenta executar o código dentro deste bloco.
* **except:** Se uma exceção ocorrer no bloco `try`, a execução imediatamente salta para o bloco `except`. Você pode especificar tipos de exceção específicos para capturar e tratar apenas essas exceções. Se nenhum tipo de exceção for especificado, ele captura todas as exceções.

#### Exemplo de try-except

```python
try:
    # Código que pode gerar uma exceção
    resultado = 10 / 0
except ZeroDivisionError:
    # Código que executa se a exceção ZeroDivisionError for levantada
    print("Divisão por zero não é permitida.")
```

### if

O `if` é uma estrutura de controle de fluxo que permite ao programa executar diferentes ações com base em diferentes condições. Se a condição avaliada pelo `if` for verdadeira (`True`), o bloco de código indentado sob ele será executado. Se a condição for falsa (`False`), o bloco de código será ignorado.

* **if:** Avalia uma condição. Se a condição for verdadeira, executa o bloco de código associado.
* **elif:** Abreviação de "else if". Permite verificar múltiplas condições em sequência.
* **else:** Executa um bloco de código se todas as condições anteriores no `if` e `elif` forem falsas.

#### Exemplo de if

```python
idade = 20
if idade < 18:
    print("Menor de idade")
elif idade == 18:
    print("Exatamente 18 anos")
else:
    print("Maior de idade")
```

Ambas as estruturas, `try-except` e `if`, são fundamentais para a criação de programas em Python que são capazes de lidar com situações inesperadas (como erros de execução) e tomar decisões com base em condições, permitindo assim que você construa programas mais robustos, flexíveis e seguros.

## Exercícios

Aqui estão cinco exercícios que envolvem `TypeError`, verificação de tipo (`type check`), o uso de `try-except` para tratamento de exceções e a utilização da estrutura condicional `if`. Esses exercícios aumentam progressivamente em dificuldade e abordam situações práticas onde você pode aplicar esses conceitos.

### Exercício 21: Conversor de Temperatura

Escreva um programa que converta a temperatura de Celsius para Fahrenheit. O programa deve solicitar ao usuário a temperatura em Celsius e, utilizando `try-except`, garantir que a entrada seja numérica, tratando qualquer `ValueError`. Imprima o resultado em Fahrenheit ou uma mensagem de erro se a entrada não for válida.

### Exercício 22: Verificador de Palíndromo

Crie um programa que verifica se uma palavra ou frase é um palíndromo (lê-se igualmente de trás para frente, desconsiderando espaços e pontuações). Utilize `try-except` para garantir que a entrada seja uma string. Dica: Utilize a função `isinstance()` para verificar o tipo da entrada.

### Exercício 23: Calculadora Simples

Desenvolva uma calculadora simples que aceite duas entradas numéricas e um operador (+, -, *, /) do usuário. Use `try-except` para lidar com divisões por zero e entradas não numéricas. Utilize `if-elif-else` para realizar a operação matemática baseada no operador fornecido. Imprima o resultado ou uma mensagem de erro apropriada.

### Exercício 24: Classificador de Números

Escreva um programa que solicite ao usuário para digitar um número. Utilize `try-except` para assegurar que a entrada seja numérica e utilize `if-elif-else` para classificar o número como "positivo", "negativo" ou "zero". Adicionalmente, identifique se o número é "par" ou "ímpar".

### Exercício 25: Conversão de Tipo com Validação

Crie um script que solicite ao usuário uma lista de números separados por vírgula. O programa deve converter a string de entrada em uma lista de números inteiros. Utilize `try-except` para tratar a conversão de cada número e validar que cada elemento da lista convertida é um inteiro. Se a conversão falhar ou um elemento não for um inteiro, imprima uma mensagem de erro. Se a conversão for bem-sucedida para todos os elementos, imprima a lista de inteiros.

## Exercícios Resolvidos

### Exercício 21: Conversor de Temperatura

```python
try:
    celsius = float(input("Digite a temperatura em Celsius: "))
    fahrenheit = (celsius * 9/5) + 32
    print(f"{celsius}°C é igual a {fahrenheit}°F.")
except ValueError:
    print("Por favor, digite um número válido para a temperatura.")
```

### Exercício 22: Verificador de Palíndromo

```python
entrada = input("Digite uma palavra ou frase: ")
if isinstance(entrada, str):
    formatado = entrada.replace(" ", "").lower()
    if formatado == formatado[::-1]:
        print("É um palíndromo.")
    else:
        print("Não é um palíndromo.")
else:
    print("Entrada inválida. Por favor, digite uma palavra ou frase.")
```

### Exercício 23: Calculadora Simples

```python
try:
    num1 = float(input("Digite o primeiro número: "))
    num2 = float(input("Digite o segundo número: "))
    operador = input("Digite o operador (+, -, *, /): ")
    if operador == '+':
        resultado = num1 + num2
    elif operador == '-':
        resultado = num1 - num2
    elif operador == '*':
        resultado = num1 * num2
    elif operador == '/' and num2 != 0:
        resultado = num1 / num2
    else:
        print("Operador inválido ou divisão por zero.")
    print("Resultado:", resultado)
except ValueError:
    print("Erro: Entrada inválida. Certifique-se de inserir números.")
```

### Exercício 24: Classificador de Números

```python
try:
    numero = int(input("Digite um número: "))
    if numero > 0:
        print("Positivo")
    elif numero < 0:
        print("Negativo")
    else:
        print("Zero")
    if numero % 2 == 0:
        print("Par")
    else:
        print("Ímpar")
except ValueError:
    print("Por favor, digite um número inteiro válido.")
```

### Exercício 25: Conversão de Tipo com Validação

```python
entrada_lista = input("Digite uma lista de números separados por vírgula: ")
numeros_str = entrada_lista.split(",")
numeros_int = []
try:
    for num in numeros_str:
        numeros_int.append(int(num.strip()))
    print("Lista de inteiros:", numeros_int)
except ValueError:
    print("Erro: certifique-se de que todos os elementos são números inteiros válidos.")
```

![imagem_03](./pics/3.png)

### Desafio - Refatorar o projeto da aula anterior evitando Bugs!

Para resolver os bugs identificados — tratamento de entradas inválidas que não podem ser convertidas para um número de ponto flutuante e prevenção de valores negativos para salário e bônus, você pode modificar o código diretamente. Isso envolve adicionar verificações adicionais após a tentativa de conversão para garantir que os valores sejam positivos.

![imagem_05](./pics/5.png)

```python
# Solicita ao usuário que digite seu nome
try:
    nome = input("Digite seu nome: ")

    # Verifica se o nome está vazio
    if len(nome) == 0:
        raise ValueError("O nome não pode estar vazio.")
    # Verifica se há números no nome
    elif any(char.isdigit() for char in nome):
        raise ValueError("O nome não deve conter números.")
    else:
        print("Nome válido:", nome)
except ValueError as e:
    print(e)

# Solicita ao usuário que digite o valor do seu salário e converte para float

try:
    salario = float(input("Digite o valor do seu salário: "))
    if salario < 0:
        print("Por favor, digite um valor positivo para o salário.")
except ValueError:
    print("Entrada inválida para o salário. Por favor, digite um número.")

# Solicita ao usuário que digite o valor do bônus recebido e converte para float
try:
    bonus_recebido = float(input("Digite o valor do bônus recebido: "))
    if bonus_recebido < 0:
        print("Por favor, digite um valor positivo para o bônus.")
except ValueError:
    print("Entrada inválida para o bônus. Por favor, digite um número.")

# Assumindo uma lógica de cálculo para o bônus final e KPI
bonus_final = bonus_recebido * 1.2  # Exemplo, ajuste conforme necessário
kpi = (salario + bonus_final) / 1000  # Exemplo simples de KPI

# Imprime as informações para o usuário
print(f"Seu KPI é: {kpi:.2f}")
print(f"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_final:.2f}.")
```

![imagem_04](./pics/4.png)

================================================
File: /Bootcamp - Python para dados/aula02/desafio.py
================================================
### Desafio - Refatorar o projeto da aula anterior evitando Bugs!

# 1) Solicita ao usuário que digite seu nome

# 2) Solicita ao usuário que digite o valor do seu salário
# Converte a entrada para um número de ponto flutuante

# 3) Solicita ao usuário que digite o valor do bônus recebido
# Converte a entrada para um número de ponto flutuante

# 4) Calcule o valor do bônus final

# 5) Imprime a mensagem personalizada incluindo o nome do usuário, salário e bônus

# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?

================================================
File: /Bootcamp - Python para dados/aula02/desafio_resolvido.py
================================================
# Solicita ao usuário que digite seu nome
try:
    nome = input("Digite seu nome: ")

    # Verifica se o nome está vazio
    if len(nome) == 0:
        raise ValueError("O nome não pode estar vazio.")
        exit()
    # Verifica se há números no nome
    elif any(char.isdigit() for char in nome):
        raise ValueError("O nome não deve conter números.")
        exit()
    else:
        print("Nome válido:", nome)
except ValueError as e:
    print(e)
    exit()

# Solicita ao usuário que digite o valor do seu salário e converte para float

try:
    salario = float(input("Digite o valor do seu salário: "))
    if salario < 0:
        print("Por favor, digite um valor positivo para o salário.")
except ValueError:
    print("Entrada inválida para o salário. Por favor, digite um número.")
    exit()

# Solicita ao usuário que digite o valor do bônus recebido e converte para float
try:
    bonus = float(input("Digite o valor do bônus recebido: "))
    if bonus < 0:
        print("Por favor, digite um valor positivo para o bônus.")
except ValueError:
    print("Entrada inválida para o bônus. Por favor, digite um número.")
    exit()

bonus_recebido = 1000 + salario * bonus  # Exemplo simples de KPI

# Imprime as informações para o usuário
print(f"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.")

================================================
File: /Bootcamp - Python para dados/aula02/exercicios.py
================================================
# #### Inteiros (`int`)

# 1. Escreva um programa que soma dois números inteiros inseridos pelo usuário.
# 2. Crie um programa que receba um número do usuário e calcule o resto da divisão desse número por 5.
# 3. Desenvolva um programa que multiplique dois números fornecidos pelo usuário e mostre o resultado.
# 4. Faça um programa que peça dois números inteiros e imprima a divisão inteira do primeiro pelo segundo.
# 5. Escreva um programa que calcule o quadrado de um número fornecido pelo usuário.

# #### Números de Ponto Flutuante (`float`)

# 6. Escreva um programa que receba dois números flutuantes e realize sua adição.
# 7. Crie um programa que calcule a média de dois números flutuantes fornecidos pelo usuário.
# 8. Desenvolva um programa que calcule a potência de um número (base e expoente fornecidos pelo usuário).
# 9. Faça um programa que converta a temperatura de Celsius para Fahrenheit.
# 10. Escreva um programa que calcule a área de um círculo, recebendo o raio como entrada.

# #### Strings (`str`)

# 11. Escreva um programa que receba uma string do usuário e a converta para maiúsculas.
# 12. Crie um programa que receba o nome completo do usuário e imprima o nome com todas as letras minúsculas.
# 13. Desenvolva um programa que peça ao usuário para inserir uma frase e, em seguida, imprima esta frase sem espaços em branco no início e no final.
# 14. Faça um programa que peça ao usuário para digitar uma data no formato "dd/mm/aaaa" e, em seguida, imprima o dia, o mês e o ano separadamente.
# 15. Escreva um programa que concatene duas strings fornecidas pelo usuário.

# #### Booleanos (`bool`)

# 16. Escreva um programa que avalie duas expressões booleanas inseridas pelo usuário e retorne o resultado da operação AND entre elas.
# 17. Crie um programa que receba dois valores booleanos do usuário e retorne o resultado da operação OR.
# 18. Desenvolva um programa que peça ao usuário para inserir um valor booleano e, em seguida, inverta esse valor.
# 19. Faça um programa que compare se dois números fornecidos pelo usuário são iguais.
# 20. Escreva um programa que verifique se dois números fornecidos pelo usuário são diferentes.

# #### try-except e if

# 21: Conversor de Temperatura
# 22: Verificador de Palíndromo
# 23: Calculadora Simples
# 24: Classificador de Números
# 25: Conversão de Tipo com Validação



================================================
File: /Bootcamp - Python para dados/aula02/aovivo/desafio.py
================================================
CONSTANTE_BONUS = 1000

# 1) Solicita ao usuário que digite seu nome
#nome_usuario = input("Digite o seu nome: ")

# nome_usuario = 33 isso e um erro?

nome_usuario = input("Digite o seu nome: ")

if nome_usuario.isdigit():
    print("Voce digitou seu nome errado")
    exit()
elif len(nome_usuario) == 0:
    print("Voce nao digitou nada")
    exit()
elif nome_usuario.isspace():
    print("Voce digitou so espaco")
    exit()

# 2) Solicita ao usuário que digite o valor do seu salário
# Converte a entrada para um número de ponto flutuante
salario_usuario = float(input("Digite o seu salario: "))

# 3) Solicita ao usuário que digite o valor do bônus recebido
# Converte a entrada para um número de ponto flutuante
bonus_usuario = float(input("Digite o seu bonus: "))

# 4) Calcule o valor do bônus final

valor_do_bonus = CONSTANTE_BONUS + salario_usuario * bonus_usuario

# 5) Imprime a mensagem personalizada incluindo o nome do usuário e o valor do bonus
print(f"O usuario {nome_usuario} possui o bonus de {valor_do_bonus}")

# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?

================================================
File: /Bootcamp - Python para dados/aula02/aovivo/exercicios.py
================================================
import math

# #### Inteiros (`int`)

# 1. Escreva um programa que soma dois números inteiros inseridos pelo usuário.
# 2. Crie um programa que receba um número do usuário e calcule o resto da divisão desse número por 5.
# 3. Desenvolva um programa que multiplique dois números fornecidos pelo usuário e mostre o resultado.
# 4. Faça um programa que peça dois números inteiros e imprima a divisão inteira do primeiro pelo segundo.

numero_01 = int(input("Inserir um numero inteiro: "))
numero_02 = int(input("Inserir outro numero inteiro: "))
resultado = numero_01 // numero_02
print(resultado)

# 5. Escreva um programa que calcule o quadrado de um número fornecido pelo usuário.

# #### Números de Ponto Flutuante (`float`)

# 6. Escreva um programa que receba dois números flutuantes e realize sua adição.
# 7. Crie um programa que calcule a média de dois números flutuantes fornecidos pelo usuário.
# 8. Desenvolva um programa que calcule a potência de um número (base e expoente fornecidos pelo usuário).
# 9. Faça um programa que converta a temperatura de Celsius para Fahrenheit.
# 10. Escreva um programa que calcule a área de um círculo, recebendo o raio como entrada.

#raio_do_circulo = float(input("Digite o raio: "))
#area_do_circulo = math.pi * raio_do_circulo ** 2
# area_do_circulo_formatada = "{:.2f}".format(area_do_circulo)
#print(f"{area_do_circulo:.2f}")

# #### Strings (`str`)

# 11. Escreva um programa que receba uma string do usuário e a converta para maiúsculas.
# 12. Crie um programa que receba o nome completo do usuário e imprima o nome com todas as letras minúsculas.
# 13. Desenvolva um programa que peça ao usuário para inserir uma frase e, em seguida, imprima esta frase sem espaços em branco no início e no final.
# 14. Faça um programa que peça ao usuário para digitar uma data no formato "dd/mm/aaaa" e, em seguida, imprima o dia, o mês e o ano separadamente.
# 15. Escreva um programa que concatene duas strings fornecidas pelo usuário.

# data_do_usuario = input("Insira uma data no formato dd/mm/aaaa: ")
# lista_de_dia_mes_ano = data_do_usuario.split("/")
# print(f"O elemento 1 e o: {lista_de_dia_mes_ano[0]}")
# print(f"O elemento 2 e o: {lista_de_dia_mes_ano[1]}")
# print(f"O elemento 3 e o: {lista_de_dia_mes_ano[2]}")

# #### Booleanos (`bool`)

# 16. Escreva um programa que avalie duas expressões booleanas inseridas pelo usuário e retorne o resultado da operação AND entre elas.
# 17. Crie um programa que receba dois valores booleanos do usuário e retorne o resultado da operação OR.
# 18. Desenvolva um programa que peça ao usuário para inserir um valor booleano e, em seguida, inverta esse valor.
# 19. Faça um programa que compare se dois números fornecidos pelo usuário são iguais.
# 20. Escreva um programa que verifique se dois números fornecidos pelo usuário são diferentes.

# #### try-except e if

# 21: Conversor de Temperatura
# 22: Verificador de Palíndromo
# 23: Calculadora Simples
# 24: Classificador de Números
# 25: Conversão de Tipo com Validação

================================================
File: /Bootcamp - Python para dados/aula02/aovivo/main.py
================================================
# Exemplo que causa TypeError

# try:
#     resultado = len(3)
#     print(resultado)
# except TypeError as e:
#     print(e)
# else:
#     print("tudo ocorreu bem")
# finally:
#     print("o importante e participar")  

# numero = int(input("Insira um numero :"))
# if isinstance(numero, ):
#     print("A variável é um inteiro.")
# else:
#     print("A variável não é um inteiro.")



================================================
File: /Bootcamp - Python para dados/aula03/README.md
================================================
# Aula 03: DEBUG, IF, FOR, While, Listas e Dicionários em Python

![imagem_01](./pics/1.png)

Bem-vindo à terceira aula do bootcamp!

Hoje, vamos explorar estruturas de controle de fluxo como if, for, e while. 

Usamos estrutura de Controle de Fluxo para tomar decisões!

Databricks tem workflow
![imagem_06](./pics/6.webp)

Airflow principal ferramenta de workflow
![imagem_07](./pics/7.png)

### Estruturas de Controle de Fluxo

Exploraremos como utilizar `if` para tomar decisões baseadas em condições, `for` para iterar sobre sequências de dados, e `while` para executar blocos de código enquanto uma condição for verdadeira.

Para saber mais:
[Doc](https://docs.python.org/pt-br/3/tutorial/controlflow.html)

![imagem_02](./pics/2.png)

## Estruturas de Controle de Fluxo

O if é uma estrutura condicional fundamental em Python que avalia se uma condição é verdadeira (True) e, se for, executa um bloco de código. Se a condição inicial não for verdadeira, você pode usar elif (else if) para verificar condições adicionais, e else para executar um bloco de código quando nenhuma das condições anteriores for verdadeira.

Provavelmente o mais conhecido comando de controle de fluxo é o if. Por exemplo:

```python
x = int(input("Please enter an integer: "))

if x < 0:
    x = 0
    print('Negative changed to zero')
elif x == 0:
    print('Zero')
elif x == 1:
    print('Single')
else:
    print('More')
```

### Exercício 1: Verificação de Qualidade de Dados

Você está analisando um conjunto de dados de vendas e precisa garantir que todos os registros tenham valores positivos para `quantidade` e `preço`. Escreva um programa que verifique esses campos e imprima "Dados válidos" se ambos forem positivos ou "Dados inválidos" caso contrário.

```python
quantidade = 10  # Exemplo de valor, substitua com input do usuário se necessário
preço = 20  # Exemplo de valor, substitua com input do usuário se necessário

if quantidade > 0 and preço > 0:
    print("Dados válidos")
else:
    print("Dados inválidos")
```

### Exercício 2: Classificação de Dados de Sensor

Imagine que você está trabalhando com dados de sensores IoT. Os dados incluem medições de temperatura. Você precisa classificar cada leitura como 'Baixa', 'Normal' ou 'Alta'. Considerando que:

* Temperatura < 18°C é 'Baixa'
* Temperatura >= 18°C e <= 26°C é 'Normal'
* Temperatura > 26°C é 'Alta'

```python
temperatura = 22  # Exemplo de valor, substitua com input do usuário se necessário

if temperatura < 18:
    print("Baixa")
elif 18 <= temperatura <= 26:
    print("Normal")
else:
    print("Alta")
```

### Exercício 3: Filtragem de Logs por Severidade

Você está analisando logs de uma aplicação e precisa filtrar mensagens com severidade 'ERROR'. Dado um registro de log em formato de dicionário como `log = {'timestamp': '2021-06-23 10:00:00', 'level': 'ERROR', 'message': 'Falha na conexão'}`, escreva um programa que imprima a mensagem se a severidade for 'ERROR'.

```python
log = {'timestamp': '2021-06-23 10:00:00', 'level': 'ERROR', 'message': 'Falha na conexão'}

if log['level'] == 'ERROR':
    print(log['message'])
```

### Exercício 4: Validação de Dados de Entrada

Antes de processar os dados de usuários em um sistema de recomendação, você precisa garantir que cada usuário tenha idade entre 18 e 65 anos e tenha fornecido um email válido. Escreva um programa que valide essas condições e imprima "Dados de usuário válidos" ou o erro específico encontrado.

```python
idade = 25  # Exemplo de valor, substitua com input do usuário se necessário
email = "usuario@exemplo.com"  # Exemplo de valor, substitua com input do usuário se necessário

if not 18 <= idade <= 65:
    print("Idade fora do intervalo permitido")
elif "@" not in email or "." not in email:
    print("Email inválido")
else:
    print("Dados de usuário válidos")
```

### Exercício 5: Detecção de Anomalias em Dados de Transações

Você está trabalhando em um sistema de detecção de fraude e precisa identificar transações suspeitas. Uma transação é considerada suspeita se o valor for superior a R$ 10.000 ou se ocorrer fora do horário comercial (antes das 9h ou depois das 18h). Dada uma transação como `transacao = {'valor': 12000, 'hora': 20}`, verifique se ela é suspeita.

```python
transacao = {'valor': 12000, 'hora': 20}

if transacao['valor'] > 10000 or transacao['hora'] < 9 or transacao['hora'] > 18:
    print("Transação suspeita")
else:
    print("Transação normal")
```

### FOR

O loop `for` é utilizado para iterar sobre os itens de qualquer sequência, como listas, strings, ou objetos de dicionário, e executar um bloco de código para cada item. É especialmente útil quando você precisa executar uma operação para cada elemento de uma coleção.

O comando for em Python é um pouco diferente do que costuma ser em C ou Pascal. Ao invés de sempre iterar sobre uma progressão aritmética de números (como no Pascal), ou permitir ao usuário definir o passo de iteração e a condição de parada (como C), o comando for do Python itera sobre os itens de qualquer sequência (seja uma lista ou uma string), na ordem que aparecem na sequência. Por exemplo:

```python
# Measure some strings:
words = ['cat', 'window', 'defenestrate']
for w in words:
    print(w, len(w))
```

```python
# Measure some strings:
nome = ['Luciano']
for letra in nome:
    print(letra)
```

Se você precisa iterar sobre sequências numéricas, a função embutida `range()` é a resposta. Ela gera progressões aritméticas:


```python
for i in range(5):
    print(i)
```

O ponto de parada fornecido nunca é incluído na lista; range(10) gera uma lista com 10 valores, exatamente os índices válidos para uma sequência de comprimento 10. É possível iniciar o intervalo com outro número, ou alterar a razão da progressão (inclusive com passo negativo):


```python
list(range(5, 10))
[5, 6, 7, 8, 9]

list(range(0, 10, 3))
[0, 3, 6, 9]

list(range(-10, -100, -30))
[-10, -40, -70]
```

Para iterar sobre os índices de uma sequência, combine range() e len() da seguinte forma:

```python
a = ['Mary', 'had', 'a', 'little', 'lamb']
for i in range(len(a)):
    print(i, a[i])
```

[Material sobre Dicionários](https://www.youtube.com/watch?v=ZWj8o692qGY)

#### 6. Contagem de Palavras em Textos

**Objetivo:** Dado um texto, contar quantas vezes cada palavra única aparece nele.

```python
texto = "a raposa marrom salta sobre o cachorro preguiçoso"
palavras = texto.split()
contagem_palavras = {}

for palavra in palavras:
    if palavra in contagem_palavras:
        contagem_palavras[palavra] += 1
    else:
        contagem_palavras[palavra] = 1

print(contagem_palavras)
```

#### 7. Normalização de Dados

**Objetivo:** Normalizar uma lista de números para que fiquem na escala de 0 a 1.

```python
numeros = [10, 20, 30, 40, 50]
minimo = min(numeros)
maximo = max(numeros)
normalizados = [(x - minimo) / (maximo - minimo) for x in numeros]

print(normalizados)
```

#### 8. Filtragem de Dados Faltantes

**Objetivo:** Dada uma lista de dicionários representando dados de usuários, filtrar aqueles que têm um campo específico faltando.

```python
usuarios = [
    {"nome": "Alice", "email": "alice@example.com"},
    {"nome": "Bob", "email": ""},
    {"nome": "Carol", "email": "carol@example.com"}
]

usuarios_validos = [usuario for usuario in usuarios if usuario["email"]]

print(usuarios_validos)
```

#### 9. Extração de Subconjuntos de Dados

**Objetivo:** Dada uma lista de números, extrair apenas aqueles que são pares.

```python
numeros = range(1, 11)
pares = [x for x in numeros if x % 2 == 0]

print(pares)
```

#### 10. Agregação de Dados por Categoria

**Objetivo:** Dado um conjunto de registros de vendas, calcular o total de vendas por categoria.

```python
vendas = [
    {"categoria": "eletrônicos", "valor": 1200},
    {"categoria": "livros", "valor": 200},
    {"categoria": "eletrônicos", "valor": 800}
]

total_por_categoria = {}
for venda in vendas:
    categoria = venda["categoria"]
    valor = venda["valor"]
    if categoria in total_por_categoria:
        total_por_categoria[categoria] += valor
    else:
        total_por_categoria[categoria] = valor

print(total_por_categoria)
```

### Exercícios com WHILE

O loop while é uma estrutura de controle de fluxo fundamental em Python, permitindo executar um bloco de código repetidamente enquanto uma condição especificada é avaliada como verdadeira (True). Na engenharia de dados, o uso do while pode ser extremamente útil para diversas tarefas, como monitoramento contínuo de fontes de dados, execução de processos de ETL (Extract, Transform, Load) até que não haja mais dados para processar, ou mesmo para implementar tentativas de reconexão automáticas a serviços ou bancos de dados quando a primeira tentativa falha.

#### Exemplo de Uso do while em Engenharia de Dados
Um cenário comum em engenharia de dados é a necessidade de executar uma tarefa de maneira periódica, como verificar novos dados em um diretório, fazer polling de uma API para novas respostas ou monitorar mudanças em um banco de dados. Nestes casos, um loop while pode ser utilizado para manter o script rodando continuamente ou até que uma condição específica seja atingida (por exemplo, um sinal para desligar ou uma condição de erro).

#### Exemplo Prático: while True com Pausa

Um exemplo direto do uso de while True em Python é criar um loop infinito que executa uma ação a cada intervalo definido, como imprimir uma mensagem a cada 10 segundos. Isso pode ser útil para monitorar processos ou dados em tempo real com uma verificação periódica.

```python
import time

while True:
    print("Verificando novos dados...")
    # Aqui você pode adicionar o código para verificar novos dados,
    # por exemplo, checar a existência de novos arquivos em um diretório,
    # fazer uma consulta a um banco de dados ou API, etc.
    
    time.sleep(10)  # Pausa o loop por 10 segundos
```
Neste exemplo, o while True cria um loop infinito, que é uma maneira poderosa de manter um script rodando continuamente. O print simula a ação de verificar novos dados, e o time.sleep(10) pausa a execução do loop por 10 segundos antes da próxima iteração. Essa abordagem é simples, mas eficaz para muitos cenários de monitoramento e polling em engenharia de dados, permitindo que o script execute uma verificação ou tarefa de maneira periódica.

Contudo, é importante usar loops infinitos com cautela para evitar criar condições em que o script possa consumir recursos desnecessários ou tornar-se difícil de encerrar de forma controlada. Em ambientes de produção, outras abordagens como agendamento de tarefas (por exemplo, usando cron jobs em sistemas Unix) ou o uso de sistemas de enfileiramento de mensagens e triggers de banco de dados podem ser mais adequados para algumas dessas tarefas.

#### 11. Leitura de Dados até Flag

**Objetivo:** Ler dados de entrada até que uma palavra-chave específica ("sair") seja fornecida.

```python
dados = []
entrada = ""
while entrada.lower() != "sair":
    entrada = input("Digite um valor (ou 'sair' para terminar): ")
    if entrada.lower() != "sair":
```

#### 12. Validação de Entrada

**Objetivo:** Solicitar ao usuário um número dentro de um intervalo específico até que a entrada seja válida.

```python
numero = int(input("Digite um número entre 1 e 10: "))
while numero < 1 or numero > 10:
    print("Número fora do intervalo!")
    numero = int(input("Por favor, digite um número entre 1 e 10: "))

print("Número válido!")
```

#### 13. Consumo de API Simulado

**Objetivo:** Simular o consumo de uma API paginada, onde cada "página" de dados é processada em loop até que não haja mais páginas.

```python
pagina_atual = 1
paginas_totais = 5  # Simulação, na prática, isso viria da API

while pagina_atual <= paginas_totais:
    print(f"Processando página {pagina_atual} de {paginas_totais}")
    # Aqui iria o código para processar os dados da página
    pagina_atual += 1

print("Todas as páginas foram processadas.")
```

#### 14. Tentativas de Conexão

**Objetivo:** Simular tentativas de reconexão a um serviço com um limite máximo de tentativas.

```python
tentativas_maximas = 5
tentativa = 1

while tentativa <= tentativas_maximas:
    print(f"Tentativa {tentativa} de {tentativas_maximas}")
    # Simulação de uma tentativa de conexão
    # Aqui iria o código para tentar conectar
    if True:  # Suponha que a conexão foi bem-sucedida
        print("Conexão bem-sucedida!")
        break
    tentativa += 1
else:
    print("Falha ao conectar após várias tentativas.")
```

#### 15. Processamento de Dados com Condição de Parada

**Objetivo:** Processar itens de uma lista até encontrar um valor específico que indica a parada.

```python
itens = [1, 2, 3, "parar", 4, 5]

i = 0
while i < len(itens):
    if itens[i] == "parar":
        print("Parada encontrada, encerrando o processamento.")
        break
    # Processa o item
    print(f"Processando item: {itens[i]}")
    i += 1
```
## Estruturas de Controle de Fluxo

![imagem_03](./pics/3.png)

Integre na solução anterior um fluxo de While que repita o fluxo até que o usuário insira as informações corretas
    
##### Solução
```python
# Inicializa as variáveis para o controle do loop
nome_valido = False
salario_valido = False
bonus_valido = False

# Loop para verificar o nome
while not nome_valido:
    try:
        nome = input("Digite seu nome: ")
        if len(nome) == 0:
            raise ValueError("O nome não pode estar vazio.")
        elif any(char.isdigit() for char in nome):
            raise ValueError("O nome não deve conter números.")
        else:
            print("Nome válido:", nome)
            nome_valido = True
    except ValueError as e:
        print(e)

# Loop para verificar o salário
while not salario_valido:
    try:
        salario = float(input("Digite o valor do seu salário: "))
        if salario < 0:
            print("Por favor, digite um valor positivo para o salário.")
        else:
            salario_valido = True
    except ValueError:
        print("Entrada inválida para o salário. Por favor, digite um número.")

# Loop para verificar o bônus
while not bonus_valido:
    try:
        bonus = float(input("Digite o valor do bônus recebido: "))
        if bonus < 0:
            print("Por favor, digite um valor positivo para o bônus.")
        else:
            bonus_valido = True
    except ValueError:
        print("Entrada inválida para o bônus. Por favor, digite um número.")

bonus_recebido = 1000 + salario * bonus  # Exemplo simples de cálculo de bônus

# Imprime as informações para o usuário
print(f"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.")
```

![imagem_0](./pics/4.png)


================================================
File: /Bootcamp - Python para dados/aula03/desafio.py
================================================
# Integre na solução anterior um fluxo de While 
# que repita o fluxo até que o usuário insira as 
# informações corretas

# Solicita ao usuário que digite seu nome
nome_valido = False
salario_valido = False
bonus_valido = False

while not nome_valido:
    try:
        nome = input("Digite seu nome: ")

        # Verifica se o nome está vazio
        if len(nome) == 0:
            raise ValueError("O nome não pode estar vazio.")
        # Verifica se há números no nome
        elif any(char.isdigit() for char in nome):
            raise ValueError("O nome não deve conter números.")
        else:
            print("Nome válido:", nome)
            nome_valido = True
    except ValueError as e:
        print(e)

# Solicita ao usuário que digite o valor do seu salário e converte para float

try:
    salario = float(input("Digite o valor do seu salário: "))
    if salario < 0:
        print("Por favor, digite um valor positivo para o salário.")
except ValueError:
    print("Entrada inválida para o salário. Por favor, digite um número.")
    exit()

# Solicita ao usuário que digite o valor do bônus recebido e converte para float
try:
    bonus = float(input("Digite o valor do bônus recebido: "))
    if bonus < 0:
        print("Por favor, digite um valor positivo para o bônus.")
except ValueError:
    print("Entrada inválida para o bônus. Por favor, digite um número.")
    exit()

bonus_recebido = 1000 + salario * bonus  # Exemplo simples de KPI

# Imprime as informações para o usuário
print(f"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.")

================================================
File: /Bootcamp - Python para dados/aula03/exercicios.py
================================================
### Exercício 1: Verificação de Qualidade de Dados
# Você está analisando um conjunto de dados de vendas e precisa garantir 
# que todos os registros tenham valores positivos para `quantidade` e `preço`. 
# Escreva um programa que verifique esses campos e imprima "Dados válidos" se ambos 
# forem positivos ou "Dados inválidos" caso contrário.

### Exercício 2: Classificação de Dados de Sensor
# Imagine que você está trabalhando com dados de sensores IoT. 
# Os dados incluem medições de temperatura. Você precisa classificar cada leitura 
# como 'Baixa', 'Normal' ou 'Alta'. Considerando que:

### Exercício 3: Filtragem de Logs por Severidade
# Você está analisando logs de uma aplicação e precisa filtrar mensagens 
# com severidade 'ERROR'. Dado um registro de log em formato de dicionário 
# como `log = {'timestamp': '2021-06-23 10:00:00', 'level': 'ERROR', 'message': 'Falha na conexão'}`, 
# escreva um programa que imprima a mensagem se a severidade for 'ERROR'.

### Exercício 4: Validação de Dados de Entrada
# Antes de processar os dados de usuários em um sistema de recomendação, 
# você precisa garantir que cada usuário tenha idade entre 18 e 65 anos e tenha 
# fornecido um email válido. Escreva um programa que valide essas condições 
# e imprima "Dados de usuário válidos" ou o erro específico encontrado.

### Exercício 5: Detecção de Anomalias em Dados de Transações
# Você está trabalhando em um sistema de detecção de fraude e precisa identificar 
# transações suspeitas. Uma transação é considerada suspeita se o valor for superior 
# a R$ 10.000 ou se ocorrer fora do horário comercial (antes das 9h ou depois das 18h). 
# Dada uma transação como `transacao = {'valor': 12000, 'hora': 20}`, verifique se ela é suspeita.

### Exercício 6. Contagem de Palavras em Textos
# Objetivo:** Dado um texto, contar quantas vezes cada palavra única aparece nele.

### Exercício 7. Normalização de Dados
# Objetivo:** Normalizar uma lista de números para que fiquem na escala de 0 a 1.

### Exercício 8. Filtragem de Dados Faltantes
# Objetivo:** Dada uma lista de dicionários representando dados de usuários, filtrar aqueles que têm um campo específico faltando

### Exercício 9. Extração de Subconjuntos de Dados
# Objetivo:** Dada uma lista de números, extrair apenas aqueles que são pares.

### Exercício 10. Agregação de Dados por Categoria
# Objetivo:** Dado um conjunto de registros de vendas, calcular o total de vendas por categoria.

### Exercícios com WHILE

### Exercício 11. Leitura de Dados até Flag
# Ler dados de entrada até que uma palavra-chave específica ("sair") seja fornecida.

### Exercício 12. Validação de Entrada
# Solicitar ao usuário um número dentro de um intervalo específico até que a entrada seja válida.

### Exercício 13. Consumo de API Simulado
# Simular o consumo de uma API paginada, onde cada "página" de dados é processada em loop até que não haja mais páginas.

### Exercício 14. Tentativas de Conexão
# Simular tentativas de reconexão a um serviço com um limite máximo de tentativas.

### Exercício 15. Processamento de Dados com Condição de Parada
# Processar itens de uma lista até encontrar um valor específico que indica a parada.

================================================
File: /Bootcamp - Python para dados/aula03/aovivo/README.md
================================================
# aula04_bootcamp


================================================
File: /Bootcamp - Python para dados/aula03/aovivo/arquivos.py
================================================
import csv

caminho_do_arquivo: str = "exemplo.csv"

arquivo_csv: list = []

with open(file=caminho_do_arquivo, mode="r", encoding='utf-8') as arquivo:
    leitor_csv: csv.DictReader = csv.DictReader(arquivo)

    for linha in leitor_csv:
        arquivo_csv.append(linha)

print(arquivo_csv)

================================================
File: /Bootcamp - Python para dados/aula03/aovivo/aula_algoritmo.py
================================================
lista_de_numeros: list = [40,50,60,70,0,-408593,1,50]
lista_de_numeros_02: list = [40,60,70,0,-408593,1,50]
lista_de_numeros_03: list = [40,60,70,0,1,50]

# [40,50,60,70,0,-408593,1,50]
# [50,60,,700,-408593,1,50]

nome = "luciano"

def ordernar_lista(numeros: list) -> list:
    
    nova_lista_de_numeros = []
    
    try:
        nova_lista_de_numeros = numeros.copy()
        
        for i in range(len(nova_lista_de_numeros)):
            for j in range(i+1, len(nova_lista_de_numeros)):
                if nova_lista_de_numeros[i] > nova_lista_de_numeros[j]:
                    nova_lista_de_numeros[i], nova_lista_de_numeros[j] = nova_lista_de_numeros[j], nova_lista_de_numeros[i]
    
    except:
        print("Voce colocou uma str e ao inves de uma lista")
    
    return nova_lista_de_numeros

ordernar_lista(nome)

================================================
File: /Bootcamp - Python para dados/aula03/aovivo/data_science.py
================================================
from aula_algoritmo import ordernar_lista_de_numeros

lista = [3,5,10,-1,-3804]

nova_lista = ordernar_lista_de_numeros(numeros = lista)
print(nova_lista)

================================================
File: /Bootcamp - Python para dados/aula03/aovivo/desafio.py
================================================
# Integre na solução anterior um fluxo de While 
# que repita o fluxo até que o usuário insira as 
# informações corretas

# Solicita ao usuário que digite seu nome

nome_valido = False
salario_valido = False
bonus_valido = False

while nome_valido is not True:
    try:
        nome = "Luciano"

        # Verifica se o nome está vazio
        if len(nome) == 0:
            raise ValueError("O nome não pode estar vazio.")
        # Verifica se há números no nome
        elif nome.isdigit():
            raise ValueError("O nome não deve conter números.") 
        else:
            nome_valido = True
            print("Nome válido:", nome)
    except ValueError as e:
        print(e)
# Solicita ao usuário 
# que digite o valor do seu salário e converte para float

while salario_valido is not True:
    try:
        salario = 2000
        if salario < 0:
            print("Por favor, digite um valor positivo para o salário.")
        else:
            salario_valido = True
    except ValueError:
        print("Entrada inválida para o salário. Por favor, digite um número.")

# Solicita ao usuário que digite o valor do bônus recebido e converte para float
        
while bonus_valido is not True:        
    try:
        bonus = 3.0
        if bonus < 0:
            print("Por favor, digite um valor positivo para o bônus.")
        else:
            bonus_valido = True
    except ValueError:
        print("Entrada inválida para o bônus. Por favor, digite um número.")

bonus_recebido = 1000 + salario * bonus  # Exemplo simples de KPI

# Imprime as informações para o usuário
print(f"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.")

================================================
File: /Bootcamp - Python para dados/aula03/aovivo/exemplo_lista.py
================================================
from typing import Dict, Optional, Any

import json

lista: Any = ["Sapato", 39, 10.38, True]

produto_01: Dict[str, Any] = {
    "nome":"Sapato",
    "quantidade":39,
    "preco": 10.38,
    "diponibilidade": True
}

produto_02: dict = {
    "nome":"Televisao",
    "quantidade":10,
    "preco": 70.38,
    "diponibilidade": "false"
}

carrinho: list = []

carrinho.append(produto_01)
carrinho.append(produto_02)

carrinho_json = json.dumps(carrinho)
print(carrinho_json)


================================================
File: /Bootcamp - Python para dados/aula03/aovivo/exercicios.py
================================================
# Crie um dicionário para armazenar informações de um livro, 
# incluindo título, autor e ano de publicação. Imprima cada informação.

from typing import Dict, Any

livro_01: Dict[str, Any] = {
    "Titulo": "Game of Thrones",
    "Autor": "Estagiario",
    "Ano": 2005
}

livro_02: Dict[str, Any] = {
    "Titulo": "Game of Thrones 2",
    "Autor": "Estagiario",
    "Ano": 2007
}

lista_de_livros = []

lista_de_livros.append(livro_01)
lista_de_livros.append(livro_02)

# print(lista_de_livros)

lista_de_livros_usando_dict:dict = {
    "livro_01": {"Titulo": "Game of Thrones",
    "Autor": "Estagiario",
    "Ano": 2005},

    "livro_02": {    "Titulo": "Game of Thrones 2",
    "Autor": "Estagiario",
    "Ano": 2007}
}

================================================
File: /Bootcamp - Python para dados/aula03/aovivo/gustavo.py
================================================
nome_valido: bool = False
salario_valido: bool = False
bonus_valido: bool = False

================================================
File: /Bootcamp - Python para dados/aula03/aovivo/main.py
================================================
nome_valido: bool = False
salario_valido: bool = False
bonus_valido: bool = False

while not nome_valido:
    try:
        nome: str = input("Digite seu nome: ")

        # Verifica se o nome está vazio
        if len(nome) == 0:
            raise ValueError("O nome não pode estar vazio.")
        # Verifica se há números no nome
        elif any(char.isdigit() for char in nome):
            raise ValueError("O nome não deve conter números.")
        else:
            print("Nome válido:", nome)
            nome_valido = True
    except ValueError as e:
        print(e)

# Solicita ao usuário que digite o valor do seu salário e converte para float

try:
    salario: float = float(input("Digite o valor do seu salário: "))
    if salario < 0:
        print("Por favor, digite um valor positivo para o salário.")
except ValueError:
    print("Entrada inválida para o salário. Por favor, digite um número.")
    exit()

# Solicita ao usuário que digite o valor do bônus recebido e converte para float
try:
    bonus: float = float(input("Digite o valor do bônus recebido: "))
    if bonus < 0:
        print("Por favor, digite um valor positivo para o bônus.")
except ValueError:
    print("Entrada inválida para o bônus. Por favor, digite um número.")
    exit()

bonus_recebido: float = 1000 + salario * bonus  # Exemplo simples de KPI


# Imprime as informações para o usuário
print(f"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.")

================================================
File: /Bootcamp - Python para dados/aula04/README.md
================================================
# Aula 04 | Type hint, Tipos complexos (Dicionários vs DataFrames Vs Tabelas Vs Excel) e Funções

Bem-vindos à quarta aula de Python e SQL focada em Engenharia de Dados. Nesta aula você vai aprender sobre: Type Hint, Listas e Dicionários e Funções. Esses elementos são essenciais para a manipulação de dados, ajudando na organização, interpretação e análise eficiente das informações. 

![imagem_01](./pic/1.jpg)

Vamos começar com uma introdução a cada um desses temas para nos prepararmos para o nosso primeiro prsojeto, como ler 1 bilhão de linhas!

![imagem_05](./pic/5.jpeg)

E o nosso workshop de sabado, dia 24 as 9am, como validar 1 bilhão de linhass.

![imagem_06](./pic/6.jpeg)

![imagem_02](./pic/2.jpg)

## 1. Type Hint

O uso de Type Hint ajuda a tornar o código mais legível e seguro, especificando o tipo de dados esperados por funções e variáveis. Na engenharia de dados, isso é especialmente útil para garantir que as funções de manipulação e análise de dados recebam os dados no formato correto, reduzindo erros em tempo de execução.

Para demonstrar como utilizar Type Hints com tipos primitivos em Python, vamos criar quatro variáveis representando os tipos mais comuns: int para números inteiros, float para números de ponto flutuante, str para strings (cadeias de caracteres) e bool para valores booleanos. Type Hints são usados para indicar o tipo de uma variável no momento da sua declaração, melhorando a legibilidade do código e facilitando a detecção de erros.

Sem Type Hint
```python
idade = 30
altura = 1.75
nome = "Alice"
is_estudante = True
```

Com Type Hint
```python
idade: int = 30
altura: float = 1.75
nome: str = "Alice"
estudante: bool = True
```

O uso de Type Hint ajuda a tornar o código mais legível e seguro, especificando o tipo de dados esperados por funções e variáveis. Na engenharia de dados, isso é especialmente útil para garantir que as funções de manipulação e análise de dados recebam os dados no formato correto, reduzindo erros em tempo de execução.

Na Python, a tipagem de funções é facilitada pelo uso de "Type Hints" (Dicas de Tipo), uma característica introduzida no Python 3.5 através do PEP 484. Os Type Hints permitem aos desenvolvedores especificar os tipos de dados esperados para os parâmetros de uma função e o tipo de dado que a função deve retornar. Embora essas dicas de tipo não sejam estritamente aplicadas em tempo de execução, elas são extremamente úteis para ferramentas de análise estática de código, melhorando a legibilidade do código e ajudando na detecção precoce de erros.

### Tipagem Fraca vs. Forte

* **Tipagem Forte**: Em linguagens com tipagem forte, uma vez que uma variável é atribuída a um tipo, não pode ser automaticamente tratada como outro tipo sem uma conversão explícita. Python é um exemplo de linguagem com tipagem forte. Isso significa que operações que misturam tipos incompatíveis (como adicionar um número a uma string) resultarão em erro.
    
* **Tipagem Fraca**: Linguagens com tipagem fraca permitem maior flexibilidade nas operações entre diferentes tipos, fazendo conversões de tipo implícitas. JavaScript é um exemplo clássico, onde você pode adicionar números a strings sem erros, resultando em uma concatenação de texto.
    
### Tipagem Estática vs. Dinâmica

* **Tipagem Estática**: Linguagens de tipagem estática, como Java e C++, exigem que o tipo de cada variável seja declarado explicitamente no momento da compilação. Isso ajuda a detectar erros de tipo antes da execução do programa, aumentando a segurança do tipo e potencialmente melhorando o desempenho.
    
* **Tipagem Dinâmica**: Python é um exemplo de linguagem com tipagem dinâmica, onde os tipos são inferidos em tempo de execução e não precisam ser declarados explicitamente. Isso oferece flexibilidade e rapidez no desenvolvimento, mas pode aumentar o risco de erros de tipo que só serão detectados em tempo de execução.

Exercício será tipar o desafio da aula 03

```python
nome_valido = False
salario_valido = False
bonus_valido = False

while not nome_valido:
    try:
        nome = input("Digite seu nome: ")

        # Verifica se o nome está vazio
        if len(nome) == 0:
            raise ValueError("O nome não pode estar vazio.")
        # Verifica se há números no nome
        elif any(char.isdigit() for char in nome):
            raise ValueError("O nome não deve conter números.")
        else:
            print("Nome válido:", nome)
            nome_valido = True
    except ValueError as e:
        print(e)

# Solicita ao usuário que digite o valor do seu salário e converte para float

try:
    salario = float(input("Digite o valor do seu salário: "))
    if salario < 0:
        print("Por favor, digite um valor positivo para o salário.")
except ValueError:
    print("Entrada inválida para o salário. Por favor, digite um número.")
    exit()

# Solicita ao usuário que digite o valor do bônus recebido e converte para float
try:
    bonus = float(input("Digite o valor do bônus recebido: "))
    if bonus < 0:
        print("Por favor, digite um valor positivo para o bônus.")
except ValueError:
    print("Entrada inválida para o bônus. Por favor, digite um número.")
    exit()

bonus_recebido = 1000 + salario * bonus  # Exemplo simples de KPI

# Imprime as informações para o usuário
print(f"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.")
```

## 2. Listas e Dicionários

### Importância na Engenharia de Dados

Listas e dicionários são estruturas de dados versáteis que permitem armazenar e manipular coleções de dados de forma eficiente. Na engenharia de dados, essas estruturas são fundamentais para organizar dados coletados de diversas fontes, facilitando operações como filtragem, busca, agregação e transformação de dados.

### Exercícios de Listas e Dicionários

1. Crie uma lista com os números de 1 a 10 e use um loop para imprimir cada número elevado ao quadrado.
2. Dada a lista `["Python", "Java", "C++", "JavaScript"]`, remova o item "C++" e adicione "Ruby".
3. Crie um dicionário para armazenar informações de um livro, incluindo título, autor e ano de publicação. Imprima cada informação.
4. Escreva um programa que conta o número de ocorrências de cada caractere em uma string usando um dicionário.
5. Dada a lista `["maçã", "banana", "cereja"]` e o dicionário `{"maçã": 0.45, "banana": 0.30, "cereja": 0.65}`, calcule o preço total da lista de compras.

### Exercícios de Listas e Dicionários resolvidos

## Resoluções de Listas e Dicionários

### 1. Lista de números ao quadrado

```python
numeros = list(range(1, 11))
for numero in numeros:
    print(quadrados**2)
```

### 2. Modificar lista de linguagens

```python
linguagens = ["Python", "Java", "C++", "JavaScript"]
linguagens.remove("C++")
linguagens.append("Ruby")
print(linguagens)
```

### 3. Informações de um livro

```python
livro = {"titulo": "1984", "autor": "George Orwell", "ano": 1949}
for chave, valor in livro.items():
    print(f"{chave}: {valor}")
```

### 4. Contar ocorrências de caracteres

```python
def contar_caracteres(s):
    contagem = {}
    for caractere in s:
        contagem[caractere] = contagem.get(caractere, 0) + 1
    return contagem

print(contar_caracteres("engenharia de dados"))
```

### 5. Preço total da lista de compras

```python
lista_compras = ["maçã", "banana", "cereja"]
precos = {"maçã": 0.45, "banana": 0.30, "cereja": 0.65}
total = sum(precos[item] for item in lista_compras)
print(f"Preço total: {total}")
```

## Exercícios intermediários e mais avançados

### 6. Eliminação de Duplicatas

**Objetivo:** Dada uma lista de emails, remover todos os duplicados.

```python
emails = ["user@example.com", "admin@example.com", "user@example.com", "manager@example.com"]
emails_unicos = list(set(emails))

print(emails_unicos)
```

#### 7. Filtragem de Dados

**Objetivo:** Dada uma lista de idades, filtrar apenas aquelas que são maiores ou iguais a 18.

```python
idades = [22, 15, 30, 17, 18]
idades_validas = [idade for idade in idades if idade >= 18]

print(idades_validas)
```

#### 8. Ordenação Personalizada

**Objetivo:** Dada uma lista de dicionários representando pessoas, ordená-las pelo nome.

```python
pessoas = [
    {"nome": "Alice", "idade": 30},
    {"nome": "Bob", "idade": 25},
    {"nome": "Carol", "idade": 20}
]
pessoas.sort(key=lambda pessoa: pessoa["nome"])

print(pessoas)
```

#### 9. Agregação de Dados

**Objetivo:** Dado um conjunto de números, calcular a média.

```python
numeros = [10, 20, 30, 40, 50]
media = sum(numeros) / len(numeros)

print("Média:", media)
```

#### 10. Divisão de Dados em Grupos

**Objetivo:** Dada uma lista de valores, dividir em duas listas: uma para valores pares e outra para ímpares.

```python
valores = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
pares = [valor for valor in valores if valor % 2 == 0]
impares = [valor for valor in valores if valor % 2 != 0]

print("Pares:", pares)
print("Ímpares:", impares)
```

### Exercícios com Dicionários

#### 11. Atualização de Dados

**Objetivo:** Dada uma lista de dicionários representando produtos, atualizar o preço de um produto específico.

```python
produtos = [
    {"id": 1, "nome": "Teclado", "preço": 100},
    {"id": 2, "nome": "Mouse", "preço": 80},
    {"id": 3, "nome": "Monitor", "preço": 300}
]

# Atualizar o preço do produto com id 2 para 90
for produto in produtos:
    if produto["id"] == 2:
        produto["preço"] = 90

print(produtos)
```

#### 12. Fusão de Dicionários

**Objetivo:** Dados dois dicionários, fundi-los em um único dicionário.

```python
dicionario1 = {"a": 1, "b": 2}
dicionario2 = {"c": 3, "d": 4}

dicionario_fundido = {**dicionario1, **dicionario2}

print(dicionario_fundido)
```

#### 13. Filtragem de Dados em Dicionário

**Objetivo:** Dado um dicionário de estoque de produtos, filtrar aqueles com quantidade maior que 0.

```python
estoque = {"Teclado": 10, "Mouse": 0, "Monitor": 3, "CPU": 0}

estoque_positivo = {produto: quantidade for produto, quantidade in estoque.items() if quantidade > 0}

print(estoque_positivo)
```

#### 14. Extração de Chaves e Valores

**Objetivo:** Dado um dicionário, criar listas separadas para suas chaves e valores.

```python
dicionario = {"a": 1, "b": 2, "c": 3}
chaves = list(dicionario.keys())
valores = list(dicionario.values())

print("Chaves:", chaves)
print("Valores:", valores)
```

#### 15. Contagem de Frequência de Itens

**Objetivo:** Dada uma string, contar a frequência de cada caractere usando um dicionário.

```python
texto = "engenharia de dados"
frequencia = {}

for caractere in texto:
    if caractere in frequencia:
        frequencia[caractere] += 1
    else:
        frequencia[caractere] = 1

print(frequencia)
```

## 3.Lendo arquivos

Para ler um arquivo CSV em Python utilizando o módulo nativo, você pode usar a combinação do comando with open... para abrir o arquivo e o método .reader() do módulo csv para ler o arquivo linha por linha. O uso de with assegura que o arquivo será fechado corretamente após sua leitura, mesmo que ocorram erros durante o processo. Abaixo está um exemplo básico de como realizar essa operação:

```python
import csv

# Caminho para o arquivo CSV
caminho_arquivo = 'exemplo.csv'

# Inicializa uma lista vazia para armazenar os dados
dados = []

# Usa o gerenciador de contexto `with` para abrir o arquivo
with open(caminho_arquivo, mode='r', encoding='utf-8') as arquivo:
    # Cria um objeto leitor de CSV
    leitor_csv = csv.DictReader(arquivo)
    
    # Itera sobre as linhas do arquivo CSV
    for linha in leitor_csv:
        # Adiciona cada linha (um dicionário) à lista de dados
        dados.append(linha)

# Exibe os dados lidos do arquivo CSV
for registro in dados:
    print(registro)
```

## 4. Funções

### Importância na Engenharia de Dados

Funções permitem modularizar e reutilizar código, essencial para processar e analisar grandes conjuntos de dados. Na engenharia de dados, funções são usadas para encapsular lógicas de transformação, limpeza, agregação e análise de dados, tornando o código mais organizado e mantendo a qualidade do código.

As funções em programação são abstrações poderosas que permitem encapsular blocos de código para realizar tarefas específicas. Elas servem não apenas para organizar o código e torná-lo mais legível, mas também para abstrair complexidades, permitindo que os programadores pensem em problemas em um nível mais alto. Uma função bem projetada pode ser vista como um "mini-programa" dentro de um programa maior, com sua própria lógica e dados de entrada e saída.

Um exemplo clássico dessa abstração é a ordenação de uma lista. Vamos primeiro desenvolver uma função simples em Python que ordena uma lista usando o algoritmo de ordenação por seleção, um método simples mas eficaz para listas pequenas e médias. Em seguida, mostraremos como essa tarefa pode ser realizada de forma mais direta usando o método `sort()` built-in do Python, que é uma abstração fornecida pela linguagem para realizar a mesma tarefa.

### Função de Ordenação Personalizada

```python
# Implementação do algoritmo de ordenação por seleção
lista = [64, 34, 25, 12, 22, 11, 90]

for i in range(len(lista)):
    for j in range(i+1, len(lista)):
        if lista[i] > lista[j]:
            lista[i], lista[j] = lista[j], lista[i]

# Ordenando a lista
print("Lista ordenada com função personalizada:", lista)
```

### Usando o Método Built-in `sort()`

O Python fornece uma abstração poderosa através do método `sort()`, que pode ordenar listas in-place de maneira eficiente e com uma sintaxe simples.

```python
# Lista de exemplo
lista_exemplo = [64, 34, 25, 12, 22, 11, 90]

# Ordenando a lista com sort()
lista_exemplo.sort()

print("Lista ordenada com método built-in:", lista_exemplo)
```

A comparação entre a função de ordenação personalizada e o método `sort()` ilustra perfeitamente como as abstrações em programação, como funções e métodos built-in, podem simplificar significativamente o desenvolvimento de software. Enquanto a implementação manual de um algoritmo de ordenação é uma ótima maneira de entender os princípios da computação e algoritmos, na prática, utilizar abstrações fornecidas pela linguagem pode economizar tempo e evitar erros, permitindo que os desenvolvedores se concentrem na lógica de negócios e nos aspectos de alto nível de seus programas.

#### Exemplo: Transformação de Dados com Funções

Suponhamos a necessidade de limpar e transformar nomes de usuários em um conjunto de dados. Uma função dedicada pode ser implementada para essa tarefa.

```python
def normalizar_nome(nome: str) -> str:
    return nome.strip().lower()

nomes = [" Alice ", "BOB", "Carlos"]
nomes_normalizados = [normalizar_nome(nome) for nome em nomes]
print(nomes_normalizados)
```

Cada um desses temas desempenha um papel crucial na engenharia de dados, permitindo a manipulação eficiente de dados, garantindo a qualidade do código e facilitando a análise de dados complexos. Esses exemplos ilustram como listas, dicionários, type hints e funções podem ser aplicados para resolver problemas comuns encontrados nesse campo.

### Exercícios de Funções

16. Escreva uma função que receba uma lista de números e retorne a soma de todos os números.
17. Crie uma função que receba um número como argumento e retorne `True` se o número for primo e `False` caso contrário.
18. Desenvolva uma função que receba uma string como argumento e retorne essa string revertida.
19. Implemente uma função que receba dois argumentos: uma lista de números e um número. A função deve retornar todas as combinações de pares na lista que somem ao número dado.
20. Escreva uma função que receba um dicionário e retorne uma lista de chaves ordenadas

O padrão de nomeação de funções em Python segue convenções que são amplamente aceitas pela comunidade Python, conforme recomendado no PEP 8, o guia de estilo para a codificação em Python. Adotar esses padrões não só melhora a legibilidade do código, mas também facilita a compreensão e a manutenção por outros desenvolvedores, incluindo aqueles novos ao projeto.

### Padrões de Nomes de Funções

* **Nomes Claros e Descritivos**: O nome de uma função deve ser descritivo o suficiente para indicar sua finalidade ou o que ela faz. Por exemplo, `calcular_area_circulo` é mais descritivo do que simplesmente `area`.
    
* **Letras Minúsculas com Sublinhados**: Funções em Python devem ser nomeadas usando letras minúsculas, com palavras separadas por sublinhados para melhorar a legibilidade. Este estilo é algumas vezes referido como snake_case. Por exemplo, `carregar_dados_usuario` é um bom exemplo.
    
* **Evitar Nomes Genéricos**: Nomes como `processo`, `executar`, ou `fazer_algo` são muito genéricos e não fornecem informações suficientes sobre o que a função faz. Prefira nomes que ofereçam um nível adequado de detalhe.
    
* **Evitar Abreviações Obscuras**: Embora abreviações possam encurtar o nome de uma função, elas podem tornar o código menos acessível para outros desenvolvedores. Por exemplo, `calc_media_notas` é preferível a `cmn`.
    
* **Prefixos com Verbo**: Muitas vezes, funções realizam ações, então é útil iniciar o nome da função com um verbo que descreve essa ação, como `obter_`, `calcular_`, `processar_`, `validar_` ou `limpar_`.

Na Python, a tipagem de funções é facilitada pelo uso de "Type Hints" (Dicas de Tipo), uma característica introduzida no Python 3.5 através do PEP 484. Os Type Hints permitem aos desenvolvedores especificar os tipos de dados esperados para os parâmetros de uma função e o tipo de dado que a função deve retornar. Embora essas dicas de tipo não sejam estritamente aplicadas em tempo de execução, elas são extremamente úteis para ferramentas de análise estática de código, melhorando a legibilidade do código e ajudando na detecção precoce de erros.

### Tipagem dos Parâmetros

Você pode especificar o tipo de cada parâmetro ao definir uma função. Isso indica claramente o tipo de argumento que a função espera.

```python
def saudacao(nome: str, idade: int) -> str:
    return f"Olá, {nome}, você tem {idade} anos."
```

### Parâmetros com Valores Default

Python permite definir valores default para os parâmetros, o que significa que a função pode ser chamada sem fornecer todos os argumentos, desde que os omitidos tenham um valor padrão definido. A tipagem funciona da mesma forma, com o tipo sendo especificado antes do sinal de igual.

```python
def saudacao(nome: str, idade: int = 30) -> str:
    return f"Olá, {nome}, você tem {idade} anos."
```


![imagem_03](./pic/3.jpg)

Refatorar nosso código usando Dicionário, Type Hint e Funcões.


![imagem_04](./pic/4.jpg)

Duvidas

================================================
File: /Bootcamp - Python para dados/aula04/exercicios.py
================================================
import csv

# Caminho para o arquivo CSV
caminho_arquivo = 'exemplo.csv'

# Inicializa uma lista vazia para armazenar os dados
dados = []

# Usa o gerenciador de contexto `with` para abrir o arquivo
with open(caminho_arquivo, mode='r', encoding='utf-8') as arquivo:
    # Cria um objeto leitor de CSV
    leitor_csv = csv.DictReader(arquivo)
    
    # Itera sobre as linhas do arquivo CSV
    for linha in leitor_csv:
        # Adiciona cada linha (um dicionário) à lista de dados
        dados.append(linha)

# Exibe os dados lidos do arquivo CSV
for registro in dados:
    print(registro)

================================================
File: /Bootcamp - Python para dados/aula04/ordem.py
================================================
lista = [64, 34, 25, 12, 22, 11, 90]

def ordernar_lista(lista: list) -> list:
    lista_ordenada = lista.copy()

    for i in range(len(lista)):
        for j in range(i+1, len(lista)):
            if lista[i] > lista[j]:
                lista[i], lista[j] = lista[j], lista[i]

    return lilista_ordenadasta

print(lista)

================================================
File: /Bootcamp - Python para dados/aula05/README.md
================================================
# Projeto 01: Um Bilhão de Linhas: Desafio de Processamento de Dados com Python

![imagem_01](./pic/1.jpg)

## Introdução

O objetivo deste projeto é demonstrar como processar eficientemente um arquivo de dados massivo contendo 1 bilhão de linhas (~14GB), especificamente para calcular estatísticas (Incluindo agregação e ordenação que são operações pesadas) utilizando Python. 

Este desafio foi inspirado no [The One Billion Row Challenge](https://github.com/gunnarmorling/1brc), originalmente proposto para Java.

O arquivo de dados consiste em medições de temperatura de várias estações meteorológicas. Cada registro segue o formato `<string: nome da estação>;<double: medição>`, com a temperatura sendo apresentada com precisão de uma casa decimal.

Aqui estão dez linhas de exemplo do arquivo:

```
Hamburg;12.0
Bulawayo;8.9
Palembang;38.8
St. Johns;15.2
Cracow;12.6
Bridgetown;26.9
Istanbul;6.2
Roseau;34.4
Conakry;31.2
Istanbul;23.0
```

O desafio é desenvolver um programa Python capaz de ler esse arquivo e calcular a temperatura mínima, média (arredondada para uma casa decimal) e máxima para cada estação, exibindo os resultados em uma tabela ordenada por nome da estação.

| station      | min_temperature | mean_temperature | max_temperature |
|--------------|-----------------|------------------|-----------------|
| Abha         | -31.1           | 18.0             | 66.5            |
| Abidjan      | -25.9           | 26.0             | 74.6            |
| Abéché       | -19.8           | 29.4             | 79.9            |
| Accra        | -24.8           | 26.4             | 76.3            |
| Addis Ababa  | -31.8           | 16.0             | 63.9            |
| Adelaide     | -31.8           | 17.3             | 71.5            |
| Aden         | -19.6           | 29.1             | 78.3            |
| Ahvaz        | -24.0           | 25.4             | 72.6            |
| Albuquerque  | -35.0           | 14.0             | 61.9            |
| Alexandra    | -40.1           | 11.0             | 67.9            |
| ...          | ...             | ...              | ...             |
| Yangon       | -23.6           | 27.5             | 77.3            |
| Yaoundé      | -26.2           | 23.8             | 73.4            |
| Yellowknife  | -53.4           | -4.3             | 46.7            |
| Yerevan      | -38.6           | 12.4             | 62.8            |
| Yinchuan     | -45.2           | 9.0              | 56.9            |
| Zagreb       | -39.2           | 10.7             | 58.1            |
| Zanzibar City| -26.5           | 26.0             | 75.2            |
| Zürich       | -42.0           | 9.3              | 63.6            |
| Ürümqi       | -42.1           | 7.4              | 56.7            |
| İzmir        | -34.4           | 17.9             | 67.9            |

## Desafio

![imagem_03](./pic/3.jpg)

- Clonar o repositorio do projeto [link](https://github.com/lvgalvao/One-Billion-Row-Challenge-Python)
- Rodar o código utilizando Pyenv, Poetry e VSCode
- Instalar as dependencias
- Conseguir rodar todos os testes também
- Após isso, aplicar algum dos conteudos que vimos durante a aula (estrutura de dados, type hint, estrutura condicional, try catch, funções etc) e propor uma melhoria no projeto através de uma PR

================================================
File: /Bootcamp - Python para dados/aula06/aovivo/README.md
================================================
# aula06_bootcamp


================================================
File: /Bootcamp - Python para dados/aula06/aovivo/main.py
================================================
is_nome_aluno: int = 0

while is_nome_aluno is not True:
    nome_aluno = input("Digirw uma classe")
    if isinstance(nome_aluno, str):
        nome_aluno_maiusculo = nome_aluno.upper()
        print(nome_aluno_maiusculo)
        is_nome_aluno = True
    else:
        print("voce digitou uma classe errada, precisa ser str")
        is_nome_aluno = is_nome_aluno + 1


================================================
File: /Bootcamp - Python para dados/aula06/aovivo/main_02.py
================================================
# Integre na solução anterior um fluxo de While
# que repita o fluxo até que o usuário insira as
# informações corretas

# Solicita ao usuário que digite seu nome
nome_valido = False
salario_valido = False
bonus_valido = False

while not nome_valido:
    try:
        nome = input("Digite seu nome: ")

        # Verifica se o nome está vazio
        if len(nome) == 0:
            raise ValueError("O nome não pode estar vazio.")
        # Verifica se há números no nome
        elif any(char.isdigit() for char in nome):
            raise ValueError("O nome não deve conter números.")
        else:
            print("Nome válido:", nome)
            nome_valido = True
    except ValueError as e:
        print(e)

# Solicita ao usuário que digite o valor do seu salário e converte para float

try:
    salario = float(input("Digite o valor do seu salário: "))
    if salario < 0:
        print("Por favor, digite um valor positivo para o salário.")
except ValueError:
    print("Entrada inválida para o salário. Por favor, digite um número.")
    exit()

# Solicita ao usuário que digite o valor do bônus recebido e converte para float
try:
    bonus = float(input("Digite o valor do bônus recebido: "))
    if bonus < 0:
        print("Por favor, digite um valor positivo para o bônus.")
except ValueError:
    print("Entrada inválida para o bônus. Por favor, digite um número.")
    exit()

bonus_recebido = 1000 + salario * bonus  # Exemplo simples de KPI

# Imprime as informações para o usuário
print(
    f"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}."
)


================================================
File: /Bootcamp - Python para dados/aula06/aovivo/pyproject.toml
================================================
[tool.poetry]
name = "aula06-bootcamp"
version = "0.1.0"
description = ""
authors = ["lvgalvaofilho <lvgalvaofilho@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
black = "^24.2.0"
flake8 = "^7.0.0"
isort = "^5.13.2"
taskipy = "^1.12.2"
pre-commit = "^3.6.2"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.isort]
profile = "black"

[tool.taskipy.tasks]
format = """
isort main.py
black main.py
flake8 main.py
"""


================================================
File: /Bootcamp - Python para dados/aula06/aovivo/.flake8
================================================
[flake8]
max-line-length = 89
extend-ignore = E203,E701,W291


================================================
File: /Bootcamp - Python para dados/aula06/aovivo/.pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
        args: [--markdown-linebreak-ext=md]
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-toml
      - id: detect-private-key
      - id: check-added-large-files
  - repo: https://github.com/psf/black-pre-commit-mirror
    rev: 24.1.1
    hooks:
      - id: black
        language_version: python3.11
  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
        name: isort (python)
  - repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks:
      - id: flake8


================================================
File: /Bootcamp - Python para dados/aula06/aovivo/.python-version
================================================
3.11.5


================================================
File: /Bootcamp - Python para dados/aula06/aovivo/helper/aws.py
================================================
def ler_arquivo_s3():
    pass


def salvar_arquivo_s3():
    pass


================================================
File: /Bootcamp - Python para dados/aula06/aovivo/helper/azure.py
================================================
def ler_arquivo_s3():
    pass


def salvar_arquivo_s3():
    pass


================================================
File: /Bootcamp - Python para dados/aula07/README.md
================================================
# Aula 07: Funções em Python e Estrutura de Dados - Parte 1

![imagem_01](./pic/1.jpg)

As funções em Python são uma das estruturas fundamentais da linguagem, permitindo a reutilização de código, a organização e a modularidade dos programas. Este guia aborda desde a motivação até a aplicação prática de funções, incluindo keywords, nomes, e como utilizá-las efetivamente.

## Conteúdo

![imagem_02](./pic/2.jpg)

### Motivação

A principal motivação para usar funções em Python é a **reutilização de código**. Funções permitem que você escreva um bloco de código uma vez e o execute múltiplas vezes, possivelmente com diferentes argumentos, para produzir diferentes resultados. Isso ajuda a tornar o código mais **legível**, **modular**, e **fácil de debugar**.

### Definindo Funções

Para criar uma função em Python, você usa a keyword `def`, seguida de um nome de função, parênteses `()` contendo zero ou mais "parâmetros", e dois pontos `:`. O bloco de código indentado que segue é o corpo da função.

```python
def minha_funcao():
    return "Hello, World!"
```

### Nomes de Funções

Os nomes das funções seguem as mesmas regras de nomes de variáveis em Python: podem conter letras, números (não como primeiro caractere) e underscores (`_`), mas não espaços ou caracteres especiais. Nomes de funções devem ser descritivos e, por convenção, utilizam `snake_case`.

### Parâmetros e Argumentos

* **Parâmetros** são as variáveis listadas nos parênteses na definição da função. Eles são como placeholders para os dados que a função irá processar.
    
* **Argumentos** são os valores reais passados para a função quando ela é chamada.
    

```python
def soma(a, b):
    return a + b
```

### Palavras-chave importantes

* `def` inicia a definição de uma função.
* `return` é usado para retornar um valor da função. Se omitido, a função retorna `None` por padrão.
* `pass` pode ser usado como um placeholder para uma função vazia, significando "nada".

### Chamando Funções

Para chamar uma função, use o nome da função seguido por parênteses contendo os argumentos necessários.

```python
resultado = soma(5, 3)
print(resultado)  # Saída: 8
```

### Valores Padrão e Argumentos Nomeados

Funções podem ter parâmetros com valores padrão, permitindo que sejam chamadas com menos argumentos.

```python
def cumprimentar(nome, mensagem="Olá"):
    print(f"{mensagem}, {nome}!")
```

Você também pode chamar funções com argumentos nomeados para maior clareza.

```python
cumprimentar(mensagem="Bem-vindo", nome="João")
```

## Exercícios

Vamos revisar funções adicionando type hints e Pydantic

1. **Calcular Média de Valores em uma Lista**

```python
from typing import List

def calcular_media(valores: List[float]) -> float:
    return sum(valores) / len(valores)
```

2. **Filtrar Dados Acima de um Limite**

```python
def filtrar_acima_de(valores: List[float], limite: float) -> List[float]:
    resultado = []
    for valor in valores:
        if valor > limite:
            resultado.append(valor)
    return resultados
```

3. **Contar Valores Únicos em uma Lista**

```python
def contar_valores_unicos(lista: List[int]) -> int:
    return len(set(lista))
```

4. **Converter Celsius para Fahrenheit em uma Lista**

```python
def celsius_para_fahrenheit(temperaturas_celsius: List[float]) -> List[float]:
    return [(9/5) * temp + 32 for temp in temperaturas_celsius]
```

5. **Calcular Desvio Padrão de uma Lista**

```python
def calcular_desvio_padrao(valores: List[float]) -> float:
    media = sum(valores) / len(valores)
    variancia = sum((x - media) ** 2 for x in valores) / len(valores)
    return variancia ** 0.5
```

6. **Encontrar Valores Ausentes em uma Sequência**

```python
def encontrar_valores_ausentes(sequencia: List[int]) -> List[int]:
    completo = set(range(min(sequencia), max(sequencia) + 1))
    return list(completo - set(sequencia))
```

![imagem_03](./pic/3.jpg)

Desafio: Análise de Vendas de Produtos
Objetivo: Dado um arquivo CSV contendo dados de vendas de produtos, o desafio consiste em ler os dados, processá-los em um dicionário para análise e, por fim, calcular e reportar as vendas totais por categoria de produto.

**Fluxo**:

```mermaid
graph TD;
    A[Início] --> B{Ler CSV};
    B --> C[Processar Dados];
    C --> D[Calcular Vendas];
    D --> E[Exibir Resultados];
    E --> F[Fim];
```

**Tarefas**:

1. Ler o arquivo CSV e carregar os dados.
2. Processar os dados em um dicionário, onde a chave é a categoria, e o valor é uma lista de dicionários, cada um contendo informações do produto (`Produto`, `Quantidade`, `Venda`).
3. Calcular o total de vendas (`Quantidade` * `Venda`) por categoria.

### Funções

1. **Ler CSV**:
    
    * Função: `ler_csv`
    * Entrada: Nome do arquivo CSV
    * Saída: Lista de dicionários com dados lidos
2. **Processar Dados**:
    
    * Função: `processar_dados`
    * Entrada: Lista de dicionários
    * Saída: Dicionário processado conforme descrito
3. **Calcular Vendas por Categoria**:
    
    * Função: `calcular_vendas_categoria`
    * Entrada: Dicionário processado
    * Saída: Dicionário com total de vendas por categoria

![imagem_04](./pic/4.jpg)

================================================
File: /Bootcamp - Python para dados/aula07/desafio.py
================================================
import csv

# Função para ler o arquivo CSV
def ler_csv(nome_arquivo):
    with open(nome_arquivo, mode='r', encoding='utf-8') as arquivo:
        leitor = csv.DictReader(arquivo)
        return list(leitor)

# Função para processar os dados em um dicionário
def processar_dados(dados):
    categorias = {}
    for item in dados:
        categoria = item['Categoria']
        if categoria not in categorias:
            categorias[categoria] = []
        categorias[categoria].append(item)
    return categorias

# Função para calcular o total de vendas por categoria
def calcular_vendas_categoria(dados):
    vendas_por_categoria = {}
    for categoria, itens in dados.items():
        total_vendas = sum(int(item['Quantidade']) * int(item['Venda']) for item in itens)
        vendas_por_categoria[categoria] = total_vendas
    return vendas_por_categoria

# Função principal para integrar as funções anteriores
def main():
    nome_arquivo = 'vendas.csv'
    dados_brutos = ler_csv(nome_arquivo)
    dados_processados = processar_dados(dados_brutos)
    vendas_categoria = calcular_vendas_categoria(dados_processados)
    for categoria, total in vendas_categoria.items():
        print(f'{categoria}: ${total}')

if __name__ == '__main__':
    main()


================================================
File: /Bootcamp - Python para dados/aula07/desafio_Lucas_Andrade.py
================================================
import csv 
from typing import List, Dict
import pandas as pd

def ler_csv(path: str) -> List:
    with open(path, mode = 'r', encoding= 'utf-8') as file: 
        reader = csv.DictReader(file)
        return list(reader)
    
def processar_dados(lista_dict: List[dict]) -> Dict: 
    produtos = []
    quantidades = []
    vendas = []
    for i in range(len(lista_dict)):
        produtos.append(lista_dict[i]['Produto'])
        quantidades.append(lista_dict[i]['Quantidade'])
        vendas.append(lista_dict[i]['Venda'])
    
    return {'Produto': produtos, 'Quantidade': quantidades, 'Venda': vendas}

def somar_valores(dicionario: Dict) -> List[float]:
    totais: List =[] 

    for i in range(len(dicionario['Produto'])):
        quantidade: int = int(dicionario['Quantidade'][i])
        venda: int = int(dicionario['Venda'][i])
        total = quantidade * venda 
        totais.append(total)
    
    dicionario['Total'] = totais

    return dicionario

def ler_DataFrame(dicionario_valores_categoria: Dict) -> pd.DataFrame:
    df = pd.DataFrame(dicionario_valores_categoria) 
    print(df)

dicionario_csv: Dict = ler_csv('./vendas.csv')
dicionario_processado: Dict = processar_dados(dicionario_csv)
dicionario_valores_categoria: List = somar_valores(dicionario_processado)
print(dicionario_valores_categoria)
ler_DataFrame(dicionario_valores_categoria)

================================================
File: /Bootcamp - Python para dados/aula07/desafio_com_pydantic.py
================================================
from pydantic import BaseModel, field_validator, ValidationError
from typing import List, Dict, Optional
import csv

class ItemVenda(BaseModel):
    Produto: str
    Categoria: str
    Quantidade: int
    Venda: int

    # Validador para garantir valores positivos
    @field_validator('Quantidade', 'Venda')
    def valores_positivos(cls, v):
        assert v >= 0, 'deve ser positivo'
        return v

class CategoriaDados(BaseModel):
    Categoria: str
    Itens: List[ItemVenda]
    TotalVendas: Optional[int] = 0

    def calcular_total_vendas(self):
        self.TotalVendas = sum(item.Quantidade * item.Venda for item in self.Itens)

def ler_csv(nome_arquivo: str) -> List[ItemVenda]:
    dados_validados = []
    with open(nome_arquivo, mode='r', encoding='utf-8') as arquivo:
        leitor = csv.DictReader(arquivo)
        for linha in leitor:
            try:
                item = ItemVenda(**linha)
                dados_validados.append(item)
            except ValidationError as e:
                print(f"Erro de validação: {e.json()}")
    return dados_validados

def processar_dados(dados: List[ItemVenda]) -> Dict[str, CategoriaDados]:
    categorias = {}
    for item in dados:
        if item.Categoria not in categorias:
            categorias[item.Categoria] = CategoriaDados(Categoria=item.Categoria, Itens=[])
        categorias[item.Categoria].Itens.append(item)
    return categorias

def calcular_vendas_categoria(dados: Dict[str, CategoriaDados]) -> Dict[str, int]:
    vendas_por_categoria = {}
    for categoria, dados_categoria in dados.items():
        dados_categoria.calcular_total_vendas()
        vendas_por_categoria[categoria] = dados_categoria.TotalVendas
    return vendas_por_categoria

def main():
    nome_arquivo = 'vendas.csv'
    dados_brutos = ler_csv(nome_arquivo)
    dados_processados = processar_dados(dados_brutos)
    vendas_categoria = calcular_vendas_categoria(dados_processados)
    for categoria, total in vendas_categoria.items():
        print(f'{categoria}: ${total}')

if __name__ == '__main__':
    main()

================================================
File: /Bootcamp - Python para dados/aula07/desafio_com_type_hint.py
================================================
import csv
from typing import List, Dict

# Função para ler o arquivo CSV
def ler_csv(nome_arquivo: str) -> List[Dict[str, str]]:
    with open(nome_arquivo, mode='r', encoding='utf-8') as arquivo:
        leitor = csv.DictReader(arquivo)
        return list(leitor)

# Função para processar os dados em um dicionário
def processar_dados(dados: List[Dict[str, str]]) -> Dict[str, List[Dict[str, str]]]:
    categorias = {}
    for item in dados:
        categoria = item['Categoria']
        if categoria not in categorias:
            categorias[categoria] = []
        categorias[categoria].append(item)
    return categorias

# Função para calcular o total de vendas por categoria
def calcular_vendas_categoria(dados: Dict[str, List[Dict[str, str]]]) -> Dict[str, int]:
    vendas_por_categoria = {}
    for categoria, itens in dados.items():
        total_vendas = sum(int(item['Quantidade']) * int(item['Venda']) for item in itens)
        vendas_por_categoria[categoria] = total_vendas
    return vendas_por_categoria

# Função principal para integrar as funções anteriores
def main():
    nome_arquivo = 'vendas.csv'
    dados_brutos = ler_csv(nome_arquivo)
    dados_processados = processar_dados(dados_brutos)
    vendas_categoria = calcular_vendas_categoria(dados_processados)
    for categoria, total in vendas_categoria.items():
        print(f'{categoria}: ${total}')

if __name__ == '__main__':
    main()


================================================
File: /Bootcamp - Python para dados/aula07/aovivo/README.md
================================================
# aula07_bootcamp


================================================
File: /Bootcamp - Python para dados/aula07/aovivo/cliente.py
================================================
from desafio import pipeline

print(pipeline("vendas.csv"))

load_csv("vendas.csv")

================================================
File: /Bootcamp - Python para dados/aula07/aovivo/desafio.py
================================================
import csv

from pydantic import validate_call

@validate_call
def ler_csv_para_transformar_em_um_dict(path: str) -> list[dict]:
    dados = []
    with open(file=path, mode="r", encoding="utf-8") as file:
        leitor_csv = csv.DictReader(file)
        for linha in leitor_csv:
            # Adiciona cada linha (um dicionário) à lista de dados
            dados.append(linha)
    return dados

@validate_call
def filtra_produtos_entregues(lista = list[dict]) -> list[dict]:
    lista_de_produtos_entregues = []
    for produto in lista:
        if produto.get("entregue") == "True":
            lista_de_produtos_entregues.append(produto)
    return lista_de_produtos_entregues

@validate_call
def somar_valores(lista = list[dict]) -> int:
    valor_total = 0
    for produto in lista:
        valor_total += int(produto.get("price"))
    return valor_total

@validate_call
def pipeline(path: str):
    lista_de_produtos = ler_csv_para_transformar_em_um_dict(path)
    produtos_entregues = filtra_produtos_entregues(lista_de_produtos)
    return somar_valores(produtos_entregues)






================================================
File: /Bootcamp - Python para dados/aula07/aovivo/etl.py
================================================
from pydantic import validate_call

@validate_call
def filtrar_acima_de(lista_de_salarios: list[float], salario_max: float) -> list:
    lista_de_salarios_acima: list = []
    for salario in lista_de_salarios:
        if salario > salario_max:
            lista_de_salarios_acima.append(salario)
    return lista_de_salarios_acima

================================================
File: /Bootcamp - Python para dados/aula07/aovivo/main.py
================================================
from etl import filtrar_acima_de

lista = [3000, 4000, 150000]
max = 100000 

print(filtrar_acima_de(lista_de_salarios=lista, salario_max=max))

================================================
File: /Bootcamp - Python para dados/aula07/aovivo/pyproject.toml
================================================
[tool.poetry]
name = "aula07-bootcamp"
version = "0.1.0"
description = ""
authors = ["lvgalvaofilho <lvgalvaofilho@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
pydantic = "^2.6.2"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /Bootcamp - Python para dados/aula08/README.md
================================================
# Aula 08: Funções em Python - ETL com Pandas, JSON e Parquet

![imagem_01](./pic/1.jpg)

Para realizar uma ETL (Extract, Transform, Load) simples utilizando Python e a biblioteca Pandas, vamos seguir os seguintes passos:

Extract: Ler os dados de um arquivo JSON.

Transform: Concatenar os dados extraídos em um único DataFrame e aplicar uma transformação. A transformação específica dependerá dos dados, mas vamos assumir uma operação simples como um exemplo.

Load: Salvar o DataFrame resultante em um arquivo CSV ou PARQUET. 

**Usando LOG**

## Conteúdo

![imagem_02](./pic/2.jpg)

O Loguru é uma biblioteca de logging para Python que visa trazer uma experiência de uso mais simples e poderosa do que o módulo de logging padrão do Python. Com uma API simples, Loguru oferece várias funcionalidades úteis, como rotação de arquivos, serialização JSON, envio de mensagens para múltiplos destinos, e muito mais, tudo isso sem a necessidade de configuração inicial complicada.

### O que é Logging?

Logging é o processo de gravar mensagens que documentam os eventos que ocorrem durante a execução de um software. Essas mensagens podem indicar progresso da execução, falhas, erros, ou outras informações úteis. O logging é crucial para desenvolvimento e manutenção de software, pois permite aos desenvolvedores e administradores de sistema entender o que o aplicativo está fazendo, diagnosticar problemas e monitorar o desempenho em produção.

### Como Utilizar o Loguru

Para começar a usar o Loguru, você primeiro precisa instalá-lo. Isso pode ser feito facilmente via pip:

```bash
poetry add loguru
```

Agora, vamos aos exemplos de como utilizar o Loguru em seu código Python.

#### Exemplo 1: Logging Básico

Este exemplo mostra como fazer logging básico com Loguru, incluindo mensagens de diferentes níveis de severidade.

```python
from loguru import logger

# Mensagens de log de diferentes níveis
logger.debug("Isso é uma mensagem de debug")
logger.info("Isso é uma mensagem informativa")
logger.warning("Isso é um aviso")
logger.error("Isso é um erro")
logger.critical("Isso é crítico")

# A saída será exibida no console
```

Neste exemplo, utilizamos o `logger` importado do Loguru para registrar mensagens de diferentes níveis de severidade. O Loguru se encarrega de formatar e exibir essas mensagens no console, por padrão.

#### Exemplo 2: Configuração de Arquivo de Log

Aqui, configuramos o Loguru para salvar mensagens de log em um arquivo, incluindo a rotação do arquivo baseada no tamanho.

```python
from loguru import logger

# Configurando o arquivo de log com rotação de 5MB
logger.add("meu_app.log", rotation="5 MB")

logger.info("Essa mensagem será salva no arquivo")
```

No exemplo acima, `logger.add()` é usado para adicionar um "sink" (destino) que, neste caso, é um arquivo. A opção `rotation` determina que um novo arquivo será criado sempre que o atual atingir 5MB.

#### Exemplo 3: Capturando Exceções com Log

Loguru também facilita o logging de exceções, capturando automaticamente informações de traceback.

```python
from loguru import logger

def minha_funcao():
    raise ValueError("Um erro aconteceu!")

try:
    minha_funcao()
except Exception:
    logger.exception("Uma exceção foi capturada")
```

Usando `logger.exception()`, Loguru automaticamente captura e loga o traceback da exceção, o que é extremamente útil para diagnóstico de erros.

Vamos criar um decorador utilizando o Loguru para adicionar automaticamente logs a qualquer função Python. Isso nos permite registrar automaticamente quando uma função é chamada e quando ela termina, junto com qualquer informação relevante, como argumentos da função e o resultado retornado (ou exceção lançada).

Agora, vamos ao código do decorador:

```python
from loguru import logger

def log_decorator(func):
    def wrapper(*args, **kwargs):
        logger.info(f"Chamando '{func.__name__}' com {args} e {kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.info(f"'{func.__name__}' retornou {result}")
            return result
        except Exception as e:
            logger.exception(f"'{func.__name__}' lançou uma exceção: {e}")
            raise
    return wrapper
```

Neste decorador, `log_decorator`, usamos `logger.info` para registrar quando a função decorada é chamada e o que ela retorna. Se uma exceção for lançada, usamos `logger.exception` para registrar a exceção, incluindo o traceback.

### Como Utilizar o Decorador

Agora, veja como aplicar o `log_decorator` a uma função:

```python
@log_decorator
def soma(a, b):
    return a + b

@log_decorator
def falha():
    raise ValueError("Um erro intencional")

# Testando as funções decoradas
soma(5, 3)  # Isso irá logar a chamada e o retorno
try:
    falha()  # Isso irá logar a chamada e a exceção
except ValueError:
    pass  # Ignora a exceção para fins de demonstração
```

Ao decorar as funções `soma` e `falha` com `@log_decorator`, automaticamente logamos a entrada e saída (ou exceção) dessas funções sem alterar o corpo delas. Isso é especialmente útil para debugar, monitorar a performance de aplicações ou simplesmente manter um registro de quais funções estão sendo chamadas e com quais argumentos.

### Benefícios do Uso de Decoradores com Loguru

O uso de decoradores em conjunto com o Loguru fornece uma abordagem elegante e poderosa para adicionar logs a aplicações Python. Sem a necessidade de modificar o corpo da função, podemos facilmente adicionar funcionalidades de logging, o que torna o código mais limpo, mantém a separação de preocupações e facilita a manutenção e o debugging.

Além disso, ao centralizar a lógica de logging no decorador, promovemos a reutilização de código e garantimos uma forma consistente de logar informações através de diferentes partes de uma aplicação.

### Conclusão

O Loguru oferece uma abordagem moderna e conveniente para logging em Python, simplificando muitos aspectos que requerem configuração manual detalhada com o módulo de logging padrão do Python. Seja para desenvolvimento, depuração ou produção, adicionar logging ao seu aplicativo com Loguru pode melhorar significativamente a visibilidade e a capacidade de diagnóstico do seu código.

### Desafio

![imagem_03](./pic/3.jpg)

![imagem_03](./pic/pic_05.png)



================================================
File: /Bootcamp - Python para dados/aula08/etl.py
================================================
import pandas as pd
import glob
import os
import pandera as pa
from schema import VendasSchema
from pathlib import Path

@pa.check_output(VendasSchema)
def extrair_dados(pasta: str) -> pd.DataFrame:
    arquivos_json = glob.glob(os.path.join(pasta, '*.json'))
    df_list = [pd.read_json(arquivo) for arquivo in arquivos_json]
    df_total = pd.concat(df_list, ignore_index=True)
    print(df_total)
    return df_total

def transformar_dados(df: pd.DataFrame):
    df['Receita'] = df['Quantidade'] * df['Venda']
    print(df)
    return df

def carregar_dados(df: pd.DataFrame, formatos: list):

    for formato in formatos:
        if formato == 'csv':
            df.to_csv("dados.csv", index=False)
        elif formato == 'parquet':
            df.to_parquet( "dados.parquet", index=False)

def pipeline(pasta_entrada: str, formato_saida: str):
    dados = extrair_dados(pasta_entrada)
    dados_transformados = transformar_dados(dados)
    carregar_dados(dados_transformados, formato_saida)

================================================
File: /Bootcamp - Python para dados/aula08/pipeline.py
================================================
from etl import pipeline
from pathlib import Path

if __name__ == "__main__":
    # Define as pastas de entrada e saída usando pathlib
    pasta_raiz = Path(__file__).parent
    pasta_entrada = pasta_raiz / 'data'

    formato_saida = ["csv"]  # Ou 'parquet', conforme a decisão de saída

    pipeline(pasta_entrada, formato_saida)


================================================
File: /Bootcamp - Python para dados/aula08/schema.py
================================================
import pandera as pa
from pandera.typing import Series

class VendasSchema(pa.SchemaModel):
    Produto: Series[str]
    Categoria: Series[str]
    Quantidade: Series[int] = pa.Field(ge=0)  # ge=0 significa "maior ou igual a 0"
    Venda: Series[int] = pa.Field(ge=0)
    Data: Series[str]
    
    class Config:
        coerce = True
        strict = True


================================================
File: /Bootcamp - Python para dados/aula08/aovivo/README.md
================================================
# aula08_bootcamp


================================================
File: /Bootcamp - Python para dados/aula08/aovivo/etl.py
================================================
import pandas as pd
import os
import glob
# uma funcao de extract que le e consolida os json

def extrair_dados_e_consolidar(pasta: str) -> pd.DataFrame:
    arquivos_json = glob.glob(os.path.join(pasta, '*.json'))
    df_list = [pd.read_json(arquivo) for arquivo in arquivos_json]
    df_total = pd.concat(df_list, ignore_index=True)
    return df_total

# uma funcao que transforma

def calcular_kpi_de_total_de_vendas(df: pd.DataFrame) -> pd.DataFrame:
    df["Total"] = df["Quantidade"] * df["Venda"]
    return df

def carregar_dados(df: pd.DataFrame, format_saida: list):
    """
    parametro que vai ser ou "csv" ou "parquet" ou "os dois"
    """
    for formato in format_saida:
        if formato == 'csv':
            df.to_csv("dados.csv", index=False)
        if formato == 'parquet':
            df.to_parquet("dados.parquet", index=False)

extrair_dados_e_consolidar()
@extrair_dados_e_consolidar
def pipeline_calcular_kpi_de_vendas_consolidado(pasta: str, formato_de_saida: list):
    data_frame = extrair_dados_e_consolidar(pasta)
    data_frame_calculado = calcular_kpi_de_total_de_vendas(data_frame)
    carregar_dados(data_frame_calculado, formato_de_saida)

# uma funcao que da load em csv ou parquet

================================================
File: /Bootcamp - Python para dados/aula08/aovivo/pipeline.py
================================================
from etl import pipeline_calcular_kpi_de_vendas_consolidado

pasta_argumento: str = 'data'
formato_de_saida: list = ["csv", "parquet"]

pipeline_calcular_kpi_de_vendas_consolidado(pasta_argumento, formato_de_saida)


================================================
File: /Bootcamp - Python para dados/aula08/aovivo/pyproject.toml
================================================
[tool.poetry]
name = "aula08-bootcamp"
version = "0.1.0"
description = ""
authors = ["lvgalvaofilho <lvgalvaofilho@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
pandera = "^0.18.0"
pandas = "^2.2.1"
fastparquet = "^2024.2.0"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /Bootcamp - Python para dados/aula08/aovivo/.python-version
================================================
3.11.5


================================================
File: /Bootcamp - Python para dados/aula09/README.md
================================================
# Aula 09: Funções em Python - Decoradores

Na engenharia de dados, a eficiência, reusabilidade e confiabilidade do código são cruciais. Por isso, trabalhamos com decoradores.

![imagem_01](./pic/1.jpg)

**Usando LOG**

Quando queremos entender mais sobre nossa aplicação, temos duas alternativas.

- Utilizar o Print
- Utilizar o Debugger

Hoje quero apresentar uma terceira opção, que é o logging.

[![imagem_05](./pic/5.png)](https://www.linkedin.com/posts/lucianovasconcelosf_voc%C3%A9-%C3%A9-da-turma-do-print-ou-do-log-debugar-activity-7166127525763518464-z8AN?utm_source=share&utm_medium=member_desktop)

## Conteúdo

![imagem_02](./pic/2.jpg)

O Loguru é uma biblioteca de logging para Python que visa trazer uma experiência de uso mais simples e poderosa do que o módulo de logging padrão do Python. Com uma API simples, Loguru oferece várias funcionalidades úteis, como rotação de arquivos, serialização JSON, envio de mensagens para múltiplos destinos, e muito mais, tudo isso sem a necessidade de configuração inicial complicada.

### O que é Logging?

Logging é o processo de gravar mensagens que documentam os eventos que ocorrem durante a execução de um software. Essas mensagens podem indicar progresso da execução, falhas, erros, ou outras informações úteis. O logging é crucial para desenvolvimento e manutenção de software, pois permite aos desenvolvedores e administradores de sistema entender o que o aplicativo está fazendo, diagnosticar problemas e monitorar o desempenho em produção.

### Como Utilizar o Loguru

Para começar a usar o Loguru, você primeiro precisa instalá-lo. Isso pode ser feito facilmente via pip:

```bash
poetry add loguru
```

Agora, vamos aos exemplos de como utilizar o Loguru em seu código Python.

#### Exemplo 1: Logging Básico

Este exemplo mostra como fazer logging básico com Loguru

```python
from loguru import logger

logger.info("Isso é uma mensagem informativa")

# A saída será exibida no console
```

#### Exemplo 2: Configuração de Arquivo de Log

Aqui, configuramos o Loguru para salvar mensagens de log em um arquivo, incluindo a rotação do arquivo baseada no tamanho.

```python
from loguru import logger

# Configurando o arquivo de log com rotação de 5MB
logger.add("meu_app.log", rotation="5 MB")

logger.info("Essa mensagem será salva no arquivo")
```

No exemplo acima, `logger.add()` é usado para adicionar um "sink" (destino) que, neste caso, é um arquivo. A opção `rotation` determina que um novo arquivo será criado sempre que o atual atingir 5MB.

#### Exemplo 3: Capturando e salvando

Aqui está um exemplo de como configurar o `loguru` para salvar os logs tanto em um arquivo quanto exibi-los na saída padrão (`stderr`):

```python
from loguru import logger
from sys import stderr

# Configuração do logger para exibir logs no stderr e salvar em arquivo, com filtragem e formatação específicas
logger.add(
    sink=stderr,
    format="{time} <r>{level}</r> <g>{message}</g> {file}",
    level="INFO"
)

logger.add(
    "meu_arquivo_de_logs.log",  # Arquivo onde os logs serão salvos
    format="{time} {level} {message} {file}",
    level="INFO"
)

# Exemplo de uso do logger
logger.info("Este é um log de informação.")
logger.error("Este é um log de erro.")
```

Neste código, dois "sinks" são adicionados ao `logger`:

1. `stderr`, para exibir os logs, com uma formatação específica que inclui o tempo, nível de log, mensagem e arquivo de origem.

2. `"meu_arquivo_de_logs.log"`, para salvar os logs em um arquivo com uma formatação que também inclui tempo, nível, mensagem e arquivo de origem.

Os níveis de log em Python (e em muitos sistemas de logging em outras linguagens de programação) são usados para indicar a gravidade ou importância das mensagens registradas pelo aplicativo. Eles ajudam a diferenciar entre tipos de informações que estão sendo logadas, permitindo uma filtragem e análise mais eficazes dos dados de log. Aqui estão os níveis de log mais comuns, listados em ordem crescente de gravidade:

### DEBUG

* **Descrição**: O nível DEBUG é usado para informações detalhadas, tipicamente de interesse apenas quando se está diagnosticando problemas.
* **Uso**: Desenvolvedores usam este nível para obter informações detalhadas sobre o fluxo da aplicação, variáveis de estado, e para entender como o código está operando durante o desenvolvimento e a depuração.

### INFO

* **Descrição**: O nível INFO é usado para confirmar que as coisas estão funcionando conforme o esperado.
* **Uso**: Este nível é geralmente o padrão em produção para registrar eventos normais do sistema, como processos de inicialização, operações concluídas com sucesso, ou outras transações de rotina.

### WARNING

* **Descrição**: O nível WARNING indica que algo inesperado aconteceu, ou indica algum problema no futuro próximo (e.g., 'disco quase cheio'). O software está funcionando como esperado.
* **Uso**: Utiliza-se este nível para alertar sobre situações que podem necessitar de atenção mas não impedem o funcionamento do sistema. Por exemplo, usar uma função obsoleta ou problemas de performance que não requerem uma ação imediata.

### ERROR

* **Descrição**: O nível ERROR indica que devido a um problema mais grave, a execução de alguma função ou operação falhou.
* **Uso**: Este nível é usado para registrar eventos de erro que afetam a operação de uma parte do sistema ou funcionalidade, mas não necessariamente o sistema como um todo. Erros que são capturados e gerenciados ainda podem ser logados neste nível.

### CRITICAL

* **Descrição**: O nível CRITICAL indica um erro grave que impede a continuação da execução do programa.
* **Uso**: É usado para erros que necessitam de atenção imediata, como um falha crítica no sistema que pode resultar em parada total do serviço ou aplicação. Este nível deve ser reservado para os problemas mais sérios.

### Como Utilizar

A seleção do nível de log adequado para diferentes mensagens permite que os desenvolvedores e administradores de sistema configurem os logs para capturar apenas as informações de que precisam. Por exemplo, em um ambiente de desenvolvimento, você pode querer ver todos os logs, desde DEBUG até CRITICAL, para entender completamente o comportamento da aplicação. Em contraste, em um ambiente de produção, você pode configurar para registrar apenas WARNING, ERROR, e CRITICAL, para reduzir o volume de dados gerados e se concentrar em problemas que necessitam de atenção.


#### Exemplo 4: Capturando Exceções com Log

Loguru também facilita o logging de exceções, capturando automaticamente informações de traceback.

```python
from loguru import logger

def minha_funcao():
    raise ValueError("Um erro aconteceu!")

try:
    minha_funcao()
except Exception:
    logger.exception("Uma exceção foi capturada")
```

Usando `logger.exception()`, Loguru automaticamente captura e loga o traceback da exceção, o que é extremamente útil para diagnóstico de erros.

Vamos criar um decorador utilizando o Loguru para adicionar automaticamente logs a qualquer função Python. Isso nos permite registrar automaticamente quando uma função é chamada e quando ela termina, junto com qualquer informação relevante, como argumentos da função e o resultado retornado (ou exceção lançada).

#### Exemplo 5: Capturando Exceções com Log

Agora, vamos ao código do decorador:

```python
from loguru import logger
from sys import stderr
from functools import wraps

logger.remove()

logger.add(
                sink=stderr,
                format="{time} <r>{level}</r> <g>{message}</g> {file}",
                level="INFO"
            )

logger.add(
                "meu_arquivo_de_logs.log",
                format="{time} {level} {message} {file}",
                level="INFO"
            )

def log_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger.info(f"Chamando função '{func.__name__}' com args {args} e kwargs {kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.info(f"Função '{func.__name__}' retornou {result}")
            return result
        except Exception as e:
            logger.exception(f"Exceção capturada em '{func.__name__}': {e}")
            raise  # Re-lança a exceção para não alterar o comportamento da função decorada
    return wrapper
```

Neste decorador, `log_decorator`, usamos `logger.info` para registrar quando a função decorada é chamada e o que ela retorna. Se uma exceção for lançada, usamos `logger.exception` para registrar a exceção, incluindo o traceback.

### Como Utilizar o Decorador

Agora, veja como aplicar o `log_decorator` a uma função:

```python
@log_decorator
def soma(a, b):
    return a + b

@log_decorator
def falha():
    raise ValueError("Um erro intencional")

# Testando as funções decoradas
soma(5, 3)  # Isso irá logar a chamada e o retorno
try:
    falha()  # Isso irá logar a chamada e a exceção
except ValueError:
    pass  # Ignora a exceção para fins de demonstração
```

Ao decorar as funções `soma` e `falha` com `@log_decorator`, automaticamente logamos a entrada e saída (ou exceção) dessas funções sem alterar o corpo delas. Isso é especialmente útil para debugar, monitorar a performance de aplicações ou simplesmente manter um registro de quais funções estão sendo chamadas e com quais argumentos.

### Benefícios do Uso de Decoradores com Loguru

O uso de decoradores em conjunto com o Loguru fornece uma abordagem elegante e poderosa para adicionar logs a aplicações Python. Sem a necessidade de modificar o corpo da função, podemos facilmente adicionar funcionalidades de logging, o que torna o código mais limpo, mantém a separação de preocupações e facilita a manutenção e o debugging.

Além disso, ao centralizar a lógica de logging no decorador, promovemos a reutilização de código e garantimos uma forma consistente de logar informações através de diferentes partes de uma aplicação.

### Conclusão

O Loguru oferece uma abordagem moderna e conveniente para logging em Python, simplificando muitos aspectos que requerem configuração manual detalhada com o módulo de logging padrão do Python. Seja para desenvolvimento, depuração ou produção, adicionar logging ao seu aplicativo com Loguru pode melhorar significativamente a visibilidade e a capacidade de diagnóstico do seu código.

### Desafio

![imagem_03](./pic/3.jpg)

Aplicar decorador de Log, Timer e Qualidade em nossa ETL

![imagem_03](./pic/pic_05.png)



================================================
File: /Bootcamp - Python para dados/aula09/etl.py
================================================
import pandas as pd
import os
import glob
# uma funcao de extract que le e consolida os json

def extrair_dados_e_consolidar(pasta: str) -> pd.DataFrame:
    arquivos_json = glob.glob(os.path.join(pasta, '*.json'))
    df_list = [pd.read_json(arquivo) for arquivo in arquivos_json]
    df_total = pd.concat(df_list, ignore_index=True)
    return df_total

# uma funcao que transforma

def calcular_kpi_de_total_de_vendas(df: pd.DataFrame) -> pd.DataFrame:
    df["Total"] = df["Quantidade"] * df["Venda"]
    return df

def carregar_dados(df: pd.DataFrame, format_saida: list):
    """
    parametro que vai ser ou "csv" ou "parquet" ou "os dois"
    """
    for formato in format_saida:
        if formato == 'csv':
            df.to_csv("dados.csv", index=False)
        if formato == 'parquet':
            df.to_parquet("dados.parquet", index=False)

def pipeline_calcular_kpi_de_vendas_consolidado(pasta: str, formato_de_saida: list):
    data_frame = extrair_dados_e_consolidar(pasta)
    data_frame_calculado = calcular_kpi_de_total_de_vendas(data_frame)
    carregar_dados(data_frame_calculado, formato_de_saida)

# uma funcao que da load em csv ou parquet
    
print(carregar_dados.__doc__)

================================================
File: /Bootcamp - Python para dados/aula09/exemplo_00.py
================================================
from log import log_decorator

from timer import time_measure_decorator

from hello import hello


@hello
def soma_1(a, b):
    return a + b

soma_1(1,2)

================================================
File: /Bootcamp - Python para dados/aula09/hello.py
================================================
from functools import wraps

def hello(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        result = func(*args, **kwargs)
        print("Isso e um decorador")
        return result
    return wrapper


================================================
File: /Bootcamp - Python para dados/aula09/log.py
================================================
from loguru import logger
from sys import stderr
from functools import wraps

# Removendo os handlers existentes para evitar duplicação
logger.remove()

# Configuração do logger para stderr
logger.add(
                sink=stderr,
                format="{time} <r>{level}</r> <g>{message}</g> {file}",
                level="INFO"
            )

# Configuração do logger para arquivo de log
logger.add(
                "meu_arquivo_de_logs.log",
                format="{time} {level} {message} {file}",
                level="INFO"
            )

def log_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger.info(f"Chamando função '{func.__name__}' com args {args} e kwargs {kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.info(f"Função '{func.__name__}' retornou {result}")
            return result
        except Exception as e:
            logger.exception(f"Exceção capturada em '{func.__name__}': {e}")
            raise  # Re-lança a exceção para não alterar o comportamento da função decorada
    return wrapper


================================================
File: /Bootcamp - Python para dados/aula09/main.py
================================================
from loguru import logger

def minha_funcao():
    raise ValueError("Um erro aconteceu!")

try:
    minha_funcao()
except Exception:
    logger.exception("Uma exceção foi capturada")

================================================
File: /Bootcamp - Python para dados/aula09/pipeline.py
================================================
from etl import pipeline_calcular_kpi_de_vendas_consolidado

pasta_argumento: str = 'data'
formato_de_saida: list = ["csv", "parquet"]

pipeline_calcular_kpi_de_vendas_consolidado(pasta_argumento, formato_de_saida)


================================================
File: /Bootcamp - Python para dados/aula09/singleton_decorator.py
================================================
def singleton(cls):
    instances = {}
    def get_instance(*args, **kwargs):
        if cls not in instances:
            instances[cls] = cls(*args, **kwargs)
        return instances[cls]
    return get_instance

@singleton
class DatabaseConnection:
    def __init__(self):
        print("Inicializando uma nova instância da conexão com o banco de dados.")

# Testando o padrão Singleton
if __name__ == "__main__":
    db1 = DatabaseConnection()
    db2 = DatabaseConnection()
    
    print(f"db1 é db2? {'Sim' if db1 is db2 else 'Não'}")


================================================
File: /Bootcamp - Python para dados/aula09/tenacity_decorator.py
================================================
from tenacity import retry, stop_after_attempt, wait_fixed

@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
def get_user_input():
    user_input = input("Digite 'ok' para continuar: ")
    if user_input.lower() != 'ok':
        print("Input incorreto. Por favor, tente novamente.")
        raise ValueError("Input incorreto")
    else:
        print("Input correto. Continuando...")

# Chamar a função
try:
    get_user_input()
except Exception as e:
    print(f"Finalmente falhou após várias tentativas: {e}")


================================================
File: /Bootcamp - Python para dados/aula09/timer.py
================================================
import time
from loguru import logger
from functools import wraps


# Decorador de medida de tempo
def time_measure_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        logger.info(f"Função '{func.__name__}' executada em {end_time - start_time:.4f} segundos")
        return result
    return wrapper

================================================
File: /Bootcamp - Python para dados/aula11-15/README.md
================================================
# aula02_bootcamp

instalar pyenv

instalar poetry

instalar ....

================================================
File: /Bootcamp - Python para dados/aula11-15/OOP.md
================================================
# Orientação a Objetos (OOP) versus Programação Funcional

## Orientação a Objetos (OOP):
A Orientação a Objetos (OOP) é um paradigma de programação que se baseia no conceito de "objetos", que podem conter dados na forma de campos (também conhecidos como atributos ou propriedades) e códigos na forma de procedimentos (métodos ou funções). Os objetos são instâncias de classes, que definem as estruturas e comportamentos dos objetos. Os principais conceitos da OOP incluem encapsulamento, herança e polimorfismo.

### Características da OOP:
1. **Encapsulamento:** O encapsulamento permite ocultar detalhes de implementação dentro de um objeto, expondo apenas a interface pública.
2. **Herança:** A herança permite que uma classe herde características e comportamentos de outra classe, promovendo a reutilização de código e a organização hierárquica de classes.
3. **Polimorfismo:** O polimorfismo permite que objetos de diferentes classes sejam tratados de maneira uniforme, fornecendo interfaces comuns para comportamentos diferentes.

## Programação Funcional:
A Programação Funcional é outro paradigma de programação que se concentra na avaliação de funções matemáticas e na aplicação de funções para transformar dados. Na programação funcional, as funções são tratadas como cidadãos de primeira classe, o que significa que elas podem ser atribuídas a variáveis, passadas como argumentos para outras funções e retornadas como resultados de outras funções.

### Características da Programação Funcional:
1. **Imutabilidade:** As estruturas de dados são imutáveis, o que significa que não podem ser modificadas após serem criadas. Em vez disso, as funções de transformação retornam novas estruturas de dados.
2. **Funções Puras:** As funções na programação funcional são consideradas "puras" se retornarem o mesmo resultado para os mesmos argumentos e não tiverem efeitos colaterais observáveis.
3. **Recursão:** A recursão é comumente usada na programação funcional em vez de loops iterativos.

## Diferenças entre OOP e Programação Funcional:
1. **Abordagem de Solução de Problemas:** Na OOP, os problemas são resolvidos pensando-se em objetos e suas interações, enquanto na programação funcional, os problemas são resolvidos pensando-se em funções e suas composições.
2. **Estado e Mutabilidade:** Na OOP, os objetos podem manter estados mutáveis, enquanto na programação funcional, as estruturas de dados geralmente são imutáveis.
3. **Ênfase na Mutabilidade:** Na OOP, a mutabilidade é frequentemente aceita e até mesmo incentivada, enquanto na programação funcional, a ênfase é na imutabilidade e na evitação de efeitos colaterais.

## Conclusão:
Tanto a Orientação a Objetos quanto a Programação Funcional são paradigmas de programação poderosos, cada um com suas próprias vantagens e casos de uso. A escolha entre eles depende do problema em questão, das preferências pessoais e das necessidades do projeto. Em muitos casos, é possível combinar elementos de ambos os paradigmas para criar soluções mais flexíveis e eficientes.


================================================
File: /Bootcamp - Python para dados/aula11-15/desafio.py
================================================
CONSTANTE_BONUS = 1000

# 1) Solicita ao usuário que digite seu nome
#nome_usuario = input("Digite o seu nome: ")

# nome_usuario = 33 isso e um erro?

nome_usuario = input("Digite o seu nome: ")

if nome_usuario.isdigit():
    print("Voce digitou seu nome errado")
    exit()
elif len(nome_usuario) == 0:
    print("Voce nao digitou nada")
    exit()
elif nome_usuario.isspace():
    print("Voce digitou so espaco")
    exit()

# 2) Solicita ao usuário que digite o valor do seu salário
# Converte a entrada para um número de ponto flutuante
salario_usuario = float(input("Digite o seu salario: "))

# 3) Solicita ao usuário que digite o valor do bônus recebido
# Converte a entrada para um número de ponto flutuante
bonus_usuario = float(input("Digite o seu bonus: "))

# 4) Calcule o valor do bônus final

valor_do_bonus = CONSTANTE_BONUS + salario_usuario * bonus_usuario

# 5) Imprime a mensagem personalizada incluindo o nome do usuário e o valor do bonus
print(f"O usuario {nome_usuario} possui o bonus de {valor_do_bonus}")

# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?

================================================
File: /Bootcamp - Python para dados/aula11-15/exercicios.py
================================================
import math

# #### Inteiros (`int`)

# 1. Escreva um programa que soma dois números inteiros inseridos pelo usuário.
# 2. Crie um programa que receba um número do usuário e calcule o resto da divisão desse número por 5.
# 3. Desenvolva um programa que multiplique dois números fornecidos pelo usuário e mostre o resultado.
# 4. Faça um programa que peça dois números inteiros e imprima a divisão inteira do primeiro pelo segundo.

numero_01 = int(input("Inserir um numero inteiro: "))
numero_02 = int(input("Inserir outro numero inteiro: "))
resultado = numero_01 // numero_02
print(resultado)

# 5. Escreva um programa que calcule o quadrado de um número fornecido pelo usuário.

# #### Números de Ponto Flutuante (`float`)

# 6. Escreva um programa que receba dois números flutuantes e realize sua adição.
# 7. Crie um programa que calcule a média de dois números flutuantes fornecidos pelo usuário.
# 8. Desenvolva um programa que calcule a potência de um número (base e expoente fornecidos pelo usuário).
# 9. Faça um programa que converta a temperatura de Celsius para Fahrenheit.
# 10. Escreva um programa que calcule a área de um círculo, recebendo o raio como entrada.

#raio_do_circulo = float(input("Digite o raio: "))
#area_do_circulo = math.pi * raio_do_circulo ** 2
# area_do_circulo_formatada = "{:.2f}".format(area_do_circulo)
#print(f"{area_do_circulo:.2f}")

# #### Strings (`str`)

# 11. Escreva um programa que receba uma string do usuário e a converta para maiúsculas.
# 12. Crie um programa que receba o nome completo do usuário e imprima o nome com todas as letras minúsculas.
# 13. Desenvolva um programa que peça ao usuário para inserir uma frase e, em seguida, imprima esta frase sem espaços em branco no início e no final.
# 14. Faça um programa que peça ao usuário para digitar uma data no formato "dd/mm/aaaa" e, em seguida, imprima o dia, o mês e o ano separadamente.
# 15. Escreva um programa que concatene duas strings fornecidas pelo usuário.

# data_do_usuario = input("Insira uma data no formato dd/mm/aaaa: ")
# lista_de_dia_mes_ano = data_do_usuario.split("/")
# print(f"O elemento 1 e o: {lista_de_dia_mes_ano[0]}")
# print(f"O elemento 2 e o: {lista_de_dia_mes_ano[1]}")
# print(f"O elemento 3 e o: {lista_de_dia_mes_ano[2]}")

# #### Booleanos (`bool`)

# 16. Escreva um programa que avalie duas expressões booleanas inseridas pelo usuário e retorne o resultado da operação AND entre elas.
# 17. Crie um programa que receba dois valores booleanos do usuário e retorne o resultado da operação OR.
# 18. Desenvolva um programa que peça ao usuário para inserir um valor booleano e, em seguida, inverta esse valor.
# 19. Faça um programa que compare se dois números fornecidos pelo usuário são iguais.
# 20. Escreva um programa que verifique se dois números fornecidos pelo usuário são diferentes.

# #### try-except e if

# 21: Conversor de Temperatura
# 22: Verificador de Palíndromo
# 23: Calculadora Simples
# 24: Classificador de Números
# 25: Conversão de Tipo com Validação

================================================
File: /Bootcamp - Python para dados/aula11-15/main.py
================================================
# Exemplo que causa TypeError

# try:
#     resultado = len(3)
#     print(resultado)
# except TypeError as e:
#     print(e)
# else:
#     print("tudo ocorreu bem")
# finally:
#     print("o importante e participar")  

# numero = int(input("Insira um numero :"))
# if isinstance(numero, ):
#     print("A variável é um inteiro.")
# else:
#     print("A variável não é um inteiro.")



================================================
File: /Bootcamp - Python para dados/aula11-15/01-basico/class.py
================================================
import pandas as pd

class ProcessadorCSV:
    def __init__(self, arquivo_csv):
        self.arquivo_csv = arquivo_csv
        self.df = None
    
    def carregar_csv(self):
        # Carregar o arquivo CSV em um DataFrame
        self.df = pd.read_csv(self.arquivo_csv)
    
    def remover_celulas_vazias(self):
        # Verificar e remover células vazias
        self.df = self.df.dropna()
    
    def filtrar_por_estado(self, estado):
        # Filtrar as linhas pela coluna estado
        self.df = self.df[self.df['estado'] == estado]
    
    def processar(self, estado):
        # Carregar CSV, remover células vazias e filtrar por estado
        self.carregar_csv()
        self.remover_celulas_vazias()
        self.filtrar_por_estado(estado)
        
        return self.df

# Exemplo de uso
arquivo_csv = './exemplo.csv'  # substitua 'exemplo.csv' pelo caminho do seu arquivo CSV
estado_filtrado = 'SP'  # estado que você quer filtrar

processador = ProcessadorCSV(arquivo_csv)
df_filtrado = processador.processar(estado_filtrado)

print(df_filtrado)


================================================
File: /Bootcamp - Python para dados/aula11-15/01-basico/functional.py
================================================
import pandas as pd

def carregar_csv_e_filtrar(arquivo_csv, estado):
    # Carregar o arquivo CSV em um DataFrame
    df = pd.read_csv(arquivo_csv)
    
    # Verificar e remover células vazias
    df = df.dropna()
    
    # Filtrar as linhas pela coluna estado
    df_filtrado = df[df['estado'] == estado]
    
    return df_filtrado

# Exemplo de uso
arquivo_csv = './exemplo.csv'  # substitua 'dados.csv' pelo caminho do seu arquivo CSV
estado_filtrado = 'SP'  # estado que você quer filtrar
df_filtrado = carregar_csv_e_filtrar(arquivo_csv, estado_filtrado)

print(df_filtrado)


================================================
File: /Bootcamp - Python para dados/aula11-15/02-encapsulamento/conexao.py
================================================
from .sqlite import BancoDeDadosSQLite
from .postgre import BancoDeDadosPost
from .encaps import BancoDeDados

########## SQLITE #############

nome_arquivo = "exemplo.db"
banco_sql = BancoDeDadosSQLite(nome_arquivo)
banco_sql.conectar()

# Inserindo dados na tabela
insert_query = """
INSERT INTO usuarios (nome, email) VALUES
('João', 'joao@example.com'),
('Maria', 'maria@example.com');
"""
banco_sql.executar_query(insert_query)

banco_sql.desconectar()

########## POSTGRE #############

host = 'localhost'
porta = '5432'
banco = 'nome_do_banco'
usuario = 'usuario'
senha = 'senha'
    
banco_post = BancoDeDadosPost(host, porta, banco, usuario, senha)
banco_post.conectar()

insert_query = """
INSERT INTO usuarios (nome, email) VALUES
('João', 'joao@example.com'),
('Maria', 'maria@example.com');
"""
banco_post.executar_query(insert_query)

banco_post.desconectar()


########## ENCAPSULADO #############
banco = BancoDeDados("tipo_banco")
banco.conectar()

insert_query = """
INSERT INTO usuarios (nome, email) VALUES
('João', 'joao@example.com'),
('Maria', 'maria@example.com');
"""
banco.executar_query(insert_query)

banco.desconectar()

================================================
File: /Bootcamp - Python para dados/aula11-15/02-encapsulamento/encaps.py
================================================
import os
import sqlite3
import psycopg2

class BancoDeDados:
    def __init__(self, tipo_banco):
        self.tipo_banco = tipo_banco
        self.conexao = None

    def conectar(self):
        if self.tipo_banco == 'sqlite':
            try:
                nome_arquivo = os.getenv('NOME_ARQUIVO_SQLITE')
                self.conexao = sqlite3.connect(nome_arquivo)
                print("Conexão SQLite estabelecida com sucesso!")
            except sqlite3.Error as e:
                print("Erro ao conectar ao banco de dados SQLite:", e)
        elif self.tipo_banco == 'postgres':
            try:
                host = os.getenv('HOST_PG')
                porta = os.getenv('PORTA_PG')
                banco = os.getenv('BANCO_PG')
                usuario = os.getenv('USUARIO_PG')
                senha = os.getenv('SENHA_PG')
                self.conexao = psycopg2.connect(
                    host=host,
                    port=porta,
                    database=banco,
                    user=usuario,
                    password=senha
                )
                print("Conexão PostgreSQL estabelecida com sucesso!")
            except psycopg2.Error as e:
                print("Erro ao conectar ao banco de dados PostgreSQL:", e)
        else:
            print("Tipo de banco de dados não suportado.")

    def desconectar(self):
        if self.conexao:
            self.conexao.close()
            if self.tipo_banco == 'sqlite':
                print("Conexão SQLite fechada.")
            elif self.tipo_banco == 'postgres':
                print("Conexão PostgreSQL fechada.")

    def executar_query(self, query):
        try:
            cursor = self.conexao.cursor()
            cursor.execute(query)
            self.conexao.commit()
            print("Query executada com sucesso!")
        except (sqlite3.Error, psycopg2.Error) as e:
            print("Erro ao executar a query:", e)


# Exemplo de uso
if __name__ == "__main__":
    tipo_banco = os.getenv('TIPO_BANCO')  # 'sqlite' ou 'postgres'
    
    banco = BancoDeDados(tipo_banco)
    banco.conectar()

    # Exemplo de criação de tabela
    create_table_query = """
    CREATE TABLE IF NOT EXISTS usuarios (
        id SERIAL PRIMARY KEY,
        nome TEXT NOT NULL,
        email TEXT NOT NULL
    );
    """
    banco.executar_query(create_table_query)

    # Exemplo de inserção de dados
    insert_query = """
    INSERT INTO usuarios (nome, email) VALUES
    ('João', 'joao@example.com'),
    ('Maria', 'maria@example.com');
    """
    banco.executar_query(insert_query)

    banco.desconectar()


================================================
File: /Bootcamp - Python para dados/aula11-15/02-encapsulamento/postgre.py
================================================
import psycopg2

class BancoDeDadosPost:
    def __init__(self, host, porta, banco, usuario, senha):
        self.host = host
        self.porta = porta
        self.banco = banco
        self.usuario = usuario
        self.senha = senha
        self.conexao = None

    def conectar(self):
        try:
            self.conexao = psycopg2.connect(
                host=self.host,
                port=self.porta,
                database=self.banco,
                user=self.usuario,
                password=self.senha
            )
            print("Conexão estabelecida com sucesso!")
        except psycopg2.Error as e:
            print("Erro ao conectar ao banco de dados:", e)

    def desconectar(self):
        if self.conexao:
            self.conexao.close()
            print("Conexão fechada.")

    def executar_query(self, query):
        try:
            cursor = self.conexao.cursor()
            cursor.execute(query)
            self.conexao.commit()
            print("Query executada com sucesso!")
        except psycopg2.Error as e:
            print("Erro ao executar a query:", e)


# Exemplo de uso
if __name__ == "__main__":
    host = 'localhost'
    porta = '5432'
    banco = 'nome_do_banco'
    usuario = 'usuario'
    senha = 'senha'
    
    banco = BancoDeDadosPost(host, porta, banco, usuario, senha)
    banco.conectar()

    # Exemplo de criação de tabela
    create_table_query = """
    CREATE TABLE IF NOT EXISTS usuarios (
        id SERIAL PRIMARY KEY,
        nome TEXT NOT NULL,
        email TEXT NOT NULL
    );
    """
    banco.executar_query(create_table_query)

    # Exemplo de inserção de dados
    insert_query = """
    INSERT INTO usuarios (nome, email) VALUES
    ('João', 'joao@example.com'),
    ('Maria', 'maria@example.com');
    """
    banco.executar_query(insert_query)

    banco.desconectar()


================================================
File: /Bootcamp - Python para dados/aula11-15/02-encapsulamento/sqlite.py
================================================
import sqlite3

class BancoDeDadosSQLite:
    def __init__(self, nome_arquivo):
        self.nome_arquivo = nome_arquivo
        self.conexao = None

    def conectar(self):
        try:
            self.conexao = sqlite3.connect(self.nome_arquivo)
            print("Conexão estabelecida com sucesso!")
        except sqlite3.Error as e:
            print("Erro ao conectar ao banco de dados:", e)

    def desconectar(self):
        if self.conexao:
            self.conexao.close()
            print("Conexão fechada.")

    def executar_query(self, query):
        try:
            cursor = self.conexao.cursor()
            cursor.execute(query)
            self.conexao.commit()
            print("Query executada com sucesso!")
        except sqlite3.Error as e:
            print("Erro ao executar a query:", e)


# Exemplo de uso
if __name__ == "__main__":
    nome_arquivo = "exemplo.db"
    banco = BancoDeDadosSQLite(nome_arquivo)
    banco.conectar()

    # Criando uma tabela
    create_table_query = """
    CREATE TABLE IF NOT EXISTS usuarios (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        nome TEXT NOT NULL,
        email TEXT NOT NULL
    );
    """
    banco.executar_query(create_table_query)

    # Inserindo dados na tabela
    insert_query = """
    INSERT INTO usuarios (nome, email) VALUES
    ('João', 'joao@example.com'),
    ('Maria', 'maria@example.com');
    """
    banco.executar_query(insert_query)

    banco.desconectar()


================================================
File: /Bootcamp - Python para dados/aula11-15/03-heranca/etl.py
================================================
import pandas as pd

class ETLProcess:
    def __init__(self, fonte_dados):
        self.fonte_dados = fonte_dados

    def extrair_dados(self):
        raise NotImplementedError("Método extrair_dados deve ser implementado nas classes filhas.")

    def transformar_dados(self, dados):
        raise NotImplementedError("Método transformar_dados deve ser implementado nas classes filhas.")

    def carregar_dados(self, dados_transformados):
        raise NotImplementedError("Método carregar_dados deve ser implementado nas classes filhas.")

    def executar_etl(self):
        dados_extraidos = self.extrair_dados()
        dados_transformados = self.transformar_dados(dados_extraidos)
        self.carregar_dados(dados_transformados)


class ETLCSV(ETLProcess):
    def extrair_dados(self):
        return pd.read_csv(self.fonte_dados)

    def transformar_dados(self, dados):
        # Exemplo simples de transformação: converter todas as letras em maiúsculas
        return dados.applymap(lambda x: x.upper() if isinstance(x, str) else x)

    def carregar_dados(self, dados_transformados):
        # Aqui você pode implementar a lógica para carregar os dados transformados para onde desejar
        print("Dados transformados:")
        print(dados_transformados)


# Exemplo de uso
if __name__ == "__main__":
    fonte_csv = 'dados.csv'  # Substitua 'dados.csv' pelo caminho do seu arquivo CSV
    etl_csv = ETLCSV(fonte_csv)
    etl_csv.executar_etl()


================================================
File: /Bootcamp - Python para dados/aula11-15/04-polimorfismo/overload.py
================================================
## Em Python, a sobrecarga de método não é diretamente suportada como em algumas outras linguagens de programação, 
## mas você pode simular sobrecarga usando parâmetros padrão ou argumentos variáveis.


class Calculadora:
    def soma(self, *args):
        total = 0
        for num in args:
            total += num
        return total

# Exemplo de uso
calculadora = Calculadora()
print(calculadora.soma(1, 2))        # Saída: 3
print(calculadora.soma(1, 2, 3, 4))  # Saída: 10
print(calculadora.soma(5, 10, 15))   # Saída: 30


================================================
File: /Bootcamp - Python para dados/aula11-15/04-polimorfismo/override.py
================================================
import pandas as pd

class ETLProcess:
    def __init__(self, fonte_dados):
        self.fonte_dados = fonte_dados

    def extrair_dados(self):
        raise NotImplementedError("Método extrair_dados deve ser implementado nas classes filhas.")

    def transformar_dados(self, dados):
        raise NotImplementedError("Método transformar_dados deve ser implementado nas classes filhas.")

    def carregar_dados(self, dados_transformados):
        raise NotImplementedError("Método carregar_dados deve ser implementado nas classes filhas.")

    def executar_etl(self):
        dados_extraidos = self.extrair_dados()
        dados_transformados = self.transformar_dados(dados_extraidos)
        self.carregar_dados(dados_transformados)


class ETLCSV(ETLProcess):
    def extrair_dados(self):
        return pd.read_csv(self.fonte_dados)

    def transformar_dados(self, dados):
        # Exemplo simples de transformação: converter todas as letras em maiúsculas
        return dados.applymap(lambda x: x.upper() if isinstance(x, str) else x)

    def carregar_dados(self, dados_transformados):
        # Aqui você pode implementar a lógica para carregar os dados transformados de um arquivo CSV
        print("Dados transformados (CSV):")
        print(dados_transformados)


class ETLExcel(ETLProcess):
    def extrair_dados(self):
        return pd.read_excel(self.fonte_dados)

    def transformar_dados(self, dados):
        # Exemplo simples de transformação: converter todas as letras em minúsculas
        return dados.applymap(lambda x: x.lower() if isinstance(x, str) else x)

    def carregar_dados(self, dados_transformados):
        # Aqui você pode implementar a lógica para carregar os dados transformados de um arquivo Excel
        print("Dados transformados (Excel):")
        print(dados_transformados)


# Exemplo de uso
if __name__ == "__main__":
    fonte_csv = 'dados.csv'  # Substitua 'dados.csv' pelo caminho do seu arquivo CSV
    etl_csv = ETLCSV(fonte_csv)
    etl_csv.executar_etl()

    fonte_excel = 'dados.xlsx'  # Substitua 'dados.xlsx' pelo caminho do seu arquivo Excel
    etl_excel = ETLExcel(fonte_excel)
    etl_excel.executar_etl()


================================================
File: /Bootcamp - Python para dados/aula11-15/05-GettereSetter/basico.py
================================================
class Pessoa:
    def __init__(self, nome, idade):
        self._nome = nome
        self._idade = idade

    # Getter para o atributo 'nome'
    def get_nome(self):
        return self._nome

    # Setter para o atributo 'nome'
    def set_nome(self, novo_nome):
        self._nome = novo_nome

    # Getter para o atributo 'idade'
    def get_idade(self):
        return self._idade

    # Setter para o atributo 'idade'
    def set_idade(self, nova_idade):
        if nova_idade > 0:
            self._idade = nova_idade
        else:
            print("A idade deve ser um número positivo.")

# Exemplo de uso
pessoa = Pessoa("João", 30)

# Usando o método getter para acessar o atributo 'nome'
print("Nome:", pessoa.get_nome())  # Saída: Nome: João

# Usando o método setter para alterar o atributo 'nome'
pessoa.set_nome("Maria")
print("Novo nome:", pessoa.get_nome())  # Saída: Novo nome: Maria

# Usando o método getter para acessar o atributo 'idade'
print("Idade:", pessoa.get_idade())  # Saída: Idade: 30

# Usando o método setter para alterar o atributo 'idade'
pessoa.set_idade(25)
print("Nova idade:", pessoa.get_idade())  # Saída: Nova idade: 25

# Tentando definir uma idade negativa
pessoa.set_idade(-5)  # Saída: A idade deve ser um número positivo.


================================================
File: /Bootcamp - Python para dados/aula11-15/05-GettereSetter/decorator.py
================================================
class Pessoa:
    def __init__(self, nome, idade):
        self._nome = nome
        self._idade = idade

    @property
    def nome(self):
        return self._nome

    @nome.setter
    def nome(self, novo_nome):
        self._nome = novo_nome

    @property
    def idade(self):
        return self._idade

    @idade.setter
    def idade(self, nova_idade):
        if nova_idade > 0:
            self._idade = nova_idade
        else:
            print("A idade deve ser um número positivo.")

# Exemplo de uso
pessoa = Pessoa("João", 30)

# Usando o método getter para acessar o atributo 'nome'
print("Nome:", pessoa.nome)  # Saída: Nome: João

# Usando o método setter para alterar o atributo 'nome'
pessoa.nome = "Maria"
print("Novo nome:", pessoa.nome)  # Saída: Novo nome: Maria

# Usando o método get


================================================
File: /Bootcamp - Python para dados/aula11-15/Aula02/csv-test.py
================================================
import pandas as pd


df = pd.read_csv('./exemplo.csv')

df_filtrado = df[df['estado'] == 'SP']

df_filtrado = df[df['preço'] == '10,50']


print(df_filtrado)



df2 = pd.read_csv('./examplo2.csv')

df_filtrado2 = df2[df2['estado'] == 'DF']

df_filtrado2 = df2[df2['preço'] == '10,50']

print(df_filtrado)

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula02/src/usar.py
================================================
from interface.classes.csv_class import CsvProcessor
# import pandas as pd

arquivo_csv = './exemplo.csv'
filtro = 'estado'
limite = 'SP'

arquivo_CSV = CsvProcessor(arquivo_csv)
arquivo_CSV.carregar_csv()  # Load the CSV
print(arquivo_CSV.filtrar_por(['estado', 'preço'], ['SP', '10,50']))
# print(arquivo_CSV.df)
print("#########################")
# arquivo_csv2 = './examplo2.csv'
# filtro2 = 'estado'  # Changed to filtro2
# limite2 = 'DF'

# arquivo_CSV2 = CsvProcessor(arquivo_csv2)
# arquivo_CSV2.carregar_csv()  # Load the CSV
# print(arquivo_CSV2.filtrar_por(filtro2, limite2))  # Changed to filtro2
# print(arquivo_CSV2.sub_filtro('preço', '10,50'))


================================================
File: /Bootcamp - Python para dados/aula11-15/Aula02/src/interface/classes/csv_class.py
================================================
import pandas as pd

class CsvProcessor:
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.df = None
        self.df_filtrado = None

    def carregar_csv(self):
        self.df = pd.read_csv(self.file_path)
        return self.df  # Return the DataFrame after loading

    ## receber um str str[]
    def filtrar_por(self, colunas, atributos):
        if len(colunas) != len(atributos):
            raise ValueError("Não tem o mesmo número de colunas e atributos")
        
        if len(colunas) == 0:
            return self.df
        
        coluna_atual = colunas[0]
        atributo_atual = atributos[0]

        df_filtrado = self.df[self.df[coluna_atual] == atributo_atual]

        if len(colunas) == 1:
            return df_filtrado
        else:
            return self.filtrar_por(colunas[1:], atributos[1:])

    


================================================
File: /Bootcamp - Python para dados/aula11-15/Aula03/README.md
================================================
Minha empresa recebe arquivos nos formatos .csv e .txt em duas pastas distintas,
e preciso consolidá-los em um único dataframe. 

Qual seria a melhor abordagem para realizar essa tarefa?



data/csv_files
data/txt_files


Formato dos arquivos 
id,name


s3 -> trigger -> ec2 ou lambda ou ecs 



================================================
File: /Bootcamp - Python para dados/aula11-15/Aula03/data/txt_files/test.txt
================================================
id,name
1,fabio

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula03/src/__main__.py
================================================
import schedule
import time
from lib.classes.CsvSource import CsvSource
from lib.classes.JsonSource import JsonSource
from lib.classes.TxtSource import TxtSource

# Função para verificar novos arquivos
def check_for_new_files():
    csv_source.check_for_new_files()  # Chama o método check_for_new_files da instância
    txt_source.check_for_new_files()
    json_source.check_for_new_files()

# Agendando a execução da função check_for_new_files() a cada segundo
schedule.every(10).seconds.do(check_for_new_files)

csv_source = CsvSource()
txt_source = TxtSource()
json_source = JsonSource()

# Executa o loop principal
while True:
    schedule.run_pending()
    time.sleep(1)  # Aguarda 1 segundo para que o loop não consuma muito processamento


================================================
File: /Bootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/AbstractDataSource.py
================================================
from abc import ABC, abstractmethod

class AbstractDataSource(ABC):
    def __init__(self):
        pass
     
    @abstractmethod
    def start(self):
        raise NotImplementedError("Método não implementado")

    @abstractmethod
    def get_data(self):
        raise NotImplementedError("Método não implementado")

    @abstractmethod
    def transform_data_to_df(self):
        raise NotImplementedError("Método não implementado")

    @abstractmethod
    def save_data(self):
        raise NotImplementedError("Método não implementado")

    def hello_world(self):
        print('Hello World')

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/CsvSource.py
================================================
import os
import pandas as pd
from lib.classes.FilesSources import FilesSources


class CsvSource(FilesSources):
    def create_path(self):
        current_directory = os.getcwd()
        self.folder_path = os.path.join(current_directory, 'data', 'csv_files')
        if not os.path.exists(self.folder_path):
            os.makedirs(self.folder_path)

    def check_for_new_files(self):
        current_files = os.listdir(self.folder_path)
        new_files = [file for file in current_files if file not in self.previous_files and file.endswith('.csv')]

        if new_files:
            print("New files detected:", new_files)
            # Update the list of previous files
            self.previous_files = current_files
        else:
            print("No new CSV files detected.")
            self.get_data()

    def get_data(self):
        # Implement getting data from CSV files in the specified folder
        data_frames = []
        for file_path in self.previous_files:
            try:
                path = f'{self.folder_path}/{file_path}'
                data = pd.read_csv(path)
                data_frames.append(data)
            except Exception as e:
                print("An error occurred while reading the CSV file:", e)
        if data_frames:
            self.combined_data = pd.concat(data_frames, ignore_index=True)
            print(self.combined_data)
            return self.combined_data
        else:
            return None
    
    def transform_data_to_df(self):
        return super().transform_data_to_df()
        

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/FilesSources.py
================================================
import os
from lib.classes.AbstractDataSource import AbstractDataSource

class FilesSources(AbstractDataSource):
    def __init__(self):
        self.previous_files = []
        self.start()

    def create_path(self):
        current_directory = os.getcwd()
        self.folder_path = os.path.join(current_directory, 'data', 'extension_files')
        if not os.path.exists(self.folder_path):
            os.makedirs(self.folder_path)

    def check_for_new_files(self):
        current_files = os.listdir(self.folder_path)
        new_files = [file for file in current_files if file not in self.previous_files]

        if new_files:
            print("New files detected:", new_files)
            # Update the list of previous files
            self.previous_files = current_files
        else:
            print("No new files detected.")

    def get_data(self):
        pass
    
    def transform_data_to_df(self):
        pass

    def save_data(self):
        pass

    def show_files(self):
        print(self.previous_files)

    def start(self):
        self.create_path()

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/JsonSource.py
================================================
import json
import os
from lib.classes.FilesSources import FilesSources

class JsonSource(FilesSources):
    def create_path(self):
        current_directory = os.getcwd()
        self.folder_path = os.path.join(current_directory, 'json_files')
        if not os.path.exists(self.folder_path):
            os.makedirs(self.folder_path)

    def check_for_new_files(self):
        current_files = os.listdir(self.folder_path)
        new_files = [file for file in current_files if file not in self.previous_files and file.endswith('.json')]

        if new_files:
            print("New files detected:", new_files)
            # Update the list of previous files
            self.previous_files = current_files
        else:
            print("No new JSON files detected.")
            self.get_data()

    def read_json_file(self, file_path):
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            return data
        except Exception as e:
            print("Erro ao acessar o JSON")
            return None
    
    def get_data(self):

        data = []
        for file_path in self.previous_files:
            if file_path.endswith('.json'):
                path = os.path.join(self.folder_path, file_path)
                json_data = self.read_json_file(path)
                if json_data is not None:
                    data.append(json_data)
        print(data)
        return data

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/TxtSource.py
================================================
import os
import pandas as pd
from lib.classes.FilesSources import FilesSources


class TxtSource(FilesSources):
    def create_path(self):
        current_directory = os.getcwd()
        self.folder_path = os.path.join(current_directory, 'data', 'txt_files')
        if not os.path.exists(self.folder_path):
            os.makedirs(self.folder_path)

    def check_for_new_files(self):
        current_files = os.listdir(self.folder_path)
        new_files = [file for file in current_files if file not in self.previous_files and file.endswith('.txt')]

        if new_files:
            print("New TXT files detected:", new_files)
            # Update the list of previous files
            self.previous_files = current_files
        else:
            print("No new TXT files detected.")
            self.get_data()

    def get_data(self):
        # Implement getting data from TXT files in the specified folder
        data_frames = []
        for file_path in self.previous_files:
            try:
                path = os.path.join(self.folder_path, file_path)
                data = pd.read_csv(path, sep='\t')  # Assume que os arquivos .txt estão tabulados
                data_frames.append(data)
            except Exception as e:
                print("An error occurred while reading the TXT file:", e)
        if data_frames:
            self.combined_data = pd.concat(data_frames, ignore_index=True)
            print(self.combined_data)
            return self.combined_data
        else:
            return None


================================================
File: /Bootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/aws/s3.py
================================================
import boto3

class S3DataCollector:
    def __init__(self, access_key, secret_key, bucket_name):
        self.access_key = access_key
        self.secret_key = secret_key
        self.bucket_name = bucket_name
        self.s3_client = boto3.client('s3', aws_access_key_id=self.access_key, aws_secret_access_key=self.secret_key)

    def list_objects(self, prefix=''):
        response = self.s3_client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)
        if 'Contents' in response:
            return [obj['Key'] for obj in response['Contents']]
        else:
            return []

    def download_file(self, key, local_path):
        return self.s3_client.download_file(self.bucket_name, key, local_path)
        

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula04/Collector/main.py
================================================
from datasource.api import APICollector
from contracts.schema import CompraSchema
from tools.aws.client import S3Client

import time
import schedule

schema = CompraSchema
aws = S3Client()


def apiCollector(schema, aws, repeat):
    reponse = APICollector(schema, aws).start(repeat)
    print('Executei')
    return

schedule.every(1).minutes.do(apiCollector,schema, aws, 50)


while True:
    schedule.run_pending()
    time.sleep(1)


================================================
File: /Bootcamp - Python para dados/aula11-15/Aula04/Collector/contracts/schema.py
================================================
from typing import Union, Dict


GenericSchema = Dict[str, Union[str, float, int]]


CompraSchema: GenericSchema = {
    "ean" : int,
    "price" : float,
    "store" : int,
    "dateTime" : str
}

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula04/Collector/datasource/api.py
================================================
import requests
import pandas as pd
import datetime
from io import BytesIO
from contracts.schema import GenericSchema
from typing import List
from tools.retry import retry

class APICollector:
        def __init__ (self, schema, aws): 
            self._schema = schema
            self._aws = aws
            self._buffer = None
            return
        
        def start(self, param):
            response = self.getData(param)
            response = self.extractData(response)
            response = self.transformDf(response)
            response = self.convertToParquet(response)

            if self._buffer is not None:
                 file_name = self.fileName()
                 print(file_name)
                 self._aws.upload_file(response, file_name)
                 return True
                 
            return False
                
        @retry(requests.exceptions.RequestException, tries=5, delay=1, backoff=2)
        def getData(self, param):
            response = None
            if param > 1:
                  response = requests.get(f'http://127.0.0.1:8000/gerar_compras/{param}').json()
            else:
                 response = requests.get('http://127.0.0.1:8000/gerar_compra').json()
            return response
                
        def extractData(self, response):
            result: List[GenericSchema] = []
            for item in response:
                index = {}
                for key, value in self._schema.items():
                    if type(item.get(key)) == value:
                        index[key] = item[key]
                    else:
                        index[key] = None
                result.append(index)
            return result
        
        def transformDf(self, response):
              result = pd.DataFrame(response)
              return result
        
        def convertToParquet(self, response):
            self._buffer = BytesIO()
            try:
                with self._buffer as buffer:
                    response.to_parquet(buffer)
                    return buffer
            except Exception as e:
                    print(f"Error converting DataFrame to Parquet: {e}")
                    self._buffer = None

        def fileName(self):
             data_atual = datetime.datetime.now().isoformat()
             match = data_atual.split(".")
             return f"api/api-reponse-compra{match[0]}.parquet"

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula04/Collector/tools/retry.py
================================================
import time
from functools import wraps

def retry(exception_to_check, tries=3, delay=1, backoff=2):
    """
    Decorator que retenta a função várias vezes em caso de exceção.
    
    :param exception_to_check: A exceção (ou tuple de exceções) que deve ser capturada.
    :param tries: O número máximo de tentativas.
    :param delay: O tempo de espera inicial entre as tentativas.
    :param backoff: O fator pelo qual o atraso deve aumentar após cada tentativa.
    """
    def decorator_retry(func):
        @wraps(func)
        def wrapper_retry(*args, **kwargs):
            _tries, _delay = tries, delay
            while _tries > 1:
                try:
                    return func(*args, **kwargs)
                except exception_to_check as e:
                    print(f"{func.__name__} falhou, tentando novamente em {_delay} segundos. Tentativas restantes: {_tries - 1}")
                    time.sleep(_delay)
                    _tries -= 1
                    _delay *= backoff
            return func(*args, **kwargs)
        return wrapper_retry
    return decorator_retry

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula04/Collector/tools/aws/client.py
================================================
import boto3
from botocore.exceptions import NoCredentialsError
import sys
import os

# Suponha que você tenha uma variável de ambiente chamada "MINHA_VARIAVEL"
# Você pode acessar seu valor usando a função os.environ.get()


class S3Client:

    def __init__(self):
        
        self._envs = {
            "aws_access_key_id": os.environ.get("AWS_ACCESS_KEY_ID"),
            "aws_secret_access_key": os.environ.get("AWS_SECRET_ACCESS_KEY"),
            "region_name": os.environ.get("AWS_REGION", "us-west-1"),  # Usando um valor padrão se a variável não estiver definida
            "s3_bucket": os.environ.get("S3_BUCKET_NAME"),
            "datalake" : os.environ.get("DELTA_LAKE_S3_PATH")
        }

        for var in self._envs:
            if self._envs[var] is None:
                print(f"A variável de ambiente {var} não está definida.")
                sys.exit(1)

        self.s3 = boto3.client('s3', aws_access_key_id=self._envs["aws_access_key_id"], aws_secret_access_key=self._envs["aws_secret_access_key"], region_name=self._envs["region_name"])

    def upload_file(self, data, s3_key):
        try:
            self.s3.put_object(Body=data.getvalue(), Bucket=self._envs["s3_bucket"], Key=s3_key)
        except NoCredentialsError:
            print("Credenciais não encontradas. Certifique-se de configurar suas credenciais AWS corretamente.")

    def download_file(self, s3_key):
        try:
            file = self.s3.get_object(Bucket=self._envs["s3_bucket"], Key=s3_key)
            print(f"Download bem-sucedido para {s3_key}")
            return file
        except NoCredentialsError:
            print("Credenciais não encontradas. Certifique-se de configurar suas credenciais AWS corretamente.")
        except FileNotFoundError:
            print(f"Arquivo {s3_key} não encontrado no bucket {self._envs['s3_bucket']}.")
        except Exception as e:
            print(f"Ocorreu um erro durante o download: {e}")

    def list_object(self, prefix):
        return self.s3.list_objects(Bucket=self._envs["s3_bucket"], Prefix=prefix)['Contents']

================================================
File: /Bootcamp - Python para dados/aula11-15/Aula04/FakeApi/start.py
================================================
from fastapi import FastAPI
from faker import Faker
import pandas as pd
import random


app = FastAPI(debug=True)
fake = Faker()

file_name = 'data/products.csv'
df = pd.read_csv(file_name)
df['indice'] = range(1, len(df) +1)
df.set_index('indice', inplace=True)

lojapadraoonline = 11

@app.get("/")
async def hello_world():
    return 'Coca-Cola me patrocina!'

@app.get("/gerar_compra")
async def gerar_compra():
    index = random.randint(1, len(df)-1)
    tuple = df.iloc[index]
    return [{
            "client": fake.name(),
            "creditcard": fake.credit_card_provider(),
            "product": tuple["Product Name"],
            "ean": int(tuple["EAN"]),
            "price":  round(float(tuple["Price"])*1.2,2),
            "clientPosition": fake.location_on_land(),
            "store": lojapadraoonline,
            "dateTime": fake.iso8601()
        }]

@app.get("/gerar_compras/{numero_registro}")
async def gerar_compra(numero_registro: int):
    
    if numero_registro < 1:
        return {"error" : "O número deve ser maior que 1"}
 
    respostas = []
    for _ in range(numero_registro):
        try:
            index = random.randint(1, len(df)-1)
            tuple = df.iloc[index]
            compra = {
                    "client": fake.name(),
                    "creditcard": fake.credit_card_provider(),
                    "product": tuple["Product Name"],
                    "ean": int(tuple["EAN"]),
                    "price":  round(float(tuple["Price"])*1.2,2),
                    "clientPosition": fake.location_on_land(),
                    "store": lojapadraoonline,
                    "dateTime": fake.iso8601()
                    }
            respostas.append(compra)
        except IndexError as e:
            print(f"Erro de índice: {e}")
        except ValueError as e:
            print(f"Erro inesperado: {e}")
            compra = {
                    "client": fake.name(),
                    "creditcard": fake.credit_card_provider(),
                    "product": "error",
                    "ean": 0,
                    "price":  0.0,
                    "clientPosition": fake.location_on_land(),
                    "store": lojapadraoonline,
                    "dateTime": fake.iso8601()
                    }
            respostas.append(compra)
        except Exception as e:
            print(f"Erro inesperado: {e}")
    return respostas

================================================
File: /Bootcamp - Python para dados/aula11-15/basics/func_csv.py
================================================
import pandas as pd


def carregar_csv_e_filtrar(arquivo_csv, estado):
    # Carregar o arquivo CSV em um DataFrame
    df = pd.read_csv(arquivo_csv)
    
    # Verificar e remover células vazias
    df = df.dropna()
    
    # Filtrar as linhas pela coluna estado
    df_filtrado = df[df['estado'] == estado]
    
    return df_filtrado

arquivo_csv = './exemplo.csv'  # substitua 'dados.csv' pelo caminho do seu arquivo CSV
estado_filtrado = 'SP'  # estado que você quer filtrar
df_filtrado = carregar_csv_e_filtrar(arquivo_csv, estado_filtrado)

print(df_filtrado)

================================================
File: /Bootcamp - Python para dados/aula11-15/basics/pessoa-class.py
================================================
from datetime import datetime

class Pessoa:
    def __init__(self, nome, idade, profissao):
        self.nome = nome
        self.idade = idade
        self.profissao = profissao

    def ola(self):
        return f'Olá {self.nome}'

    def ano_nascimento(self):
        ano_atual = datetime.now().year
        idade = int(self.idade)
        ano_nascimento = ano_atual - idade
        return f'Você nasceu em: {ano_nascimento}'

    def tem_emprego(self):
        if self.profissao == "Data Eng":
            return f'Não há vagas para: {self.profissao}'
        return f'Vou te arranjar um emprego de {self.profissao}'

# Exemplo de uso:
pessoa = Pessoa("Fabio", "35", "Data Eng")
pessoa2 = Pessoa("Luciano", "33", "Data Product Manager")
print(pessoa.ola())
print(pessoa.ano_nascimento())
print(pessoa.tem_emprego())

print(pessoa2.ola())
print(pessoa2.ano_nascimento())
print(pessoa2.tem_emprego())


================================================
File: /Bootcamp - Python para dados/aula11-15/basics/pessoa.py
================================================
from datetime import datetime

def ola_Pessoa(pessoa):
    return f'Olá {pessoa["nome"]}'

def ano_nascimento_Pessoa(pessoa):
    ano_atual = datetime.now().year
    idade = int(pessoa["idade"])
    ano_nascimento = ano_atual - idade
    return f'você nasceu em: {ano_nascimento}'

def tem_emprego(pessoa):
    if pessoa["profissao"] == "Data Eng":
        return f'Não há vagas para: {pessoa["profissao"]}'
    
    return f'Vou te arranjar um emprego de {pessoa["profissao"]}'
        

pessoa = {
    "nome" : "Fabio",
    "idade" : "35",
    "profissao" : "Data Eng"
}

pessoa2 = {
    "nome" : "Luciano",
    "idade" : "33",
    "profissao" : "Data Product Manager"
}

print(pessoa)    
print(ola_Pessoa(pessoa))
print(ano_nascimento_Pessoa(pessoa))
print(tem_emprego(pessoa))

print(pessoa2)    
print(ola_Pessoa(pessoa2))
print(ano_nascimento_Pessoa(pessoa2))
print(tem_emprego(pessoa2))

================================================
File: /Bootcamp - Python para dados/aula16/README.md
================================================
# aula_16_aovivo


================================================
File: /Bootcamp - Python para dados/aula16/create.py
================================================
from typing import Optional

from sqlmodel import Field, Session, SQLModel, create_engine


class Hero(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    secret_name: str
    age: Optional[int] = None


hero_1 = Hero(name="Deadpond", secret_name="Dive Wilson")
hero_2 = Hero(name="Spider-Boy", secret_name="Pedro Parqueador")
hero_3 = Hero(name="Rusty-Man", secret_name="Tommy Sharp", age=48)


engine = create_engine("sqlite:///database.db", echo=True)


SQLModel.metadata.create_all(engine)

with Session(engine) as session:
    session.add(hero_1)
    session.add(hero_2)
    session.add(hero_3)
    session.commit()



================================================
File: /Bootcamp - Python para dados/aula16/desafio.py
================================================
from sqlmodel import SQLModel, Field, create_engine, Session, select
from typing import Optional

class Livro(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    titulo: str
    autor: str
    ano_publicacao: int
    disponivel: bool = True

# Conexão com o banco de dados (SQLite para simplicidade)
engine = create_engine("sqlite:///biblioteca.db")

# Criação da tabela
SQLModel.metadata.create_all(engine)

def adicionar_livro(livro: Livro):
    with Session(engine) as session:
        session.add(livro)
        session.commit()

def buscar_livros_por_autor(autor: str):
    with Session(engine) as session:
        livros = session.exec(select(Livro).where(Livro.autor == autor)).all()
        return livros

def atualizar_disponibilidade_livro(id_livro: int, disponivel: bool):
    with Session(engine) as session:
        livro = session.get(Livro, id_livro)
        livro.disponivel = disponivel
        session.add(livro)
        session.commit()

def remover_livro(id_livro: int):
    with Session(engine) as session:
        livro = session.get(Livro, id_livro)
        session.delete(livro)
        session.commit()

# Demonstração
if __name__ == "__main__":
    adicionar_livro(Livro(titulo="Dom Casmurro", autor="Machado de Assis", ano_publicacao=1899))
    adicionar_livro(Livro(titulo="O Pequeno Príncipe", autor="Antoine de Saint-Exupéry", ano_publicacao=1943))

    print("Livros de Machado de Assis:", buscar_livros_por_autor("Machado de Assis"))
    atualizar_disponibilidade_livro(1, False)  # Supondo que o ID do "Dom Casmurro" seja 1
    remover_livro(2)  # Supondo que o ID de "O Pequeno Príncipe" seja 2


================================================
File: /Bootcamp - Python para dados/aula16/main.py
================================================
from typing import Optional

from sqlmodel import Field, Session, SQLModel, create_engine

class Hero(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    secret_name: str
    age: Optional[int] = None

engine = create_engine("sqlite:///database.db", echo=True)

SQLModel.metadata.create_all(engine)

hero_1 = Hero(name="Spider-Boy", secret_name="Pedro Parqueador")
hero_2 = Hero(name="Rusty-Man", secret_name="Tommy Sharp", age=48)

with Session(engine) as session:
    session.add(hero_1)
    session.add(hero_2)
    session.commit()

================================================
File: /Bootcamp - Python para dados/aula16/pyproject.toml
================================================
[tool.poetry]
name = "aula-16-ao-vivo"
version = "0.1.0"
description = ""
authors = ["lvgalvaofilho <lvgalvaofilho@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
sqlmodel = "^0.0.16"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /Bootcamp - Python para dados/aula16/read.py
================================================
from typing import Optional

from sqlmodel import Field, Session, SQLModel, create_engine, select


class Hero(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    secret_name: str
    age: Optional[int] = None


engine = create_engine("sqlite:///database.db")

with Session(engine) as session:
    statement = select(Hero).where(Hero.name == "Spider-Boy")
    hero = session.exec(statement).first()
    print(hero)

================================================
File: /Bootcamp - Python para dados/aula16/read_ops.py
================================================
from sqlmodel import SQLModel, create_engine, Session
from sqlalchemy import text


# Supondo que o banco de dados seja SQLite e esteja no arquivo `database.db`
engine = create_engine("sqlite:///database.db")

# Criação da sessão
with Session(engine) as session:
    # Sua consulta SQL
    statement = text("SELECT * FROM hero;")
    
    # Executando a consulta
    results = session.exec(statement)
    
    # Fetch dos resultados
    heroes = results.fetchall()
    
    # Imprimindo os resultados
    for hero in heroes:
        print(hero)


================================================
File: /Bootcamp - Python para dados/aula16/read_ops_2.py
================================================
from sqlmodel import SQLModel, create_engine, Session
from sqlalchemy import text


# Supondo que o banco de dados seja SQLite e esteja no arquivo `database.db`
engine = create_engine("sqlite:///database.db")

# Criação da sessão
with Session(engine) as session:
    # Sua consulta SQL
    statement = text("DROP TABLE hero;")
    
    # Executando a consulta
    results = session.exec(statement)
    
    # Fetch dos resultados
    heroes = results.fetchall()
    
    # Imprimindo os resultados
    for hero in heroes:
        print(hero)


================================================
File: /Bootcamp - Python para dados/aula16/read_sql.sql
================================================
SELECT id, name, secret_name, age
FROM hero
WHERE name = "Spider-Boy"

================================================
File: /Bootcamp - Python para dados/aula16/planejado/exemplo_00_dict.py
================================================
venda = {
    "id": 1,
    "produto": "Notebook Gamer",
    "valor": 5000.00,  # Float positivo
    "quantidade": 2,   # Int positivo
    "data": "2024-03-18",
    "email_comprador": "cliente@example.com"
}

print(f"Venda de {venda['produto']} no valor de {venda['valor']} para {venda['email_comprador']}.")

================================================
File: /Bootcamp - Python para dados/aula16/planejado/exemplo_01_classe.py
================================================
from datetime import date

class Venda:
    def __init__(self, id: int, produto: str, valor: float, quantidade: int, data: date, email_comprador: str):
        self.id: int = id
        self.produto: str = produto
        self.valor: float = valor
        self.quantidade: int = quantidade
        self.data: date = data
        self.email_comprador: str = email_comprador

    def __repr__(self) -> str:
        return f"Venda(id={self.id}, produto={self.produto}, valor={self.valor}, quantidade={self.quantidade}, data={self.data}, email_comprador={self.email_comprador})"

# Exemplo de uso
venda = Venda(1, "Notebook Gamer", 5000.00, 2, date(2024, 3, 18), "cliente@example.com")
print(venda)


================================================
File: /Bootcamp - Python para dados/aula16/planejado/exemplo_02_dataclasse.py
================================================
from dataclasses import dataclass

@dataclass
class VendaDataClass:
    id: int
    produto: str
    valor: float
    quantidade: int
    data: str
    email_comprador: str

# Exemplo de uso
venda_dc = VendaDataClass(1, "Notebook Gamer", 5000.00, 2, "2024-03-18", "cliente@example.com")
print(venda_dc)



================================================
File: /Bootcamp - Python para dados/aula16/planejado/exemplo_03_classe_com_validador.py
================================================
import re
from datetime import date

class VendaValidada:
    def __init__(self, id: int, produto: str, valor: float, quantidade: int, data: date, email_comprador: str):
        if not isinstance(valor, float) or valor <= 0:
            raise ValueError("Valor deve ser um float positivo.")
        if not isinstance(quantidade, int) or quantidade <= 0:
            raise ValueError("Quantidade deve ser um int positivo.")
        if not re.match(r"[^@]+@[^@]+\.[^@]+", email_comprador):
            raise ValueError("E-mail do comprador inválido.")

        self.id: int = id
        self.produto: str = produto
        self.valor: float = valor
        self.quantidade: int = quantidade
        self.data: date = data
        self.email_comprador: str = email_comprador

    def __repr__(self) -> str:
        return f"VendaValidada(id={self.id}, produto={self.produto}, valor={self.valor}, quantidade={self.quantidade}, data={self.data}, email_comprador={self.email_comprador})"

# Exemplo de uso
venda_validada = VendaValidada(1, "Notebook Gamer", 5000.00, 2, date(2024, 3, 18), "cliente@example.com")
print(venda_validada)

try:
    venda_invalidada = VendaValidada(1, "Notebook Gamer", -5000.00, 2, date(2024, 3, 18), "cliente")
    print(venda_invalidada)
except Exception as e:
    print(e)


================================================
File: /Bootcamp - Python para dados/aula16/planejado/exemplo_04_dataclasse_com_validador.py
================================================
from dataclasses import dataclass
import re
from datetime import date

def validar_valor(valor: float) -> float:
    if valor <= 0:
        raise ValueError("Valor deve ser um float positivo.")
    return valor

def validar_quantidade(quantidade: int) -> int:
    if quantidade <= 0:
        raise ValueError("Quantidade deve ser um int positivo.")
    return quantidade

def validar_email(email: str) -> str:
    if not re.match(r"[^@]+@[^@]+\.[^@]+", email):
        raise ValueError("E-mail do comprador inválido.")
    return email

@dataclass
class VendaValidada:
    id: int
    produto: str
    valor: float
    quantidade: int
    data: date
    email_comprador: str

    def __post_init__(self):
        self.valor = validar_valor(self.valor)
        self.quantidade = validar_quantidade(self.quantidade)
        self.email_comprador = validar_email(self.email_comprador)

# Exemplo de uso com try-except
try:
    venda_validada = VendaValidada(id=1, produto="Notebook Gamer", valor=5000.00, quantidade=2, data=date(2024, 3, 18), email_comprador="cliente@example.com")
    print(venda_validada)
except ValueError as e:
    print(e)

try:
    venda_invalida = VendaValidada(id=2, produto="Mouse Sem Fio", valor=-30.00, quantidade=3, data=date(2024, 3, 19), email_comprador="clienteinvali.do")
    print(venda_invalida)
except ValueError as e:
    print(e)


================================================
File: /Bootcamp - Python para dados/aula16/planejado/exemplo_05_pydantic.py
================================================
from pydantic import BaseModel, EmailStr, ValidationError, PositiveInt, PositiveFloat
from datetime import date

class VendaPydantic(BaseModel):
    id: int
    produto: str
    valor: PositiveFloat
    quantidade: PositiveInt
    data: date
    email_comprador: EmailStr

# Exemplo de uso
try:
    venda_pydantic = VendaPydantic(id=1, produto="Notebook Gamer", valor=5000.00, quantidade=2, data=date(2024, 3, 18), email_comprador="cliente@example.com")
    print(venda_pydantic)
except ValidationError as e:
    print(e.json())

# Exemplo de uso
try:
    venda_pydantic = VendaPydantic(id=1, produto="Notebook Gamer", valor=-5000.00, quantidade=2, data=date(2024, 3, 18), email_comprador="cliente@example.com")
    print(venda_pydantic)
except ValidationError as e:
    print(e)



================================================
File: /Bootcamp - Python para dados/aula16/planejado/exemplo_06_orm.py
================================================
from datetime import date
from sqlalchemy import create_engine, Column, Integer, Float, String, Date
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from pydantic import BaseModel, EmailStr
from typing import List

# Definição do modelo SQLAlchemy
Base = declarative_base()

class VendaModel(Base):
    __tablename__ = 'vendas'
    id = Column(Integer, primary_key=True)
    produto = Column(String)
    valor = Column(Float)
    quantidade = Column(Integer)
    data = Column(Date)
    email_comprador = Column(String)

# Configuração do banco de dados SQLAlchemy
engine = create_engine('sqlite:///vendas.db', echo=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base.metadata.create_all(bind=engine)

# Modelos Pydantic
class VendaBase(BaseModel):
    produto: str
    valor: int  # Garante que o valor seja maior que 0
    quantidade: int
    data: date
    email_comprador: EmailStr

class VendaCreate(VendaBase):
    pass

class Venda(VendaBase):
    id: int

    class Config:
        from_attributes=True

# Funções para interagir com o banco de dados
def create_venda(db: Session, venda: VendaCreate) -> VendaModel:
    db_venda = VendaModel(**venda.model_dump())
    db.add(db_venda)
    db.commit()
    db.refresh(db_venda)
    return db_venda

def get_vendas(db: Session, skip: int = 0, limit: int = 100) -> List[VendaModel]:
    return db.query(VendaModel).offset(skip).limit(limit).all()

# Uso dos modelos e interação com o banco de dados
if __name__ == "__main__":
    db = SessionLocal()

    # Criando uma nova venda
    # venda_data = {
    #     "produto": "Notebook Ultra",
    #     "valor": 4500.00,
    #     "quantidade": 1,
    #     "data": date.today(),
    #     "email_comprador": "comprador@example.com"
    # }
    # nova_venda = VendaCreate(**venda_data)
    # venda_criada = create_venda(db=db, venda=nova_venda)
    # print(f"Venda criada: {venda_criada}")

    # Recuperando vendas
    vendas = get_vendas(db=db)
    print("Vendas recuperadas:")
    for venda_model in vendas:
        # Deserializando para o modelo Pydantic
        venda_pydantic = Venda.from_orm(venda_model)
        print(venda_pydantic)

================================================
File: /Bootcamp - Python para dados/aula16/planejado/exemplo_07_sql_model.py
================================================
from datetime import date
from sqlalchemy import create_engine, Column, Integer, Float, String, Date
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from pydantic import BaseModel, EmailStr
from typing import List

# Definição do modelo SQLAlchemy
Base = declarative_base()

class VendaModel(Base):
    __tablename__ = 'vendas'
    id = Column(Integer, primary_key=True)
    produto = Column(String)
    valor = Column(Float)
    quantidade = Column(Integer)
    data = Column(Date)
    email_comprador = Column(String)

# Configuração do banco de dados SQLAlchemy
engine = create_engine('sqlite:///vendas.db', echo=True)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base.metadata.create_all(bind=engine)

# Modelos Pydantic
class VendaBase(BaseModel):
    produto: str
    valor: int  # Garante que o valor seja maior que 0
    quantidade: int
    data: date
    email_comprador: EmailStr

class VendaCreate(VendaBase):
    pass

class Venda(VendaBase):
    id: int

    class Config:
        from_attributes=True

# Funções para interagir com o banco de dados
def create_venda(db: Session, venda: VendaCreate) -> VendaModel:
    db_venda = VendaModel(**venda.model_dump())
    db.add(db_venda)
    db.commit()
    db.refresh(db_venda)
    return db_venda

def get_vendas(db: Session, skip: int = 0, limit: int = 100) -> List[VendaModel]:
    return db.query(VendaModel).offset(skip).limit(limit).all()

# Uso dos modelos e interação com o banco de dados
if __name__ == "__main__":
    db = SessionLocal()

    # Criando uma nova venda
    # venda_data = {
    #     "produto": "Notebook Ultra",
    #     "valor": 4500.00,
    #     "quantidade": 1,
    #     "data": date.today(),
    #     "email_comprador": "comprador@example.com"
    # }
    # nova_venda = VendaCreate(**venda_data)
    # venda_criada = create_venda(db=db, venda=nova_venda)
    # print(f"Venda criada: {venda_criada}")

    # Recuperando vendas
    vendas = get_vendas(db=db)
    print("Vendas recuperadas:")
    for venda_model in vendas:
        # Deserializando para o modelo Pydantic
        venda_pydantic = Venda.from_orm(venda_model)
        print(venda_pydantic)

================================================
File: /Bootcamp - Python para dados/aula16/planejado/exemplo_classe_validador.py
================================================
from datetime import datetime
import re

class Venda:
    def __init__(self, id, produto, valor, quantidade, data, email_do_comprador):
        self.id = id
        self.produto = produto
        self.valor = self.validar_valor(valor)
        self.quantidade = self.validar_quantidade(quantidade)
        self.data = data
        self.email_do_comprador = self.validar_email(email_do_comprador)

    def __repr__(self):
        return f"Venda(id={self.id}, produto={self.produto}, valor={self.valor}, quantidade={self.quantidade}, data={self.data}, email_do_comprador={self.email_do_comprador})"

    def validar_valor(self, valor):
        if valor <= 0:
            raise ValueError("O valor deve ser positivo.")
        return valor

    def validar_quantidade(self, quantidade):
        if quantidade <= 0:
            raise ValueError("A quantidade deve ser positiva.")
        return quantidade

    def validar_email(self, email):
        if not re.match(r"[^@]+@[^@]+\.[^@]+", email):
            raise ValueError("Email inválido.")
        return email


================================================
File: /Bootcamp - Python para dados/aula17/README.md
================================================
# Aula 17: SQLAlchemy - Conjunto de ferramentas para manipular SQL em 

![imagem_01](./pics/1.jpg)

Bem-vindo à décima sétima aula do bootcamp!

Mapeamento Objeto-Relacional (ORM) é uma técnica que permite consultar e manipular dados de um banco de dados usando um paradigma orientado a objetos. Ao falar sobre ORM, a maioria das pessoas está se referindo a uma biblioteca que implementa a técnica de Mapeamento Objeto-Relacional, daí a frase "um ORM".

[Excalidraw:](https://link.excalidraw.com/l/8pvW6zbNUnD/3tmGeQYjxeG)

## Introdução ao SQL Alchemy

Uma biblioteca ORM é uma biblioteca completamente comum escrita na linguagem de sua escolha que encapsula o código necessário para manipular os dados, então você não usa mais SQL; você interage diretamente com um objeto na mesma linguagem que está usando.

### Por que devemos usar ORM?

- DRY: Você escreve seu modelo de dados em apenas um lugar, e é mais fácil atualizar, manter e reutilizar o código.

- Você não precisa escrever seu SQL zoado (a maioria dos programadores não são bons nisso, porque SQL é tratado como uma "sub" linguagem, quando na realidade é uma linguagem muito poderosa e complexa).

- Sanitização; usar declarações preparadas ou transações é tão fácil quanto chamar um método.

- Ela se encaixa na sua maneira natural no seu código Python.

- Ela abstrai o sistema de BD, então você pode mudá-lo sempre que quiser.

![imagem_02](./pics/2.jpg)

### Instalação

Primeiro, certifique-se de que o SQLAlchemy esteja instalado. Se não estiver, você pode instalá-lo usando pip:

```bash
pip install sqlalchemy
```

### Conectando ao SQLite (Hello world!)

SQLite é um banco de dados leve que é ótimo para aprender os fundamentos do SQLAlchemy. Aqui está um exemplo básico de como criar uma engine de conexão com um banco de dados SQLite em memória:

```python
from sqlalchemy import create_engine

# Conectar ao SQLite em memória
engine = create_engine('sqlite:///meubanco.db', echo=True)

print("Conexão com SQLite estabelecida.")
```

![engine](./pics/engine.png)

[Atende diferentes "Dialect"](https://docs.sqlalchemy.org/en/20/core/engines.html)  

### Criando nosso MAPPING

![engine](./pics/mapping.png)


Antes de inserir ou consultar dados, precisamos definir os modelos e criar tabelas correspondentes no banco de dados:

```python
from sqlalchemy.orm import declarative_base
from sqlalchemy import Column, Integer, String

Base = declarative_base()

class Usuario(Base):
    __tablename__ = 'usuarios'
    
    id = Column(Integer, primary_key=True)
    nome = Column(String)
    idade = Column(Integer)

# Criar as tabelas no banco de dados
Base.metadata.create_all(engine)
```

### Criando Sessões e Inserindo Dados

As sessões no SQLAlchemy são usadas para manter um 'workspace' de todas as operações de objetos que você deseja sincronizar com o banco de dados:

```python
from sqlalchemy.orm import sessionmaker

Session = sessionmaker(bind=engine)
session = Session()

novo_usuario = Usuario(nome='João', idade=28)
session.add(novo_usuario)
session.commit()

print("Usuário inserido com sucesso.")
```

### Consultando Dados

Agora, vamos consultar os dados para verificar a inserção:

```python
usuario = session.query(Usuario).filter_by(nome='João').first()
print(f"Usuário encontrado: {usuario.nome}, Idade: {usuario.idade}")
```

### Utilizando Session com o With

O gerenciador de contexto `with` em Python, especialmente quando usado com SQLAlchemy, é uma maneira elegante e segura de garantir que os recursos, como conexões de banco de dados e sessões, sejam apropriadamente gerenciados. Ao usar o `with`, você se beneficia da entrada e saída automática de contextos, o que significa que ao final do bloco `with`, o SQLAlchemy automaticamente fecha a sessão ou faz o commit/rollback, dependendo do resultado da operação. Isso ajuda a prevenir vazamentos de conexão e garante que as transações sejam devidamente gerenciadas.

### Vantagens do Uso do `with` com SQLAlchemy

* **Gerenciamento automático de transações**: As transações são automaticamente commitadas ou revertidas dependendo se exceções foram lançadas dentro do bloco.
* **Fechamento automático de sessões**: Isso garante que os recursos sejam liberados de maneira oportuna, evitando vazamentos de conexão.

### Exemplo Sem Usar `with`

Sem utilizar o gerenciador de contexto, você precisa manualmente gerenciar a sessão, incluindo commits, rollbacks e o fechamento da sessão:

```python
from sqlalchemy.orm import sessionmaker
# assumindo que engine já foi criado

Session = sessionmaker(bind=engine)
session = Session()

try:
    novo_usuario = Usuario(nome='Ana', idade=25)
    session.add(novo_usuario)
    session.commit()
except:
    session.rollback()
    raise
finally:
    session.close()
```

Neste exemplo, todos os passos para garantir que a sessão seja devidamente gerida são explícitos: comitar as alterações, lidar com exceções, fazer rollback se algo der errado, e, por fim, fechar a sessão.

### Exemplo Usando `with`

Quando você utiliza o gerenciador de contexto `with`, muitas dessas etapas são automatizadas:

```python
from sqlalchemy.orm import sessionmaker, Session
# assumindo que engine já foi criado

Session = sessionmaker(bind=engine)

with Session() as session:
    novo_usuario = Usuario(nome='Ana', idade=25)
    session.add(novo_usuario)
    # O commit é feito automaticamente aqui, se não houver exceções
    # O rollback é automaticamente chamado se uma exceção ocorrer
    # A sessão é fechada automaticamente ao sair do bloco with
```

No exemplo acima, o SQLAlchemy lida com o commit, rollback e fechamento da sessão automaticamente. Se uma exceção ocorrer dentro do bloco `with`, um rollback é chamado. Quando o bloco `with` é concluído sem erros, o commit é realizado, e em ambos os casos, a sessão é fechada automaticamente no final.

### Conclusão

A principal vantagem de usar o gerenciador de contexto `with` com SQLAlchemy (ou qualquer outro recurso que necessite de gerenciamento de estado e liberação de recursos) é reduzir a verbosidade do código e minimizar a chance de erros, como esquecer de fechar uma sessão ou fazer rollback de uma transação falha. Ele promove um código mais limpo, seguro e legível.

### Desafio

![imagem_03](./pics/3.jpg)

### Desafio Intermediário de SQLAlchemy: Tabelas de Produto e Fornecedor

Este desafio focará na criação de duas tabelas relacionadas, `Produto` e `Fornecedor`, utilizando SQLAlchemy. Cada produto terá um fornecedor associado, demonstrando o uso de chaves estrangeiras para estabelecer relações entre tabelas. Além disso, você realizará inserções nessas tabelas para praticar a manipulação de dados.

#### Passo 1: Configuração Inicial

Primeiro, certifique-se de ter o SQLAlchemy instalado. Se não, instale-o usando pip:

```bash
pip install sqlalchemy
```

#### Passo 2: Definição dos Modelos

```python
from sqlalchemy import create_engine, Column, Integer, String, ForeignKey
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, sessionmaker

Base = declarative_base()

class Fornecedor(Base):
    __tablename__ = 'fornecedores'
    id = Column(Integer, primary_key=True)
    nome = Column(String(50), nullable=False)
    telefone = Column(String(20))
    email = Column(String(50))
    endereco = Column(String(100))

class Produto(Base):
    __tablename__ = 'produtos'
    id = Column(Integer, primary_key=True)
    nome = Column(String(50), nullable=False)
    descricao = Column(String(200))
    preco = Column(Integer)
    fornecedor_id = Column(Integer, ForeignKey('fornecedores.id'))
    
    # Estabelece a relação entre Produto e Fornecedor
    fornecedor = relationship("Fornecedor")
```

#### Passo 3: Criando o Banco de Dados e as Tabelas

```python
engine = create_engine('sqlite:///:memory:', echo=True)
Base.metadata.create_all(engine)

Session = sessionmaker(bind=engine)
session = Session()
```

#### Passo 4: Inserções nas Tabelas

Primeiro, vamos inserir alguns fornecedores:

```python
fornecedores = [
    Fornecedor(nome="Fornecedor A", telefone="12345678", email="contato@a.com", endereco="Endereço A"),
    Fornecedor(nome="Fornecedor B", telefone="87654321", email="contato@b.com", endereco="Endereço B"),
    Fornecedor(nome="Fornecedor C", telefone="12348765", email="contato@c.com", endereco="Endereço C"),
    Fornecedor(nome="Fornecedor D", telefone="56781234", email="contato@d.com", endereco="Endereço D"),
    Fornecedor(nome="Fornecedor E", telefone="43217865", email="contato@e.com", endereco="Endereço E")
]

session.add_all(fornecedores)
session.commit()
```

Em seguida, inserimos alguns produtos, cada um vinculado a um fornecedor:

```python
produtos = [
    Produto(nome="Produto 1", descricao="Descrição do Produto 1", preco=100, fornecedor_id=1),
    Produto(nome="Produto 2", descricao="Descrição do Produto 2", preco=200, fornecedor_id=2),
    Produto(nome="Produto 3", descricao="Descrição do Produto 3", preco=300, fornecedor_id=3),
    Produto(nome="Produto 4", descricao="Descrição do Produto 4", preco=400, fornecedor_id=4),
    Produto(nome="Produto 5", descricao="Descrição do Produto 5", preco=500, fornecedor_id=5)
]

session.add_all(produtos)
session.commit()
```

#### Passo 5: Consulta dos Dados

Para verificar se tudo correu como esperado, você pode fazer uma consulta simples para listar todos os produtos e seus fornecedores:

```python
for produto in session.query(Produto).all():
    print(f"Produto: {produto.nome}, Fornecedor: {produto.fornecedor.nome}")
```

Este desafio cobre conceitos intermediários como a criação de tabelas relacionadas, inserção de dados com chaves estrangeiras e consultas básicas no SQLAlchemy. Ele oferece uma boa prática para quem está aprendendo a manipular relações entre tabelas em um contexto de banco de dados relacional usando ORM.

Vamos criar um exemplo prático que demonstra a utilização de `JOIN` e `GROUP BY` tanto em SQL puro quanto usando o SQLAlchemy (ORM). O objetivo é obter a soma dos preços dos produtos agrupados por fornecedor.

### Cenário

Temos duas tabelas: `fornecedores` e `produtos`. Cada produto tem um `fornecedor_id` que o vincula a um fornecedor específico na tabela `fornecedores`.

### SQL Puro

Para realizar essa operação em SQL puro, você pode usar a seguinte query:

```sql
SELECT fornecedores.nome, SUM(produtos.preco) AS total_preco
FROM produtos
JOIN fornecedores ON produtos.fornecedor_id = fornecedores.id
GROUP BY fornecedores.nome;
```

Esta query junta as tabelas `produtos` e `fornecedores` através do `fornecedor_id`, agrupa os resultados pelo nome do fornecedor e, para cada grupo, calcula a soma dos preços dos produtos associados a esse fornecedor.

### SQLAlchemy (ORM)

Para realizar a mesma operação usando SQLAlchemy, você seguiria estes passos:

```python
from sqlalchemy import func
from sqlalchemy.orm import sessionmaker
# Supondo que engine já foi definido anteriormente e os modelos Produto e Fornecedor foram definidos conforme o exemplo anterior.

Session = sessionmaker(bind=engine)
session = Session()

resultado = session.query(
    Fornecedor.nome,
    func.sum(Produto.preco).label('total_preco')
).join(Produto, Fornecedor.id == Produto.fornecedor_id
).group_by(Fornecedor.nome).all()

for nome, total_preco in resultado:
    print(f"Fornecedor: {nome}, Total Preço: {total_preco}")
```

No exemplo acima com SQLAlchemy, utilizamos o método `query()` para construir uma consulta que seleciona o nome do fornecedor e a soma dos preços dos produtos. Usamos `join()` para juntar as tabelas `Produto` e `Fornecedor` baseadas na chave estrangeira. `group_by()` é utilizado para agrupar os resultados pelo nome do fornecedor, e `func.sum()` calcula a soma dos preços dos produtos para cada grupo.

### Conclusão

Ambos os métodos, SQL puro e SQLAlchemy, alcançam o mesmo resultado: agrupam os produtos por fornecedor e calculam a soma dos preços dos produtos para cada fornecedor. A principal diferença está na abordagem: enquanto o SQL puro é mais direto e requer que você escreva a query explicitamente, o SQLAlchemy abstrai a construção da query, permitindo que você utilize métodos Python e relações entre modelos para definir a consulta. A escolha entre um ou outro dependerá das suas necessidades específicas, preferências de desenvolvimento e o contexto do seu projeto.

================================================
File: /Bootcamp - Python para dados/aula17/desafio.py
================================================
from sqlalchemy import create_engine, Column, Integer, String, ForeignKey
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import relationship, sessionmaker
from sqlalchemy.exc import SQLAlchemyError  # Importa exceções do SQLAlchemy

Base = declarative_base()

class Fornecedor(Base):
    __tablename__ = 'fornecedores'
    id = Column(Integer, primary_key=True)
    nome = Column(String(50), nullable=False)
    telefone = Column(String(20))
    email = Column(String(50))
    endereco = Column(String(100))

class Produto(Base):
    __tablename__ = 'produtos'
    id = Column(Integer, primary_key=True)
    nome = Column(String(50), nullable=False)
    descricao = Column(String(200))
    preco = Column(Integer)
    fornecedor_id = Column(Integer, ForeignKey('fornecedores.id'))
    fornecedor = relationship("Fornecedor")  # Relação entre Produto e Fornecedor

engine = create_engine('sqlite:///desafio.db', echo=True)
Base.metadata.create_all(engine)

Session = sessionmaker(bind=engine)

# Inserindo fornecedores
try:
    with Session() as session:  # Usando a sessão corretamente com o gerenciador de contexto
        fornecedores = [
            Fornecedor(nome="Fornecedor A", telefone="12345678", email="contato@a.com", endereco="Endereço A"),
            Fornecedor(nome="Fornecedor B", telefone="87654321", email="contato@b.com", endereco="Endereço B"),
            Fornecedor(nome="Fornecedor C", telefone="12348765", email="contato@c.com", endereco="Endereço C"),
            Fornecedor(nome="Fornecedor D", telefone="56781234", email="contato@d.com", endereco="Endereço D"),
            Fornecedor(nome="Fornecedor E", telefone="43217865", email="contato@e.com", endereco="Endereço E")
        ]
        session.add_all(fornecedores)
        session.commit()
except SQLAlchemyError as e:  # Capturando exceções do SQLAlchemy
    print(f"Erro ao inserir fornecedores: {e}")

# Inserindo produtos
try:
    with Session() as session:  # Corrigindo a utilização da sessão
        produtos = [
            Produto(nome="Produto 1", descricao="Descrição do Produto 1", preco=100, fornecedor_id=1),
            Produto(nome="Produto 2", descricao="Descrição do Produto 2", preco=200, fornecedor_id=2),
            Produto(nome="Produto 3", descricao="Descrição do Produto 3", preco=300, fornecedor_id=3),
            Produto(nome="Produto 4", descricao="Descrição do Produto 4", preco=400, fornecedor_id=4),
            Produto(nome="Produto 5", descricao="Descrição do Produto 5", preco=500, fornecedor_id=5)
        ]
        session.add_all(produtos)
        session.commit()
except SQLAlchemyError as e:
    print(f"Erro ao inserir produtos: {e}")

from sqlalchemy import func
from sqlalchemy.orm import sessionmaker
# Supondo que engine já foi definido anteriormente e os modelos Produto e Fornecedor foram definidos conforme o exemplo anterior.

Session = sessionmaker(bind=engine)
session = Session()

resultado = session.query(
    Fornecedor.nome,
    func.sum(Produto.preco).label('total_preco')
).join(Produto, Fornecedor.id == Produto.fornecedor_id
).group_by(Fornecedor.nome).all()

for nome, total_preco in resultado:
    print(f"Fornecedor: {nome}, Total Preço: {total_preco}")


================================================
File: /Bootcamp - Python para dados/aula17/desafio_query.py
================================================
from sqlalchemy import func
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine, Column, Integer, String, ForeignKey
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import relationship, sessionmaker
from sqlalchemy.exc import SQLAlchemyError  # Importa exceções do SQLAlchemy

# Supondo que engine já foi definido anteriormente e os modelos Produto e Fornecedor foram definidos conforme o exemplo anterior.

Base = declarative_base()

class Fornecedor(Base):
    __tablename__ = 'fornecedores'
    id = Column(Integer, primary_key=True)
    nome = Column(String(50), nullable=False)
    telefone = Column(String(20))
    email = Column(String(50))
    endereco = Column(String(100))

class Produto(Base):
    __tablename__ = 'produtos'
    id = Column(Integer, primary_key=True)
    nome = Column(String(50), nullable=False)
    descricao = Column(String(200))
    preco = Column(Integer)
    fornecedor_id = Column(Integer, ForeignKey('fornecedores.id'))
    fornecedor = relationship("Fornecedor")  # Relação entre Produto e Fornecedor

engine = create_engine('sqlite:///desafio.db', echo=True)
Base.metadata.create_all(engine)

Session = sessionmaker(bind=engine)
session = Session()

resultado = session.query(
    Fornecedor.nome,
    func.sum(Produto.preco).label('total_preco')
).join(Produto, Fornecedor.id == Produto.fornecedor_id
).group_by(Fornecedor.nome).all()

for nome, total_preco in resultado:
    print(f"Fornecedor: {nome}, Total Preço: {total_preco}")

================================================
File: /Bootcamp - Python para dados/aula17/exercicio_01.py
================================================
from sqlalchemy import create_engine

# Conectar ao SQLite em memória
engine = create_engine('sqlite:///meubanco.db', echo=True)

## dialetos
## engine = create_engine("postgresql+psycopg2://scott:tiger@localhost:5432/mydatabase")


print("Conexão com SQLite estabelecida.")

================================================
File: /Bootcamp - Python para dados/aula17/exercicio_02.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from sqlalchemy import Column, Integer, String


# Conectar ao SQLite em memória
engine = create_engine('sqlite:///meubanco.db', echo=True)

print("Conexão com SQLite estabelecida.")

Base = declarative_base()

class Usuario(Base):
    __tablename__ = 'usuarios'
    
    id = Column(Integer, primary_key=True)
    nome = Column(String, nullable=False)
    idade = Column(Integer, nullable=False)

Base.metadata.create_all(engine)

print("Tabela Criada com SQLite estabelecida.")

from sqlalchemy.orm import sessionmaker

Session = sessionmaker(bind=engine)
session = Session()

novo_usuario = Usuario(nome='João', idade=28)
session.add(novo_usuario)
session.commit()

print("Usuário inserido com sucesso.")

usuario = session.query(Usuario).filter_by(nome='João').first()
print(f"Usuário encontrado: {usuario.nome}, Idade: {usuario.idade}")

================================================
File: /Bootcamp - Python para dados/aula17/exercicio_03.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from sqlalchemy import Column, Integer, String


# Conectar ao SQLite em memória
engine = create_engine('sqlite:///meubanco.db', echo=True)

print("Conexão com SQLite estabelecida.")

Base = declarative_base()

class Usuario(Base):
    __tablename__ = 'usuarios'
    
    id = Column(Integer, primary_key=True)
    nome = Column(String, nullable=False)
    idade = Column(Integer, nullable=False)

Base.metadata.create_all(engine)

print("Tabela Criada com SQLite estabelecida.")

from sqlalchemy.orm import sessionmaker

Session = sessionmaker(bind=engine)
session = Session()

print("Usuário inserido com sucesso.")

usuarios = session.query(Usuario).all()

for usuario in usuarios:
    print(f"ID: {usuario.id}, Nome: {usuario.nome}")

================================================
File: /Bootcamp - Python para dados/aula17/exercicio_04.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from sqlalchemy import Column, Integer, String


# Conectar ao SQLite em memória
engine = create_engine('sqlite:///meubanco.db', echo=True)

print("Conexão com SQLite estabelecida.")

Base = declarative_base()

class Usuario(Base):
    __tablename__ = 'usuarios'
    
    id = Column(Integer, primary_key=True)
    nome = Column(String, nullable=True)
    idade = Column(Integer, nullable=True)

Base.metadata.create_all(engine)

print("Tabela Criada com SQLite estabelecida.")

from sqlalchemy.orm import sessionmaker

from sqlalchemy.orm import sessionmaker
# assumindo que engine já foi criado

Session = sessionmaker(bind=engine)
session = Session()

try:
    with Session() as session:
        novo_usuario = Usuario(cadeira=40)
        session.add(novo_usuario)
        # O commit é feito automaticamente aqui, se não houver exceções
        # O rollback é automaticamente chamado se uma exceção ocorrer
        # A sessão é fechada automaticamente ao sair do bloco with
except TypeError as e:
    print(e)

================================================
File: /Bootcamp - Python para dados/aula17/pyproject.toml
================================================
[tool.poetry]
name = "sqlachemyexemplo"
version = "0.1.0"
description = ""
authors = ["luciano"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
sqlalchemy = "^2.0.28"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /Bootcamp - Python para dados/aula18/README.md
================================================
# Aula 18: API - Conjunto de ferramentas para manipular SQL em 

![imagem_01](./pics/1.jpg)

Bem-vindo à décima oitava aula do bootcamp!

# O que é uma API?

Uma API (Application Programming  Interface) é um conjunto de rotinas e padrões (contratos) estabelecidos por uma aplicação, para que outras aplicações possam utilizar as funcionalidades dessa aplicação.

![Imagem](assets/server-server.png)

# Por que usar uma API?

Nos últimos anos, a Internet se transformou de uma rede de servidores web que serviam principalmente páginas estáticas para navegadores de internet...

![Internet](https://thefloppydisk.files.wordpress.com/2013/05/web10.png?w=1248)

...em uma arquitetura cliente-servidor, onde aplicativos web e mobile se comunicam com diferentes aplicações, cada vez mais por meio de APIs RESTful simples, mas poderosas.

![Imagem](https://thefloppydisk.files.wordpress.com/2013/05/web20.png?w=1245)

# As regras do jogo

Basicamente uma API é um contrato que define como uma aplicação vai se comunicar com a outra. Como os dados serão enviados e recebidos.

![Contrato](pics/contract.png)

# O que é uma API REST?

REST é um acrônimo para REpresentational STATE Transfer, que é um estilo de arquitetura para sistemas distribuídos.

![Rest](pics/apirest.png)

# Como se comunicar com ela?

- Nosso protocolo (ex: https)

- Nosso servidor tem um endereço (ex: pokeapi.co)

- Nosso servidor tem uma porta (ex: 8080 para http e 443 para https)

- E precisamos acessar um recurso ou como constumamos chamar, endpoint ou rota (ex: /api/character)

``` 
https://pokeapi.co/api/v2/pokemon/15
```

# Nossos verbos

O protocolo HTTP é a base usada por trás das APIs REST e as "requisita" utilizando diversos "tipos". Os mais comuns são:

## O que é o CRUD? 

Create, Read, Update e Delete

- POST: (Create) Criar um recurso
- GET: (Read) Obter um recurso
- PUT: (Update) Atualizar um recurso
- DELETE: Remover um recurso

# Qual a diferença entre REST e RESTful?

REST é um estilo de arquitetura para sistemas distribuídos, enquanto RESTful é a implementação desse estilo.

# Vamos para a prática?

Vamos usar o VScode e o terminal para conectar e salvar os dados de uma API em um banco de dados.

================================================
File: /Bootcamp - Python para dados/aula18/exemplo_00.py
================================================
import requests

response = requests.get(f"https://pokeapi.co/api/v2/pokemon/15")
data = response.json()
data_types = data['types']  # Supondo que 'data' é o dicionário com os dados do Pokémon
types_list = []
for type_info in data_types:
    types_list.append(type_info['type']['name'])
types = ', '.join(types_list)
print(data['name'], types)

lista_exemplo = ["luciano", "fabio", "bruno"]
lista_unica = ', '.join(lista_exemplo)
print(lista_unica)

================================================
File: /Bootcamp - Python para dados/aula18/exemplo_01.py
================================================
import requests

from pydantic import BaseModel

class PokemonSchema(BaseModel):
    name: int
    type: str

    class Config:
        orm_mode = True

def pegar_pokemon(id: int) -> PokemonSchema:
    response = requests.get(f"https://pokeapi.co/api/v2/pokemon/{id}")
    data = response.json()
    print(data)
    data_types = data['types']  # Supondo que 'data' é o dicionário com os dados do Pokémon
    types_list = []
    for type_info in data_types:
        types_list.append(type_info['type']['name'])
    types = ', '.join(types_list)
    return PokemonSchema(name=data['name'], type=types)

from pydantic import BaseModel

pokemon = pegar_pokemon(24)
print(pokemon)

================================================
File: /Bootcamp - Python para dados/aula18/exemplo_02_json.py
================================================
import requests
import json

from pydantic import BaseModel

class PokemonSchema(BaseModel):
    name: int
    type: str

    class Config:
        orm_mode = True

def pegar_pokemon(id: int) -> PokemonSchema:
    response = requests.get(f"https://pokeapi.co/api/v2/pokemon/{id}")
    data = response.json()
    with open(f"{data['name']}.json", 'w') as f:
            json.dump(data, f)
    data_types = data['types']  # Supondo que 'data' é o dicionário com os dados do Pokémon
    types_list = []
    for type_info in data_types:
        types_list.append(type_info['type']['name'])
    types = ', '.join(types_list)
    return PokemonSchema(name=data['name'], type=types)

from pydantic import BaseModel

pokemon = pegar_pokemon(24)
print(pokemon)

================================================
File: /Bootcamp - Python para dados/aula18/json_exemplo.py
================================================
json_pokemon = {
    
}

================================================
File: /Bootcamp - Python para dados/aula18/pyproject.toml
================================================
[tool.poetry]
name = "aula-18"
version = "0.1.0"
description = ""
authors = ["luciano"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
requests = "^2.31.0"
sqlalchemy = "^2.0.28"
pydantic = "^2.6.4"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


================================================
File: /Bootcamp - Python para dados/aula18/src/controller.py
================================================
import requests
from db import SessionLocal, engine, Base
from models import Pokemon
from schema import PokemonSchema

Base.metadata.create_all(bind=engine)

def fetch_pokemon_data(pokemon_id: int):
    response = requests.get(f"https://pokeapi.co/api/v2/pokemon/{pokemon_id}")
    print(response)
    if response.status_code == 200:
        data = response.json()
        types = ', '.join(type['type']['name'] for type in data['types'])
        return PokemonSchema(name=data['name'], type=types)
    else:
        return None

def add_pokemon_to_db(pokemon_schema: PokemonSchema) -> Pokemon:
    with SessionLocal() as db:
        db_pokemon = Pokemon(name=pokemon_schema.name, type=pokemon_schema.type)
        db.add(db_pokemon)
        db.commit()
        db.refresh(db_pokemon)
    return db_pokemon



================================================
File: /Bootcamp - Python para dados/aula18/src/db.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

SQLALCHEMY_DATABASE_URL = "sqlite:///./pokemon.db"
engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()


================================================
File: /Bootcamp - Python para dados/aula18/src/main.py
================================================
import time
import random
from controller import fetch_pokemon_data, add_pokemon_to_db

def main():
    while True:
        pokemon_id = random.randint(1, 350)  # Gera um ID aleatório entre 1 e 350
        pokemon_schema = fetch_pokemon_data(pokemon_id)
        if pokemon_schema:
            print(f"Adicionando {pokemon_schema.name} ao banco de dados.")
            add_pokemon_to_db(pokemon_schema)
        else:
            print(f"Não foi possível obter dados para o Pokémon com ID {pokemon_id}.")
        time.sleep(10)

if __name__ == "__main__":
    main()

================================================
File: /Bootcamp - Python para dados/aula18/src/models.py
================================================
from sqlalchemy import Column, Integer, String, DateTime
from sqlalchemy.sql import func
from db import Base

class Pokemon(Base):
    __tablename__ = 'pokemons'
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String)
    type = Column(String)
    created_at = Column(DateTime, default=func.now())  # Campo adicionado


================================================
File: /Bootcamp - Python para dados/aula18/src/schema.py
================================================
from pydantic import BaseModel

class PokemonSchema(BaseModel):
    name: str
    type: str

    class Config:
        orm_mode = True


================================================
File: /Bootcamp - Python para dados/aula19/README.md
================================================
# Aula 19: Fazendo nossa API

## O que é FastAPI?

FastAPI é uma estrutura (framework) web de alto desempenho para construir APIs com Python 3.6+ baseada em tipos de dados declarativos (graças ao Pydantic) e no padrão ASGI (Asynchronous Server Gateway Interface). Ele é projetado para ser fácil de usar, rápido para aprender e altamente eficiente em termos de desempenho, oferecendo suporte nativo a tipos de dados Python, tipagem de dados, validação automática de entrada e documentação interativa automática (gerada automaticamente pelo Swagger UI e ReDoc).

Principais características:

1. **Rápido**: Utiliza Python assíncrono e técnicas de otimização para alto desempenho.
2. **Fácil de usar**: Possui uma sintaxe declarativa e intuitiva, permitindo uma rápida prototipação.
3. **Tipagem de dados**: Utiliza a tipagem de dados Python para garantir a segurança e a consistência dos dados.
4. **Documentação automática**: Gera automaticamente documentação interativa para sua API.
5. **Suporte a OpenAPI e Swagger**: Total compatibilidade com esses padrões, permitindo integração com outras ferramentas.

## Exemplos de uso do FastAPI:

### 1. Criando uma API básica:

```python
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get("/")
def read_root():
    return {"Hello": "World"}


@app.get("/items/{item_id}")
def read_item(item_id: int, q: Union[str, None] = None):
    return {"item_id": item_id, "q": q}
```

### 2. Definindo modelos de dados com Pydantic:

```python
from typing import Union

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    name: str
    price: float
    is_offer: Union[bool, None] = None


@app.get("/")
def read_root():
    return {"Hello": "World"}


@app.get("/items/{item_id}")
def read_item(item_id: int, q: Union[str, None] = None):
    return {"item_id": item_id, "q": q}


@app.put("/items/{item_id}")
def update_item(item_id: int, item: Item):
    return {"item_name": item.name, "item_id": item_id}
```

### Desafio

Criar nosso primeiro CRUD

1. **`POST /items/`: Cria um novo item**
    
    Esta rota permite criar um novo item no banco de dados. O cliente envia os dados do novo item no corpo da solicitação HTTP e o servidor adiciona esse item ao banco de dados. Aqui está como funciona:
    
    * **Verbo HTTP**: POST
    * **Endpoint**: `/items/`
    * **Ação**: Cria um novo item no banco de dados.
    * **Requisitos**: O corpo da solicitação deve conter os dados do novo item.
    * **Resposta**: Retorna o novo item criado.
2. **`GET /items/`: Retorna uma lista paginada de itens**
    
    Esta rota permite recuperar uma lista paginada de itens do banco de dados. O cliente pode especificar opcionalmente os parâmetros `skip` (quantos itens pular) e `limit` (quantos itens retornar) para paginação. Aqui está como funciona:
    
    * **Verbo HTTP**: GET
    * **Endpoint**: `/items/`
    * **Ação**: Retorna uma lista paginada de itens do banco de dados.
    * **Parâmetros de consulta**: `skip` (opcional, padrão = 0) e `limit` (opcional, padrão = 10).
    * **Resposta**: Retorna uma lista de itens conforme especificado pelos parâmetros de consulta.
3. **`GET /items/{item_id}`: Retorna um item específico com base no ID**
    
    Esta rota permite recuperar um item específico do banco de dados com base no ID fornecido. Aqui está como funciona:
    
    * **Verbo HTTP**: GET
    * **Endpoint**: `/items/{item_id}`
    * **Ação**: Retorna um item específico com base no ID fornecido.
    * **Parâmetros de caminho**: `item_id` (ID do item a ser recuperado).
    * **Resposta**: Retorna o item correspondente ao ID fornecido.
4. **`PUT /items/{item_id}`: Atualiza um item existente com base no ID**
    
    Esta rota permite atualizar os dados de um item existente no banco de dados com base no ID fornecido. O cliente envia os novos dados do item no corpo da solicitação HTTP. Aqui está como funciona:
    
    * **Verbo HTTP**: PUT
    * **Endpoint**: `/items/{item_id}`
    * **Ação**: Atualiza um item existente com base no ID fornecido.
    * **Parâmetros de caminho**: `item_id` (ID do item a ser atualizado).
    * **Requisitos**: O corpo da solicitação deve conter os novos dados do item.
    * **Resposta**: Retorna o item atualizado.
5. **`DELETE /items/{item_id}`: Exclui um item existente com base no ID**
    
    Esta rota permite excluir um item existente no banco de dados com base no ID fornecido. Aqui está como funciona:
    
    * **Verbo HTTP**: DELETE
    * **Endpoint**: `/items/{item_id}`
    * **Ação**: Exclui um item existente com base no ID fornecido.
    * **Parâmetros de caminho**: `item_id` (ID do item a ser excluído).
    * **Resposta**: Retorna o item excluído.

Essas operações fornecem uma API completa para gerenciar itens no banco de dados, permitindo criar, recuperar, atualizar e excluir itens de forma eficiente e segura. Certifique-se de que as operações estejam de acordo com os requisitos do seu projeto e que você implemente a lógica necessária para garantir a consistência e a segurança dos dados.

================================================
File: /Bootcamp - Python para dados/aula19/Dockerfile
================================================
FROM python:3.12

# Instalando o Poetry
RUN pip install poetry

# Copiar o conteúdo do diretório atual para o contêiner
COPY . /src

# Definir o diretório de trabalho
WORKDIR /src

# Instalar as dependências do projeto com Poetry
RUN poetry install

# Expor a porta em que a aplicação estará escutando
EXPOSE 8501

# Definir o entrypoint para executar o servidor Uvicorn
ENTRYPOINT ["poetry", "run", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8501"]


================================================
File: /Bootcamp - Python para dados/aula19/src/database.py
================================================
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base

SQLALCHEMY_DATABASE_URL = "sqlite:///./test.db"
engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()



================================================
File: /Bootcamp - Python para dados/aula19/src/main.py
================================================
from fastapi import FastAPI, HTTPException, Depends
from sqlalchemy.orm import Session
import models
import database
from typing import List, Union
from schema import Item, ItemBase, ItemCreate

app = FastAPI()

models.Base.metadata.create_all(bind=database.engine)

@app.post("/items/", response_model=Item)
def create_item(item: ItemCreate, db: Session = Depends(database.get_db)):
    db_item = models.Item(**item.dict())
    db.add(db_item)
    db.commit()
    db.refresh(db_item)
    return db_item

@app.get("/items/", response_model=List[Item])
def read_items(skip: int = 0, limit: int = 10, db: Session = Depends(database.get_db)):
    items = db.query(models.Item).offset(skip).limit(limit).all()
    return items

@app.get("/items/{item_id}", response_model=Item)
def read_item(item_id: int, db: Session = Depends(database.get_db)):
    db_item = db.query(models.Item).filter(models.Item.id == item_id).first()
    if db_item is None:
        raise HTTPException(status_code=404, detail="Item not found")
    return db_item

@app.put("/items/{item_id}", response_model=Item)
def update_item(item_id: int, item: ItemCreate, db: Session = Depends(database.get_db)):
    db_item = db.query(models.Item).filter(models.Item.id == item_id).first()
    if db_item is None:
        raise HTTPException(status_code=404, detail="Item not found")
    for key, value in item.dict().items():
        setattr(db_item, key, value)
    db.commit()
    db.refresh(db_item)
    return db_item

@app.delete("/items/{item_id}", response_model=Item)
def delete_item(item_id: int, db: Session = Depends(database.get_db)):
    db_item = db.query(models.Item).filter(models.Item.id == item_id).first()
    if db_item is None:
        raise HTTPException(status_code=404, detail="Item not found")
    db.delete(db_item)
    db.commit()
    return db_item


================================================
File: /Bootcamp - Python para dados/aula19/src/models.py
================================================
from sqlalchemy import Column, Integer, String, Float
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Item(Base):
    __tablename__ = 'items'

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True)
    price = Column(Float)
    is_offer = Column(String, nullable=True)


================================================
File: /Bootcamp - Python para dados/aula19/src/schema.py
================================================
from pydantic import BaseModel
from typing import Union

class ItemBase(BaseModel):
    name: str
    price: float
    is_offer: Union[bool, None] = None

class ItemCreate(ItemBase):
    pass

class Item(ItemBase):
    id: int

================================================
File: /Bootcamp - Python para dados/aula20/README.md
================================================
# CRUD FASTAPI POSTGRES STREAMLIT

Você sabe o que é CRUD?

![Imagem CRUD](assets/crud.jpeg)

A BlackFriday ta chegando. Você sabe como que o Iphone fica mais barato? Você sabe como que o vídeo game é cadastrado? Você sabia que quando abre o seu navegador, nada mais é do que o seu browser fazendo um SELECT no banco do Mercado Livre 🤯

Você precisa conhecer o CRUD.

O principal responsável por tornar isso possível é o ORM

![Imagem ORM](assets/orm.png)

## Instalação via docker

```bash
docker-compose up -d --build
```

### Uso

Frontend:
Acesse o endereço http://localhost:8501

### Documentação

Backend:
Acesse o endereço http://localhost:8000/docs

## Nossa estrutura de pastas e arquivos

```bash
├── README.md # arquivo com a documentação do projeto
├── backend # pasta do backend (FastAPI, SQLAlchemy, Uvicorn, Pydantic)
├── frontend # pasta do frontend (Streamlit, Requests, Pandas)
├── docker-compose.yml # arquivo de configuração do docker-compose (backend, frontend, postgres)
├── poetry.lock # arquivo de lock do poetry
└── pyproject.toml # arquivo de configuração do poetry
```

## Nosso Backend

Nosso backend vai ser uma API, que será responsável por fazer a comunicação entre o nosso frontend com o banco de dados. Vamos detalhar cada uma das pastas e arquivos do nosso backend.

### FastAPI

O FastAPI é um framework web para construir APIs com Python. Ele é baseado no Starlette, que é um framework assíncrono para construir APIs. O FastAPI é um framework que está crescendo muito, e que tem uma curva de aprendizado muito baixa, pois ele é muito parecido com o Flask.

### Uvicorn

O Uvicorn é um servidor web assíncrono, que é baseado no ASGI, que é uma especificação para servidores web assíncronos. O Uvicorn é o servidor web recomendado pelo FastAPI, e é o servidor que vamos utilizar nesse projeto.

### SQLAlchemy

O SQLAlchemy é uma biblioteca para fazer a comunicação com o banco de dados. Ele é um ORM (Object Relational Mapper), que é uma técnica de mapeamento objeto-relacional que permite fazer a comunicação com o banco de dados utilizando objetos.

Uma das principais vantagens de trabalhar com o SQLAlchemy é que ele é compatível com vários bancos de dados, como MySQL, PostgreSQL, SQLite, Oracle, Microsoft SQL Server, Firebird, Sybase e até mesmo o Microsoft Access.

Além disso, ele realiza a sanitização dos dados, evitando ataques de SQL Injection.

![imagem](assets/sqlinjection.jpeg)

Outro ponto, é que você pode trabalhar com métodos nativos do Python, como por exemplo o filter, que é muito utilizado para fazer filtros em listas. Isso facilita muito a nossa vida, pois não precisamos aprender uma nova linguagem para fazer a comunicação com o banco de dados. Quem tiver familidade com Pandas, vai se sentir em casa.

### Pydantic

O Pydantic é uma biblioteca para fazer a validação de dados. Ele é utilizado pelo FastAPI para fazer a validação dos dados que são recebidos na API, e também para definir os tipos de dados que são retornados pela API.

## docker-compose.yml

Esse arquivo `docker-compose.yml` define uma aplicação composta por três serviços: `postgres`, `backend` e `frontend`, e cria uma rede chamada `mynetwork`. Vou explicar cada parte em detalhes:

### Services:

#### Postgres:

* `image: postgres:latest`: Esse serviço utiliza a imagem mais recente do PostgreSQL disponível no Docker Hub.
* `volumes`: Mapeia o diretório `/var/lib/postgresql/data` dentro do contêiner do PostgreSQL para um volume chamado `postgres_data` no sistema hospedeiro. Isso permite que os dados do banco de dados persistam mesmo quando o contêiner é desligado.
* `environment`: Define variáveis de ambiente para configurar o banco de dados PostgreSQL, como nome do banco de dados (`POSTGRES_DB`), nome de usuário (`POSTGRES_USER`) e senha (`POSTGRES_PASSWORD`).
* `networks`: Define que este serviço está na rede chamada `mynetwork`.

#### Backend:

* `build`: Especifica que o Docker deve construir uma imagem para esse serviço, usando um Dockerfile localizado no diretório `./backend`.
* `volumes`: Mapeia o diretório `./backend` (no sistema hospedeiro) para o diretório `/app` dentro do contêiner. Isso permite que as alterações no código fonte do backend sejam refletidas no contêiner em tempo real.
* `environment`: Define a variável de ambiente `DATABASE_URL`, que especifica a URL de conexão com o banco de dados PostgreSQL.
* `ports`: Mapeia a porta `8000` do sistema hospedeiro para a porta `8000` do contêiner, permitindo que o serviço seja acessado através da porta `8000`.
* `depends_on`: Indica que este serviço depende do serviço `postgres`, garantindo que o banco de dados esteja pronto antes que o backend seja iniciado.
* `networks`: Também define que este serviço está na rede `mynetwork`.

#### Frontend:

* `build`: Similar ao backend, especifica que o Docker deve construir uma imagem para este serviço, usando um Dockerfile localizado no diretório `./frontend`.
* `volumes`: Mapeia o diretório `./frontend` (no sistema hospedeiro) para o diretório `/app` dentro do contêiner, permitindo alterações em tempo real.
* `ports`: Mapeia a porta `8501` do sistema hospedeiro para a porta `8501` do contêiner, permitindo acesso ao frontend através da porta `8501`.
* `networks`: Define que este serviço também está na rede `mynetwork`.

### Networks:

* `mynetwork`: Define uma rede personalizada para os serviços se comunicarem entre si.

### Volumes:

* `postgres_data`: Define um volume para armazenar os dados do banco de dados PostgreSQL.

### Comando `docker-compose up`:

Quando você executa `docker-compose up`, o Docker Compose lerá o arquivo `docker-compose.yml`, criará os serviços conforme as definições especificadas e os iniciará. Isso significa que os contêineres para o banco de dados PostgreSQL, o backend e o frontend serão criados e conectados à rede `mynetwork`. O banco de dados será configurado com os detalhes fornecidos (nome do banco de dados, usuário e senha), e as imagens para os serviços de backend e frontend serão construídas a partir dos Dockerfiles fornecidos. Uma vez iniciados, você poderá acessar o backend através de `http://localhost:8000` e o frontend através de `http://localhost:8501`. Os dados do banco de dados serão persistidos no volume `postgres_data`.

## Nossa estrutura de pastas e arquivos

```bash
├── backend
│   ├── Dockerfile # arquivo de configuração do Docker
│   ├── crud.py # arquivo com as funções de CRUD utilizando o SQL Alchemy ORM
│   ├── database.py # arquivo com a configuração do banco de dados utilizando o SQL Alchemy 
│   ├── main.py
│   ├── models.py
│   ├── requirements.txt
│   ├── router.py
│   └── schemas.py
```

## Arquivo `database.py`

O arquivo `database.py` é responsável por fazer a configuração do banco de dados utilizando o SQLAlchemy. Ele é responsável por criar a conexão com o banco de dados, e também por criar a sessão do banco de dados.

Caso queira mudar de banco de dados, você só precisa mudar a URL de conexão, que está na variável SQLALCHEMY_DATABASE_URL. o SQLAlchemy é compatível com vários bancos de dados, como MySQL, PostgreSQL, SQLite, Oracle, Microsoft SQL Server, Firebird, Sybase e até mesmo o Microsoft Access.

Os principais pontos desse arquivo é a engine, que é a conexão com o banco de dados, e o SessionLocal, que é a sessão do banco de dados. O SessionLocal é quem executada as queries no banco de dados.

Lembrar sempre de:

1) Declarar a URL do banco
2) Criar a engine usando o 'create_engine'
3) Criar a sessão do banco
4) Criar a Base do ORM (nosso Model vai herdar ele)
5) Criar um gerador de sessão para ser reutilizado

## Arquivo `models.py`

O arquivo `models.py` é responsável por definir os modelos do SQLAlchemy, que são as classes que definem as tabelas do banco de dados. Esses modelos são utilizados para fazer a comunicação com o banco de dados.

É aqui que definimos o nome da tabela, os campos e os tipos de dados. Conseguimos incluir campos gerados aleatoriamente, como o id e o created_at. Para o id, ao incluir o campo Integer, com o parâmetro primary_key=True, o SQLAlchemy já entende que esse campo é o id da tabela. Para o created_at, ao incluir o campo DateTime, com o parâmetro default=datetime, o SQLAlchemy já entende que esse campo é a data de criação da tabela.

Lembrar:

1) O models é agnóstico ao banco, ele não sabe qual é o banco que é criado! Ele vai importar o base do database!

2) Declarar sua Tabela

## Arquivo `schemas.py`

O arquivo `schemas.py` é responsável por definir os schemas do Pydantic, que são as classes que definem os tipos de dados que serão utilizados na API. Esses schemas são utilizados para fazer a validação dos dados que são recebidos na API, e também para definir os tipos de dados que são retornados pela API.

O pydantic é a principal biblioteca para validação de dados em Python. Ela é utilizada pelo FastAPI para fazer a validação dos dados recebidos na API, e também para definir os tipos de dados que são retornados pela API.

Além disso, ela possui uma integração muito boa com o SQLAlchemy, que é a biblioteca que utilizamos para fazer a comunicação com o banco de dados.

Outra vantagem são os seus tipos pré-definidos, que facilitam muito a nossa vida. Por exemplo, se você quer definir um campo que aceita apenas números positivos, você pode utilizar o PositiveInt. Se você quer definir um campo que aceita apenas determinadas categorias, você pode utilizar o construtor constrains.

Detalhe que criamos schemas diferentes para os retornos da nossa API. Isso é uma boa prática, pois permite que você tenha mais flexibilidade para alterar os schemas no futuro.

Temos o schema `ProductBase`, que é o schema base para o cadastro de produtos. Esse schema é utilizado para fazer a validação dos dados que são recebidos na API, e também para definir os tipos de dados que são retornados pela API.

Temos o schema `ProductCreate`, que é o schema que é retornado pela API. Ele é uma classe que herda do schema `ProductBase`, e possui um campo a mais, que é o id. Esse campo é utilizado para identificar o produto no banco de dados.

Temos o schema `ProductResponse`, que é o schema que é retornado pela API. Ele é uma classe que herda do schema `ProductBase`, e possui dois campos a mais, que é o id e o created_at. Esses campos são gerados pelo nosso banco de dados.

Temos o schema `ProductUpdate`, que é o schema que é recebido pela API para update. Ele possui os campos opcionais, pois não é necessário enviar todos os campos para fazer o update.

## Arquivo `crud.py`

O arquivo `crud.py` é responsável por definir as funções de CRUD utilizando o SQLAlchemy ORM. Essas funções são utilizadas para fazer a comunicação com o banco de dados. É nele que definimos as funções de listagem, criação, atualização e remoção de produtos. É onde os dados são persistidos no banco de dados.

## Arquivo `router.py`

O arquivo `router.py` é responsável por definir as rotas da API utilizando o FastAPI. É aqui que definimos as rotas, e também as funções que serão executadas em cada rota. Todas as funções definidas aqui recebem um parâmetro, que é o parâmetro request, que é o objeto que contém os dados da requisição.

Os principais parametros são o path, que é o caminho da rota, o methods, que são os métodos HTTP que a rota aceita, e o response_model, que é o schema que é retornado pela rota.

```python
@router.post("/products/", response_model=ProductResponse)
```
Importante destacar que o FastAPI utiliza o conceito de type hints, que são as anotações de tipos. Isso permite que o FastAPI faça a validação dos dados que são recebidos na API, e também para definir os tipos de dados que são retornados pela API. Por exemplo, ao definir o parâmetro product do tipo ProductResponse, o FastAPI já entende que os dados recebidos nesse parâmetro devem ser do tipo ProductResponse.

Conseguimos também retornar parâmetros pelo nosso path, no caso do delete, por exemplo, precisamos passar o id do produto que queremos deletar. Para isso, utilizamos o path /products/{product_id}, e definimos o parâmetro product_id na função delete_product.

```python
@router.get("/products/{product_id}", response_model=ProductResponse)
def read_product_route(product_id: int, db: Session = Depends(get_db)):
    db_product = get_product(db, product_id=product_id)
    if db_product is None:
        raise HTTPException(status_code=404, detail="Product not found")
    return db_product
```

## Arquivo `main.py`

O arquivo `main.py` é responsável por definir a aplicação do FastAPI, e também por definir o servidor web Uvicorn. É aqui que definimos o servidor web, e também as configurações do servidor web, como o host e a porta.


## Nosso Frontend

Nosso frontend vai ser uma aplicação que vai consumir a nossa API, e vai ser responsável por fazer o cadastro, alteração e remoção de produtos. Vamos detalhar cada uma das pastas e arquivos do nosso frontend.

### Streamlit

O Streamlit é uma biblioteca para construir aplicações web com Python. Ele é muito utilizado para construir dashboards, e também para construir aplicações que consomem APIs.

### Requests

O Requests é uma biblioteca para fazer requisições HTTP com Python. Ele é muito utilizado para consumir APIs, e também para fazer web scraping.

### Pandas

O Pandas é uma biblioteca para manipulação de dados com Python. Ele é muito utilizado para fazer análise de dados, e também para construir dashboards.



## Deploy <> Em construção





### AWS ECS

Além disso, nesse projeto vamos apresentar como colocar em produção um projeto utilizando containers Docker, utilizando o AWS ECS (Amazon Elastic Container Service).

Se você quer ter toda a facilidade do Docker, garantir que o seu ambiente de desenvolvimento e de produção são idênticos, e ainda ter a possibilidade de escalar a sua aplicação, esse projeto é para você.

A AWS ECS é um serviço de orquestração de containers, que permite que você execute containers Docker de forma escalável e altamente disponível. Com ele, você não precisa se preocupar com a infraestrutura, pois a AWS cuida de tudo para você.

### AMAZON ECS

É um serviço de orquestração de containers, que permite que você execute containers Docker de forma escalável e altamente disponível. A vantagem principal é que você não precisa se preocupar com a orquestração dos containers (Kubernetes) mas tenha todas as vantagens de utilizar containers Docker.

### AMAZON ECS FARGATE

O ECS Fargate é um serviço que permite que você execute containers Docker sem precisar gerenciar servidores. Ou seja, todo o gerenciamento de servidores, balanceamento de carga, auto scaling, etc, é feito pela AWS. É um serviço ainda mais gerenciado que o ECS, pois você não precisa se preocupar com a infraestrutura.

### Conceitos

[Imagem arquitetura](assets/arquitetura.png)

#### Cluster

Um cluster é um grupo de instâncias EC2 (máquinas) que executam as suas tarefas. Ou seja, as máquinas onde os meus containers vão ser executados.

#### Task Definition

Uma task definition é um arquivo de configuração (com a formatação JSON) que define como a sua aplicação vai ser executada. Nesse arquivo você define qual imagem Docker vai ser utilizada, qual o poder computacional necessário, qual o volume que vai ser utilizado, etc.

#### Task

Uma task é uma instância de uma task definition. Ou seja, é uma execução da sua aplicação. Por exemplo, se você tem uma task definition que define que a sua aplicação vai ser executada com 2 instâncias, você terá 2 tasks executando a sua aplicação. Aplicado ao Airflow que vimos no Workshop 02, podemos subir mais de uma instância do Airflow, para garantir que a nossa aplicação vai estar sempre disponível. Além disso, podemos configurar para subir mais instâncias quando a CPU estiver alta, por exemplo.

#### Service

Um service é um grupo de tasks que são executadas juntas. Por exemplo, se você tem uma task definition que define que a sua aplicação vai ser executada com 2 instâncias, você terá 2 tasks executando a sua aplicação. Essas 2 tasks formam um service. Se alguma tarefa falhar, o service vai garantir que ela vai ser executada novamente. O service também pode ser utilizado para balancear a carga entre as tasks.



================================================
File: /Bootcamp - Python para dados/aula20/docker-compose.yml
================================================
version: '3.8'
services:
  postgres:
    image: postgres:latest
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: mydatabase
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    networks:
      - mynetwork

  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    volumes:
      - ./backend:/app
    environment:
      DATABASE_URL: postgresql://user:password@postgres/mydatabase
    ports:
      - "8000:8000"
    depends_on:
      - postgres
    networks:
      - mynetwork

  frontend:
    build: 
      context: ./frontend
      dockerfile: Dockerfile
    volumes:
      - ./frontend:/app
    ports:
      - "8501:8501"
    networks:
      - mynetwork

networks:
  mynetwork:

volumes:
  postgres_data:


================================================
File: /Bootcamp - Python para dados/aula20/pyproject.toml
================================================
[tool.poetry]
name = "crud-fastapi-postgres-streamlit"
version = "0.1.0"
description = ""
authors = ["Luciano Filho <lvgalvaofilho@gmail.com>"]
readme = "README.md"
packages = [{ include = "crud_fastapi_postgres_streamlit" }]

[tool.poetry.dependencies]
python = "^3.11.5"
fastapi = "^0.104.1"
uvicorn = "^0.24.0.post1"
streamlit = "^1.28.1"
sqlalchemy = "^2.0.23"
plotly = "^5.18.0"


[tool.poetry.group.docs.dependencies]
mkdocs = "^1.5.3"


[tool.poetry.group.dev.dependencies]
pytest = "^7.4.3"
taskipy = "^1.12.0"
isort = "5.12.0"
ruff = "0.1.5"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.taskipy.tasks]
lint = "ruff . & isort ."
kill = "kill -9 $(lsof -t -i:8000)"
run = "uvicorn backend.main:app --reload & streamlit run frontend/app.py"


================================================
File: /Bootcamp - Python para dados/aula20/.dockerignore
================================================
# Include any files or directories that you don't want to be copied to your
# container here (e.g., local build artifacts, temporary files, etc.).
#
# For more help, visit the .dockerignore file reference guide at
# https://docs.docker.com/engine/reference/builder/#dockerignore-file

**/.DS_Store
**/__pycache__
**/.venv
**/.classpath
**/.dockerignore
**/.env
**/.git
**/.gitignore
**/.project
**/.settings
**/.toolstarget
**/.vs
**/.vscode
**/*.*proj.user
**/*.dbmdl
**/*.jfm
**/bin
**/charts
**/docker-compose*
**/compose*
**/Dockerfile*
**/node_modules
**/npm-debug.log
**/obj
**/secrets.dev.yaml
**/values.dev.yaml
LICENSE
README.md


================================================
File: /Bootcamp - Python para dados/aula20/.python-version
================================================
3.11


================================================
File: /Bootcamp - Python para dados/aula20/backend/Dockerfile
================================================
# Dockerfile-backend

# Imagem base
FROM python:3.9

# Definir o diretório de trabalho no container
WORKDIR /app

# Copiar os arquivos de dependências e instalar
COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir --upgrade -r /app/requirements.txt

# Copiar o restante dos arquivos do projeto
COPY . /app

# Comando para executar a aplicação
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

================================================
File: /Bootcamp - Python para dados/aula20/backend/crud.py
================================================
from sqlalchemy.orm import Session
from schemas import ProductUpdate, ProductCreate
from models import ProductModel


def get_product(db: Session, product_id: int):
    """
    funcao que recebe um id e retorna somente ele
    """
    return db.query(ProductModel).filter(ProductModel.id == product_id).first()


def get_products(db: Session):
    """
    funcao que retorna todos os elementos
    """
    return db.query(ProductModel).all()


def create_product(db: Session, product: ProductCreate):
    db_product = ProductModel(**product.model_dump())
    db.add(db_product)
    db.commit()
    db.refresh(db_product)
    return db_product


def delete_product(db: Session, product_id: int):
    db_product = db.query(ProductModel).filter(ProductModel.id == product_id).first()
    db.delete(db_product)
    db.commit()
    return db_product


def update_product(db: Session, product_id: int, product: ProductUpdate):
    db_product = db.query(ProductModel).filter(ProductModel.id == product_id).first()

    if db_product is None:
        return None

    if product.name is not None:
        db_product.name = product.name
    if product.description is not None:
        db_product.description = product.description
    if product.price is not None:
        db_product.price = product.price
    if product.categoria is not None:
        db_product.categoria = product.categoria
    if product.email_fornecedor is not None:
        db_product.email_fornecedor = product.email_fornecedor

    db.commit()
    return db_product


================================================
File: /Bootcamp - Python para dados/aula20/backend/database.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

SQLALCHEMY_DATABASE_URL = "postgresql://user:password@postgres/mydatabase"

# Cria o motor do banco de dados, é o conecta com o banco
engine = create_engine(SQLALCHEMY_DATABASE_URL)

# Sessão de banco de dados, é quem vai executar as queries
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Base para os modelos declarativos
Base = declarative_base()


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


================================================
File: /Bootcamp - Python para dados/aula20/backend/main.py
================================================
from fastapi import FastAPI
from database import engine
import models
from router import router

models.Base.metadata.create_all(bind=engine)

app = FastAPI()
app.include_router(router)


================================================
File: /Bootcamp - Python para dados/aula20/backend/models.py
================================================
from sqlalchemy import Column, Integer, String, Float, DateTime
from sqlalchemy.sql import func
from database import Base


class ProductModel(Base):
    __tablename__ = "products"  # esse será o nome da tabela

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True)
    description = Column(String, index=True)
    price = Column(Float, index=True)
    categoria = Column(String, index=True)
    email_fornecedor = Column(String, index=True)
    created_at = Column(DateTime(timezone=True), default=func.now(), index=True)


================================================
File: /Bootcamp - Python para dados/aula20/backend/requirements.txt
================================================
fastapi
uvicorn
SQLAlchemy
email-validator
psycopg2-binary

================================================
File: /Bootcamp - Python para dados/aula20/backend/router.py
================================================
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from database import SessionLocal, get_db
from schemas import ProductResponse, ProductUpdate, ProductCreate
from typing import List
from crud import (
    create_product,
    get_products,
    get_product,
    delete_product,
    update_product,
)

router = APIRouter()


@router.post("/products/", response_model=ProductResponse)
def create_product_route(product: ProductCreate, db: Session = Depends(get_db)):
    return create_product(db=db, product=product)


@router.get("/products/", response_model=List[ProductResponse])
def read_all_products_route(db: Session = Depends(get_db)):
    products = get_products(db)
    return products


@router.get("/products/{product_id}", response_model=ProductResponse)
def read_product_route(product_id: int, db: Session = Depends(get_db)):
    db_product = get_product(db, product_id=product_id)
    if db_product is None:
        raise HTTPException(status_code=404, detail="Product not found")
    return db_product


@router.delete("/products/{product_id}", response_model=ProductResponse)
def detele_product_route(product_id: int, db: Session = Depends(get_db)):
    db_product = delete_product(db, product_id=product_id)
    if db_product is None:
        raise HTTPException(status_code=404, detail="Product not found")
    return db_product


@router.put("/products/{product_id}", response_model=ProductResponse)
def update_product_route(
    product_id: int, product: ProductUpdate, db: Session = Depends(get_db)
):
    db_product = update_product(db, product_id=product_id, product=product)
    if db_product is None:
        raise HTTPException(status_code=404, detail="Product not found")
    return db_product


================================================
File: /Bootcamp - Python para dados/aula20/backend/schemas.py
================================================
from pydantic import BaseModel, PositiveFloat, EmailStr, validator, Field
from enum import Enum
from datetime import datetime
from typing import Optional


class CategoriaBase(Enum):
    categoria1 = "Eletrônico"
    categoria2 = "Eletrodoméstico"
    categoria3 = "Móveis"
    categoria4 = "Roupas"
    categoria5 = "Calçados"


class ProductBase(BaseModel):
    name: str
    description: Optional[str] = None
    price: PositiveFloat
    categoria: str
    email_fornecedor: EmailStr

    @validator("categoria")
    def check_categoria(cls, v):
        if v in [item.value for item in CategoriaBase]:
            return v
        raise ValueError("Categoria inválida")


class ProductCreate(ProductBase):
    pass


class ProductResponse(ProductBase):
    id: int
    created_at: datetime

    class Config:
        orm_mode = True


class ProductUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    price: Optional[PositiveFloat] = None
    categoria: Optional[str] = None
    email_fornecedor: Optional[EmailStr] = None

    @validator("categoria", pre=True, always=True)
    def check_categoria(cls, v):
        if v is None:
            return v
        if v in [item.value for item in CategoriaBase]:
            return v
        raise ValueError("Categoria inválida")


================================================
File: /Bootcamp - Python para dados/aula20/backend/.dockerignore
================================================
# Include any files or directories that you don't want to be copied to your
# container here (e.g., local build artifacts, temporary files, etc.).
#
# For more help, visit the .dockerignore file reference guide at
# https://docs.docker.com/engine/reference/builder/#dockerignore-file

**/.DS_Store
**/__pycache__
**/.venv
**/.classpath
**/.dockerignore
**/.env
**/.git
**/.gitignore
**/.project
**/.settings
**/.toolstarget
**/.vs
**/.vscode
**/*.*proj.user
**/*.dbmdl
**/*.jfm
**/bin
**/charts
**/docker-compose*
**/compose*
**/Dockerfile*
**/node_modules
**/npm-debug.log
**/obj
**/secrets.dev.yaml
**/values.dev.yaml
LICENSE
README.md


================================================
File: /Bootcamp - Python para dados/aula20/frontend/Dockerfile
================================================
# Dockerfile-frontend

# Imagem base
FROM python:3.9

# Definir o diretório de trabalho no container
WORKDIR /app

# Copiar os arquivos de dependências e instalar
COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir --upgrade -r /app/requirements.txt

# Copiar o restante dos arquivos do projeto
COPY . /app

# Comando para executar a aplicação
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]


================================================
File: /Bootcamp - Python para dados/aula20/frontend/app.py
================================================
import streamlit as st
import requests
import pandas as pd

st.set_page_config(layout="wide")

st.image("logo.png", width=200)

st.title("Gerenciamento de Produtos")


# Função auxiliar para exibir mensagens de erro detalhadas
def show_response_message(response):
    if response.status_code == 200:
        st.success("Operação realizada com sucesso!")
    else:
        try:
            data = response.json()
            if "detail" in data:
                # Se o erro for uma lista, extraia as mensagens de cada erro
                if isinstance(data["detail"], list):
                    errors = "\n".join([error["msg"] for error in data["detail"]])
                    st.error(f"Erro: {errors}")
                else:
                    # Caso contrário, mostre a mensagem de erro diretamente
                    st.error(f"Erro: {data['detail']}")
        except ValueError:
            st.error("Erro desconhecido. Não foi possível decodificar a resposta.")


# Adicionar Produto
with st.expander("Adicionar um Novo Produto"):
    with st.form("new_product"):
        name = st.text_input("Nome do Produto")
        description = st.text_area("Descrição do Produto")
        price = st.number_input("Preço", min_value=0.01, format="%f")
        categoria = st.selectbox(
            "Categoria",
            ["Eletrônico", "Eletrodoméstico", "Móveis", "Roupas", "Calçados"],
        )
        email_fornecedor = st.text_input("Email do Fornecedor")
        submit_button = st.form_submit_button("Adicionar Produto")

        if submit_button:
            response = requests.post(
                "http://backend:8000/products/",
                json={
                    "name": name,
                    "description": description,
                    "price": price,
                    "categoria": categoria,
                    "email_fornecedor": email_fornecedor,
                },
            )
            show_response_message(response)
# Visualizar Produtos
with st.expander("Visualizar Produtos"):
    if st.button("Exibir Todos os Produtos"):
        response = requests.get("http://backend:8000/products/")
        if response.status_code == 200:
            product = response.json()
            df = pd.DataFrame(product)

            df = df[
                [
                    "id",
                    "name",
                    "description",
                    "price",
                    "categoria",
                    "email_fornecedor",
                    "created_at",
                ]
            ]

            # Exibe o DataFrame sem o índice
            st.write(df.to_html(index=False), unsafe_allow_html=True)
        else:
            show_response_message(response)

# Obter Detalhes de um Produto
with st.expander("Obter Detalhes de um Produto"):
    get_id = st.number_input("ID do Produto", min_value=1, format="%d")
    if st.button("Buscar Produto"):
        response = requests.get(f"http://backend:8000/products/{get_id}")
        if response.status_code == 200:
            product = response.json()
            df = pd.DataFrame([product])

            df = df[
                [
                    "id",
                    "name",
                    "description",
                    "price",
                    "categoria",
                    "email_fornecedor",
                    "created_at",
                ]
            ]

            # Exibe o DataFrame sem o índice
            st.write(df.to_html(index=False), unsafe_allow_html=True)
        else:
            show_response_message(response)

# Deletar Produto
with st.expander("Deletar Produto"):
    delete_id = st.number_input("ID do Produto para Deletar", min_value=1, format="%d")
    if st.button("Deletar Produto"):
        response = requests.delete(f"http://backend:8000/products/{delete_id}")
        show_response_message(response)

# Atualizar Produto
with st.expander("Atualizar Produto"):
    with st.form("update_product"):
        update_id = st.number_input("ID do Produto", min_value=1, format="%d")
        new_name = st.text_input("Novo Nome do Produto")
        new_description = st.text_area("Nova Descrição do Produto")
        new_price = st.number_input(
            "Novo Preço",
            min_value=0.01,
            format="%f",
        )
        new_categoria = st.selectbox(
            "Nova Categoria",
            ["Eletrônico", "Eletrodoméstico", "Móveis", "Roupas", "Calçados"],
        )
        new_email = st.text_input("Novo Email do Fornecedor")

        update_button = st.form_submit_button("Atualizar Produto")

        if update_button:
            update_data = {}
            if new_name:
                update_data["name"] = new_name
            if new_description:
                update_data["description"] = new_description
            if new_price > 0:
                update_data["price"] = new_price
            if new_email:
                update_data["email_fornecedor"] = new_email
            if new_categoria:
                update_data["categoria"] = new_categoria

            if update_data:
                response = requests.put(
                    f"http://backend:8000/products/{update_id}", json=update_data
                )
                show_response_message(response)
            else:
                st.error("Nenhuma informação fornecida para atualização")


================================================
File: /Bootcamp - Python para dados/aula20/frontend/requirements.txt
================================================
streamlit
requests
pandas

================================================
File: /Bootcamp - Python para dados/aula20/frontend/.dockerignore
================================================
# Include any files or directories that you don't want to be copied to your
# container here (e.g., local build artifacts, temporary files, etc.).
#
# For more help, visit the .dockerignore file reference guide at
# https://docs.docker.com/engine/reference/builder/#dockerignore-file

**/.DS_Store
**/__pycache__
**/.venv
**/.classpath
**/.dockerignore
**/.env
**/.git
**/.gitignore
**/.project
**/.settings
**/.toolstarget
**/.vs
**/.vscode
**/*.*proj.user
**/*.dbmdl
**/*.jfm
**/bin
**/charts
**/docker-compose*
**/compose*
**/Dockerfile*
**/node_modules
**/npm-debug.log
**/obj
**/secrets.dev.yaml
**/values.dev.yaml
LICENSE
README.md


================================================
File: /Bootcamp - Python para dados/aula20/frontend/.streamlit/config.toml
================================================
[theme]
primaryColor = "#1E2250"             # Amarelo - similar ao botão do Mercado Livre
backgroundColor = "#ffffff"          # Amarelo claro para o fundo
secondaryBackgroundColor = "#f5f5f5" # Azul claro para fundo dos widgets
textColor = "#2d377a"                # Azul - similar ao texto do Mercado Livre
font = "sans serif"


================================================
File: /Bootcamp - SQL e Analytics/README.md
================================================
# Bootcamp de SQL e Analytics

Bem-vindos ao nosso Bootcamp de SQL e Analytics, um programa intensivo projetado para equipar você com habilidades de engenharia de dados e análise de dados. 

Nosso bootcamp é repleto de recursos e atividades interativas para garantir uma experiência de aprendizado prática. Aqui estão os principais componentes do nosso programa:

#### 1. **Repositório no GitHub**

* **Link:** [Data Engineering Roadmap](https://github.com/lvgalvao/data-engineering-roadmap)
* **Descrição:** Este repositório é a sua porta de entrada para os recursos de conteúdos da jornada de dados. Ele contém todos os materiais didáticos, exercícios práticos, datasets para análise e scripts que você utilizará durante o bootcamp. Cada módulo está detalhadamente documentado para que você possa seguir o caminho de aprendizagem com clareza e estrutura.

#### 2. **Plataforma Alpaclass**

* **Link:** [Jornada de Dados](https://jornadadedados.alpaclass.com/s/conteudos)
* **Descrição:** A Alpaclass é a nossa plataforma de aprendizado dedicada, onde você encontrará os vídeos das aulas. O conteúdo está organizado de forma sequencial para facilitar o seu progresso de um tópico para o próximo, com marcos claros para monitorar seu desenvolvimento ao longo do curso. Caso tenha alguma dúvida, deixar no comentário do vídeo incluindo também o tempo.

#### 3. **Aulas ao Vivo**

* **Horário:** Diariamente ao meio-dia
* **Plataforma:** Google Meet
* **Descrição:** Nossas aulas ao vivo são uma oportunidade para interagir com os instrutores e colegas. Durante estas sessões, você pode esclarecer dúvidas, explorar conceitos complexos e participar de discussões em tempo real. 

#### 4. **Grupo do WhatsApp**

* **Descrição:** Um espaço dedicado para suporte contínuo, o grupo do WhatsApp permite que você discuta desafios, compartilhe insights e peça ajuda tanto aos seus colegas quanto aos instrutores. Este canal está sempre ativo, assegurando que você nunca fique sem suporte.

* **Foto do Grupo:** 

![Foto grupo](https://github.com/lvgalvao/data-engineering-roadmap/blob/main/Bootcamp%20-%20SQL%20e%20Analytics/pics/grupo_alunos.png)


#### 5. **Repositório no Excalidraw**

* **Link:** [Excalidraw](https://link.excalidraw.com/l/8pvW6zbNUnD/3ktnOgfFeRK)
* **Objetivo:** Conceituar e expor graficamente estruturas de dados, fluxos de trabalho e arquiteturas de sistemas.

* **Descrição:** O repositório no Excalidraw serve como uma ferramenta visual para compreender melhor os conceitos abstratos e complexos que você encontrará no curso. Aqui, você terá acesso a diagramas e esquemas que ilustram de forma clara e eficaz os tópicos de engenharia e análise de dados, facilitando a compreensão e o aprendizado.

#### 6. **Conteúdo**

- *[Aula-01](./Aula-01)* - **Visão Geral e Preparação do ambiente SQL**
- *[Aula-02](./Aula-02)* - **SQL para Analytics: Nossas primeiras consultas**
- *[Aula-03](./Aula-03)* - **SQL para Analytics: Join and Having in SQL**
- *[Aula-04](./Aula-04)* - **Windows Function**
- *[Aula-05](./Aula-05)* - **Projeto de Análise de dados**
- *[Aula-06](./Aula-06)* - **CTE vs Subqueries vs Views vs Temporary Tables vs Materialized Views**
- *[Aula-07](./Aula-07)* - **Stored Procedures**
- *[Aula-08](./Aula-08)* - **CTE vs Subqueries vs Views vs Temporary Tables vs Materialized Views**
- *[Aula-09](./Aula-09)* - **Triggers (Gatilhos) e Projeto Prático II**
- *[Aula-10](./Aula-10)* - **Transação**
- *[Aula-11](./Aula-11)* - **Ordem de consulta**
- *[Aula-12](./Aula-12)* - **Database Indexing**
- *[Aula-13](./Aula-13)* - **Database Partition**


## Por Que Participar?
Participar do nosso Bootcamp de SQL e Analytics não apenas fortalece suas habilidades técnicas, mas também expande sua rede profissional através de interações com colegas e especialistas da indústria. Ao final do programa, você estará bem equipado para enfrentar desafios complexos de dados e fazer contribuições significativas no campo da engenharia e análise de dados.


================================================
File: /Bootcamp - SQL e Analytics/Aula-01/README.md
================================================
# Aula 01 - Visão Geral e Preparação do ambiente SQL

## Introdução

Bem-vindos ao nosso workshop sobre SQL e PostgreSQL. Hoje, vamos mergulhar nos conceitos básicos de bancos de dados e como o PostgreSQL pode ser utilizado para gerenciar dados de forma eficiente. Nosso objetivo é garantir que todos vocês tenham uma boa base para explorar mais sobre SQL e operações de banco de dados nos próximos dias.

## Por que Postgres?

PostgreSQL é um sistema de gerenciamento de banco de dados relacional (RDBMS) desenvolvido no Departamento de Ciência da Computação da Universidade da Califórnia em Berkeley. POSTGRES foi pioneiro em muitos conceitos que só se tornaram disponíveis em alguns sistemas de banco de dados comerciais muito mais tarde:

* complex queries
* foreign keys
* triggers
* updatable views
* transactional integrity

Além disso, o PostgreSQL pode ser estendido pelo usuário de várias maneiras, por exemplo, adicionando novos

* data types
* functions
* operators
* aggregate functions
* index methods
* procedural languages

## Informações Adicionais 

Além do conteúdo do curso, recomendo alguns outros lugares para estudo.

[Documentação](https://www.postgresql.org/docs/current/index.html) Documentação oficial do Postgres, todas as features estão aqui.


[Wiki](https://wiki.postgresql.org/wiki/Main_Page) A wiki do PostgreSQL contém a lista de Perguntas Frequentes (FAQ), lista de tarefas pendentes (TODO) e informações detalhadas sobre muitos outros tópicos.

[Site](https://www.postgresql.org/) na Web O site do PostgreSQL fornece detalhes sobre a última versão e outras informações para tornar seu trabalho ou lazer com o PostgreSQL mais produtivo.

[Comunidade](https://github.com/postgres/postgres) O código O PostgreSQL é um projeto de código aberto. Como tal, depende da comunidade de usuários para suporte contínuo. À medida que você começa a usar o PostgreSQL, dependerá de outros para obter ajuda, seja através da documentação ou através das listas de discussão. Considere devolver o seu conhecimento. Leia as listas de discussão e responda às perguntas. Se você aprender algo que não está na documentação, escreva e contribua com isso. Se você adicionar recursos ao código, contribua com eles.

## Instalação

Antes de poder usar o PostgreSQL, você precisa instalá-lo, é claro. É possível que o PostgreSQL já esteja instalado em seu local, seja porque foi incluído na distribuição do seu sistema operacional ou porque o administrador do sistema já o instalou.

Se você não tem certeza se o PostgreSQL já está disponível ou se você pode usá-lo para suas experimentações, então você pode instalá-lo por conta própria. Fazer isso não é difícil e pode ser um bom exercício.

- Instalando o postgres Local

## Fundamentos da Arquitetura

Antes de prosseguirmos, é importante que você entenda a arquitetura básica do sistema PostgreSQL. Compreender como as partes do PostgreSQL interagem tornará tudo mais fácil.

No jargão de tecnologia, o PostgreSQL utiliza um modelo cliente/servidor. 

Um processo servidor, que gerencia os arquivos de banco de dados, aceita conexões com o banco de dados de aplicações cliente e executa ações no banco de dados em nome dos clientes. O programa do servidor de banco de dados é chamado de postgres.

A aplicação cliente do usuário (frontend) que deseja realizar operações de banco de dados. As aplicações cliente podem ser muito diversas em natureza: um cliente pode ser uma ferramenta orientada a texto, uma aplicação gráfica, um servidor web que acessa o banco de dados para exibir páginas web ou uma ferramenta especializada de manutenção de banco de dados. Algumas aplicações cliente são fornecidas com a distribuição do PostgreSQL; a maioria é desenvolvida pelos usuários.

Como é típico em aplicações cliente/servidor, o cliente e o servidor podem estar em hosts diferentes. Nesse caso, eles se comunicam por uma conexão de rede TCP/IP. Você deve ter isso em mente, porque os arquivos que podem ser acessados em uma máquina cliente podem não ser acessíveis (ou podem ser acessíveis apenas com um nome de arquivo diferente) na máquina do servidor de banco de dados.

O servidor PostgreSQL pode lidar com múltiplas conexões simultâneas de clientes. Para alcançar isso, ele inicia (“forks”) um novo processo para cada conexão. A partir desse ponto, o cliente e o novo processo servidor se comunicam sem intervenção do processo postgres original. Assim, o processo servidor supervisor está sempre em execução, aguardando conexões de clientes, enquanto os processos de clientes e servidores associados vêm e vão. (Tudo isso, é claro, é invisível para o usuário. Só mencionamos isso aqui para completude.)

## Criando um Banco de Dados

O primeiro teste para verificar se você pode acessar o servidor de banco de dados é tentar criar um banco de dados. Um servidor PostgreSQL em execução pode gerenciar vários bancos de dados. Tipicamente, um banco de dados separado é usado para cada projeto ou usuário.

Para isso vamos entrar dentro do nosso cliente `pgAdmin 4`

Também podemos nos conectar em servidores remoto, ex: `Render`

## Criando nosso Schema

![Northwind database](https://github.com/pthom/northwind_psql/raw/master/ER.png)

Para este projeto, vamos utilizar um script SQL simples que preencherá um banco de dados com o famoso exemplo [Northwind](https://github.com/pthom/northwind_psql), adaptado para o PostgreSQL. Esse script configurará o banco de dados Northwind no PostgreSQL, criando todas as tabelas necessárias e inserindo dados de exemplo para que você possa começar a trabalhar imediatamente com consultas e análises SQL em um contexto prático. Este banco de dados de exemplo é uma ótima ferramenta para aprender e praticar as operações e técnicas de SQL, especialmente útil para entender como manipular dados relacionais em um ambiente realista.

## Primeiros comandos

Vamos agora para um guia introdutório para operações básicas de SQL utilizando o banco de dados Northwind. Cada comando SQL será explicado com uma breve introdução para ajudar no entendimento e aplicação prática.

#### Exemplo de Seleção Completa

Para selecionar todos os dados de uma tabela:

```sql
-- Exibe todos os registros da tabela Customers
SELECT * FROM customers;
```

#### Seleção de Colunas Específicas

Para selecionar colunas específicas:

```sql
-- Exibe o nome de contato e a cidade dos clientes
SELECT contact_name, city FROM customers;
```

#### Utilizando DISTINCT

Para selecionar valores distintos:

```sql
-- Lista todos os países dos clientes
SELECT country FROM customers;
-- Lista os países sem repetição
SELECT DISTINCT country FROM customers;
-- Conta quantos países únicos existem
SELECT COUNT(DISTINCT country) FROM customers;
```

#### Cláusula WHERE

Para filtrar registros:

```sql
-- Seleciona todos os clientes do México
SELECT * FROM customers WHERE country='Mexico';
-- Seleciona clientes com ID específico
SELECT * FROM customers WHERE customer_id='ANATR';
-- Utiliza AND para múltiplos critérios
SELECT * FROM customers WHERE country='Germany' AND city='Berlin';
-- Utiliza OR para mais de uma cidade
SELECT * FROM customers WHERE city='Berlin' OR city='Aachen';
-- Utiliza NOT para excluir a Alemanha
SELECT * FROM customers WHERE country<>'Germany';
-- Combina AND, OR e NOT
SELECT * FROM customers WHERE country='Germany' AND (city='Berlin' OR city='Aachen');
-- Exclui clientes da Alemanha e EUA
SELECT * FROM customers WHERE country<>'Germany' AND country<>'USA';
```

#### ORDER BY

Para ordenar os resultados:

```sql
-- Ordena clientes pelo país
SELECT * FROM customers ORDER BY country;
-- Ordena por país em ordem descendente
SELECT * FROM customers ORDER BY country DESC;
-- Ordena por país e nome do contato
SELECT * FROM customers ORDER BY country, contact_name;
-- Ordena por país em ordem ascendente e nome em ordem descendente
SELECT * FROM customers ORDER BY country ASC, contact_name DESC;
```

#### Utilizando LIKE e IN
Para busca por padrões e listas de valores:

```sql
-- Clientes com nome de contato começando por "a"
SELECT * FROM customers WHERE contact_name LIKE 'a%';
-- Clientes com nome de contato não começando por "a"
SELECT * FROM customers WHERE contact_name NOT LIKE 'a%';
-- Clientes de países específicos
SELECT * FROM customers WHERE country IN ('Germany', 'France', 'UK');
-- Clientes não localizados em 'Germany', 'France', 'UK'
SELECT * FROM customers WHERE country NOT IN ('Germany', 'France', 'UK');
```

#### Desafio

- Instalar o Postgres
- Criar o projeto Northwind local
- Realizar todos os comandos acima


================================================
File: /Bootcamp - SQL e Analytics/Aula-02/README.md
================================================
# Aula 02 - SQL para Analytics: Nossas primeiras consultas

## Objetivo

Realizar nossas primeiras consultas no banco Northwind

## Uma tangente antes de realizarmos nossas primeiras consultas

SQL, ou Structured Query Language, é uma linguagem de programação projetada para gerenciar dados armazenados em um sistema de gerenciamento de banco de dados relacional (RDBMS). SQL possui vários componentes, cada um responsável por diferentes tipos de tarefas e operações que podem ser executadas em um banco de dados. Esses componentes incluem DDL, DML, DCL, e DQL, entre outros. Aqui está um resumo de cada um deles:

Cada componente da linguagem SQL tem um papel fundamental na gestão e no uso de bancos de dados, e diferentes tipos de profissionais de tecnologia podem utilizar esses comandos para desempenhar suas funções específicas. Vamos detalhar quem geralmente é responsável por cada tipo de comando e qual o objetivo de cada um dos componentes mencionados (DDL, DML, DQL, DCL, TCL):

### 1. DDL (Data Definition Language)

O DDL ou Linguagem de Definição de Dados é usado para definir e modificar a estrutura do banco de dados e seus objetos, como tabelas, índices, restrições, esquemas, entre outros. Comandos DDL incluem:

* **CREATE**: Usado para criar novos objetos no banco de dados, como tabelas, índices, funções, vistas, triggers, etc.
* **ALTER**: Modifica a estrutura de um objeto existente no banco de dados, por exemplo, adicionando uma coluna a uma tabela ou alterando características de uma coluna existente.
* **DROP**: Remove objetos do banco de dados.
* **TRUNCATE**: Remove todos os registros de uma tabela, liberando o espaço ocupado por esses registros.

* **Responsável**: Administradores de banco de dados (DBAs) e desenvolvedores de banco de dados.
* **Objetivo**: O DDL é usado para criar e modificar a estrutura do banco de dados e de seus objetos. Esses comandos ajudam a definir como os dados são organizados, armazenados, e como as relações entre eles são estabelecidas. Eles são essenciais durante a fase de design do banco de dados e quando são necessárias mudanças na estrutura.

### 2. DML (Data Manipulation Language)

O DML ou Linguagem de Manipulação de Dados é usado para gerenciar dados dentro dos objetos (como tabelas). Inclui comandos para inserir, modificar e deletar dados:

* **INSERT**: Insere dados em uma tabela.
* **UPDATE**: Altera dados existentes em uma tabela.
* **DELETE**: Remove dados de uma tabela.
* **MERGE**: Uma operação que permite inserir, atualizar ou deletar registros em uma tabela com base em um conjunto de condições determinadas.

* **Responsável**: Desenvolvedores de software, analistas de dados e, ocasionalmente, usuários finais através de interfaces que executam comandos DML por trás dos panos.
* **Objetivo**: O DML é crucial para o gerenciamento dos dados dentro das tabelas. Ele é utilizado para inserir, atualizar, deletar e manipular dados armazenados. Analistas de dados podem usar DML para preparar conjuntos de dados para análise, enquanto os desenvolvedores o utilizam para implementar a lógica de negócios.

### 3. DQL (Data Query Language)

O DQL ou Linguagem de Consulta de Dados é fundamentalmente usado para realizar consultas nos dados. O comando mais conhecido na DQL é o **SELECT**, que é utilizado para recuperar dados de uma ou mais tabelas.

* **Responsável**: Analistas de dados, cientistas de dados, e qualquer usuário que necessite extrair informações do banco de dados.
* **Objetivo**: O DQL é usado para consultar e recuperar dados. É fundamental para gerar relatórios, realizar análises, e fornecer dados que ajudem na tomada de decisões. O comando `SELECT`, parte do DQL, é um dos mais usados e é essencial para qualquer tarefa que requer visualização ou análise de dados.

### 4. DCL (Data Control Language)

O DCL ou Linguagem de Controle de Dados inclui comandos relacionados à segurança na acessibilidade dos dados no banco de dados. Isso envolve comandos para conceder e revogar permissões de acesso:

* **GRANT**: Concede permissões de acesso aos usuários.
* **REVOKE**: Remove permissões de acesso.

* **Responsável**: Administradores de banco de dados.
* **Objetivo**: O DCL é usado para configurar permissões em um banco de dados, garantindo que apenas usuários autorizados possam acessar, modificar, ou administrar os dados. Isso é crucial para a segurança e a governança de dados, protegendo informações sensíveis e mantendo a integridade do sistema.

### 5. TCL (Transaction Control Language)

O TCL ou Linguagem de Controle de Transação é usado para gerenciar transações no banco de dados. Transações são importantes para manter a integridade dos dados e garantir que operações múltiplas sejam concluídas com sucesso ou não sejam realizadas de todo:

* **COMMIT**: Confirma uma transação, tornando todas as mudanças permanentes no banco de dados.
* **ROLLBACK**: Desfaz todas as mudanças feitas durante a transação atual.
* **SAVEPOINT**: Define um ponto na transação que pode ser usado para um rollback parcial.

* **Responsável**: Desenvolvedores de software e administradores de banco de dados.
* **Objetivo**: O TCL é usado para gerenciar transações no banco de dados, garantindo que as operações sejam completadas com sucesso ou revertidas em caso de erro. Isso é essencial para manter a consistência e integridade dos dados, especialmente em ambientes onde múltiplas transações ocorrem simultaneamente.

Essa separação de responsabilidades ajuda a manter a organização e eficiência das operações do banco de dados, além de garantir que as ações executadas em um ambiente de banco de dados sejam seguras e alinhadas com as necessidades da organização.

## Se olharmos os comandos que fizemos ontem...

1) Esse comando é de qual subconjunto?

```sql
SELECT * FROM customers WHERE country='Mexico';
```

2) Esse comando é de qual subconjunto?

```sql
INSERT INTO customers VALUES ('ALFKI', 'Alfreds Futterkiste', 'Maria Anders', 'Sales Representative', 'Obere Str. 57', 'Berlin', NULL, '12209', 'Germany', '030-0074321', '030-0076545');
INSERT INTO customers VALUES ('ANATR', 'Ana Trujillo Emparedados y helados', 'Ana Trujillo', 'Owner', 'Avda. de la Constitución 2222', 'México D.F.', NULL, '05021', 'Mexico', '(5) 555-4729', '(5) 555-3745');
INSERT INTO customers VALUES ('ANTON', 'Antonio Moreno Taquería', 'Antonio Moreno', 'Owner', 'Mataderos  2312', 'México D.F.', NULL, '05023', 'Mexico', '(5) 555-3932', NULL);
INSERT INTO customers VALUES ('AROUT', 'Around the Horn', 'Thomas Hardy', 'Sales Representative', '120 Hanover Sq.', 'London', NULL, 'WA1 1DP', 'UK', '(171) 555-7788', '(171) 555-6750');
INSERT INTO customers VALUES ('BERGS', 'Berglunds snabbköp', 'Christina Berglund', 'Order Administrator', 'Berguvsvägen  8', 'Luleå', NULL, 'S-958 22', 'Sweden', '0921-12 34 65', '0921-12 34 67');
```

3) Esse comando é de qual subconjunto?

```sql
CREATE TABLE suppliers (
    supplier_id smallint NOT NULL,
    company_name character varying(40) NOT NULL,
    contact_name character varying(30),
    contact_title character varying(30),
    address character varying(60),
    city character varying(15),
    region character varying(15),
    postal_code character varying(10),
    country character varying(15),
    phone character varying(24),
    fax character varying(24),
    homepage text
);
```

4) Esse comando é de qual subconjunto?

```sql
SET statement_timeout = 0;
SET lock_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SET check_function_bodies = false;
SET client_min_messages = warning;
```

## Agora vamos para nossas primeiras QUERY? (Data Query Language)

Data Query Language (DQL) é um subconjunto da linguagem SQL (Structured Query Language) utilizado especificamente para consultar dados em bancos de dados. DQL é fundamental para extrair informações, realizar análises e gerar relatórios a partir dos dados armazenados em um sistema de gerenciamento de banco de dados relacional (RDBMS). O principal comando em DQL é o `SELECT`, que é amplamente utilizado para selecionar dados de uma ou mais tabelas.

**Objetivos da DQL**

O principal objetivo da DQL é permitir que usuários e aplicações recuperem dados de forma eficiente e precisa de um banco de dados. DQL proporciona a flexibilidade para especificar exatamente quais dados são necessários, como devem ser filtrados, agrupados, ordenados e transformados. Isso torna a DQL uma ferramenta essencial para:

* **Análise de dados**: Extrair conjuntos de dados para análise e tomada de decisão baseada em evidências.
* **Geração de relatórios**: Criar relatórios detalhados que ajudam as organizações a entender o desempenho operacional e estratégico.
* **Visualização de dados**: Alimentar ferramentas de visualização com dados que ajudam a representar informações complexas de maneira compreensível.
* **Auditoria e monitoramento**: Acompanhar e revisar operações e transações para conformidade e segurança.

**Como começar com DQL**

Para começar a usar DQL, é essencial ter um conhecimento básico de SQL e entender a estrutura dos dados dentro do banco de dados com o qual você está trabalhando. Aqui estão alguns passos para começar:

1. **Entenda o esquema do banco de dados**: Conheça as tabelas, colunas, tipos de dados e relações entre as tabelas.
2. **Aprenda os fundamentos do comando `SELECT`**: Comece com consultas simples para selecionar colunas específicas de uma tabela.
3. **Use cláusulas para refinar suas consultas**:
    * **WHERE**: Para filtrar registros.
    * **GROUP BY**: Para agrupar registros.
    * **HAVING**: Para filtrar grupos.
    * **ORDER BY**: Para ordenar os resultados.
4. **Pratique com dados de exemplo**: Use um banco de dados de exemplo para praticar suas consultas e testar diferentes cenários.

**Principais comandos da DQL**

* **SELECT**: O comando mais fundamental em DQL, usado para selecionar dados de uma ou mais tabelas.
    
    ```sql
    SELECT * FROM customers;
    select contact_name, city from customers;
    ```
    
* **DISTINCT**: Usado com `SELECT` para retornar apenas valores distintos.
    
    ```sql
    select country from customers;
    select distinct country from customers;
    select count(distinct country) from customers;
    ```

* **WHERE**: Usado para filtrar.

```sql
-- Seleciona todos os clientes do México
SELECT * FROM customers WHERE country='Mexico';
-- Seleciona clientes com ID específico
SELECT * FROM customers WHERE customer_id='ANATR';
-- Utiliza AND para múltiplos critérios
SELECT * FROM customers WHERE country='Germany' AND city='Berlin';
-- Utiliza OR para mais de uma cidade
SELECT * FROM customers WHERE city='Berlin' OR city='Aachen';
-- Utiliza NOT para excluir a Alemanha
SELECT * FROM customers WHERE country<>'Germany';
-- Combina AND, OR e NOT
SELECT * FROM customers WHERE country='Germany' AND (city='Berlin' OR city='Aachen');
-- Exclui clientes da Alemanha e EUA
SELECT * FROM customers WHERE country<>'Germany' AND country<>'USA';
```

### Mais operadores

Os operadores de comparação no SQL são essenciais para filtrar registros em consultas com base em condições específicas. Vamos examinar cada um dos operadores que você mencionou (`<`, `>`, `<=`, `>=`, `<>`) com exemplos práticos. Suponhamos que temos uma tabela chamada `products` com uma coluna `unit_price` para o preço dos produtos e uma coluna `units_in_stock` para o número de itens em estoque.

### Operador `<` (Menor que)

```sql
-- Seleciona todos os produtos com preço menor que 20
SELECT * FROM products
WHERE unit_price < 20;
```

### Operador `>` (Maior que)

```sql
-- Seleciona todos os produtos com preço maior que 100
SELECT * FROM products
WHERE unit_price > 100;
```

### Operador `<=` (Menor ou igual a)

```sql
-- Seleciona todos os produtos com preço menor ou igual a 50
SELECT * FROM products
WHERE unit_price <= 50;
```

### Operador `>=` (Maior ou igual a)

```sql
-- Seleciona todos os produtos com quantidade em estoque maior ou igual a 10
SELECT * FROM products
WHERE units_in_stock >= 10;
```

### Operador `<>` (Diferente de)

```sql
-- Seleciona todos os produtos cujo preço não é 30
SELECT * FROM products
WHERE unit_price <> 30;
```

### Combinação de Operadores

Você também pode combinar vários operadores em uma única consulta para criar condições mais específicas:

```sql
-- Seleciona todos os produtos com preço entre 50 e 100 (exclusive)
SELECT * FROM products
WHERE unit_price >= 50 AND unit_price < 100;
```

```sql
-- Seleciona todos os produtos com preço fora do intervalo 20 a 40
SELECT * FROM products
WHERE unit_price < 20 OR unit_price > 40;
```

* **Is null and is not null**: Usado em conjunto com o `where` para criar regras mais complexas de filtro nos registros.

```sql
SELECT * FROM customers
WHERE contact_name is Null;

SELECT * FROM customers
WHERE contact_name is not null;
```

* **LIKE**

```SQL
-- Nome do cliente começando com "a":
SELECT * FROM customers
WHERE contact_name LIKE 'a%';
```

Para tratar as strings como maiúsculas ou minúsculas em uma consulta SQL, você pode usar as funções `UPPER()` ou `LOWER()`, respectivamente. Essas funções convertem todas as letras em uma string para maiúsculas ou minúsculas, permitindo que você faça comparações de forma mais flexível, ignorando a diferença entre maiúsculas e minúsculas.

Aqui está como você pode modificar a consulta para encontrar todos os clientes cujo nome começa com a letra "a", independentemente de ser maiúscula ou minúscula:

### Para encontrar nomes que começam com "a" em maiúscula ou minúscula:

```sql
SELECT * FROM customers
WHERE LOWER(contact_name) LIKE 'a%';
```

Essa consulta converte todo o `contact_name` para minúsculas antes de fazer a comparação, o que torna a busca insensível a maiúsculas e minúsculas.

### Para encontrar nomes que começam com "A" em maiúscula:

```sql
SELECT * FROM customers
WHERE UPPER(contact_name) LIKE 'A%';
```

Essa consulta converte todo o `contact_name` para maiúsculas antes de fazer a comparação, garantindo que apenas os nomes que começam com "A" maiúscula sejam selecionados.

Usar `UPPER()` ou `LOWER()` é uma prática comum para garantir que as condições aplicadas em campos de texto não sejam afetadas por diferenças de capitalização nas entradas de dados.

```sql
-- Nome do cliente terminando com "a":
SELECT * FROM customers
WHERE contact_name LIKE '%a';

-- Nome do cliente que possui "or" em qualquer posição:
SELECT * FROM customers
WHERE contact_name LIKE '%or%';

-- Nome do cliente com "r" na segunda posição:
SELECT * FROM customers
WHERE contact_name LIKE '_r%';

-- Nome do cliente que começa com "A" e tem pelo menos 3 caracteres de comprimento:
SELECT * FROM customers
WHERE contact_name LIKE 'A_%_%';

-- Nome do contato que começa com "A" e termina com "o":
SELECT * FROM customers
WHERE contact_name LIKE 'A%o';

-- Nome do cliente que NÃO começa com "a":
SELECT * FROM customers
WHERE contact_name NOT LIKE 'A%';

-- Usando o curinga [charlist] (SQL server)
SELECT * FROM customers
WHERE city LIKE '[BSP]%';

-- Usando o curinga Similar To (Postgres)
SELECT * FROM customers
WHERE city SIMILAR TO '(B|S|P)%';

-- Usando o MySQL (coitado, tem nada)
SELECT * FROM customers
WHERE (city LIKE 'B%' OR city LIKE 'S%' OR city LIKE 'P%');
```

* **Operador IN**

```sql
-- localizado na "Alemanha", "França" ou "Reino Unido":
SELECT * FROM customers
WHERE country IN ('Germany', 'France', 'UK');

-- NÃO localizado na "Alemanha", "França" ou "Reino Unido":
SELECT * FROM customers
WHERE country NOT IN ('Germany', 'France', 'UK');

-- Só para dar um gostinho de uma subqueyr... Seleciona todos os clientes que são dos mesmos países que os fornecedores:

SELECT * FROM customers
WHERE country IN (SELECT country FROM suppliers);

-- Exemplo com BETWEEN
SELECT * FROM products
WHERE unit_price BETWEEN 10 AND 20;

-- Exemplo com NOT BETWEEN
SELECT * FROM products
WHERE unit_price NOT BETWEEN 10 AND 20;

-- Seleciona todos os produtos com preço ENTRE 10 e 20. Adicionalmente, não mostra produtos com CategoryID de 1, 2 ou 3:
SELECT * FROM products
WHERE (unit_price BETWEEN 10 AND 20) AND category_id NOT IN (1, 2, 3);
```

```sql
--selects todos os produtos entre 'Carnarvon Tigers' e 'Mozzarella di Giovanni':
select * from products
where product_name between 'Carnarvon Tigers' and 'Mozzarella di Giovanni'
order by product_name;

--Selecione todas as ordens BETWEEN '04-July-1996' e '09-July-1996':
select * from orders
where order_date between '07/04/1996' and '07/09/1996';
```

* **Tangente sobre diferentes bancos**

O comando SQL que você mencionou é específico para PostgreSQL e não necessariamente padrão em todos os SGBDs (Sistemas de Gerenciamento de Banco de Dados). Cada SGBD pode ter funções e formatos de data ligeiramente diferentes. No entanto, a estrutura básica do comando `SELECT` e a cláusula `WHERE` usando `BETWEEN` são bastante universais.

Aqui estão algumas variantes para outros SGBDs populares:

### SQL Server

Para formatar datas em SQL Server, você usaria a função `CONVERT` ou `FORMAT` (a partir do SQL Server 2012):

```sql
-- Usando CONVERT
SELECT CONVERT(VARCHAR, order_date, 120) FROM orders
WHERE order_date BETWEEN '1996-04-07' AND '1996-09-07';

-- Usando FORMAT
SELECT FORMAT(order_date, 'yyyy-MM-dd') FROM orders
WHERE order_date BETWEEN '1996-04-07' AND '1996-09-07';
```

### MySQL

MySQL utiliza a função `DATE_FORMAT` para formatar datas:

```sql
SELECT DATE_FORMAT(order_date, '%Y-%m-%d') FROM orders
WHERE order_date BETWEEN '1996-04-07' AND '1996-09-07';
```

### Oracle

Oracle também usa a função `TO_CHAR` como PostgreSQL para formatação de datas:

```sql
SELECT TO_CHAR(order_date, 'YYYY-MM-DD') FROM orders
WHERE order_date BETWEEN TO_DATE('1996-04-07', 'YYYY-MM-DD') AND TO_DATE('1996-09-07', 'YYYY-MM-DD');
```

### SQLite

SQLite não tem uma função dedicada para formatar datas, mas você pode usar funções de string para manipular formatos de data padrão:

```sql
SELECT strftime('%Y-%m-%d', order_date) FROM orders
WHERE order_date BETWEEN '1996-04-07' AND '1996-09-07';
```

* **Funções Agregadas** (COUNT, MAX, MIN, SUM, AVG): Usadas para realizar cálculos em um conjunto de valores.

As funções agregadas são uma ferramenta fundamental na linguagem SQL, utilizadas para realizar cálculos sobre um conjunto de valores e retornar um único valor resultante. Essas funções são especialmente úteis em operações que envolvem a análise estatística de dados, como a obtenção de médias, somas, valores máximos e mínimos, entre outros. Ao operar em conjuntos de dados, as funções agregadas permitem extrair insights significativos, suportar decisões de negócios, e simplificar dados complexos em informações gerenciáveis.
    
As funções agregadas geralmente são usadas em consultas SQL com a cláusula GROUP BY, que agrupa linhas que têm os mesmos valores em colunas especificadas. No entanto, podem ser usadas sem GROUP BY para resumir todos os dados de uma tabela. Aqui estão as principais funções agregadas e como são aplicadas:

```sql
-- Exemplo de MIN()
SELECT MIN(unit_price) AS preco_minimo
FROM products;

-- Exemplo de MAX()
SELECT MAX(unit_price) AS preco_maximo
FROM products;

-- Exemplo de COUNT()
SELECT COUNT(*) AS total_de_produtos
FROM products;

-- Exemplo de AVG()
SELECT AVG(unit_price) AS preco_medio
FROM products;

-- Exemplo de SUM()
SELECT SUM(quantity) AS quantidade_total_de_order_details
FROM order_details;
```

### Práticas Recomendadas

* **Precisão de dados**: Ao usar `AVG()` e `SUM()`, esteja ciente do tipo de dados da coluna para evitar imprecisões, especialmente com dados flutuantes.
* **NULLs**: Lembre-se de que a maioria das funções agregadas ignora valores `NULL`, exceto `COUNT(*)`, que conta todas as linhas, incluindo aquelas com valores `NULL`.
* **Performance**: Em tabelas muito grandes, operações agregadas podem ser custosas em termos de desempenho. Considere usar índices adequados ou realizar pré-agregações quando aplicável.
* **Clareza**: Ao usar `GROUP BY`, assegure-se de que todas as colunas não agregadas na sua cláusula `SELECT` estejam incluídas na cláusula `GROUP BY`.

### Exemplo de MIN() com GROUP BY

```sql
-- Calcula o menor preço unitário de produtos em cada categoria
SELECT category_id, MIN(unit_price) AS preco_minimo
FROM products
GROUP BY category_id;
```

### Exemplo de MAX() com GROUP BY

```sql
-- Calcula o maior preço unitário de produtos em cada categoria
SELECT category_id, MAX(unit_price) AS preco_maximo
FROM products
GROUP BY category_id;
```

### Exemplo de COUNT() com GROUP BY

```sql
-- Conta o número total de produtos em cada categoria
SELECT category_id, COUNT(*) AS total_de_produtos
FROM products
GROUP BY category_id;
```

### Exemplo de AVG() com GROUP BY

```sql
-- Calcula o preço médio unitário de produtos em cada categoria
SELECT category_id, AVG(unit_price) AS preco_medio
FROM products
GROUP BY category_id;
```

### Exemplo de SUM() com GROUP BY

```sql
-- Calcula a quantidade total de produtos pedidos por pedido
SELECT order_id, SUM(quantity) AS quantidade_total_por_pedido
FROM order_details
GROUP BY order_id;
```

* **Desafio**

1. Obter todas as colunas das tabelas Clientes, Pedidos e Fornecedores

```sql
SELECT * FROM customers;
SELECT * FROM orders;
SELECT * FROM suppliers;
```

2. Obter todos os Clientes em ordem alfabética por país e nome

```sql
SELECT *
FROM customers
ORDER BY country, contact_name;
```

3. Obter os 5 pedidos mais antigos

```sql
SELECT * 
FROM orders 
ORDER BY order_date
LIMIT 5;
```

4. Obter a contagem de todos os Pedidos feitos durante 1997

```sql
SELECT COUNT(*) AS "Number of Orders During 1997"
FROM orders
WHERE order_date BETWEEN '1997-1-1' AND '1997-12-31';
```

5. Obter os nomes de todas as pessoas de contato onde a pessoa é um gerente, em ordem alfabética

```sql
SELECT contact_name
FROM customers
WHERE contact_title LIKE '%Manager%'
ORDER BY contact_name;
```

6. Obter todos os pedidos feitos em 19 de maio de 1997

```sql
SELECT *
FROM orders
WHERE order_date = '1997-05-19';
```

================================================
File: /Bootcamp - SQL e Analytics/Aula-02/desafio.sql
================================================
-- 1. Obter todas as colunas das tabelas Clientes, Pedidos e Fornecedores

-- 2. Obter todos os Clientes em ordem alfabética por país e nome

-- 3. Obter os 5 pedidos mais antigos

-- 4. Obter a contagem de todos os Pedidos feitos durante 1997

-- 5. Obter os nomes de todas as pessoas de contato onde a pessoa é um gerente, em ordem alfabética

-- 6. Obter todos os pedidos feitos em 19 de maio de 1997


================================================
File: /Bootcamp - SQL e Analytics/Aula-03/README.md
================================================
# Aula 03 - SQL para Analytics: Join and Having in SQL

## Introdução aos Joins em SQL

Joins em SQL são fundamentais para combinar registros de duas ou mais tabelas em um banco de dados com base em uma condição comum, geralmente uma chave estrangeira. Essa técnica permite que dados relacionados, que são armazenados em tabelas separadas, sejam consultados juntos de forma eficiente e coerente. 

Os joins são essenciais para consultar dados complexos e para aplicações em que a normalização do banco de dados resulta em distribuição de informações por diversas tabelas.

Existem vários tipos de joins, cada um com seu uso específico dependendo das necessidades da consulta:

1. **Inner Join**: Retorna registros que têm correspondência em ambas as tabelas.
2. **Left Join (ou Left Outer Join)**: Retorna todos os registros da tabela esquerda e os registros correspondentes da tabela direita. Se não houver correspondência, os resultados da tabela direita terão valores `NULL`.
3. **Right Join (ou Right Outer Join)**: Retorna todos os registros da tabela direita e os registros correspondentes da tabela esquerda. Se não houver correspondência, os resultados da tabela esquerda terão valores `NULL`.
4. **Full Join (ou Full Outer Join)**: Retorna registros quando há uma correspondência em uma das tabelas. Se não houver correspondência, ainda assim, o resultado aparecerá com `NULL` nos campos da tabela sem correspondência.

### 1. Criar um relatório para todos os pedidos de 1996 e seus clientes

**Inner Join**

**Uso**: Utilizado quando você precisa de registros que têm correspondência exata em ambas as tabelas. 

**Exemplo Prático**: Se quisermos encontrar todos os pedidos de 1996 e os detalhes dos clientes que fizeram esses pedidos, usamos um Inner Join. Isso garante que só obteremos os pedidos que possuem um cliente correspondente e que foram feitos em 1996.

```sql
-- Cria um relatório para todos os pedidos de 1996 e seus clientes (152 linhas)
SELECT *
FROM orders o
INNER JOIN customers c ON o.customer_id = c.customer_id
WHERE EXTRACT(YEAR FROM o.order_date) = 1996; -- EXTRACT(part FROM date) part pode ser YEAR, MONTH, DAY, etc
```

Gustavo trouxe tambem essa implementacao

'WHERE DATE_PART('YEAR', o.order_date) = 1996'

No SQL server pode usar o 

'WHERE YEAR(o.order_date) = 1996'

No contexto da sua consulta, o uso de EXTRACT na cláusula WHERE serve especificamente para aplicar um filtro nos dados retornados pelo SELECT, garantindo que apenas registros do ano de 1996 sejam incluídos no conjunto de resultados. Este é um uso legítimo e comum de EXTRACT para manipulação de condições baseadas em datas dentro de cláusulas WHERE.

### 2. Criar um relatório que mostra o número de funcionários e clientes de cada cidade que tem funcionários

**Left Join**

**Uso**: Usado quando você quer todos os registros da primeira (esquerda) tabela, com os correspondentes da segunda (direita) tabela. Se não houver correspondência, a segunda tabela terá campos `NULL`. 

**Exemplo Prático**: Se precisarmos listar todas as cidades onde temos funcionários, e também queremos saber quantos clientes temos nessas cidades, mesmo que não haja clientes, usamos um Left Join.

```sql
-- Cria um relatório que mostra o número de funcionários e clientes de cada cidade que tem funcionários (5 linhas)
SELECT e.city AS cidade, 
       COUNT(DISTINCT e.employee_id) AS numero_de_funcionarios, 
       COUNT(DISTINCT c.customer_id) AS numero_de_clientes
FROM employees e 
LEFT JOIN customers c ON e.city = c.city
GROUP BY e.city
ORDER BY cidade;
```

### Exemplo de Resultados da Consulta

| cidade | numero_de_funcionarios | numero_de_clientes |
| --- | --- | --- |
| Kirkland | 1 | 1 |
| London | 4 | 6 |
| Redmond | 1 | 0 |
| Seattle | 2 | 1 |
| Tacoma | 1 | 0 |

### Descrição da Tabela

* **cidade**: O nome da cidade onde os funcionários e clientes estão localizados.
* **numero_de_funcionarios**: Contagem dos funcionários distintos nessa cidade. Este número vem diretamente da tabela `employees`.
* **numero_de_clientes**: Contagem dos clientes distintos que têm a mesma cidade que os funcionários. Se não houver clientes em uma cidade onde há funcionários, o número será 0.

### Explicação Detalhada

* **Kirkland**: Tem um equilíbrio entre o número de funcionários e clientes, com ambos os valores sendo 1. Isso indica uma correspondência direta entre locais de funcionários e clientes.
* **London**: Apresenta uma maior concentração tanto de funcionários quanto de clientes, com mais clientes (6) do que funcionários (4), indicando uma forte presença de ambos na cidade.
* **Redmond**: Tem 1 funcionário, mas nenhum cliente registrado nesta cidade, sugerindo que, embora a empresa tenha presença laboral aqui, não há clientes registrados.
* **Seattle**: Tem 2 funcionários e apenas 1 cliente, mostrando uma presença menor de clientes em relação aos funcionários.
* **Tacoma**: Similar a Redmond, tem funcionários (1) mas nenhum cliente, o que pode indicar uma área onde a empresa opera, mas ainda não estabeleceu uma base de clientes.

Essa análise é particularmente útil para entender como os recursos humanos da empresa (funcionários) estão distribuídos em relação à sua base de clientes em diferentes locais. Isso pode ajudar a identificar cidades onde a empresa pode precisar intensificar esforços de aquisição de clientes ou avaliar a eficácia de suas operações e estratégias de mercado locais.


### 3. Criar um relatório que mostra o número de funcionários e clientes de cada cidade que tem clientes

**Right Join**

**Uso**: É o inverso do Left Join e é menos comum. Usado quando queremos todos os registros da segunda (direita) tabela e os correspondentes da primeira (esquerda) tabela. 

**Exemplo Prático**: Para listar todas as cidades onde temos clientes, e também contar quantos funcionários temos nessas cidades, usamos um Right Join.

```sql
-- Cria um relatório que mostra o número de funcionários e clientes de cada cidade que tem clientes (69 linhas)
SELECT c.city AS cidade, 
       COUNT(DISTINCT c.customer_id) AS numero_de_clientes, 
       COUNT(DISTINCT e.employee_id) AS numero_de_funcionarios
FROM employees e 
RIGHT JOIN customers c ON e.city = c.city
GROUP BY c.city
ORDER BY cidade;
```

### Diferenças Principais do `RIGHT JOIN`

1. **Foco na Tabela à Direita**: Ao contrário do `LEFT JOIN` que foca na tabela à esquerda, o `RIGHT JOIN` garante que todos os registros da tabela à direita (neste caso, `customers`) estejam presentes no resultado. Se não houver correspondência na tabela à esquerda (`employees`), as colunas relacionadas desta tabela aparecerão como `NULL`.
    
2. **Exibição de Dados Não Correspondentes**: Como mostrado, o `RIGHT JOIN` pode exibir linhas onde não há correspondência na tabela à esquerda, o que é útil para identificar dados que estão apenas na tabela à direita. No contexto de um negócio, isso pode destacar áreas (ou dados) que requerem atenção, como clientes em locais onde a empresa não tem funcionários representados.
    
3. **Utilização Estratégica para Análise de Dados**: O `RIGHT JOIN` é menos comum que o `LEFT JOIN` porque muitas vezes as tabelas são organizadas de modo que a tabela mais importante (ou abrangente) seja colocada à esquerda da consulta. No entanto, o `RIGHT JOIN` é útil quando a tabela à direita é prioritária e queremos garantir que todos os seus registros sejam analisados.


### 4. Criar um relatório que mostra o número de funcionários e clientes de cada cidade

* **Análise Completa de Dados**: O `FULL JOIN` é útil quando você precisa de uma visão completa dos dados em duas tabelas relacionadas, especialmente para identificar onde os dados estão faltando em uma ou ambas as tabelas.
* **Relatórios Abrangentes**: Permite criar relatórios que mostram todas as possíveis relações entre duas tabelas, incluindo onde as relações não existem.
* **Análise de Lacunas de Dados**: Ajuda a identificar lacunas nos dados de ambas as tabelas simultaneamente, facilitando análises de cobertura e consistência entre conjuntos de dados.

**Full Join**

**Uso**: Utilizado quando queremos a união de Left Join e Right Join, mostrando todos os registros de ambas as tabelas, e preenchendo com `NULL` onde não há correspondência. 

**Exemplo Prático**: Para listar todas as cidades onde temos clientes ou funcionários, e contar ambos em cada cidade, usamos um Full Join.

```sql
-- Cria um relatório que mostra o número de funcionários e clientes de cada cidade (71 linhas)
SELECT
	COALESCE(e.city, c.city) AS cidade,
	COUNT(DISTINCT e.employee_id) AS numero_de_funcionarios,
	COUNT(DISTINCT c.customer_id) AS numero_de_clientes
FROM employees e 
FULL JOIN customers c ON e.city = c.city
GROUP BY e.city, c.city
ORDER BY cidade;
```

Esta consulta retorna uma lista de todas as cidades conhecidas por ambas as tabelas, junto com a contagem de funcionários e clientes em cada cidade. Aqui estão alguns cenários possíveis no resultado:

### Análise do Resultado

O resultado do `FULL JOIN` mostra:

* A maioria das cidades listadas tem clientes, mas não funcionários (indicado por "0" no número de funcionários).
* Em algumas cidades, como "Kirkland", "Redmond", "Seattle", e "Tacoma", há funcionários e/ou clientes, mostrando correspondência direta entre as tabelas.
* Notavelmente, em cidades como "London" e "Madrid", o número de clientes é significativamente maior do que o de funcionários, o que pode indicar centros de alta atividade de clientes sem uma proporção correspondente de suporte de funcionários.

### Observações Importantes

1. **Cidades com apenas Clientes**: A maioria das cidades no resultado possui clientes, mas não funcionários. Isso pode sugerir que a empresa tem uma ampla base de clientes geograficamente, mas uma distribuição mais limitada de sua força de trabalho.
    
2. **Cidades com Funcionários e sem Clientes**: Cidades como "Redmond" e "Tacoma" têm funcionários, mas nenhuma contagem de clientes listada, indicando que há operações da empresa sem correspondente atividade de clientes registrada nesses locais.
    
3. **Concentrações de Clientes e Funcionários**: Em cidades como "London", "Seattle", e "Sao Paulo", há uma concentração significativa de clientes e alguma presença de funcionários, sugerindo centros operacionais ou mercados importantes para a empresa.
    
4. **Ausência de Dados em Algumas Cidades**: Algumas cidades têm zero funcionários e clientes, indicando que pode haver um erro de dados, cidades listadas incorretamente, ou simplesmente que não há atividade de funcionários ou clientes registrados nesses locais.
    

### Implicações Estratégicas

A partir desses dados, a empresa poderia considerar várias ações estratégicas:

* **Expansão de Funcionários**: Investir em recursos humanos nas cidades com altos números de clientes, mas baixa presença de funcionários, para melhorar o suporte e a satisfação do cliente.
    
* **Análise de Mercado**: Realizar uma análise mais aprofundada sobre por que certas cidades têm alta atividade de clientes e ajustar as estratégias de marketing e vendas conforme necessário.
    
* **Revisão de Dados**: Verificar a precisão dos dados para entender melhor as discrepâncias ou ausências nas contagens de funcionários e clientes.
    
Este exemplo realça o valor de usar `FULL JOIN` para obter uma visão completa da relação entre duas variáveis críticas (funcionários e clientes) e como essa informação pode ser usada para insights estratégicos.

## Having

### 1. Criar um relatório que mostra a quantidade total de produtos (da tabela order_details)

```sql
-- Cria um relatório que mostra a quantidade total de produtos encomendados.
-- Mostra apenas registros para produtos para os quais a quantidade encomendada é menor que 200 (5 linhas)
SELECT o.product_id, p.product_name, SUM(o.quantity) AS quantidade_total
FROM order_details o
JOIN products p ON p.product_id = o.product_id
GROUP BY o.product_id, p.product_name
HAVING SUM(o.quantity) < 200
ORDER BY quantidade_total DESC;
```

### 2. Criar um relatório que mostra o total de pedidos por cliente desde 31 de dezembro de 1996

```sql
-- Cria um relatório que mostra o total de pedidos por cliente desde 31 de dezembro de 1996.
-- O relatório deve retornar apenas linhas para as quais o total de pedidos é maior que 15 (5 linhas)
SELECT customer_id, COUNT(order_id) AS total_de_pedidos
FROM orders
WHERE order_date > '1996-12-31'
GROUP BY customer_id
HAVING COUNT(order_id) > 15
ORDER BY total_de_pedidos;
```

### Explicação das Consultas Convertidas

**Consulta 1:**

* **Seleção e Junção**: A consulta seleciona o `product_id` e `product_name` da tabela `products` e junta com a tabela `order_details` pelo `product_id`.
* **Agrupamento e Filtragem**: Os dados são agrupados por `product_id` e `product_name`, e a função agregada `SUM(o.quantity)` calcula a quantidade total de cada produto encomendado. A cláusula `HAVING` é usada para filtrar produtos cuja quantidade total encomendada é menor que 200.

**Consulta 2:**

* **Filtragem de Data**: A consulta filtra os pedidos realizados após 31 de dezembro de 1996.
* **Agrupamento e Contagem**: Agrupa os pedidos pelo `customer_id` e conta o número de pedidos feitos por cada cliente usando `COUNT(order_id)`.
* **Filtragem de Resultados**: Utiliza a cláusula `HAVING` para incluir apenas os clientes que fizeram mais de 15 pedidos desde a data especificada.

================================================
File: /Bootcamp - SQL e Analytics/Aula-03/desafio.sql
================================================
-- 1. Cria um relatório para todos os pedidos de 1996 e seus clientes (152 linhas)

-- 2. Cria um relatório que mostra o número de funcionários e clientes de cada cidade que tem funcionários (5 linhas)

-- 3. Cria um relatório que mostra o número de funcionários e clientes de cada cidade que tem clientes (69 linhas)

-- 4.Cria um relatório que mostra o número de funcionários e clientes de cada cidade (71 linhas)

-- 5. Cria um relatório que mostra a quantidade total de produtos encomendados.
-- Mostra apenas registros para produtos para os quais a quantidade encomendada é menor que 200 (5 linhas)

-- 6. Cria um relatório que mostra o total de pedidos por cliente desde 31 de dezembro de 1996.
-- O relatório deve retornar apenas linhas para as quais o total de pedidos é maior que 15 (5 linhas)


================================================
File: /Bootcamp - SQL e Analytics/Aula-04/README.md
================================================
# Aula 04 - Windows Function

Documentação Postgres:  https://www.postgresql.org/docs/current/functions-window.html

## Com o que vimos até aqui: Group By

Com SQL que vimos até agora conseguimos dois tipos de resultado: todas as linhas (com ou sem filtro/where) ou linhas agrupadas (group by)

Cálcular:
Quantos produtos únicos existem?
Quantos produtos no total?
Qual é o valor total pago?

```sql
SELECT order_id,
       COUNT(order_id) AS unique_product,
       SUM(quantity) AS total_quantity,
       SUM(unit_price * quantity) AS total_price
FROM order_details
GROUP BY order_id
ORDER BY order_id;
```

## Com Windows Function

As `Windows Function` permitem uma análise de dados eficiente e precisa, ao possibilitar cálculos dentro de `partições ou linhas específicas`. Elas são cruciais para tarefas como classificação, agregação e análise de tendências em consultas SQL.

Essas funções são aplicadas a cada linha de um conjunto de resultados, e utilizam uma cláusula `OVER()` para determinar como cada linha é processada dentro de uma "janela", permitindo controle sobre o comportamento da função dentro de um grupo de dados ordenados.

Window Functions Syntax componentes
```sql
window_function_name(arg1, arg2, ...) OVER (
  [PARTITION BY partition_expression, ...]
  [ORDER BY sort_expression [ASC | DESC], ...]
)
```

* **window_function_name**: Este é o nome da função de janela que você deseja usar, como SUM, RANK, LEAD, etc.

* **arg1, arg2, ...:** Estes são os argumentos que você passa para a função de janela, se ela exigir algum. Por exemplo, para a função SUM, você especificaria a coluna que deseja somar.

* **OVER**: Principal conceito das windows functions, ele que cria essa "Janela" onde fazem nossos cálculos

* **PARTITION BY:** Esta cláusula opcional divide o conjunto de resultados em partições ou grupos. A função de janela opera independentemente dentro de cada partição.

* **ORDER BY:** Esta cláusula opcional especifica a ordem em que as linhas são processadas dentro de cada partição. Você pode especificar a ordem ascendente (ASC) ou descendente (DESC).

```sql
SELECT DISTINCT order_id,
   COUNT(order_id) OVER (PARTITION BY order_id) AS unique_product,
   SUM(quantity) OVER (PARTITION BY order_id) AS total_quantity,
   SUM(unit_price * quantity) OVER (PARTITION BY order_id) AS total_price
FROM order_details
ORDER BY order_id;
```

## MIN (), MAX (), AVG ()

Quais são os valores mínimo, máximo e médio de frete pago por cada cliente? (tabela orders)

### Usando Group by

```sql
SELECT customer_id,
   MIN(freight) AS min_freight,
   MAX(freight) AS max_freight,
   AVG(freight) AS avg_freight
FROM orders
GROUP BY customer_id
ORDER BY customer_id;
```

### Detalhes da Consulta Ajustada:

* **`customer_id`**: Seleciona o identificador único do cliente da tabela `orders`.
* **`MIN(freight) AS min_freight`**: Calcula o valor mínimo de frete para cada cliente.
* **`MAX(freight) AS max_freight`**: Calcula o valor máximo de frete para cada cliente.
* **`AVG(freight) AS avg_freight`**: Calcula o valor médio de frete para cada cliente.

### Explicação:

* A função `MIN` extrai o menor valor de frete registrado para cada cliente.
* A função `MAX` obtém o maior valor de frete registrado para cada cliente.
* A função `AVG` fornece o valor médio de frete por cliente, útil para entender o custo médio de envio associado a cada um.
* `GROUP BY customer_id` agrupa os registros por `customer_id`, permitindo que as funções agregadas calculem seus resultados para cada grupo de cliente.
* `ORDER BY customer_id` garante que os resultados sejam apresentados em ordem crescente de `customer_id`, facilitando a leitura e a análise dos dados.

### Usando Windows Function

```sql
SELECT DISTINCT customer_id,
   MIN(freight) OVER (PARTITION BY customer_id) AS min_freight,
   MAX(freight) OVER (PARTITION BY customer_id) AS max_freight,
   AVG(freight) OVER (PARTITION BY customer_id) AS avg_freight
FROM orders
ORDER BY customer_id;
```

### Explicação da Consulta Ajustada:

* **`customer_id`**: Seleciona o identificador único do cliente da tabela `orders`.
* **`MIN(freight) OVER (PARTITION BY customer_id)`**: Utiliza a função de janela `MIN` para calcular o valor mínimo de frete para cada grupo de registros que têm o mesmo `customer_id`.
* **`MAX(freight) OVER (PARTITION BY customer_id)`**: Utiliza a função de janela `MAX` para calcular o valor máximo de frete para cada `customer_id`.
* **`AVG(freight) OVER (PARTITION BY customer_id)`**: Utiliza a função de janela `AVG` para calcular o valor médio de frete para cada `customer_id`.

### Características das Funções de Janela:

* **Funções de Janela (`OVER`)**: As funções de janela permitem que você execute cálculos sobre um conjunto de linhas relacionadas a cada entrada. Ao usar o `PARTITION BY customer_id`, a função de janela é reiniciada para cada novo `customer_id`. Isso significa que cada cálculo de `MIN`, `MAX`, e `AVG` é confinado ao conjunto de ordens de cada cliente individualmente.
* **`DISTINCT`**: A cláusula `DISTINCT` é utilizada para garantir que cada `customer_id` apareça apenas uma vez nos resultados finais, juntamente com seus respectivos valores de frete mínimo, máximo e médio. Isso é necessário porque as funções de janela calculam valores para cada linha, e sem `DISTINCT`, cada `customer_id` poderia aparecer múltiplas vezes se houver várias ordens por cliente.

## Colapso

Para ilustrar como a cláusula `GROUP BY` influencia os resultados de uma consulta SQL e por que ela pode "colapsar" as linhas para uma única linha por grupo, vou dar um exemplo baseado nas funções de agregação `MIN`, `MAX`, e `AVG` que discutimos anteriormente. Essas funções são frequentemente usadas para calcular estatísticas resumidas dentro de cada grupo especificado por `GROUP BY`.

### Exemplo sem GROUP BY

Considere a seguinte consulta sem usar `GROUP BY`:

```sql 
-- 830 linhas
SELECT customer_id, freight
FROM orders;
```

Essa consulta simplesmente seleciona o `customer_id` e o `freight` de cada ordem. Se houver múltiplas ordens para cada cliente, cada ordem aparecerá como uma linha separada no conjunto de resultados.

### Exemplo com GROUP BY

Agora, vamos adicionar `GROUP BY` e funções de agregação:

```sql
-- 89 linhas
SELECT customer_id,
       MIN(freight) AS min_freight,
       MAX(freight) AS max_freight,
       AVG(freight) AS avg_freight
FROM orders
GROUP BY customer_id
ORDER BY customer_id;
```

### O que acontece aqui:

* **`GROUP BY customer_id`**: Esta cláusula agrupa todas as entradas na tabela `orders` que têm o mesmo `customer_id`. Para cada grupo, a consulta calcula os valores mínimo, máximo e médio de `freight`.
    
* **Agregações (`MIN`, `MAX`, `AVG`)**: Cada uma dessas funções de agregação opera sobre o conjunto de `freight` dentro do grupo especificado pelo `customer_id`. Apenas um valor para cada função de agregação é retornado por grupo.
    

### Por que "colapsa" as linhas:

* Quando usamos `GROUP BY`, a consulta não retorna mais uma linha para cada entrada na tabela `orders`. Em vez disso, ela retorna uma linha para cada grupo de `customer_id`, onde cada linha contém o `customer_id` e os valores agregados de `freight` para esse grupo. Isso significa que se um cliente tem várias ordens, você não verá cada ordem individualmente; em vez disso, você verá uma linha resumida com as estatísticas de frete para todas as ordens desse cliente.

### Limitação do SELECT com GROUP BY:

* Se você tentar selecionar uma coluna que não está incluída na cláusula `GROUP BY` e que não é uma expressão agregada, a consulta falhará. Por exemplo, a consulta a seguir resultará em erro porque `order_date` não está em uma função agregada nem no `GROUP BY`:

```sql
SELECT customer_id, order_date, AVG(freight) AS avg_freight
FROM orders
GROUP BY customer_id;
```

### Mensagem de Erro Típica:

* Em muitos sistemas de gerenciamento de banco de dados, como PostgreSQL ou MySQL, essa consulta resultaria em um erro como: "column "orders.order_date" must appear in the GROUP BY clause or be used in an aggregate function".

Este exemplo mostra claramente como o `GROUP BY` "colapsa" as linhas em grupos, permitindo cálculos resumidos, mas também impõe restrições sobre quais colunas podem ser selecionadas diretamente.

Para ilustrar como evitar o "colapso" das linhas utilizando funções de janela (window functions) em vez de `GROUP BY`, vamos utilizar as mesmas estatísticas de frete (mínimo, máximo e médio) por cliente, mas manter todas as linhas de pedidos individuais visíveis no conjunto de resultados. As funções de janela permitem calcular agregações enquanto ainda se mantém cada linha distinta na saída.

### Consulta com Funções de Janela

Aqui está como você pode escrever uma consulta que utiliza funções de janela para calcular o frete mínimo, máximo e médio para cada cliente sem colapsar as linhas:

```sql
SELECT 
    customer_id,
    order_id,  -- Mantendo a visibilidade de cada pedido
    freight,
    MIN(freight) OVER (PARTITION BY customer_id) AS min_freight,
    MAX(freight) OVER (PARTITION BY customer_id) AS max_freight,
    AVG(freight) OVER (PARTITION BY customer_id) AS avg_freight
FROM orders
ORDER BY customer_id, order_id;
```

### Explicação da Consulta

* **Seleção de Colunas**: `customer_id`, `order_id`, e `freight` são selecionados diretamente, o que mantém cada linha de pedido individual visível no resultado.
* **Funções de Janela**: `MIN(freight) OVER`, `MAX(freight) OVER`, e `AVG(freight) OVER` são aplicadas com a cláusula `PARTITION BY customer_id`. Isso significa que as estatísticas de frete são calculadas para cada grupo de `customer_id`, mas a aplicação é feita sem agrupar as linhas em um único resultado por cliente. Cada linha no conjunto de resultados original mantém sua identidade única.
* **`PARTITION BY customer_id`**: Assegura que as funções de janela são recalculadas para cada cliente. Cada pedido mantém sua linha, mas agora também inclui as informações agregadas de frete específicas para o cliente ao qual o pedido pertence.
* **`ORDER BY customer_id, order_id`**: Ordena os resultados primeiro por `customer_id` e depois por `order_id`, facilitando a leitura dos dados.

### Vantagens das Funções de Janela

* **Preservação de Dados Detalhados**: Ao contrário do `GROUP BY`, que agrega e reduz os dados a uma linha por grupo, as funções de janela mantêm cada linha individual do conjunto de dados original visível. Isso é útil para análises detalhadas onde você precisa ver tanto os valores agregados quanto os dados de linha individual.
* **Flexibilidade**: Você pode calcular múltiplas métricas de agregação em diferentes partições dentro da mesma consulta sem múltiplas passagens pelos dados ou subconsultas complexas.

Este método é especialmente útil em relatórios e análises detalhadas onde tanto o contexto agregado quanto os detalhes de cada evento individual (neste caso, cada pedido) são importantes para uma compreensão completa dos dados.

Funções de classificação de janela no SQL são um conjunto de ferramentas valiosas usadas para atribuir classificações, posições ou números sequenciais às linhas dentro de um conjunto de resultados com base em critérios específicos.

Elas são aplicadas em vários cenários, como criar leaderboards, classificar produtos por vendas, identificar os melhores desempenhos ou acompanhar mudanças ao longo do tempo. Essas funções são ferramentas poderosas para obter insights e tomar decisões informadas na análise de dados.

### 2.1 RANK(), DENSE_RANK() e ROW_NUMBER()

* **RANK()**: Atribui um rank único a cada linha, deixando lacunas em caso de empates.
* **DENSE_RANK()**: Atribui um rank único a cada linha, com ranks contínuos para linhas empatadas.
* **ROW_NUMBER()**: Atribui um número inteiro sequencial único a cada linha, independentemente de empates, sem lacunas.

### Exemplo: Classificação dos produtos mais venvidos POR order ID

ex: o mesmo produto pode ficar em primeiro por ter vendido muito por ORDER e depois ficar em segundo por ter vendido muito por ORDER

```sql
SELECT  
  o.order_id, 
  p.product_name, 
  (o.unit_price * o.quantity) AS total_sale,
  ROW_NUMBER() OVER (ORDER BY (o.unit_price * o.quantity) DESC) AS order_rn, 
  RANK() OVER (ORDER BY (o.unit_price * o.quantity) DESC) AS order_rank, 
  DENSE_RANK() OVER (ORDER BY (o.unit_price * o.quantity) DESC) AS order_dense
FROM  
  order_details o
JOIN 
  products p ON p.product_id = o.product_id;
```

### Explicação da Consulta

* **Seleção de Dados**: A consulta seleciona o `order_id`, `product_name` da tabela `products`, e calcula `total_sale` como o produto de `unit_price` e `quantity` da tabela `order_details`.
    
* **Funções de Classificação**:
    
    * **`ROW_NUMBER()`**: Atribui um número sequencial a cada linha baseada no total de vendas (`total_sale`), ordenado do maior para o menor. Cada linha recebe um número único dentro do conjunto de resultados inteiro.
    * **`RANK()`**: Atribui um rank a cada linha baseado no `total_sale`, onde linhas com valores iguais recebem o mesmo rank, e o próximo rank disponível considera os empates (por exemplo, se dois itens compartilham o primeiro lugar, o próximo item será o terceiro).
    * **`DENSE_RANK()`**: Funciona de forma similar ao `RANK()`, mas os ranks subsequentes não têm lacunas. Se dois itens estão empatados no primeiro lugar, o próximo item será o segundo.
* **`JOIN`**: A junção entre `order_details` e `products` é feita pelo `product_id`, permitindo que o nome do produto seja incluído nos resultados baseados nos IDs correspondentes em ambas as tabelas.
    
## Este relatório apresenta o ID de cada pedido juntamente com o total de vendas e a classificação percentual e a distribuição cumulativa do valor de cada venda em relação ao valor total das vendas para o mesmo pedido. Esses cálculos são realizados com base no preço unitário e na quantidade de produtos vendidos em cada pedido.

### Exemplo: Classificação dos produtos mais venvidos usnado SUB QUERY

```sql
SELECT  
  sales.product_name, 
  total_sale,
  ROW_NUMBER() OVER (ORDER BY total_sale DESC) AS order_rn, 
  RANK() OVER (ORDER BY total_sale DESC) AS order_rank, 
  DENSE_RANK() OVER (ORDER BY total_sale DESC) AS order_dense
FROM (
  SELECT 
    p.product_name, 
    SUM(o.unit_price * o.quantity) AS total_sale
  FROM  
    order_details o
  JOIN 
    products p ON p.product_id = o.product_id
  GROUP BY p.product_name
) AS sales
ORDER BY sales.product_name;
```

### Utilidade da Consulta

Esta consulta é útil para análises de vendas, onde é necessário identificar os produtos mais vendidos, bem como sua classificação em termos de receita gerada. Ela permite que os analistas vejam rapidamente quais produtos geram mais receita e como eles se classificam em relação uns aos outros, facilitando decisões estratégicas relacionadas a estoque, promoções e planejamento de vendas.

### Funções PERCENT_RANK() e CUME_DIST()

Ambos retornam um valor entre 0 e 1

* **PERCENT_RANK()**: Calcula o rank relativo de uma linha específica dentro do conjunto de resultados como uma porcentagem. É computado usando a seguinte fórmula:
    * RANK é o rank da linha dentro do conjunto de resultados.
    * N é o número total de linhas no conjunto de resultados.
    * PERCENT_RANK = (RANK - 1) / (N - 1)
* **CUME_DIST()**: Calcula a distribuição acumulada de um valor no conjunto de resultados. Representa a proporção de linhas que são menores ou iguais à linha atual. A fórmula é a seguinte:
    * CUME_DIST = (Número de linhas com valores <= linha atual) / (Número total de linhas)

Ambas as funções PERCENT_RANK() e CUME_DIST() são valiosas para entender a distribuição e posição de pontos de dados dentro de um conjunto de dados, particularmente em cenários onde você deseja comparar a posição de um valor específico com a distribuição geral de dados.

```sql
SELECT  
  order_id, 
  unit_price * quantity AS total_sale,
  ROUND(CAST(PERCENT_RANK() OVER (PARTITION BY order_id 
    ORDER BY (unit_price * quantity) DESC) AS numeric), 2) AS order_percent_rank,
  ROUND(CAST(CUME_DIST() OVER (PARTITION BY order_id 
    ORDER BY (unit_price * quantity) DESC) AS numeric), 2) AS order_cume_dist
FROM  
  order_details;
```

### Explicação da Consulta Ajustada:

* **Seleção de Dados**: A consulta seleciona o `order_id` e calcula `total_sale` como o produto de `unit_price` e `quantity`.
* **Funções de Janela**:
    * **`PERCENT_RANK()`**: Aplicada com uma partição por `order_id` e ordenada pelo `total_sale` de forma descendente, calcula a posição percentual de cada venda em relação a todas as outras no mesmo pedido.
    * **`CUME_DIST()`**: Similarmente, calcula a distribuição acumulada das vendas, indicando a proporção de vendas que não excedem o `total_sale` da linha atual dentro de cada pedido.
* **Arredondamento**: Os resultados de `PERCENT_RANK()` e `CUME_DIST()` são arredondados para duas casas decimais para facilitar a interpretação.

Esta consulta é útil para análises detalhadas de desempenho de vendas dentro de pedidos, permitindo que gestores e analistas identifiquem rapidamente quais itens contribuem mais

A função NTILE() no SQL é usada para dividir o conjunto de resultados em um número especificado de partes aproximadamente iguais ou "faixas" e atribuir um número de grupo ou "bucket" a cada linha com base em sua posição dentro do conjunto de resultados ordenado.

```sql
NTILE(n) OVER (ORDER BY coluna)
```

* **n**: O número de faixas ou grupos que você deseja criar.
* **ORDER BY coluna**: A coluna pela qual você deseja ordenar o conjunto de resultados antes de aplicar a função NTILE().

### Exemplo: Listar funcionários dividindo-os em 3 grupos

```sql
SELECT first_name, last_name, title,
   NTILE(3) OVER (ORDER BY first_name) AS group_number
FROM employees;
```

### Explicação da Consulta Ajustada:

* **Seleção de Dados**: A consulta seleciona `first_name`, `last_name` e `title` da tabela `employees`.
* **NTILE(3) OVER (ORDER BY first_name)**: Aplica a função NTILE para dividir os funcionários em 3 grupos baseados na ordem alfabética de seus primeiros nomes. Cada funcionário receberá um número de grupo (`group_number`) que indica a qual dos três grupos ele pertence.

Esta consulta é útil para análises que requerem a distribuição equitativa dos dados em grupos especificados, como para balanceamento de cargas de trabalho, análises segmentadas, ou mesmo para fins de relatórios onde a divisão em grupos facilita a visualização e o entendimento dos dados.

LAG(), LEAD()

* **LAG()**: Permite acessar o valor da linha anterior dentro de um conjunto de resultados. Isso é particularmente útil para fazer comparações com a linha atual ou identificar tendências ao longo do tempo.
* **LEAD()**: Permite acessar o valor da próxima linha dentro de um conjunto de resultados, possibilitando comparações com a linha subsequente.

### Exemplo: Ordenando os custos de envio pagos pelos clientes de acordo com suas datas de pedido:

```sql
SELECT 
  customer_id, 
  TO_CHAR(order_date, 'YYYY-MM-DD') AS order_date, 
  shippers.company_name AS shipper_name, 
  LAG(freight) OVER (PARTITION BY customer_id ORDER BY order_date DESC) AS previous_order_freight, 
  freight AS order_freight, 
  LEAD(freight) OVER (PARTITION BY customer_id ORDER BY order_date DESC) AS next_order_freight
FROM 
  orders
JOIN 
  shippers ON shippers.shipper_id = orders.ship_via;
```

* **LEAD() e LAG(): Estas funções de janela são usadas para acessar dados de linhas anteriores ou subsequentes dentro de uma partição definida, muito úteis para comparar o valor de frete entre ordens consecutivas de um mesmo cliente.


================================================
File: /Bootcamp - SQL e Analytics/Aula-04/desafio.sql
================================================
-- Faça a classificação dos produtos mais venvidos usando usando RANK(), DENSE_RANK() e ROW_NUMBER()
-- Essa questão tem 2 implementações, veja uma que utiliza subquery e uma que não utiliza.
-- Tabelas utilizadasFROM order_details o JOIN products p ON p.product_id = o.product_id;

-- Listar funcionários dividindo-os em 3 grupos usando NTILE
-- FROM employees;

-- Ordenando os custos de envio pagos pelos clientes de acordo 
-- com suas datas de pedido, mostrando o custo anterior e o custo posterior usando LAG e LEAD:
-- FROM orders JOIN shippers ON shippers.shipper_id = orders.ship_via;

-- Desafio extra: questão intrevista Google
-- https://medium.com/@aggarwalakshima/interview-question-asked-by-google-and-difference-among-row-number-rank-and-dense-rank-4ca08f888486#:~:text=ROW_NUMBER()%20always%20provides%20unique,a%20continuous%20sequence%20of%20ranks.
-- https://platform.stratascratch.com/coding/10351-activity-rank?code_type=3
-- https://www.youtube.com/watch?v=db-qdlp8u3o

================================================
File: /Bootcamp - SQL e Analytics/Aula-05/README.md
================================================
## Aula 05 - Projeto de Análise de dados

Verificar o projeto completo no Github

https://github.com/lvgalvao/Northwind-SQL-Analytics/tree/main


================================================
File: /Bootcamp - SQL e Analytics/Aula-06/README.md
================================================
## Aula 06 - CTE vs Subqueries vs Views vs Temporary Tables vs Materialized Views

[Apostila completa](https://www.linkedin.com/feed/update/urn:li:activity:7190722950499577856/)

1. **CTE (Common Table Expressions):**
    
    * **Onde usar:** As CTEs são úteis quando você precisa dividir uma consulta em partes mais gerenciáveis ou quando deseja reutilizar uma subconsulta várias vezes na mesma consulta principal.
    * **Vantagens:**
        * Permitem escrever consultas mais legíveis e organizadas, dividindo a lógica em partes distintas.
        * Podem ser referenciadas várias vezes na mesma consulta.
    * **Desvantagens:**
        * Podem não ser tão eficientes quanto outras técnicas, especialmente se a CTE for referenciada várias vezes ou se a consulta for muito complexa.

    ```sql
    WITH TotalRevenues AS (
        SELECT 
            customers.company_name, 
            SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total
        FROM customers
        INNER JOIN orders ON customers.customer_id = orders.customer_id
        INNER JOIN order_details ON order_details.order_id = orders.order_id
        GROUP BY customers.company_name
    )
    SELECT * FROM TotalRevenues;
    ```

2. **Subqueries:**
    
    * **Onde usar:** Subqueries são úteis quando você precisa de resultados intermediários para filtrar ou agregar dados em uma consulta principal.
    * **Vantagens:**
        * São simples de escrever e entender, especialmente para consultas simples.
        * Podem ser aninhadas dentro de outras subqueries ou consultas principais.
    * **Desvantagens:**
        * Pode tornar consultas complexas difíceis de entender e manter.
        * Em algumas situações, podem não ser tão eficientes quanto outras técnicas, especialmente se as subqueries forem executadas várias vezes.

    ```sql
    SELECT product_id FROM (
	SELECT product_id 
	FROM (
		SELECT product_id, rank
		FROM (SELECT 
				product_id,
				SUM( det.quantity * det.unit_price * ( 1 - det.discount )) sold_value,
				RANK() OVER (ORDER BY SUM( det.quantity * det.unit_price * ( 1 - det.discount )) DESC) rank -- WINDOWS FUNCTION
			FROM order_details det
			GROUP BY det.product_id
			ORDER BY rank)
		WHERE rank <= 5 )
	WHERE product_id BETWEEN 35 and 65 )
    ORDER BY product_id DESC
    ```

    Refatorando a subquery acima para CTEs

    ```sql
    WITH CalculatedValues AS (
    -- Calcula o valor vendido e o rank para cada produto
    SELECT 
        product_id,
        SUM(det.quantity * det.unit_price * (1 - det.discount)) AS sold_value,
        RANK() OVER (ORDER BY SUM(det.quantity * det.unit_price * (1 - det.discount)) DESC) AS rank
    FROM order_details det
    GROUP BY product_id
    ),
    TopRankedProducts AS (
        -- Seleciona apenas os produtos com rank entre os top 5
        SELECT 
            product_id
        FROM CalculatedValues
        WHERE rank <= 5
    ),
    FilteredProducts AS (
        -- Filtra os produtos com IDs entre 35 e 65
        SELECT 
            product_id
        FROM TopRankedProducts
        WHERE product_id BETWEEN 35 AND 65
    )
    -- Seleciona e ordena os produtos finais
    SELECT product_id
    FROM FilteredProducts
    ORDER BY product_id DESC;
    ```

3. **Views:**
    
    * **Onde usar:** As views são úteis quando você precisa reutilizar uma consulta em várias consultas ou quando deseja simplificar consultas complexas dividindo-as em partes menores.
    * **Vantagens:**
        * Permitem abstrair a lógica de consulta complexa em um objeto de banco de dados reutilizável.
        * Facilitam a segurança, pois você pode conceder permissões de acesso à view em vez das tabelas subjacentes.
    * **Desvantagens:**
        * As views não armazenam dados fisicamente, então elas precisam ser reavaliadas sempre que são consultadas, o que pode impactar o desempenho.
        * Se uma view depende de outras views ou tabelas, a complexidade pode aumentar.

    ```sql
    CREATE VIEW TotalRevenues AS
    SELECT 
        customers.company_name, 
        SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total
    FROM customers
    INNER JOIN orders ON customers.customer_id = orders.customer_id
    INNER JOIN order_details ON order_details.order_id = orders.order_id
    GROUP BY customers.company_name;
    
    SELECT * FROM TotalRevenues;
    ```

    ```sql
    GRANT SELECT ON TotalRevenues TO user1;
    ```

4. **Temporary Tables / Staging / Testes ETL :**
    
    * **Onde usar:** Tabelas temporárias são úteis quando você precisa armazenar dados temporários para uso em uma sessão de banco de dados ou em uma consulta específica.
    * **Vantagens:**
        * Permitem armazenar resultados intermediários de uma consulta complexa para reutilização posterior.
        * Podem ser indexadas para melhorar o desempenho em consultas subsequentes.
    * **Desvantagens:**
        * Podem consumir recursos do banco de dados, especialmente se forem grandes.
        * Exigem gerenciamento explícito para limpar os dados após o uso.

    ```sql
    CREATE TEMP TABLE TempTotalRevenues AS
    SELECT 
        customers.company_name, 
        SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total
    FROM customers
    INNER JOIN orders ON customers.customer_id = orders.customer_id
    INNER JOIN order_details ON order_details.order_id = orders.order_id
    GROUP BY customers.company_name;
    
    SELECT * FROM TempTotalRevenues;
    ```

5. **Materialized Views / Snapshot:**

    Definição da Oracle: https://oracle-base.com/articles/misc/materialized-views
    
    * **Onde usar:** Materialized views são úteis quando você precisa pré-calcular e armazenar resultados de consultas complexas para consultas frequentes ou análises de desempenho.
    * **Vantagens:**
        * Permitem armazenar fisicamente os resultados de uma consulta, melhorando significativamente o desempenho em consultas subsequentes.
        * Reduzem a carga no banco de dados, já que os resultados são pré-calculados e armazenados.
    * **Desvantagens:**
        * **Precisam ser atualizadas** regularmente para manter os dados atualizados, o que pode consumir recursos do sistema.
        * A introdução de dados redundantes pode aumentar os requisitos de armazenamento.

    ```sql
    CREATE MATERIALIZED VIEW MaterializedTotalRevenues AS
    SELECT 
        customers.company_name, 
        SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total
    FROM customers
    INNER JOIN orders ON customers.customer_id = orders.customer_id
    INNER JOIN order_details ON order_details.order_id = orders.order_id
    GROUP BY customers.company_name;
    
    SELECT * FROM MaterializedTotalRevenues;
    ```

    ```sql
    REFRESH MATERIALIZED VIEW MaterializedTotalRevenues;
    ```

    ```sql
    REFRESH MATERIALIZED VIEW CONCURRENTLY MaterializedTotalRevenues
    ```

* **Performance**

```sql
WITH TotalRevenues AS (
    SELECT 
        customers.company_name, 
        SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total
    FROM customers
    INNER JOIN orders ON customers.customer_id = orders.customer_id
    INNER JOIN order_details ON order_details.order_id = orders.order_id
    CROSS JOIN products -- Junção cruzada com a tabela de produtos para aumentar a carga da consulta
    GROUP BY customers.company_name
)
SELECT * FROM TotalRevenues;
```

```sql
-- Criação da tabela temporária
CREATE TEMP TABLE TotalRevenues AS
SELECT 
    *
FROM customers
INNER JOIN orders ON customers.customer_id = orders.customer_id
INNER JOIN order_details ON order_details.order_id = orders.order_id
CROSS JOIN products; -- Junção cruzada com a tabela de produtos para aumentar a carga da consulta

-- Consulta na tabela temporária
SELECT * FROM TotalRevenues;
```


Em resumo, cada técnica tem seu lugar e uso apropriado, dependendo dos requisitos específicos de cada situação. As CTEs e subqueries são úteis para consultas simples ou interações temporárias com os dados, enquanto as views e as tabelas temporárias são mais adequadas para consultas e manipulações de dados mais complexas. As materialized views são ideais para consultas frequentes ou análises de desempenho, onde o desempenho é crucial e os dados podem ser pré-calculados e armazenados fisicamente.

* **Materialized view vs Table**

1. Armazenamento de Dados:
    
    * Tabela Normal: Armazena dados fisicamente no banco de dados.
    * Materialized View: Armazena os resultados de uma consulta como uma tabela física.
2. Atualização Automática:
    
    * Tabela Normal: Os dados são atualizados manual ou automaticamente através de operações de inserção, atualização e exclusão.
    * Materialized View: Os dados não são atualizados automaticamente. Eles precisam ser atualizados manualmente usando o comando `REFRESH MATERIALIZED VIEW`.
3. Desempenho:
    
    * Tabela Normal: As consultas são executadas diretamente nos dados armazenados na tabela.
    * Materialized View: As consultas são executadas nos dados armazenados na materialized view, o que pode melhorar o desempenho de consultas complexas ou frequentemente usadas.
4. Uso de Espaço em Disco:
    
    * Tabela Normal: Pode ocupar mais espaço em disco devido ao armazenamento físico de dados.
    * Materialized View: Pode ocupar menos espaço em disco, pois armazena apenas os resultados da consulta, não os dados brutos.
5. Flexibilidade:
    
    * Tabela Normal: Os dados são atualizados em tempo real e podem ser manipulados diretamente.
    * Materialized View: Os resultados da consulta são estáticos e precisam ser atualizados manualmente. Eles são usados principalmente para armazenar resultados de consultas complexas que não mudam com frequência.



* **DROP TEMP TABLE**

    https://www.youtube.com/watch?v=vKvnIa6S-nQ&t=3682s

    https://www.reddit.com/r/SQL/comments/tg4hei/sql_server_temporary_tables/

    https://www.theknowledgeacademy.com/blog/how-to-create-temp-table-in-sql/#:~:text=To%20add%20data%20to%20a%20Temp%20Table%2C%20you'll%20use,%2C%20column2%2C%20...)

    https://www.reddit.com/r/SQL/comments/tg4hei/sql_server_temporary_tables/

================================================
File: /Bootcamp - SQL e Analytics/Aula-06/table.sql
================================================
SELECT product_id,
    sum((((quantity)::double precision * unit_price) * ((1)::double precision - discount))) AS sold_value,
    rank() OVER (ORDER BY (sum((((quantity)::double precision * unit_price) * ((1)::double precision - discount)))) DESC) AS rank
   FROM order_details det
  GROUP BY product_id

================================================
File: /Bootcamp - SQL e Analytics/Aula-07/README.md
================================================
## Aula 07 - Stored Procedures

* **Stored Procedures vs Views**

As Views e Stored Procedures são ambos recursos poderosos em bancos de dados relacionais, mas têm propósitos e funcionalidades distintas.

**Views:**

* As Views são abstrações de consulta que permitem aos usuários definir consultas complexas e frequentemente usadas como uma única entidade.
* Elas são essencialmente consultas SQL pré-definidas que são armazenadas no banco de dados e tratadas como tabelas virtuais.
* As Views simplificam o acesso aos dados, ocultando a complexidade das consultas subjacentes e fornecendo uma interface consistente para os usuários.
* Elas são úteis para simplificar consultas frequentes, segmentar permissões de acesso aos dados e abstrair a complexidade do modelo de dados subjacente.

**Stored Procedures:**

* As Stored Procedures são abstrações de transações que consistem em um conjunto de instruções SQL pré-compiladas e armazenadas no banco de dados.
* Elas são usadas para encapsular operações de banco de dados mais complexas, como atualizações, inserções, exclusões e outras transações.
* As Stored Procedures podem aceitar parâmetros de entrada e retornar valores de saída, o que as torna altamente flexíveis e reutilizáveis em diferentes partes de um aplicativo.
* Elas oferecem maior controle sobre as operações de banco de dados e permitem a execução de lógica de negócios no lado do servidor.

## Criando um novo Banco de dados


Para ilustrar o processo, vamos estabelecer um novo banco de dados que simula um ambiente bancário convencional.

Começaremos criando um novo banco de dados. Em seguida, empregaremos os comandos `CREATE TABLE` e `INSERT INTO`.

```sql
CREATE TABLE IF NOT EXISTS clients (
    id SERIAL PRIMARY KEY NOT NULL,
    limite INTEGER NOT NULL,
    saldo INTEGER NOT NULL
);

CREATE TABLE IF NOT EXISTS transactions (
    id SERIAL PRIMARY KEY NOT NULL,
    tipo CHAR(1) NOT NULL,
    descricao VARCHAR(10) NOT NULL,
    valor INTEGER NOT NULL,
    cliente_id INTEGER NOT NULL,
    realizada_em TIMESTAMP NOT NULL DEFAULT NOW()
);
```

Caso queira usar UUID (para cenários de produção)

```
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE TABLE IF NOT EXISTS clients (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    limite INTEGER NOT NULL,
    saldo INTEGER NOT NULL
);
```


CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE TABLE IF NOT EXISTS clients (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    limite INTEGER NOT NULL,
    saldo INTEGER NOT NULL,
    CHECK (saldo >= limite)
);



**CREATE TABLE:**

* O comando `CREATE TABLE` é usado para criar uma nova tabela no banco de dados.
* O `IF NOT EXISTS` é uma cláusula opcional que garante que a tabela só será criada se ainda não existir no banco de dados, evitando erros caso a tabela já exista.
* Em seguida, é especificada o nome da tabela (`clients` e `transactions` neste caso), seguido por uma lista de colunas e suas definições.
* Cada coluna é definida com um nome, um tipo de dado e opcionalmente outras restrições, como a definição de uma chave primária (`PRIMARY KEY`) e a obrigatoriedade de não ser nula (`NOT NULL`).

```sql
INSERT INTO clients (limite, saldo)
VALUES
    (10000, 0),
    (80000, 0),
    (1000000, 0),
    (10000000, 0),
    (500000, 0);
```

**INSERT INTO:**

* O comando `INSERT INTO` é usado para adicionar novos registros a uma tabela existente.
* Na cláusula `INSERT INTO`, é especificado o nome da tabela (`clients` neste caso) seguido da lista de colunas entre parênteses, se necessário.
* Em seguida, a cláusula `VALUES` é usada para especificar os valores a serem inseridos nas colunas correspondentes.
* Cada linha de valores corresponde a um novo registro a ser inserido na tabela, com os valores na mesma ordem que as colunas foram listadas.

Em resumo, esses comandos são fundamentais para definir a estrutura e inserir dados em tabelas no banco de dados, criando assim a base para armazenar e manipular informações de forma organizada e eficiente.

## Vamos agora simular uma transação bancaria

Para realizar a compra de um Carro, de 80 mil reais, no cliente 1.

Vamos realizar esse processo em 2 etapas.

A primeira será um comando de `INSERT INTO` e depois um comando de `UPDATE`

```sql
INSERT INTO transactions (tipo, descricao, valor, cliente_id)
VALUES ('d', 'Compra de carro', 80000, 1)
```

```sql
UPDATE clients
SET saldo = saldo + CASE WHEN 'd' = 'd' THEN -80000 ELSE 80000 END
WHERE id = 1; -- Substitua pelo ID do cliente desejado
```

* **Vamos olhar a situação do cliente 1 agora**

```sql
SELECT saldo, limite 
FROM clients
WHERE id = 1
```

## Vamos precisar corrigir isso

O comando `DELETE` é uma instrução do SQL usada para remover registros de uma tabela com base em uma condição específica. Ele permite que você exclua dados de uma tabela de banco de dados de forma controlada e precisa. Aqui estão alguns pontos-chave sobre o comando `DELETE`:

1. **Sintaxe Básica**: A sintaxe básica do comando `DELETE` é a seguinte:
    
    ```sql
    DELETE FROM nome_da_tabela
    WHERE condição;
    ```
    
2. **Cláusula WHERE**: A cláusula `WHERE` é opcional, mas geralmente é usada para especificar quais registros devem ser excluídos. Se não for especificada, todos os registros da tabela serão excluídos.
    
3. **Remoção Condicional**: Você pode usar a cláusula `WHERE` para definir uma condição para determinar quais registros serão excluídos. Apenas os registros que atendem a essa condição serão removidos.
    
4. **Impacto da Exclusão**: O comando `DELETE` remove permanentemente os registros da tabela, o que significa que os dados excluídos não podem ser recuperados.
    
5. **Uso Cauteloso**: É importante usar o comando `DELETE` com cuidado, especialmente sem uma cláusula `WHERE` específica, pois ele pode resultar na exclusão de todos os registros da tabela.
    
6. **Transações**: Assim como outros comandos SQL de modificação de dados, como `INSERT` e `UPDATE`, o comando `DELETE` pode ser usado dentro de transações para garantir a consistência e a integridade dos dados.
    

No exemplo que você forneceu:

```sql
DELETE FROM transactions
WHERE id = 1;
```

Este comando remove o registro da tabela `transactions` onde o `id` é igual a `1`. Isso resultará na exclusão permanente desse registro específico da tabela. Certifique-se sempre de usar o comando `DELETE` com cuidado e de verificar duas vezes a condição antes de executá-lo para evitar a exclusão acidental de dados importantes.

```sql
DELETE FROM transactions
WHERE id = 1;
```

Vamos precisar voltar também com o saldo atual do cliente, que era de 0.

```sql
UPDATE clients
SET saldo = 0
WHERE id = 1;
```

## Como evitar isso? Stored Procedures

Stored Procedures são rotinas armazenadas no banco de dados que contêm uma série de instruções SQL e podem ser executadas por aplicativos ou usuários conectados ao banco de dados. Elas oferecem várias vantagens, como:

1. **Reutilização de código:** As stored procedures permitem que blocos de código SQL sejam escritos uma vez e reutilizados em várias partes do aplicativo.
    
2. **Desempenho:** Por serem compiladas e armazenadas no banco de dados, as stored procedures podem ser executadas de forma mais eficiente do que várias consultas SQL enviadas separadamente pelo aplicativo.
    
3. **Segurança:** As stored procedures podem ajudar a proteger o banco de dados, pois os aplicativos só precisam de permissão para executar a stored procedure, não para acessar diretamente as tabelas.
    
4. **Abstração de dados:** Elas podem ser usadas para ocultar a complexidade do modelo de dados subjacente, fornecendo uma interface simplificada para os usuários ou aplicativos.
    
5. **Controle de transações:** As stored procedures podem incluir instruções de controle de transações para garantir a integridade dos dados durante operações complexas.

Vamos entender cada parte da stored procedure `realizar_transacao`:

1. **Criação da Procedure:**
    
    ```sql
    CREATE OR REPLACE PROCEDURE realizar_transacao(
        IN p_tipo CHAR(1),
        IN p_descricao VARCHAR(10),
        IN p_valor INTEGER,
        IN p_cliente_id INTEGER
    )
    ```
    
    * Esta declaração cria ou substitui uma stored procedure chamada `realizar_transacao`.
    * A procedure tem quatro parâmetros de entrada: `p_tipo`, `p_descricao`, `p_valor` e `p_cliente_id`, cada um com seu tipo de dado especificado.
2. **Definição da Linguagem:**
    
    Sobre a languages na documentação do postgresql tem 4 linguagens padrões  disponíveis: PL/pgSQL (Chapter 43), PL/Tcl (Chapter 44), PL/Perl (Chapter 45), and PL/Python (Chapter 46)

    ```sql
    LANGUAGE plpgsql
    ```
    
    * Define a linguagem da stored procedure como PL/pgSQL, que é uma linguagem procedural para o PostgreSQL.
3. **Corpo da Procedure:**
    
    ```sql
    AS $$
    DECLARE
        saldo_atual INTEGER;
        limite_cliente INTEGER;
    BEGIN
        -- Corpo da procedure...
    END;
    $$;
    ```
    
    * O corpo da stored procedure é definido entre `AS $$` e `$$;`.
    * Dentro do corpo, declaramos variáveis locais usando `DECLARE`.
    * A execução da procedure ocorre entre `BEGIN` e `END;`.
4. **Obtenção de Dados:**
    
    ```sql
    -- Obtém o saldo atual e o limite do cliente
    SELECT saldo, limite INTO saldo_atual, limite_cliente
    FROM clients
    WHERE id = p_cliente_id;
    ```
    
    * Esta parte do código executa uma consulta para obter o saldo atual e o limite do cliente com o ID fornecido.
5. **Verificação da Transação:**
    
    ```sql
    -- Verifica se a transação é válida com base no saldo e no limite
    IF p_tipo = 'd' AND saldo_atual - p_valor < -limite_cliente THEN
        RAISE EXCEPTION 'Saldo insuficiente para realizar a transação';
    END IF;
    ```
    
    * Aqui, é feita uma verificação para garantir que a transação seja válida com base no tipo de transação ('d' para débito) e se o saldo atual menos o valor da transação é menor que o limite de crédito do cliente. Se a condição for verdadeira, uma exceção é lançada.
6. **Atualização do Saldo:**
    
    ```sql
    -- Atualiza o saldo do cliente
    UPDATE clients
    SET saldo = saldo + CASE WHEN p_tipo = 'd' THEN -p_valor ELSE p_valor END
    WHERE id = p_cliente_id;
    ```
    
    * Nesta parte, o saldo do cliente é atualizado com base no tipo de transação. Se for um débito ('d'), o valor é subtraído do saldo atual; caso contrário, é adicionado.
7. **Inserção da Transação:**
    
    ```sql
    -- Insere uma nova transação
    INSERT INTO transactions (tipo, descricao, valor, cliente_id)
    VALUES (p_tipo, p_descricao, p_valor, p_cliente_id);
    ```
    
    * Por fim, uma nova transação é inserida na tabela `transactions` com os detalhes fornecidos.

Essa stored procedure encapsula todo o processo de realização de uma transação bancária, desde a validação do saldo e limite do cliente até a atualização do saldo e a inserção da transação. Ela oferece uma maneira conveniente e segura de executar essas operações de forma consistente e controlada.

Para chamar a stored procedure `realizar_transacao` com os parâmetros fornecidos, você pode executar o seguinte comando SQL no PostgreSQL:

```sql
CALL realizar_transacao('d', 'carro', 80000, 1);
```

Isso invocará a procedure `realizar_transacao` com os parâmetros fornecidos:

* `p_tipo`: 'd'
* `p_descricao`: 'carro'
* `p_valor`: 80000
* `p_cliente_id`: 1

Certifique-se de executar esse comando em um ambiente onde a stored procedure `realizar_transacao` esteja definida e acessível.

## Desafio

Criar uma stored procedure "ver_extrato" para fornecer uma visão detalhada do extrato bancário de um cliente, incluindo seu saldo atual e as informações das últimas 10 transações realizadas. Esta operação recebe como entrada o ID do cliente e retorna uma mensagem com o saldo atual do cliente e uma lista das últimas 10 transações, contendo o ID da transação, o tipo de transação (depósito ou retirada), uma breve descrição, o valor da transação e a data em que foi realizada.

**Explicação Detalhada:**

1. **Entrada de Parâmetros:**
    
    * A stored procedure recebe o ID do cliente como parâmetro de entrada.
2. **Obtenção do Saldo Atual:**
    
    * É realizada uma consulta na tabela "clients" para obter o saldo atual do cliente com base no ID fornecido.
3. **Exibição do Saldo Atual:**
    
    * O saldo atual do cliente é exibido por meio de uma mensagem de aviso.
4. **Obtenção das Últimas 10 Transações:**
    
    * É realizada uma consulta na tabela "transactions" para obter as últimas 10 transações do cliente, ordenadas pela data de realização em ordem decrescente.
5. **Exibição das Transações:**
    
    * Utilizando um loop `FOR`, cada transação é iterada e suas informações são exibidas por meio de mensagens de aviso.
    * Para cada transação, são exibidos o ID da transação, o tipo de transação (depósito ou retirada), uma breve descrição da transação, o valor da transação e a data em que foi realizada.
    * O loop é interrompido após exibir as informações das últimas 10 transações.

```sql
CREATE OR REPLACE PROCEDURE ver_extrato(
    IN p_cliente_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    saldo_atual INTEGER;
    transacao RECORD;
    contador INTEGER := 0;
BEGIN
    -- Obtém o saldo atual do cliente
    SELECT saldo INTO saldo_atual
    FROM clients
    WHERE id = p_cliente_id;

    -- Retorna o saldo atual do cliente
    RAISE NOTICE 'Saldo atual do cliente: %', saldo_atual;

    -- Retorna as 10 últimas transações do cliente
    RAISE NOTICE 'Últimas 10 transações do cliente:';
    FOR transacao IN
        SELECT *
        FROM transactions
        WHERE cliente_id = p_cliente_id
        ORDER BY realizada_em DESC
        LIMIT 10
    LOOP
        contador := contador + 1;
        RAISE NOTICE 'ID: %, Tipo: %, Descrição: %, Valor: %, Data: %', transacao.id, transacao.tipo, transacao.descricao, transacao.valor, transacao.realizada_em;
        EXIT WHEN contador >= 10;
    END LOOP;
END;
$$;
```

================================================
File: /Bootcamp - SQL e Analytics/Aula-07/create_table.sql
================================================
CREATE TABLE IF NOT EXISTS clients (
    id SERIAL PRIMARY KEY NOT NULL,
    limite INTEGER NOT NULL,
    saldo INTEGER NOT NULL
);

CREATE TABLE IF NOT EXISTS transactions (
    id SERIAL PRIMARY KEY NOT NULL,
    tipo CHAR(1) NOT NULL,
    descricao VARCHAR(10) NOT NULL,
    valor INTEGER NOT NULL,
    cliente_id INTEGER NOT NULL,
    realizada_em TIMESTAMP NOT NULL DEFAULT NOW()
);

INSERT INTO clients (limite, saldo)
VALUES
    (100000, 0),
    (80000, 0),
    (1000000, 0),
    (10000000, 0),
    (500000, 0);



================================================
File: /Bootcamp - SQL e Analytics/Aula-07/python.py
================================================
@app.post("/clientes/{cliente_id}/transacoes", response_model=schemas.ClienteResponse)
async def post_transacao(cliente_id: int, 
                         transacao: schemas.TransactionCreateRequest, 
                         session: AsyncSession = Depends(get_session)):
    
    result = await session.execute(" CALL realizar_transacao(transacao.type, transacao.description, transacao.value, cliente_id)")

    return cliente

================================================
File: /Bootcamp - SQL e Analytics/Aula-07/store_procedure.sql
================================================
CREATE OR REPLACE PROCEDURE ver_extrato(
    IN p_cliente_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    saldo_atual INTEGER;
    transacao RECORD;
    contador INTEGER := 0;
BEGIN
    -- Obtém o saldo atual do cliente
    SELECT saldo INTO saldo_atual
    FROM clients
    WHERE id = p_cliente_id;

    -- Retorna o saldo atual do cliente
    RAISE NOTICE 'Saldo atual do cliente: %', saldo_atual;

    -- Retorna as 10 últimas transações do cliente
    RAISE NOTICE 'Últimas 10 transações do cliente:';
    FOR transacao IN
        SELECT *
        FROM transactions
        WHERE cliente_id = p_cliente_id
        ORDER BY realizada_em DESC
        LIMIT 10
    LOOP
        contador := contador + 1;
        RAISE NOTICE 'ID: %, Tipo: %, Descrição: %, Valor: %, Data: %', transacao.id, transacao.tipo, transacao.descricao, transacao.valor, transacao.realizada_em;
        EXIT WHEN contador >= 10;
    END LOOP;
END;
$$;


================================================
File: /Bootcamp - SQL e Analytics/Aula-07/store_procedure_com_saldo_transacao.sql
================================================
CREATE OR REPLACE PROCEDURE realizar_transacao(
IN p_tipo CHAT(1),
IN p_descricao VARCHAR(10),
IN p_valor INTEGER,
IN p_cliente_id UUID
)
LANGUAGE plpgsql
AS $$
DECLARE
    saldo_atual INTEGER;
    limite_cliente INTEGER;
	saldo_apos_transacao INTEGER;
BEGIN
    SELECT saldo, limite INTO saldo_atual, limite_cliente
	FROM clients
	WHERE id = p_cliente_id;
	
	RAISE NOTICE 'Saldo atual do cliente: %', saldo_atual;
	RAISE NOTICE 'Limite atual do cliente: %', limite_cliente;
	
	IF p_tipo = 'd' AND saldo_atual - p_valor < - limite_cliente THEN
		RAISE EXCEPTION 'Limite inferior ao necessario da transacao'
	END IF;
	
	UPDATE clients
    SET saldo = saldo + CASE WHEN p_tipo = 'd' THEN -p_valor ELSE p_valor END
    WHERE id = p_cliente_id;

    INSERT INTO transactions (tipo, descricao, valor, cliente_id)
    VALUES (p_tipo, p_descricao, p_valor, p_cliente_id);
	
	SELECT saldo INTO saldo_apos_transacao
	FROM clients
	WHERE id = p_cliente_id;
	
	RAISE NOTICE 'Saldo cliente apos transacao: %', saldo_apos_transacao;
END;
$$;

================================================
File: /Bootcamp - SQL e Analytics/Aula-08/README.md
================================================
## Aula 07 - Stored Procedures

* **Stored Procedures vs Views**

As Views e Stored Procedures são ambos recursos poderosos em bancos de dados relacionais, mas têm propósitos e funcionalidades distintas.

**Views:**

* As Views são abstrações de consulta que permitem aos usuários definir consultas complexas e frequentemente usadas como uma única entidade.
* Elas são essencialmente consultas SQL pré-definidas que são armazenadas no banco de dados e tratadas como tabelas virtuais.
* As Views simplificam o acesso aos dados, ocultando a complexidade das consultas subjacentes e fornecendo uma interface consistente para os usuários.
* Elas são úteis para simplificar consultas frequentes, segmentar permissões de acesso aos dados e abstrair a complexidade do modelo de dados subjacente.

**Stored Procedures:**

* As Stored Procedures são abstrações de transações que consistem em um conjunto de instruções SQL pré-compiladas e armazenadas no banco de dados.
* Elas são usadas para encapsular operações de banco de dados mais complexas, como atualizações, inserções, exclusões e outras transações.
* As Stored Procedures podem aceitar parâmetros de entrada e retornar valores de saída, o que as torna altamente flexíveis e reutilizáveis em diferentes partes de um aplicativo.
* Elas oferecem maior controle sobre as operações de banco de dados e permitem a execução de lógica de negócios no lado do servidor.

## Criando um novo Banco de dados


Para ilustrar o processo, vamos estabelecer um novo banco de dados que simula um ambiente bancário convencional.

Começaremos criando um novo banco de dados. Em seguida, empregaremos os comandos `CREATE TABLE` e `INSERT INTO`.

```sql
CREATE TABLE IF NOT EXISTS clients (
    id SERIAL PRIMARY KEY NOT NULL,
    limite INTEGER NOT NULL,
    saldo INTEGER NOT NULL
);

CREATE TABLE IF NOT EXISTS transactions (
    id SERIAL PRIMARY KEY NOT NULL,
    tipo CHAR(1) NOT NULL,
    descricao VARCHAR(10) NOT NULL,
    valor INTEGER NOT NULL,
    cliente_id INTEGER NOT NULL,
    realizada_em TIMESTAMP NOT NULL DEFAULT NOW()
);
```

Caso queira usar UUID (para cenários de produção)

```
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE TABLE IF NOT EXISTS clients (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    limite INTEGER NOT NULL,
    saldo INTEGER NOT NULL
);
```


CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE TABLE IF NOT EXISTS clients (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    limite INTEGER NOT NULL,
    saldo INTEGER NOT NULL,
    CHECK (saldo >= limite)
);



**CREATE TABLE:**

* O comando `CREATE TABLE` é usado para criar uma nova tabela no banco de dados.
* O `IF NOT EXISTS` é uma cláusula opcional que garante que a tabela só será criada se ainda não existir no banco de dados, evitando erros caso a tabela já exista.
* Em seguida, é especificada o nome da tabela (`clients` e `transactions` neste caso), seguido por uma lista de colunas e suas definições.
* Cada coluna é definida com um nome, um tipo de dado e opcionalmente outras restrições, como a definição de uma chave primária (`PRIMARY KEY`) e a obrigatoriedade de não ser nula (`NOT NULL`).

```sql
INSERT INTO clients (limite, saldo)
VALUES
    (10000, 0),
    (80000, 0),
    (1000000, 0),
    (10000000, 0),
    (500000, 0);
```

**INSERT INTO:**

* O comando `INSERT INTO` é usado para adicionar novos registros a uma tabela existente.
* Na cláusula `INSERT INTO`, é especificado o nome da tabela (`clients` neste caso) seguido da lista de colunas entre parênteses, se necessário.
* Em seguida, a cláusula `VALUES` é usada para especificar os valores a serem inseridos nas colunas correspondentes.
* Cada linha de valores corresponde a um novo registro a ser inserido na tabela, com os valores na mesma ordem que as colunas foram listadas.

Em resumo, esses comandos são fundamentais para definir a estrutura e inserir dados em tabelas no banco de dados, criando assim a base para armazenar e manipular informações de forma organizada e eficiente.

## Vamos agora simular uma transação bancaria

Para realizar a compra de um Carro, de 80 mil reais, no cliente 1.

Vamos realizar esse processo em 2 etapas.

A primeira será um comando de `INSERT INTO` e depois um comando de `UPDATE`

```sql
INSERT INTO transactions (tipo, descricao, valor, cliente_id)
VALUES ('d', 'Compra de carro', 80000, 1)
```

```sql
UPDATE clients
SET saldo = saldo + CASE WHEN 'd' = 'd' THEN -80000 ELSE 80000 END
WHERE id = 1; -- Substitua pelo ID do cliente desejado
```

* **Vamos olhar a situação do cliente 1 agora**

```sql
SELECT saldo, limite 
FROM clients
WHERE id = 1
```

## Vamos precisar corrigir isso

O comando `DELETE` é uma instrução do SQL usada para remover registros de uma tabela com base em uma condição específica. Ele permite que você exclua dados de uma tabela de banco de dados de forma controlada e precisa. Aqui estão alguns pontos-chave sobre o comando `DELETE`:

1. **Sintaxe Básica**: A sintaxe básica do comando `DELETE` é a seguinte:
    
    ```sql
    DELETE FROM nome_da_tabela
    WHERE condição;
    ```
    
2. **Cláusula WHERE**: A cláusula `WHERE` é opcional, mas geralmente é usada para especificar quais registros devem ser excluídos. Se não for especificada, todos os registros da tabela serão excluídos.
    
3. **Remoção Condicional**: Você pode usar a cláusula `WHERE` para definir uma condição para determinar quais registros serão excluídos. Apenas os registros que atendem a essa condição serão removidos.
    
4. **Impacto da Exclusão**: O comando `DELETE` remove permanentemente os registros da tabela, o que significa que os dados excluídos não podem ser recuperados.
    
5. **Uso Cauteloso**: É importante usar o comando `DELETE` com cuidado, especialmente sem uma cláusula `WHERE` específica, pois ele pode resultar na exclusão de todos os registros da tabela.
    
6. **Transações**: Assim como outros comandos SQL de modificação de dados, como `INSERT` e `UPDATE`, o comando `DELETE` pode ser usado dentro de transações para garantir a consistência e a integridade dos dados.
    

No exemplo que você forneceu:

```sql
DELETE FROM transactions
WHERE id = 1;
```

Este comando remove o registro da tabela `transactions` onde o `id` é igual a `1`. Isso resultará na exclusão permanente desse registro específico da tabela. Certifique-se sempre de usar o comando `DELETE` com cuidado e de verificar duas vezes a condição antes de executá-lo para evitar a exclusão acidental de dados importantes.

```sql
DELETE FROM transactions
WHERE id = 1;
```

Vamos precisar voltar também com o saldo atual do cliente, que era de 0.

```sql
UPDATE clients
SET saldo = 0
WHERE id = 1;
```

## Como evitar isso? Stored Procedures

Stored Procedures são rotinas armazenadas no banco de dados que contêm uma série de instruções SQL e podem ser executadas por aplicativos ou usuários conectados ao banco de dados. Elas oferecem várias vantagens, como:

1. **Reutilização de código:** As stored procedures permitem que blocos de código SQL sejam escritos uma vez e reutilizados em várias partes do aplicativo.
    
2. **Desempenho:** Por serem compiladas e armazenadas no banco de dados, as stored procedures podem ser executadas de forma mais eficiente do que várias consultas SQL enviadas separadamente pelo aplicativo.
    
3. **Segurança:** As stored procedures podem ajudar a proteger o banco de dados, pois os aplicativos só precisam de permissão para executar a stored procedure, não para acessar diretamente as tabelas.
    
4. **Abstração de dados:** Elas podem ser usadas para ocultar a complexidade do modelo de dados subjacente, fornecendo uma interface simplificada para os usuários ou aplicativos.
    
5. **Controle de transações:** As stored procedures podem incluir instruções de controle de transações para garantir a integridade dos dados durante operações complexas.

Vamos entender cada parte da stored procedure `realizar_transacao`:

1. **Criação da Procedure:**
    
    ```sql
    CREATE OR REPLACE PROCEDURE realizar_transacao(
        IN p_tipo CHAR(1),
        IN p_descricao VARCHAR(10),
        IN p_valor INTEGER,
        IN p_cliente_id INTEGER
    )
    ```
    
    * Esta declaração cria ou substitui uma stored procedure chamada `realizar_transacao`.
    * A procedure tem quatro parâmetros de entrada: `p_tipo`, `p_descricao`, `p_valor` e `p_cliente_id`, cada um com seu tipo de dado especificado.
2. **Definição da Linguagem:**
    
    Sobre a languages na documentação do postgresql tem 4 linguagens padrões  disponíveis: PL/pgSQL (Chapter 43), PL/Tcl (Chapter 44), PL/Perl (Chapter 45), and PL/Python (Chapter 46)

    ```sql
    LANGUAGE plpgsql
    ```
    
    * Define a linguagem da stored procedure como PL/pgSQL, que é uma linguagem procedural para o PostgreSQL.
3. **Corpo da Procedure:**
    
    ```sql
    AS $$
    DECLARE
        saldo_atual INTEGER;
        limite_cliente INTEGER;
    BEGIN
        -- Corpo da procedure...
    END;
    $$;
    ```
    
    * O corpo da stored procedure é definido entre `AS $$` e `$$;`.
    * Dentro do corpo, declaramos variáveis locais usando `DECLARE`.
    * A execução da procedure ocorre entre `BEGIN` e `END;`.
4. **Obtenção de Dados:**
    
    ```sql
    -- Obtém o saldo atual e o limite do cliente
    SELECT saldo, limite INTO saldo_atual, limite_cliente
    FROM clients
    WHERE id = p_cliente_id;
    ```
    
    * Esta parte do código executa uma consulta para obter o saldo atual e o limite do cliente com o ID fornecido.
5. **Verificação da Transação:**
    
    ```sql
    -- Verifica se a transação é válida com base no saldo e no limite
    IF p_tipo = 'd' AND saldo_atual - p_valor < -limite_cliente THEN
        RAISE EXCEPTION 'Saldo insuficiente para realizar a transação';
    END IF;
    ```
    
    * Aqui, é feita uma verificação para garantir que a transação seja válida com base no tipo de transação ('d' para débito) e se o saldo atual menos o valor da transação é menor que o limite de crédito do cliente. Se a condição for verdadeira, uma exceção é lançada.
6. **Atualização do Saldo:**
    
    ```sql
    -- Atualiza o saldo do cliente
    UPDATE clients
    SET saldo = saldo + CASE WHEN p_tipo = 'd' THEN -p_valor ELSE p_valor END
    WHERE id = p_cliente_id;
    ```
    
    * Nesta parte, o saldo do cliente é atualizado com base no tipo de transação. Se for um débito ('d'), o valor é subtraído do saldo atual; caso contrário, é adicionado.
7. **Inserção da Transação:**
    
    ```sql
    -- Insere uma nova transação
    INSERT INTO transactions (tipo, descricao, valor, cliente_id)
    VALUES (p_tipo, p_descricao, p_valor, p_cliente_id);
    ```
    
    * Por fim, uma nova transação é inserida na tabela `transactions` com os detalhes fornecidos.

Essa stored procedure encapsula todo o processo de realização de uma transação bancária, desde a validação do saldo e limite do cliente até a atualização do saldo e a inserção da transação. Ela oferece uma maneira conveniente e segura de executar essas operações de forma consistente e controlada.

Para chamar a stored procedure `realizar_transacao` com os parâmetros fornecidos, você pode executar o seguinte comando SQL no PostgreSQL:

```sql
CALL realizar_transacao('d', 'carro', 80000, 1);
```

Isso invocará a procedure `realizar_transacao` com os parâmetros fornecidos:

* `p_tipo`: 'd'
* `p_descricao`: 'carro'
* `p_valor`: 80000
* `p_cliente_id`: 1

Certifique-se de executar esse comando em um ambiente onde a stored procedure `realizar_transacao` esteja definida e acessível.

## Desafio

Criar uma stored procedure "ver_extrato" para fornecer uma visão detalhada do extrato bancário de um cliente, incluindo seu saldo atual e as informações das últimas 10 transações realizadas. Esta operação recebe como entrada o ID do cliente e retorna uma mensagem com o saldo atual do cliente e uma lista das últimas 10 transações, contendo o ID da transação, o tipo de transação (depósito ou retirada), uma breve descrição, o valor da transação e a data em que foi realizada.

**Explicação Detalhada:**

1. **Entrada de Parâmetros:**
    
    * A stored procedure recebe o ID do cliente como parâmetro de entrada.
2. **Obtenção do Saldo Atual:**
    
    * É realizada uma consulta na tabela "clients" para obter o saldo atual do cliente com base no ID fornecido.
3. **Exibição do Saldo Atual:**
    
    * O saldo atual do cliente é exibido por meio de uma mensagem de aviso.
4. **Obtenção das Últimas 10 Transações:**
    
    * É realizada uma consulta na tabela "transactions" para obter as últimas 10 transações do cliente, ordenadas pela data de realização em ordem decrescente.
5. **Exibição das Transações:**
    
    * Utilizando um loop `FOR`, cada transação é iterada e suas informações são exibidas por meio de mensagens de aviso.
    * Para cada transação, são exibidos o ID da transação, o tipo de transação (depósito ou retirada), uma breve descrição da transação, o valor da transação e a data em que foi realizada.
    * O loop é interrompido após exibir as informações das últimas 10 transações.

```sql
CREATE OR REPLACE PROCEDURE ver_extrato(
    IN p_cliente_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    saldo_atual INTEGER;
    transacao RECORD;
    contador INTEGER := 0;
BEGIN
    -- Obtém o saldo atual do cliente
    SELECT saldo INTO saldo_atual
    FROM clients
    WHERE id = p_cliente_id;

    -- Retorna o saldo atual do cliente
    RAISE NOTICE 'Saldo atual do cliente: %', saldo_atual;

    -- Retorna as 10 últimas transações do cliente
    RAISE NOTICE 'Últimas 10 transações do cliente:';
    FOR transacao IN
        SELECT *
        FROM transactions
        WHERE cliente_id = p_cliente_id
        ORDER BY realizada_em DESC
        LIMIT 10
    LOOP
        contador := contador + 1;
        RAISE NOTICE 'ID: %, Tipo: %, Descrição: %, Valor: %, Data: %', transacao.id, transacao.tipo, transacao.descricao, transacao.valor, transacao.realizada_em;
        EXIT WHEN contador >= 10;
    END LOOP;
END;
$$;
```

================================================
File: /Bootcamp - SQL e Analytics/Aula-08/table.sql
================================================
SELECT product_id,
    sum((((quantity)::double precision * unit_price) * ((1)::double precision - discount))) AS sold_value,
    rank() OVER (ORDER BY (sum((((quantity)::double precision * unit_price) * ((1)::double precision - discount)))) DESC) AS rank
   FROM order_details det
  GROUP BY product_id

================================================
File: /Bootcamp - SQL e Analytics/Aula-09/README.md
================================================
## Aula 09 - Triggers (Gatilhos) e Projeto Prático II

Papars recomendados https://github.com/rxin/db-readings?tab=readme-ov-file

## O que sâo Triggers?

#### 1. O que são Triggers

* **Definição**: Triggers são procedimentos armazenados, que são automaticamente executados ou disparados quando eventos específicos ocorrem em uma tabela ou visão.
* **Funcionamento**: Eles são executados em resposta a eventos como INSERT, UPDATE ou DELETE.

#### 2. Por que usamos Triggers em projetos

* **Automatização de tarefas**: Para realizar ações automáticas que são necessárias após modificações na base de dados, como manutenção de logs ou atualização de tabelas relacionadas.
* **Integridade de dados**: Garantir a consistência e a validação de dados ao aplicar regras de negócio diretamente no banco de dados.

#### 3. Origem e finalidade da criação dos Triggers

* **História**: Os triggers foram criados para oferecer uma maneira de responder automaticamente a eventos de modificação em bancos de dados, permitindo a execução de procedimentos de forma automática e transparente.
* **Problemas resolvidos**: Antes dos triggers, muitas dessas tarefas precisavam ser controladas manualmente no código da aplicação, o que poderia levar a erros e inconsistências.

1. **Tabela Funcionario**:
    
    * Armazena os dados dos funcionários, incluindo ID, nome, salário e data de contratação.
2. **Tabela Funcionario_Auditoria**:
    
    * Armazena o histórico de alterações dos salários dos funcionários, incluindo o salário antigo, o novo salário e a data da modificação.

```sql
-- Criação da tabela Funcionario
CREATE TABLE Funcionario (
    id SERIAL PRIMARY KEY,
    nome VARCHAR(100),
    salario DECIMAL(10, 2),
    dtcontratacao DATE
);

-- Criação da tabela Funcionario_Auditoria
CREATE TABLE Funcionario_Auditoria (
    id INT,
    salario_antigo DECIMAL(10, 2),
    novo_salario DECIMAL(10, 2),
    data_de_modificacao_do_salario TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (id) REFERENCES Funcionario(id)
);

-- Inserção de dados na tabela Funcionario
INSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('Maria', 5000.00, '2021-06-01');
INSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('João', 4500.00, '2021-07-15');
INSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('Ana', 4000.00, '2022-01-10');
INSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('Pedro', 5500.00, '2022-03-20');
INSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('Lucas', 4700.00, '2022-05-25');
```

### Criação do Trigger

O trigger `trg_salario_modificado` será disparado após uma atualização no salário na tabela `Funcionario`. Ele registrará os detalhes da modificação na tabela `Funcionario_Auditoria`.

```sql
-- Criação do Trigger para auditoria de alterações de salário
CREATE OR REPLACE FUNCTION registrar_auditoria_salario() RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO Funcionario_Auditoria (id, salario_antigo, novo_salario)
    VALUES (OLD.id, OLD.salario, NEW.salario);
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_salario_modificado
AFTER UPDATE OF salario ON Funcionario
FOR EACH ROW
EXECUTE FUNCTION registrar_auditoria_salario();
```

Esse exemplo cria uma infraestrutura completa para monitorar as alterações de salário, garantindo que qualquer ajuste seja devidamente registrado, oferecendo uma trilha de auditoria clara e útil para análises futuras.

Para verificar se o trigger está funcionando corretamente, podemos realizar um comando `UPDATE` no salário de um dos funcionários e, em seguida, consultar a tabela `Funcionario_Auditoria` para ver se a alteração foi registrada conforme esperado. Vamos fazer isso com o funcionário cujo nome é "Ana".

### Comando de Atualização do Salário

```sql
-- Atualiza o salário da Ana
UPDATE Funcionario SET salario = 4300.00 WHERE nome = 'Ana';
```

### Consulta à Tabela de Auditoria

Após realizar a atualização, podemos verificar a tabela `Funcionario_Auditoria` para garantir que o registro da mudança de salário foi feito.

```sql
-- Consulta à tabela Funcionario_Auditoria para verificar as mudanças
SELECT * FROM Funcionario_Auditoria WHERE id = (SELECT id FROM Funcionario WHERE nome = 'Ana');
```

Este comando SQL irá retornar os registros da tabela de auditoria que correspondem ao funcionário "Ana". Você deve ver uma linha com o salário antigo (4000.00), o novo salário (4300.00) e a data/hora da modificação, indicando que o trigger operou conforme o esperado.

## Exemplo com desafio de Estoque

Neste exercício, você irá implementar um sistema simples de gestão de estoque para uma loja que vende camisetas como Basica, Dados e Verao. A loja precisa garantir que as vendas sejam registradas apenas se houver estoque suficiente para atender os pedidos. Você será responsável por criar um trigger no banco de dados que previna a inserção de vendas que excedam a quantidade disponível dos produtos.

```sql
-- Criação da tabela Produto
CREATE TABLE Produto (
    cod_prod INT PRIMARY KEY,
    descricao VARCHAR(50) UNIQUE,
    qtde_disponivel INT NOT NULL DEFAULT 0
);

-- Inserção de produtos
INSERT INTO Produto VALUES (1, 'Basica', 10);
INSERT INTO Produto VALUES (2, 'Dados', 5);
INSERT INTO Produto VALUES (3, 'Verao', 15);

-- Criação da tabela RegistroVendas
CREATE TABLE RegistroVendas (
    cod_venda SERIAL PRIMARY KEY,
    cod_prod INT,
    qtde_vendida INT,
    FOREIGN KEY (cod_prod) REFERENCES Produto(cod_prod) ON DELETE CASCADE
);
```

```sql
-- Criação de um TRIGGER
CREATE OR REPLACE FUNCTION verifica_estoque() RETURNS TRIGGER AS $$
DECLARE
    qted_atual INTEGER;
BEGIN
    SELECT qtde_disponivel INTO qted_atual
    FROM Produto WHERE cod_prod = NEW.cod_prod;
    IF qted_atual < NEW.qtde_vendida THEN
        RAISE EXCEPTION 'Quantidade indisponivel em estoque'
    ELSE
        UPDATE Produto SET qtde_disponivel = qtde_disponivel - NEW.qtde_vendida
        WHERE cod_prod = NEW.cod_prod;
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_verifica_estoque 
BEFORE INSERT ON RegistroVendas
FOR EACH ROW 
EXECUTE FUNCTION verifica_estoque();
```
    
```sql
-- Tentativa de venda de 5 unidades de Basico (deve ser bem-sucedida, pois há 10 unidades disponíveis)
INSERT INTO RegistroVendas (cod_prod, qtde_vendida) VALUES (1, 5);

-- Tentativa de venda de 6 unidades de Dados (deve ser bem-sucedida, pois há 5 unidades disponíveis e a quantidade vendida não excede o estoque)
INSERT INTO RegistroVendas (cod_prod, qtde_vendida) VALUES (2, 5);

-- Tentativa de venda de 16 unidades de Versao (deve falhar, pois só há 15 unidades disponíveis)
INSERT INTO RegistroVendas (cod_prod, qtde_vendida) VALUES (3, 16);
```


================================================
File: /Bootcamp - SQL e Analytics/Aula-10/README.md
================================================
## Transação

### O que é uma transação?

- Uma coleção de "queries"
- Uma unidade de trabalho
Muitas vezes precisamos mais de uma "querie" para querer o que queremos
ex: para fazer uma transação financeira, precisamos selecionar uma conta e verificar se ela possui o dnheiro (SELECT), fazer a remoção do dinheiro da conta que irá transferir o dinheiro (UPDATE) e fazer o incremento do dinheiro na conta alvo (UPDATE). Tudo isso precisa estar dentro da mesma transação.
- Toda transação inicia com um BEGIN
- Toda transação finaliza com um COMMIT (em memória)
- Toda transação, pode falhar, precisa de um ROLLBACK 
- Normalmente transações são usadas para MODIFICAR dados, mas é possível ter uma transação com somente leitura , exemplo: você quer gerar um relatório e quer que esses dados sejam confiáveis e ter uma SNAPSHOT daquela cenário

## Atomicidade

- Uma transação tem que ser "indivisivel"
- Ou seja, todas as "queries" em uma transação precisam ter sucesso
- Exemplo: se não existir a segunda pessoa, se você não tiver 100 reais, se cair a luz no meio dessa transação, etc. Ela volta para o estado anterior e nada acontece.

```sql
-- Criar tabela
CREATE TABLE exemplo (
    id SERIAL PRIMARY KEY,
    nome VARCHAR(50)
);

-- Inserir dados
INSERT INTO exemplo (nome) VALUES ('A'), ('B'), ('C');

SELECT * FROM exemplo
```

Read comitted:
```

exemplo 1 t1
```
BEGIN;
SELECT nome, count(nome) FROM exemplo
GROUP by nome

exemplo 1 t2
```
BEGIN;
INSERT INTO exemplo (nome) VALUES ('A');
COMMIT;
```

exemplo 1 t1
```sql
SELECT nome FROM exemplo;
```


Como evitar isso?

exemplo 1 t1
```sql
SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN;
SELECT nome, count(nome) FROM exemplo
GROUP by nome;
SELECT * FROM exemplo;
COMMIT;
```

exemplo 2 t2
```
BEGIN;
INSERT INTO exemplo (nome) VALUES ('A');
COMMIT;
```

-- Configuração para Serializable
```sql
SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;
BEGIN;
SELECT * FROM exemplo;

-- Configuração para Serializable
SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;
BEGIN;
SELECT * FROM exemplo;

-- Voltar pro T1
INSERT INTO exemplo (nome) VALUES ('A');
COMMIT;

-- Voltar pro T2
INSERT INTO exemplo (nome) VALUES ('A');
COMMIT;

================================================
File: /Bootcamp - SQL e Analytics/Aula-11/README.md
================================================
# Aula 11 : Ordem de consulta

Para maximizar a velocidade da sua consulta em qualquer mecanismo SQL, é essencial entender a ordem de execução do SQL. Embora seja possível trabalhar sem esse conhecimento, recomendo a leitura deste artigo para obter um entendimento rápido sobre isso.

O mecanismo SQL não segue a mesma ordem que você define na sua consulta, portanto, é crucial lembrar disso. Por exemplo, embora comecemos com uma instrução SELECT, o mecanismo não começará com esse comando. Neste artigo, examinaremos uma consulta complexa passo a passo para entender como o mecanismo SQL opera nos bastidores.

Importante: todos os exemplos são feitos no PostgreSQL, então as sintaxes podem variar de mecanismo para mecanismo. Ainda assim, esse conceito é aplicável a todos os outros tipos de mecanismos SQL.

Definir a Consulta
Para este exemplo, gostaria de discutir uma consulta típica usada em fluxos de trabalho do mundo real. Suponha que temos um banco de dados para carros com uma tabela para diferentes modelos, e cada modelo tem suas próprias especificações de motor listadas em uma tabela separada. Para ilustrar isso, podemos criar tabelas para este cenário.

```sql
DROP TABLE IF EXISTS cars, engines;
CREATE TABLE cars (
 manufacturer VARCHAR(64),
 model VARCHAR(64),
 country VARCHAR(64),
 engine_name VARCHAR(64),
 year INT
);
CREATE TABLE engines (
 name VARCHAR(64),
 horse_power INT
);

INSERT INTO cars
VALUES 
 ('BMW', 'M4', 'Germany', 'S58B30T0-353', 2021),
 ('BMW', 'M4', 'Germany', 'S58B30T0-375', 2021),
 ('Chevrolet', 'Corvette', 'USA', 'LT6', 2023),
 ('Chevrolet', 'Corvette', 'USA', 'LT2', 2023),
 ('Audi', 'R8', 'Germany', 'DOHC FSI V10-5.2-456', 2019),
 ('McLaren', 'GT', 'UK', 'M840TE', 2019),
 ('Mercedes', 'AMG C 63 S E', 'Germany', 'M139L', 2023);
 
INSERT INTO engines
VALUES 
 ('S58B30T0-353', 473),
 ('S58B30T0-375', 510),
 ('LT6', 670),
 ('LT2', 495),
 ('DOHC FSI V10-5.2-456', 612),
 ('M840TE', 612),
 ('M139L', 469);
```

Para alcançar nosso objetivo de identificar os dois carros mais potentes da Alemanha, estaremos olhando para automóveis modernos que não sejam mais antigos do que oito anos. Para isso, usaremos instruções SQL conhecidas como SELECT, FROM, JOIN, WHERE, GROUP BY, HAVING, ORDER BY e LIMIT.

```sql
SELECT
  cars.manufacturer,
  cars.model,
  cars.country,
  cars.year,
  MAX(engines.horse_power) as maximum_horse_power
FROM cars
JOIN engines ON cars.engine_name = engines.name
WHERE cars.year > 2015 AND cars.country = 'Germany'
GROUP BY cars.manufacturer, cars.model, cars.country, cars.year
HAVING MAX(engines.horse_power)> 200
ORDER BY maximum_horse_power DESC
LIMIT 2
```

Saída da consulta — os dois carros alemães mais potentes do nosso banco de dados de amostra

Agora que temos nossa consulta, vamos entender como o mecanismo a ordena ao executar. Aqui está a ordem:

1. FROM
2. JOIN (e ON)
3. WHERE
4. GROUP BY
5. HAVING
6. SELECT
7. ORDER BY
8. LIMIT

Fonte: https://blog.bytebytego.com/p/ep50-visualizing-a-sql-query

É importante notar que, antes de executar a consulta, o mecanismo SQL cria um plano de execução para reduzir o consumo de recursos. Este plano oferece detalhes valiosos como custos estimados, algoritmos de junção, ordem das operações e mais. Este é um resultado abrangente, e pode ser acessado se necessário.

Passo a Passo
FROM e JOIN
```sql
FROM cars
JOIN engines
```
Ao iniciar uma consulta SQL, o mecanismo precisa saber quais tabelas usar. Isso é realizado começando com uma instrução FROM. Você pode adicionar mais tabelas usando a palavra-chave JOIN,

 desde que compartilhem uma coluna comum que será usada na consulta. É um processo direto que você deve ter em mente.

ON
```sql
ON cars.engine_name = engines.name
```
A seguir na sequência vem o ON, onde definimos como juntar diferentes tabelas. Este processo também envolve o uso de índices pré-definidos, como B-tree e Bitmap, para acelerar os cálculos. É importante notar que existem vários tipos de índices que podem ajudar neste caso. Estas duas etapas requerem uma quantidade considerável de processamento, portanto, é crucial focar e começar a otimizar neste ponto.

WHERE
```sql
WHERE cars.year > 2015 AND cars.country = 'Germany'
```
Ao analisar nossos dados, é importante ter em mente que usar a cláusula WHERE apenas com colunas indexadas pode melhorar o desempenho, especialmente ao lidar com grandes tabelas. Além disso, pode ser benéfico filtrar dados em subconsultas ou CTEs antes da declaração WHERE em alguns cenários para aumentar ainda mais o desempenho.

No entanto, é importante notar que muitos problemas relacionados ao desempenho de consultas estão além do escopo deste artigo. Recomendo aprofundar-se nesses problemas e experimentar várias técnicas para escrever consultas mais rápidas.

GROUP BY e HAVING
```sql
GROUP BY cars.manufacturer, cars.model, cars.country, cars.year
HAVING MAX(engines.horse_power) > 200
```
A seguir na sequência, precisamos seguir a ordem especificada da consulta adequadamente. Depois disso, temos que determinar todas as agregações necessárias que temos que realizar. Quando se trata da cláusula HAVING, é intrigante porque não podemos empregar um alias da linha SELECT. Isso ocorre porque o motor SQL ainda não está ciente desta definição.

SELECT
```sql
SELECT
  cars.manufacturer,
  cars.model,
  cars.country,
  cars.year,
  MAX(engines.horse_power) as maximum_horse_power
```
Uma vez que todos os passos necessários foram completados, prosseguimos para executar a instrução SELECT. Neste ponto, simplesmente especificamos as colunas a serem incluídas na saída final. É importante ter em mente que muitas operações, como mesclagem e agregação, já foram concluídas nesta fase.

ORDER BY e LIMIT
```sql
ORDER BY maximum_horse_power DESC
LIMIT 2
```
Uma vez que executamos os comandos finais, tomamos conhecimento dos aliases que mencionamos na instrução SELECT. Como resultado, podemos utilizar o alias maximum_horse_power em vez do nome da função, embora ainda possamos usar este último. É melhor evitar ordenar uma grande quantidade de dados de saída, pois isso pode consumir uma quantidade significativa de tempo.

## Conclusão

O plano de execução que você visualizou para sua consulta SQL no PostgreSQL detalha como o mecanismo de banco de dados planeja buscar e processar os dados necessários para produzir o resultado desejado. Vamos analisar cada etapa e entender a ordem em que ocorrem, explicando o que cada uma representa:

1. **Seq Scan on cars as cars**
   - **Descrição**: Uma varredura sequencial (Seq Scan) é realizada na tabela `cars`. 
   - **Filtros aplicados**: A consulta verifica cada linha para ver se o ano (`year`) é maior que 2015 e se o país (`country`) é 'Germany'. 
   - **Resultado**: As linhas que não atendem a esses critérios são descartadas, indicado por "Rows Removed by Filter: 3".

2. **Hash**
   - **Descrição**: Esta etapa prepara uma estrutura de dados de hash para a tabela `cars` com base nos resultados da varredura que passaram pelos filtros.
   - **Detalhes**: A hash é construída para otimizar a junção subsequente, usando colunas que serão ligadas com a outra tabela (`engines`).

3. **Seq Scan on engines as engines**
   - **Descrição**: Assim como foi feito com `cars`, uma varredura sequencial é realizada na tabela `engines`.
   - **Resultado**: Todas as 7 linhas da tabela `engines` são lidas.

4. **Hash Inner Join**
   - **Descrição**: Um Hash Join (junção por hash) é realizado entre as tabelas `cars` e `engines`.
   - **Condição de junção**: A junção é feita onde o nome do motor (`engine_name` de `cars` e `name` de `engines`) são iguais.
   - **Resultado**: O resultado são 4 linhas onde a condição de junção é verdadeira.

5. **Sort (rows=4 loops=1)**
   - **Descrição**: As linhas resultantes do Join são ordenadas. O atributo exato da ordenação não está especificado aqui, mas é provável que seja preparação para a agregação.

6. **Aggregate**
   - **Descrição**: Uma função de agregação é aplicada.
   - **Filtro**: O filtro aplicado na agregação é que a potência máxima do motor (`max(engines.horse_power)`) deve ser maior que 200.
   - **Resultado**: Após aplicar o filtro, 3 linhas permanecem.

7. **Sort**
   - **Descrição**: As linhas são novamente ordenadas, desta vez provavelmente pelo valor de potência máxima do motor em ordem descendente.

8. **Limit**
   - **Descrição**: Apenas as duas primeiras linhas do resultado ordenado são retidas, conforme especificado pela cláusula `LIMIT 2` na consulta.

A ordem das operações mostra claramente como o PostgreSQL lida com a consulta, otimizando o processo ao usar técnicas como varreduras sequenciais, hash para junção e filtragem rigorosa antes de aplicar funções de agregação e ordenação, culminando na aplicação de um limite para o resultado final. Esta abordagem ajuda a minimizar o volume de dados manipulados nas etapas finais do processamento da consulta.

## Tudo foi uma mentira

```sql
SELECT
  cars.manufacturer,
  cars.model,
  cars.engine_name,
  engines.horse_power
FROM cars
JOIN engines ON cars.engine_name = engines.name
LIMIT 2;
```

Otimização do Join: O uso do Nested Loop Inner Join sugere que o otimizador percebeu que é mais eficiente processar o join linha a linha devido ao pequeno tamanho do resultado esperado da tabela engines.

Aplicação precoce do Limit: O fato de apenas 2 linhas serem processadas na varredura da tabela engines e resultarem em 2 linhas após o join indica que o LIMIT pode estar influenciando a execução da consulta mais cedo do que o plano sugere visualmente. Isto é, o PostgreSQL está provavelmente limitando o número de linhas processadas em cada etapa para cumprir eficientemente o LIMIT.

Eficiência do Plano: Este plano mostra um uso eficiente de recursos, processando o mínimo de dados necessário para alcançar o resultado desejado, que é fundamental em grandes bases de dados ou em sistemas com recursos limitados.
Portanto, mesmo que o LIMIT apareça ao final no plano visual, sua influência é evidente em todas as etapas anteriores, demonstrando a capacidade do otimizador de consulta do PostgreSQL de integrar profundamente considerações de limitação no plano de execução global.

================================================
File: /Bootcamp - SQL e Analytics/Aula-12/README.md
================================================
# Aula 12 : Database Indexing

**Tópico 1: Índices em Bancos de Dados**

- **Introdução aos Índices:** Índices em bancos de dados são estruturas utilizadas para melhorar a eficiência de consultas, permitindo acesso rápido aos dados. Por exemplo, considere uma tabela de alunos em um banco de dados escolar. Sem índices, uma consulta para encontrar o aluno com um determinado ID exigiria uma busca sequencial na tabela. Com um índice, o banco de dados pode ir diretamente para a linha correspondente ao ID especificado.
- **Tipos de Índices:** Existem vários tipos de índices, incluindo índices de árvore B, índices hash e índices de bitmap. Cada tipo tem suas próprias características e utilizações adequadas.
- **Funcionamento dos Índices:** Os índices são geralmente criados com base em uma ou mais colunas de uma tabela. Quando uma consulta é feita usando uma dessas colunas, o banco de dados pode usar o índice correspondente para localizar rapidamente as linhas relevantes na tabela.
- **Vantagens e Desvantagens:** As vantagens dos índices incluem consultas mais rápidas e eficientes, enquanto as desvantagens incluem custo adicional de armazenamento e sobrecarga de atualização durante operações de inserção, atualização e exclusão.

**Tópico 2: Estruturas de Dados B-Tree**
- **Introdução às Estruturas de Dados B-Tree:** Uma B-Tree é uma árvore balanceada que é frequentemente usada em bancos de dados e sistemas de arquivos. Ela é projetada para permitir inserções, exclusões e pesquisas eficientes em grandes conjuntos de dados, mantendo a árvore balanceada e otimizando a profundidade da árvore.
- **Estrutura da B-Tree:** Uma B-Tree consiste em nós, onde cada nó pode ter várias chaves e vários ponteiros para outros nós. Cada nó tem um número mínimo e máximo de chaves e ponteiros, mantendo a árvore balanceada.
- **Operações em B-Tree:** As operações básicas em uma B-Tree incluem inserção, remoção e busca. Por exemplo, durante uma busca, a árvore é percorrida de acordo com a chave procurada, reduzindo eficientemente o espaço de busca a cada passo.
- **Propriedades das B-Trees:** As B-Trees possuem várias propriedades, como balanceamento automático, garantindo que a profundidade da árvore seja mantida em um nível aceitável, mesmo com muitas inserções e remoções.
- **Aplicações Práticas:** As B-Trees são amplamente utilizadas em bancos de dados para índices, como índices de chaves primárias e secundárias, devido à sua eficiência e capacidade de manipular grandes volumes de dados.

Vamos detalhar cada parte do código:

1. **Criação da Tabela com UUID:**
   - Este trecho de código cria uma tabela chamada `pessoas` com três colunas: `id`, `first_name` e `last_name`. A coluna `id` é definida como uma chave primária (`PRIMARY KEY`) e tem o tipo de dados `UUID`. O valor padrão da coluna `id` é gerado usando a extensão `uuid-ossp`, que é usada para gerar UUIDs aleatórios.
   - Exemplo:
     ```sql
     CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

     CREATE TABLE pessoas (
         id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
         first_name VARCHAR(3),
         last_name VARCHAR(3)
     );
     ```

2. **Verificação de Índices Existente:**
   - Este trecho de código verifica os índices existentes na tabela `pessoas` e exibe suas informações, como nome e definição.
   - Exemplo:
     ```sql
     SELECT 
         tablename AS "Tabela",
         indexname AS "Índice",
         indexdef AS "Definição do Índice"
     FROM 
         pg_indexes 
     WHERE 
         tablename = 'pessoas'; -- Substitua 'pessoas' pelo nome da sua tabela
     ```

3. **Dropar a Tabela com UUID e Criar uma SERIAL:**
   - Este trecho de código remove a tabela `pessoas` existente (se houver) e a recria com uma coluna `id` do tipo `SERIAL`, que é uma sequência autoincrementada. Essa abordagem é mais rápida para gerar valores de ID do que o uso de UUIDs.
   - Exemplo:
     ```sql
     CREATE TABLE pessoas (
         id SERIAL PRIMARY KEY,
         first_name VARCHAR(3),
         last_name VARCHAR(3)
     );
     ```

4. **Inserção de 1 Milhão de Registros:**
   - Este trecho de código insere 1 milhão de registros na tabela `pessoas`, gerando valores aleatórios para as colunas `first_name` e `last_name`.
   - Exemplo:
     ```sql
     INSERT INTO pessoas (first_name, last_name)
     SELECT 
         substring(md5(random()::text), 0, 3),
         substring(md5(random()::text), 0, 3)
     FROM 
         generate_series(1, 1000000);
     ```

5. **Explicação sobre a Velocidade de Geração entre SERIAL e UUID:**

Uma das razões pelas quais uma coluna do tipo `serial` é mais rápida para gerar do que uma coluna do tipo `UUID` é a forma como os valores são criados e armazenados.

1. **Serial:**
   - Uma coluna do tipo `serial` é uma sequência numérica que é automaticamente incrementada pelo PostgreSQL. Quando uma nova linha é inserida na tabela e não é especificado um valor para essa coluna, o PostgreSQL gera automaticamente o próximo número na sequência e o atribui à coluna. Esse processo é altamente eficiente, pois não envolve cálculos complicados ou geração de valores aleatórios.

2. **UUID:**
   - Por outro lado, uma coluna do tipo `UUID` geralmente armazena identificadores únicos universais (UUIDs), que são cadeias de caracteres alfanuméricos de 128 bits (ou 16 bytes) gerados usando um algoritmo específico. A geração de um UUID geralmente envolve cálculos mais complexos e aleatórios para garantir que os valores sejam únicos globalmente. Isso pode ser mais demorado em comparação com a simples incrementação de um número inteiro.

Em resumo, a geração de valores para uma coluna do tipo `serial` é mais rápida porque envolve apenas a incrementação de um número, enquanto a geração de valores para uma coluna do tipo `UUID` pode ser mais demorada devido à complexidade do algoritmo de geração e à aleatoriedade necessária para garantir a unicidade global.

Verificando o tempo e buscando somente index

Claro, aqui está o texto aprimorado com mais contexto:

---

**Verificando o tempo e buscando somente pelo índice:**

```sql
SELECT id FROM pessoas WHERE id = 100000;
EXPLAIN ANALYZE SELECT id FROM pessoas WHERE id = 100000;
```

Ao executar essas consultas, estamos analisando o desempenho da busca direta por um registro específico na tabela `pessoas`. Como estamos consultando apenas o índice associado à coluna `id`, esperamos uma execução rápida e eficiente, já que o banco de dados pode usar diretamente o índice para localizar o registro desejado.

**Buscando somente pelo índice, mas observando os detalhes da tabela:**

```sql
SELECT first_name FROM pessoas WHERE id = 100000;
EXPLAIN ANALYZE SELECT first_name FROM pessoas WHERE id = 100000;
```

Nesse caso, mesmo que estejamos consultando apenas o índice da coluna `id`, estamos selecionando uma coluna adicional, `first_name`, da tabela `pessoas`. Isso pode resultar em uma consulta mais lenta, pois o banco de dados pode precisar acessar as páginas de dados da tabela para recuperar os valores de `first_name` associados aos registros encontrados no índice.

**Buscando e trazendo dados da tabela de maneira eficiente:**

```sql
SELECT first_name FROM pessoas WHERE first_name = 'aa';
```

Agora estamos buscando registros na tabela `pessoas` com base no valor exato de `first_name`. Se houver um índice na coluna `first_name`, essa consulta deve ser executada de maneira rápida e eficiente, pois o banco de dados pode usar o índice para localizar diretamente os registros correspondentes.

**Buscando e trazendo dados da tabela da pior maneira possível:**

```sql
SELECT first_name FROM pessoas WHERE first_name LIKE '%a%';
```

Nesta consulta, estamos buscando por valores parciais de `first_name` usando a cláusula `LIKE`. Esta consulta pode ser significativamente mais lenta, especialmente em grandes conjuntos de dados, pois não aproveita eficientemente os índices. O `%` no padrão de correspondência significa que estamos buscando por qualquer valor que contenha o caractere 'a' em qualquer posição da coluna `first_name`, o que pode resultar em uma varredura completa da tabela.

**Criando nosso índice:**

```sql
CREATE INDEX first_name_index ON pessoas(first_name);
```

Aqui estamos criando um índice na coluna `first_name` da tabela `pessoas`, o que nos permitirá otimizar consultas que buscam por valores nessa coluna.

**Comparação após a criação do índice:**

```sql
SELECT first_name FROM pessoas WHERE first_name = 'aa';
```

Agora, após a criação do índice na coluna `first_name`, vamos comparar novamente a consulta que busca por valores exatos de `first_name`. Com o índice em vigor, esperamos uma melhoria significativa no desempenho dessa consulta.

**Comparando agora com o operador LIKE:**

```sql
SELECT first_name FROM pessoas WHERE first_name LIKE '%aa%';
``` 

Nesta consulta, estamos buscando valores parciais de first_name que contenham a sequência 'aa' em qualquer posição. Embora tenhamos criado um índice na coluna first_name, o operador LIKE com o uso de % antes e depois do padrão de correspondência '%aa%' não pode fazer uso eficiente desse índice.

Varredura Completa da Tabela: O operador LIKE com um % no início do padrão significa que o banco de dados precisa verificar cada valor na coluna first_name para encontrar aqueles que contêm a sequência 'aa' em qualquer posição. Isso pode exigir uma varredura completa da tabela, mesmo com um índice criado na coluna.

Uso Ineficiente do Índice: O índice criado na coluna first_name é mais útil para consultas que buscam por valores exatos ou prefixos específicos. No entanto, como o padrão de correspondência '%aa%' não possui um prefixo definido, o otimizador de consultas pode optar por não utilizar o índice, pois uma varredura completa

## Entendendo um pouco mais do Explain

Aqui estão exemplos de consultas e suas respectivas saídas explicadas usando o comando `EXPLAIN`:

1. **Consulta simples:**
```sql
EXPLAIN SELECT * FROM pessoas;
```
Essa consulta irá explicar como o PostgreSQL planeja executar a consulta `SELECT * FROM pessoas`, que simplesmente seleciona todas as colunas da tabela `pessoas`. O resultado pode incluir informações sobre como o PostgreSQL acessa os dados na tabela, como pode ser feito um scan sequencial (percorrer todas as linhas) ou se algum índice será utilizado.

2. **Consulta com ordenação por `id`:**
```sql
EXPLAIN SELECT * FROM pessoas ORDER BY id;
```
Esta consulta adiciona uma cláusula `ORDER BY id`, que ordena os resultados da consulta com base na coluna `id`. O resultado do `EXPLAIN` mostrará como o PostgreSQL planeja executar a ordenação, se um índice na coluna `id` pode ser utilizado e se a ordenação será feita em memória ou em disco.

3. **Consulta com ordenação por `last_name`:**
```sql
EXPLAIN SELECT * FROM pessoas ORDER BY last_name;
```
Similar ao exemplo anterior, esta consulta adiciona uma cláusula `ORDER BY last_name`, que ordena os resultados da consulta com base na coluna `last_name`. O resultado do `EXPLAIN` mostrará como o PostgreSQL planeja executar a ordenação, se um índice na coluna `last_name` pode ser utilizado e como a ordenação será realizada.

Ao analisar a saída do comando `EXPLAIN`, você pode identificar oportunidades de otimização de consulta, como a criação de índices adicionais, ajustes na configuração do banco de dados ou alterações na estrutura da consulta para melhorar o desempenho.

Claro, vou explicar as diferenças entre as operações de busca em índices e varredura de tabelas no contexto do PostgreSQL, e fornecer exemplos de consultas que resultam em cada tipo de operação.

1. **Table Scan (Varredura de Tabela):**
   - A varredura de tabela ocorre quando o PostgreSQL precisa examinar todas as linhas de uma tabela para atender a uma consulta. Isso pode acontecer quando não há índices adequados para a consulta ou quando o custo de usar um índice é maior do que o de percorrer a tabela inteira.
   - Exemplo:
     ```sql
     SELECT * FROM pessoas;
     ```

2. **Index Scan (Varredura de Índice):**
   - Uma varredura de índice ocorre quando o PostgreSQL utiliza um índice para acessar as linhas de uma tabela que satisfazem os critérios da consulta. O banco de dados pode usar um índice se este for mais eficiente do que uma varredura de tabela.
   - Exemplo:
     ```sql
     SELECT * FROM pessoas WHERE id = 100;
     ```
   - Se houver um índice na coluna `id`, o PostgreSQL pode realizar uma varredura de índice para localizar rapidamente as linhas com `id` igual a 100.

3. **Bitmap Index Scan (Varredura de Bitmap de Índice):**
   - Uma varredura de bitmap de índice é uma técnica utilizada pelo PostgreSQL para combinar múltiplos índices em uma única operação. Em vez de procurar diretamente nas linhas da tabela, o PostgreSQL primeiro gera "bitmaps" para cada índice individualmente, representando as linhas que satisfazem os critérios da consulta. Em seguida, ele combina esses bitmaps para encontrar as linhas que satisfazem todos os critérios.
   - Exemplo:
     ```sql
     SELECT id, first_name FROM pessoas WHERE id = 100 OR first_name = 'aa';
     ```
   - Se houver índices separados nas colunas `id` e `first_name`, o PostgreSQL pode realizar uma varredura de bitmap de índice para encontrar as linhas que têm tanto `id` igual a 100 quanto `first_name` igual a 'aa'.

Claro, vou explicar o `Index Only Scan` (Varredura Apenas no Índice) e fornecer um exemplo de consulta que resulta nesse tipo de operação.

5. **Index Only Scan (Varredura Apenas no Índice):**

- O `Index Only Scan` ocorre quando o PostgreSQL pode satisfazer uma consulta apenas usando os dados armazenados no índice, sem a necessidade de acessar a tabela subjacente. Isso é possível quando todas as colunas necessárias para a consulta estão presentes no índice, o que elimina a necessidade de acessar as páginas de dados da tabela.
- Esse tipo de operação é particularmente eficiente, pois reduz a quantidade de E/S (entrada/saída) necessária para atender à consulta, já que apenas o índice precisa ser lido em vez da tabela inteira.
- Para que um `Index Only Scan` ocorra, todas as colunas na cláusula `SELECT` devem ser cobertas pelo índice. Além disso, não deve haver nenhuma coluna não indexada referenciada na consulta.
- Este tipo de operação é especialmente útil para consultas que precisam apenas de colunas indexadas e podem melhorar significativamente o desempenho em comparação com um `Index Scan` ou `Bitmap Index Scan` seguido de uma consulta à tabela.
- Exemplo:
  ```sql
  SELECT first_name FROM pessoas WHERE first_name = 'aa';
  ```
  Se houver um índice na coluna `first_name` e se todas as consultas de seleção envolverem apenas a coluna `first_name`, o PostgreSQL pode usar um `Index Only Scan` para atender a essas consultas, acessando apenas o índice e não a tabela subjacente.

O `Index Only Scan` é uma operação de busca muito eficiente quando todas as colunas necessárias para a consulta estão cobertas pelo índice, resultando em uma redução significativa na quantidade de E/S necessária para atender à consulta.

Vamos examinar cada parte em detalhes:

1. **Custo do Índice:**
   - Este trecho de código calcula o tamanho em disco ocupado pelo índice `first_name_index`. O resultado será exibido em uma unidade de tamanho legível, como KB, MB ou GB. Isso pode ser útil para entender o impacto do índice no armazenamento do banco de dados.
   - Exemplo:
     ```sql
     SELECT pg_size_pretty(pg_relation_size('first_name_index'));
     ```
   - Este comando retornará o tamanho ocupado pelo índice `first_name_index` em disco.

2. **Tamanho Total da Coluna:**
   - Este trecho de código calcula o tamanho total ocupado pela coluna `first_name` em todas as linhas da tabela `pessoas`. O resultado será exibido em uma unidade de tamanho legível, como KB, MB ou GB. Isso pode ser útil para entender quanto espaço a coluna está consumindo no banco de dados.
   - Exemplo:
     ```sql
     SELECT pg_size_pretty(pg_column_size(first_name)::bigint) AS tamanho_total
     FROM pessoas;
     ```
   - Este comando retornará o tamanho total ocupado pela coluna `first_name` em todas as linhas da tabela `pessoas`.

3. **Tamanho Total de Todas as Colunas:**
   - Este trecho de código calcula o tamanho total ocupado por todas as colunas em todas as linhas da tabela `pessoas`. Ele soma os tamanhos individuais de todas as colunas e retorna o resultado em uma unidade de tamanho legível, como KB, MB ou GB. Isso pode ser útil para entender quanto espaço total a tabela está consumindo no banco de dados.
   - Exemplo:
     ```sql
     SELECT pg_size_pretty(SUM(pg_column_size(first_name)::bigint)) AS tamanho_total
     FROM pessoas;
     ```
   - Este comando retornará o tamanho total ocupado por todas as linhas em todas as colunas da tabela `pessoas`.

================================================
File: /Bootcamp - SQL e Analytics/Aula-12/main.py
================================================
class Node:
    def __init__(self, leaf=False):
        self.keys = []
        self.children = []
        self.leaf = leaf


class BTree:
    def __init__(self, t):
        self.root = Node(True)
        self.t = t

    def search(self, key, node=None):
        node = self.root if node == None else node
    
        i = 0
        while i < len(node.keys) and key > node.keys[i]:
            i += 1
        if i < len(node.keys) and key == node.keys[i]:
            return (node, i)
        elif node.leaf:
            return None
        else:
            return self.search(key, node.children[i])

    def split_child(self, x, i):
        t = self.t

        # y is a full child of x
        y = x.children[i]
        
        # create a new node and add it to x's list of children
        z = Node(y.leaf)
        x.children.insert(i + 1, z)

        # insert the median of the full child y into x
        x.keys.insert(i, y.keys[t - 1])

        # split apart y's keys into y & z
        z.keys = y.keys[t: (2 * t) - 1]
        y.keys = y.keys[0: t - 1]

        # if y is not a leaf, we reassign y's children to y & z
        if not y.leaf:
            z.children = y.children[t: 2 * t]
            y.children = y.children[0: t] # video incorrectly has t-1

    def insert(self, k):
        t = self.t
        root = self.root

        # if root is full, create a new node - tree's height grows by 1
        if len(root.keys) == (2 * t) - 1:
            new_root = Node()
            self.root = new_root
            new_root.children.insert(0, root)
            self.split_child(new_root, 0)
            self.insert_non_full(new_root, k)
        else:
            self.insert_non_full(root, k)

    def insert_non_full(self, x, k):
        t = self.t
        i = len(x.keys) - 1

        # find the correct spot in the leaf to insert the key
        if x.leaf:
            x.keys.append(None)
            while i >= 0 and k < x.keys[i]:
                x.keys[i + 1] = x.keys[i]
                i -= 1
            x.keys[i + 1] = k
        # if not a leaf, find the correct subtree to insert the key
        else:
            while i >= 0 and k < x.keys[i]:
                i -= 1
            i += 1
            # if child node is full, split it
            if len(x.children[i].keys) == (2 * t) - 1:
                self.split_child(x, i)
                if k > x.keys[i]:
                    i += 1
            self.insert_non_full(x.children[i], k)

    def delete(self, x, k):
        t = self.t
        i = 0

        while i < len(x.keys) and k > x.keys[i]:
            i += 1
        if x.leaf:
            if i < len(x.keys) and x.keys[i] == k:
                x.keys.pop(i)
            return

        if i < len(x.keys) and x.keys[i] == k:
            return self.delete_internal_node(x, k, i)
        elif len(x.children[i].keys) >= t:
            self.delete(x.children[i], k)
        else:
            if i != 0 and i + 2 < len(x.children):
                if len(x.children[i - 1].keys) >= t:
                    self.delete_sibling(x, i, i - 1)
                elif len(x.children[i + 1].keys) >= t:
                    self.delete_sibling(x, i, i + 1)
                else:
                    self.delete_merge(x, i, i + 1)
            elif i == 0:
                if len(x.children[i + 1].keys) >= t:
                    self.delete_sibling(x, i, i + 1)
                else:
                    self.delete_merge(x, i, i + 1)
            elif i + 1 == len(x.children):
                if len(x.children[i - 1].keys) >= t:
                    self.delete_sibling(x, i, i - 1)
                else:
                    self.delete_merge(x, i, i - 1)
            self.delete(x.children[i], k)

    def delete_internal_node(self, x, k, i):
        t = self.t
        if x.leaf:
            if x.keys[i] == k:
                x.keys.pop(i)
            return

        if len(x.children[i].keys) >= t:
            x.keys[i] = self.delete_predecessor(x.children[i])
            return
        elif len(x.children[i + 1].keys) >= t:
            x.keys[i] = self.delete_successor(x.children[i + 1])
            return
        else:
            self.delete_merge(x, i, i + 1)
            self.delete_internal_node(x.children[i], k, self.t - 1)

    def delete_predecessor(self, x):
        if x.leaf:
            return x.keys.pop()
        n = len(x.keys) - 1
        if len(x.children[n].keys) >= self.t:
            self.delete_sibling(x, n + 1, n)
        else:
            self.delete_merge(x, n, n + 1)
        self.delete_predecessor(x.children[n])

    def delete_successor(self, x):
        if x.leaf:
            return x.keys.pop(0)
        if len(x.children[1].keys) >= self.t:
            self.delete_sibling(x, 0, 1)
        else:
            self.delete_merge(x, 0, 1)
        self.delete_successor(x.children[0])

    def delete_merge(self, x, i, j):
        cnode = x.children[i]

        if j > i:
            rsnode = x.children[j]
            cnode.keys.append(x.keys[i])
            for k in range(len(rsnode.keys)):
                cnode.keys.append(rsnode.keys[k])
                if len(rsnode.children) > 0:
                    cnode.children.append(rsnode.children[k])
            if len(rsnode.children) > 0:
                cnode.children.append(rsnode.children.pop())
            new = cnode
            x.keys.pop(i)
            x.children.pop(j)
        else:
            lsnode = x.children[j]
            lsnode.keys.append(x.keys[j])
            for i in range(len(cnode.keys)):
                lsnode.keys.append(cnode.keys[i])
                if len(lsnode.children) > 0:
                    lsnode.children.append(cnode.children[i])
            if len(lsnode.children) > 0:
                lsnode.children.append(cnode.children.pop())
            new = lsnode
            x.keys.pop(j)
            x.children.pop(i)

        if x == self.root and len(x.keys) == 0:
            self.root = new

    def delete_sibling(self, x, i, j):
        cnode = x.children[i]
        if i < j:
            rsnode = x.children[j]
            cnode.keys.append(x.keys[i])
            x.keys[i] = rsnode.keys[0]
            if len(rsnode.children) > 0:
                cnode.children.append(rsnode.children[0])
                rsnode.children.pop(0)
            rsnode.keys.pop(0)
        else:
            lsnode = x.children[j]
            cnode.keys.insert(0, x.keys[i - 1])
            x.keys[i - 1] = lsnode.keys.pop()
            if len(lsnode.children) > 0:
                cnode.children.insert(0, lsnode.children.pop())

    def print_tree(self, x, level=0):
        print(f'Level {level}', end=": ")

        for i in x.keys:
            print(i, end=" ")

        print()
        level += 1

        if len(x.children) > 0:
            for i in x.children:
                self.print_tree(i, level)


def delete_example():
    first_leaf = Node(True)
    first_leaf.keys = [1, 9]

    second_leaf = Node(True)
    second_leaf.keys = [17, 19, 21]

    third_leaf = Node(True)
    third_leaf.keys = [23, 25, 27]

    fourth_leaf = Node(True)
    fourth_leaf.keys = [31, 32, 39]

    fifth_leaf = Node(True)
    fifth_leaf.keys = [41, 47, 50]

    sixth_leaf = Node(True)
    sixth_leaf.keys = [56, 60]

    seventh_leaf = Node(True)
    seventh_leaf.keys = [72, 90]

    root_left_child = Node()
    root_left_child.keys = [15, 22, 30]
    root_left_child.children.append(first_leaf)
    root_left_child.children.append(second_leaf)
    root_left_child.children.append(third_leaf)
    root_left_child.children.append(fourth_leaf)

    root_right_child = Node()
    root_right_child.keys = [55, 63]
    root_right_child.children.append(fifth_leaf)
    root_right_child.children.append(sixth_leaf)
    root_right_child.children.append(seventh_leaf)

    root = Node()
    root.keys = [40]
    root.children.append(root_left_child)
    root.children.append(root_right_child)

    B = BTree(3)
    B.root = root
    print('\n--- Original B-Tree ---\n')
    B.print_tree(B.root)

    print('\n--- Case 1: DELETED 21 ---\n')
    B.delete(B.root, 21)
    B.print_tree(B.root)

    print('\n--- Case 2a: DELETED 30 ---\n')
    B.delete(B.root, 30)
    B.print_tree(B.root)

    print('\n--- Case 2b: DELETED 27 ---\n')
    B.delete(B.root, 27)
    B.print_tree(B.root)

    print('\n--- Case 2c: DELETED 22 ---\n')
    B.delete(B.root, 22)
    B.print_tree(B.root)

    print('\n--- Case 3b: DELETED 17 ---\n')
    B.delete(B.root, 17)
    B.print_tree(B.root)

    print('\n--- Case 3a: DELETED 9 ---\n')
    B.delete(B.root, 9)
    B.print_tree(B.root)


def insert_and_search_example():
    B = BTree(3)

    for i in range(10):
        B.insert(i)

    B.print_tree(B.root)
    print()

    keys_to_search_for = [2, 9, 11, 4]
    for key in keys_to_search_for:
        if B.search(key) is not None:
            print(f'{key} is in the tree')
        else:
            print(f'{key} is NOT in the tree')


def main():
    print('\n--- INSERT & SEARCH ---\n')
    insert_and_search_example()

    delete_example()


main()

BTree.insert

BTree.delete

lista = [1,2,3,4]

lista.insert

lista.append

================================================
File: /Bootcamp - SQL e Analytics/Aula-13/README.md
================================================
# Aula 13 : Database Partition

Vamos detalhar cada parte do código:

1. **Criação da Tabela:**
   - Exemplo:
     ```sql
     CREATE TABLE pessoas (
         id SERIAL PRIMARY KEY,
         first_name VARCHAR(3),
         last_name VARCHAR(3),
         estado VARCHAR(3)
     );
     ```

2. **Inserção de 1 Milhão de Registros:**

   ```sql
   CREATE OR REPLACE FUNCTION random_estado()
   RETURNS VARCHAR(3) AS $$
   BEGIN
      RETURN CASE floor(random() * 5)
            WHEN 0 THEN 'SP'
            WHEN 1 THEN 'RJ'
            WHEN 2 THEN 'MG'
            WHEN 3 THEN 'ES'
            ELSE 'DF'
            END;
   END;
   $$ LANGUAGE plpgsql;

   -- Inserir dados na tabela pessoas com estados aleatórios
   INSERT INTO pessoas (first_name, last_name, estado)
   SELECT 
      substring(md5(random()::text), 0, 3),
      substring(md5(random()::text), 0, 3),
      random_estado()
   FROM 
      generate_series(1, 10000000);
     ```

3. **Criando um INDEX no first_name**

```sql
CREATE INDEX first_name_index ON pessoas(first_name)
```

4. **Fazendo  uma busca usando um INDEX**

```sql
SELECT COUNT(*) FROM pessoas WHERE first_name = 'aa'
```

Total query runtime: 585 msec.

5. **Fazendo  uma busca sem usar INDEX**

```sql
SELECT COUNT(*) FROM pessoas WHERE last_name = 'aa'
```

Total query runtime: 2 secs 552 msec.

6. **Vamos criar uma tabela com particionamento**


```sql
     CREATE TABLE pessoas (
         id SERIAL PRIMARY KEY,
         first_name VARCHAR(3),
         last_name VARCHAR(3),
         estado VARCHAR(3)
     ) PARTITION BY RANGE (id);
```

Opção mais simples

```sql
CREATE TABLE pessoas_part1 PARTITION OF pessoas FOR VALUES FROM (MINVALUE) TO (2000001);
CREATE TABLE pessoas_part2 PARTITION OF pessoas FOR VALUES FROM (2000001) TO (4000001);
CREATE TABLE pessoas_part3 PARTITION OF pessoas FOR VALUES FROM (4000001) TO (6000001);
CREATE TABLE pessoas_part4 PARTITION OF pessoas FOR VALUES FROM (6000001) TO (8000001);
CREATE TABLE pessoas_part5 PARTITION OF pessoas FOR VALUES FROM (8000001) TO (MAXVALUE);
```

Opção indireta

```sql
-- Criar as tabelas particionadas
CREATE TABLE pessoas_part1 (
    LIKE pessoas INCLUDING ALL,
    CHECK (id >= 1 AND id <= 2000000)
);

CREATE TABLE pessoas_part2 (
    LIKE pessoas INCLUDING ALL,
    CHECK (id > 2000000 AND id <= 4000000)
);

CREATE TABLE pessoas_part3 (
    LIKE pessoas INCLUDING ALL,
    CHECK (id > 4000000 AND id <= 6000000)
);

CREATE TABLE pessoas_part4 (
    LIKE pessoas INCLUDING ALL,
    CHECK (id > 6000000 AND id <= 8000000)
);

CREATE TABLE pessoas_part5 (
    LIKE pessoas INCLUDING ALL,
    CHECK (id > 8000000)  -- A última partição não precisa de limite superior
);
```

```sql
ALTER TABLE pessoas ATTACH PARTITION pessoas_part1 FOR VALUES FROM (MINVALUE) TO (2000001);
ALTER TABLE pessoas ATTACH PARTITION pessoas_part2 FOR VALUES FROM (2000001) TO (4000001);
ALTER TABLE pessoas ATTACH PARTITION pessoas_part3 FOR VALUES FROM (4000001) TO (6000001);
ALTER TABLE pessoas ATTACH PARTITION pessoas_part4 FOR VALUES FROM (6000001) TO (8000001);
ALTER TABLE pessoas ATTACH PARTITION pessoas_part5 FOR VALUES FROM (8000001) TO (MAXVALUE);
```

```sql
   INSERT INTO pessoas (first_name, last_name, estado)
   SELECT 
      substring(md5(random()::text), 0, 3),
      substring(md5(random()::text), 0, 3),
      random_estado()
   FROM 
      generate_series(1, 10000000);
```

## Funciona

```sql
select * from pessoas
```


## Criando com base em lista

```sql
CREATE TABLE pessoas (
    id SERIAL,
    first_name VARCHAR(3),
    last_name VARCHAR(3),
    estado VARCHAR(3),
    PRIMARY KEY (id, estado)
) PARTITION BY LIST (estado);

-- Criar as partições
CREATE TABLE pessoas_sp PARTITION OF pessoas FOR VALUES IN ('SP');
CREATE TABLE pessoas_rj PARTITION OF pessoas FOR VALUES IN ('RJ');
CREATE TABLE pessoas_mg PARTITION OF pessoas FOR VALUES IN ('MG');
CREATE TABLE pessoas_es PARTITION OF pessoas FOR VALUES IN ('ES');
CREATE TABLE pessoas_df PARTITION OF pessoas FOR VALUES IN ('DF');
```

================================================
File: /WSL/readme.md
================================================
### **Passo a passo: Configurando VS Code com WSL e acessando a pasta "projetos"**

---

### **1. Baixar e instalar a extensão WSL no VS Code**
1. Abra o **Visual Studio Code** no Windows.
2. Vá para a aba de **Extensões** (ou use `Ctrl+Shift+X`).
3. Na barra de pesquisa, digite:  
   ```
   Remote - WSL
   ```
4. Clique em **"Install"** para instalar a extensão **Remote - WSL**.
5. Após a instalação, reinicie o VS Code, se necessário.

---

### **2. Criar uma pasta chamada "projetos" na `home` usando o WSL**

1. Abra o **WSL** (pesquise "WSL" ou "Ubuntu" no menu Iniciar).
2. No terminal, digite:
   ```bash
   cd ~
   mkdir projetos
   cd projetos
   ```
3. Verifique se a pasta foi criada com:
   ```bash
   ls
   ```
   - O comando deve mostrar a pasta "projetos".

---

### **3. Acessar a pasta usando o Git Bash**

1. Abra o **Git Bash** no Windows.
2. Navegue até a pasta "projetos" com:
   ```bash
   cd //wsl$/Ubuntu/home/projetos/
   ```
3. Digite:
   ```bash
   ls
   ```
   - Se tudo estiver correto, você verá a pasta "projetos".

---

### **4. Acessar a pasta "projetos" pelo Explorador de Arquivos do Windows**

1. Abra o **Explorador de Arquivos**.
2. Na barra de endereços, digite:
   ```
   \\wsl$\Ubuntu\home\projetos\
   ```
3. Pressione **Enter**.  
   Agora você pode ver e gerenciar os arquivos dentro dessa pasta diretamente pelo Windows.

---

### **Dica Extra: Abrir a pasta diretamente no VS Code**

1. No terminal do **WSL**, navegue até a pasta:
   ```bash
   cd ~/projetos
   ```
2. Abra a pasta no VS Code com:
   ```bash
   code .
   ```
   - Isso abrirá o VS Code diretamente com a pasta "projetos" como seu workspace.

---

Agora você tem a pasta configurada, pode navegar por ela tanto no WSL quanto no Git Bash e Explorador de Arquivos, e integrá-la facilmente com o VS Code!

================================================
File: /Workshop - Git e Github/Aula_01/README.md
================================================
# Introdução ao Git - Aula 01

Bem-vindo ao workshop de introdução ao Git! Neste workshop, nosso objetivo é fornecer uma visão clara e prática sobre o uso do Git, ajudando você a aplicar essas habilidades em seus projetos de dados. O Git é uma ferramenta fundamental para o versionamento de código e colaboração em equipes de desenvolvimento, especialmente quando múltiplas pessoas estão trabalhando em um mesmo projeto.

## 📜 Problema no Desenvolvimento

### Desafios com o Desenvolvimento Colaborativo

Em projetos de software, especialmente na área de dados, é comum termos várias pessoas contribuindo ao mesmo tempo. Isso pode gerar conflitos de código, perda de trabalho e dificuldades em gerenciar diferentes versões de um mesmo arquivo. Antes do Git, essas situações frequentemente resultavam em erros e retrabalho, pois não existia um controle eficiente sobre as mudanças feitas no código.

**Perguntas para Reflexão:**

- **Como vocês costumam compartilhar código em equipe?** Será que enviar arquivos por e-mail ou usar drives compartilhados é a forma mais eficiente de colaborar?
- **Como garantir que todos os membros da equipe estão trabalhando na versão mais atual do código?** O que acontece quando diferentes pessoas fazem mudanças no mesmo arquivo simultaneamente?
- **Qual seria o impacto de perder uma semana de trabalho por causa de um conflito de código não resolvido?** Como podemos prevenir a perda de progresso no desenvolvimento de software?
- **Como vocês organizam as diferentes versões de um projeto?** Existe alguma estratégia que vocês usam para controlar quais mudanças foram feitas e por quem?
- **O que acontece quando queremos testar uma nova funcionalidade sem impactar o código que já está funcionando?** Como isolar essas mudanças para garantir que não introduzimos novos bugs?

## 📂 Vamos Criar um Projeto Simples

### 1. Criação de uma Pasta

Primeiro, vamos criar uma pasta para o nosso projeto:

```bash
mkdir projeto-git
cd projeto-git
```

### 2. Criação de um Arquivo Python

Dentro da pasta, vamos criar um arquivo Python simples:

```bash
touch main.py
```

Abra o arquivo `main.py` e adicione o seguinte código:

```python
print("Hello, World!")
```

### Exemplo de Modificações Sem o Git

Vamos seguir o exemplo onde modificamos um arquivo Python várias vezes e criamos novos arquivos, mas sem utilizar o Git para rastrear essas mudanças. Isso demonstra o problema de não ter controle de versão e como é fácil perder o histórico do que foi feito.

#### Passo 1: Criando e Modificando o Arquivo Inicial

Primeiro, criamos o arquivo `main.py` com o seguinte conteúdo:

```python
print("Hello, World!")
```

Essa é a primeira versão do arquivo. Agora, imagine que você precisa fazer algumas mudanças.

#### Passo 2: Primeira Modificação

Você modifica o arquivo `main.py` para adicionar uma nova linha de código:

```python
print("Hello, World!")
print("Primeira Modificação")
```

Essa mudança é feita diretamente no arquivo, substituindo a versão anterior. Como não estamos usando Git, a versão original do arquivo é perdida, e não temos mais acesso a ela.

#### Passo 3: Segunda Modificação

Depois de algum tempo, você decide modificar o arquivo novamente:

```python
print("Hello, World!")
print("Primeira Modificação")
print("Segunda Modificação")
```

Mais uma vez, o arquivo original e a primeira modificação são substituídos por essa nova versão. Sem o Git, não há registro das mudanças anteriores.

#### Passo 4: Terceira Modificação e Criação de um Novo Arquivo

Agora, você faz uma terceira modificação no `main.py` e também cria um novo arquivo chamado `auxiliary.py`:

**main.py:**

```python
print("Hello, World!")
print("Primeira Modificação")
print("Segunda Modificação")
print("Terceira Modificação")
```

**auxiliary.py:**

```python
def helper():
    print("Função Auxiliar")
```

Essas novas mudanças também são feitas diretamente no arquivo, substituindo tudo o que havia antes. Como não estamos usando Git, o histórico das três versões anteriores do `main.py` é completamente perdido.

### O Problema Sem Git

Sem o Git, não temos como recuperar o estado anterior do arquivo `main.py` em nenhum desses momentos. Se algo der errado, não há como voltar para uma versão anterior. Além disso, se tivermos conflitos ou dúvidas sobre o que foi mudado ao longo do tempo, não temos um histórico para consultar.

### Como o Git Resolveria Esse Problema

Se estivéssemos usando Git, cada uma dessas modificações poderia ter sido feita em um novo commit. Isso significaria que, a qualquer momento, poderíamos voltar a uma versão anterior do arquivo ou ver exatamente o que mudou entre os commits. Também poderíamos criar branches para testar novas funcionalidades sem afetar o código principal, garantindo que sempre tivéssemos uma versão estável do projeto.

## 🐧 História do Git

### História do Git: A Palestra Famosa de Linus Torvalds

Git foi criado em 2005 por Linus Torvalds, o criador do Linux, em resposta à necessidade de um sistema de controle de versão robusto e eficiente para o desenvolvimento do kernel Linux. Na época, o time de desenvolvimento enfrentava desafios significativos para gerenciar contribuições de milhares de desenvolvedores ao redor do mundo. O Git foi concebido para ser rápido, distribuído e capaz de lidar com a complexidade de projetos desse porte.

### A Famosa Palestra de Linus Torvalds

Em uma palestra bem conhecida, Linus Torvalds falou sobre a criação do Git e como ele o desenvolveu em apenas alguns dias. Com seu humor característico, ele mencionou que decidiu criar o Git durante um final de semana, após ficar frustrado com as limitações das ferramentas de controle de versão existentes na época.

Linus explicou que, ao criar o Git, ele focou em três principais características: velocidade, simplicidade no design e suporte para desenvolvimento distribuído. Ele queria uma ferramenta que fosse fácil de usar para desenvolvedores de todo o mundo, permitindo que cada um tivesse uma cópia completa do repositório, sem a necessidade de um servidor central, e que fosse rápida o suficiente para lidar com as necessidades do kernel Linux.

Durante a palestra, Linus destacou a importância da liberdade e do controle que o Git proporciona aos desenvolvedores, algo que faltava nas ferramentas anteriores. Ele também brincou sobre o fato de que, embora tenha começado o projeto em um final de semana, o Git rapidamente se tornou uma das ferramentas mais importantes e amplamente adotadas na história do desenvolvimento de software.

Essa história ilustra não só a genialidade de Linus Torvalds, mas também a urgência e a necessidade que existiam por uma solução como o Git. Desde então, o Git se tornou a ferramenta padrão para controle de versão em quase todos os projetos de software no mundo.

Para entender mais sobre Git:
- [Sobre Git com Akita e Palestra de Linus Torvalds sobre Git](https://www.youtube.com/watch?v=6Czd1Yetaac)

## 🛠️ O Que é o Git?

### Um Programa Como Qualquer Outro

Git é um programa que você instala em seu computador, semelhante a como o PowerBI é utilizado para criar dashboards. No caso do Git, ele é usado para versionar arquivos de texto, como código-fonte, de maneira eficiente. Isso permite que você:

- **Versione Arquivos:** Mantenha um histórico detalhado de todas as mudanças feitas em seus arquivos.
- **Colabore Facilmente:** Trabalhe com muitos contribuidores de forma organizada.
- **Distribua Código:** Compartilhe seu código com outras pessoas, garantindo que todos estejam sincronizados com a versão mais recente.

## 💻 Como Instalar o Git

### Windows

1. Baixe o instalador do Git [aqui](https://git-scm.com/download/win).
2. Execute o instalador e siga as instruções.

### Linux

1. Abra o terminal.
2. Execute o comando:
   
   ```bash
   sudo apt-get install git
   ```

### Mac

1. Abra o terminal.
2. Execute o comando:
   
   ```bash
   brew install git
   ```

## 🎯 Configuração do Git

Antes de começar a usar o Git, precisamos configurar o nome e o e-mail do usuário:

```bash
git config --global user.name "Seu Nome"
git config --global user.email "seu.email@exemplo.com"
```

Os arquivos de configuração do Git que armazenam as configurações feitas com os comandos `git config` são armazenados em diferentes locais, dependendo do nível de configuração:

1. **Configurações Globais (`--global`)**:
   - As configurações globais são salvas no arquivo `.gitconfig` localizado no diretório home do usuário.
   - **Localização**:
     - **Linux/Mac**: `~/.gitconfig`
     - **Windows**: `C:\Users\SeuNomeDeUsuario\.gitconfig`
   - Você pode abrir esse arquivo em um editor de texto para visualizar ou editar as configurações.

2. **Configurações de Sistema (`--system`)**:
   - As configurações de sistema são aplicadas a todos os usuários da máquina e são armazenadas no arquivo de configuração global do sistema.
   - **Localização**:
     - **Linux**: `/etc/gitconfig`
     - **Windows**: Pode estar em um caminho como `C:\Program Files\Git\etc\gitconfig`
   - Essas configurações requerem permissões de administrador para serem alteradas.

3. **Configurações Locais (por repositório)**:
   - As configurações locais são específicas para um único repositório Git e são salvas no arquivo `config` dentro da pasta `.git` do repositório.
   - **Localização**:
     - No diretório do repositório Git: `.git/config`

Você pode visualizar as configurações atuais usando os seguintes comandos:

- **Para ver todas as configurações globais**:
  ```bash
  git config --global --list
  ```

Esses comandos vão listar as configurações e seus valores, permitindo que você veja detalhes como o nome de usuário e o e-mail configurados para o Git.

### Exemplo Completo Usando Git: Passo a Passo

Vamos seguir um exemplo onde fazemos modificações em um arquivo Python e gerenciamos essas alterações usando Git. Este processo incluirá a criação de commits para cada modificação, além de explorar conceitos importantes como o `HEAD`, branches, e o comando `git checkout`.

### 1. Criação do Repositório e Primeira Modificação

#### Inicializando o Repositório

Primeiro, vamos inicializar um novo repositório Git no diretório do projeto:

```bash
git init
```

Este comando cria um repositório Git vazio, onde começaremos a rastrear nossas alterações.

#### Criando e Adicionando o Arquivo ao Controle de Versão

Vamos criar um arquivo Python chamado `main.py`:

```bash
touch main.py
```

Abra o arquivo `main.py` e adicione o

 seguinte código:

```python
print("Hello, World!")
```

Agora, vamos verificar o estado do repositório para ver como o Git está reconhecendo o arquivo:

```bash
git status
```

Você verá que `main.py` está listado como um arquivo não rastreado (untracked). Vamos adicionar esse arquivo ao Git para que ele comece a ser rastreado:

```bash
git add main.py
```

Agora, faremos o primeiro commit para salvar o estado inicial do projeto:

```bash
git commit -m "Adiciona o arquivo main.py com um simples print"
```

### 2. Primeira Modificação e Novo Commit

Agora, vamos modificar o arquivo `main.py`:

```python
print("Hello, World!")
print("Primeira Modificação")
```

Depois de fazer a modificação, vamos verificar novamente o estado dos arquivos:

```bash
git status
```

Agora observamos que temos duas opções

```mermaid
graph TD;

A[Modified main.py in Working Directory] --> B{Choose an Option};

B --> |"git add main.py"| C[Staging Area];
C --> D["git commit -m 'Update main.py'"];
D --> E[New Commit Saved in Git Repository];

B --> |"git restore main.py"| F[Working Directory Restored to Last Commit];
F --> G["Restored from Git Repository"];

subgraph Git Repository
    E
    G
end
```

### Explicação do Fluxo:

- **Opção 1: `git add`**:
    - **`git add main.py`**: As mudanças no `main.py` são movidas para a Staging Area.
    - **`git commit -m 'Update main.py'`**: Um novo commit é criado, e as mudanças são salvas na caixa do Git Repository.

- **Opção 2: `git restore`**:
    - **`git restore main.py`**: O arquivo `main.py` no Working Directory é restaurado a partir da última versão salva no Git Repository, descartando as mudanças feitas localmente.

Esse diagrama ilustra claramente como as mudanças fluem entre o Working Directory, a Staging Area, e o Git Repository, dependendo da ação escolhida (`git add` ou `git restore`).

O Git mostrará que o arquivo `main.py` foi modificado. Vamos adicionar essa modificação à área de staging e fazer um novo commit:

### 2. Primeira Modificação e Novo Commit

```bash
git add main.py
```

### Git status

Ao realizar o Git status observamos que temos 2 opções novamente

```mermaid
graph TD;

A[Modified main.py in Staging Area] --> B{Choose an Option};

B --> |"git commit -m 'Update main.py'"| C[New Commit Saved in Git Repository];
B --> |"git restore --staged main.py"| D[Unstaged, Returned to Working Directory];

subgraph Git Repository
    C
end

D --> E[main.py in Working Directory];
```

### Explicação do Fluxo:

- **Opção 1: `git commit`**:
    - **`git commit -m 'Update main.py'`**: Cria um novo commit no Git Repository, salvando as mudanças que estavam na Staging Area.

- **Opção 2: `git restore --staged`**:
    - **`git restore --staged main.py`**: Remove o arquivo `main.py` da Staging Area, retornando-o ao Working Directory sem as mudanças serem cometidas. Ele volta ao estado antes de ser adicionado à Staging Area.

Vamos seguir com o commit

### 2. Primeiro Save no Commit

```bash
git commit -m "Adiciona a primeira modificação ao arquivo main.py"
```

### Fluxo

```mermaid
graph TD;

subgraph Working Directory
    A[main.py Modified]
end

subgraph Staging Area
    B[main.py Staged]
end

subgraph Git Repository
    C[main.py Committed]
end

A --> |"git add"| B;
B --> |"git commit"| C;
C --> |"git restore"| A;
B --> |"git restore --staged"| A;
```

### 3. Segunda Modificação e Novo Commit

Vamos modificar o arquivo novamente:

```python
print("Hello, World!")
print("Primeira Modificação")
print("Segunda Modificação")
```

Novamente, adicionamos e fazemos um commit dessas mudanças:

```bash
git add main.py
git commit -m "Adiciona a segunda modificação ao arquivo main.py"
```

### 4. Terceira Modificação e Novo Commit

Finalmente, vamos adicionar uma terceira modificação:

```python
print("Hello, World!")
print("Primeira Modificação")
print("Segunda Modificação")
print("Terceira Modificação")
```

E, novamente, fazemos o commit:

```bash
git add main.py
git commit -m "Adiciona a terceira modificação ao arquivo main.py"
```

### 5. Verificando o Histórico de Commits

Agora, podemos usar o `git log` para visualizar o histórico de commits e ver todas as modificações que fizemos até agora:

```bash
git log
```

O `git log` exibirá uma lista de todos os commits, mostrando as mensagens e os identificadores únicos (hashes) dos commits.

### 6. Entendendo o `HEAD`

O `HEAD` é um apontador especial que indica o commit atual em que você está trabalhando. Normalmente, o `HEAD` aponta para a branch `main`, que é a linha principal de desenvolvimento do projeto.

**Ilustração com Mermaid:**

```mermaid
graph TD;
    A[Commit Inicial] --> B[Primeira Modificação];
    B --> C[Segunda Modificação];
    C --> D[Terceira Modificação];
    E(HEAD -> main) --> D;
```

Aqui, o `HEAD` está apontando para o commit mais recente na branch `main`. Isso significa que todas as operações, como novos commits, partirão desse ponto.

### 7. Trabalhando com Branches

Branches são como linhas do tempo paralelas no seu repositório. Elas permitem que você trabalhe em diferentes funcionalidades ou correções de bugs sem afetar a `main`.

**Criando uma Nova Branch:**

Vamos criar uma nova branch chamada `nova-feature` para trabalhar em uma nova funcionalidade:

```bash
git branch nova-feature
```

Agora, podemos mudar para essa branch e começar a trabalhar nela:

```bash
git checkout nova-feature
```

Isso muda o `HEAD` para a nova branch `nova-feature`, o que significa que qualquer commit feito agora será registrado nessa branch.

**Ilustração com Mermaid:**

```mermaid
graph TD;
    A[Commit Inicial] --> B[Primeira Modificação];
    B --> C[Segunda Modificação];
    C --> D[Terceira Modificação];
    E(HEAD -> nova-feature) --> D;
    F(nova-feature) --> D;
    G(main) --> D;
```

Aqui, a `nova-feature` diverge da `main` a partir do mesmo ponto, permitindo que você desenvolva funcionalidades de forma isolada.

### 8. Usando `git checkout` para Navegar Entre Commits e Branches

O comando `git checkout` permite que você navegue entre diferentes branches e commits. Se você quiser voltar para a branch `main`, pode usar:

```bash
git checkout main
```

Se quiser explorar o estado do projeto em um commit anterior, use:

```bash
git checkout <hash_do_commit>
```

Isso coloca você em um estado de "detached HEAD", onde você pode ver o estado do projeto naquele momento específico.

### 9. Usando `git reset` para Voltar ao Commit Anterior

Se você deseja desfazer as últimas mudanças e voltar ao estado de um commit anterior, pode usar o `git reset`. Aqui estão as opções:

- **`git reset --soft <idCommit>`**: Volta para o commit anterior e mantém todas as alterações na área de staging.

- **`git reset --mixed <idCommit>`**: Volta para o commit anterior, remove as alterações da área de staging, mas as mantém no diretório de trabalho.

- **`git reset --hard <idCommit>`**: Volta para o commit anterior e descarta completamente todas as alterações feitas após esse commit.

**Ilustração com Mermaid:**

```mermaid
graph TD;
    A[Commit Inicial] --> B[Primeira Modificação];
    B --> C[Segunda Modificação];
    C --> D[Terceira Modificação];
    E(HEAD -> nova-feature) --> D;
    F(nova-feature) --> D;
    G(main) --> D;
    H[Reset --hard] --> B;
```

Aqui, o comando `git reset --hard` move o `HEAD` de volta para a "Primeira Modificação", descartando todas as alterações feitas após esse ponto.

### 10. Criando uma Branch a Partir de um Commit Anterior

Se você quer preservar o estado atual do projeto, mas precisa voltar a um commit anterior para experimentar algo novo, pode criar uma nova branch a partir desse commit:

```bash
git checkout -b experiment <hash_do_commit>
```

Isso cria uma nova branch chamada `experiment`, começando a partir do commit que você especificou.

**Ilustração com Mermaid:**

```mermaid
graph TD;
    A[Commit Inicial] --> B[Primeira Modificação];
    B --> C[Segunda Modificação];
    C --> D[Terceira Modificação];
    E(HEAD -> experiment) --> B;
    F(main) --> D;
```

### Caso real

Vamos expandir o exemplo para incluir três branches diferentes, cada uma com uma modificação específica em um arquivo, e um branch `main` que representa o código em produção. Vou descrever a situação e depois mostrar o diagrama em Mermaid.

### Situação:

1. **Branch `main`**: Contém o código de produção, sem as novas funcionalidades que estamos desenvolvendo.
2. **Branch `feature-1`**: Modifica o `file1.py` para adicionar uma nova funcionalidade.
3. **Branch `feature-2`**: Modifica o `file2.py` para adicionar outra funcionalidade.
4. **Branch `feature-3`**: Modifica o `file3.py` para adicionar uma terceira funcionalidade.

### Passos:

1. **Criar e mudar para a branch `feature-1`**:
    ```bash
    git branch feature-1
    git checkout feature-1
    # Modificar file1.py e fazer commit
    git commit -am "Adiciona nova funcionalidade em file1.py"
    ```

2. **Criar e mudar para a branch `feature-2`**:
    ```bash
    git branch feature-2
    git checkout feature-2
    # Modificar file2.py e fazer commit
    git commit -am "Adiciona nova funcionalidade em file2.py"
    ```

3. **Criar e mudar para a branch `feature-3`**:
    ```bash
    git branch feature-3
    git checkout feature-3
    # Modificar file3.py e fazer commit
    git commit -am "Adiciona nova funcionalidade em file3.py"
    ```

### Diagrama Mermaid:

Aqui está o diagrama que ilustra essas operações:

```mermaid
graph TD;
    subgraph main [Branch: main Production]
        A[Initial Commit] --> B[Main Codebase];
    end

    subgraph feature-1 [Branch: feature-1]
        B --> C[Commit: Modifica file1.py];
    end

    subgraph feature-2 [Branch: feature-2]
        B --> D[Commit: Modifica file2.py];
    end

    subgraph feature-3 [Branch: feature-3]
        B --> E[Commit: Modifica file3.py];
    end

    A --- B;
    C --> G[feature-1];
    D --> H[feature-2];
    E --> I[feature-3];

    style B fill:#f9f,stroke:#333,stroke-width:4px;
    style C fill:#bbf,stroke:#333,stroke-width:2px;
    style D fill:#bfb,stroke:#333,stroke-width:2px;
    style E fill:#fbf,stroke:#333,stroke-width:2px;
```

### Explicação do Diagrama:

- **Branch `main`**: Representa o código em produção, onde não foram aplicadas as novas funcionalidades.
- **Branch `feature-1`**: Diverge do `main` após o commit inicial e inclui uma modificação em `file1.py`.
- **Branch `feature-2`**: Diverge do `main` após o commit inicial e inclui uma modificação em `file2.py`.
- **Branch `feature-3`**: Diverge do `main` após o commit inicial e inclui uma modificação em `file3.py`.

Cada branch permite que você trabalhe em funcionalidades diferentes de forma isolada. Os commits em cada branch representam o trabalho feito nessas funcionalidades. Quando as funcionalidades estiverem prontas e testadas, você poderá mesclar (`merge`) essas branches de volta ao `main` para que as novas funcionalidades sejam incorporadas ao código de produção.

Vamos continuar o exemplo, adicionando os comandos para mesclar as branches de funcionalidades (`feature-1`, `feature-2`, `feature-3`) de volta ao `main` quando as funcionalidades estiverem prontas.

### Situação Revisada:

1. **Branch `main`**: Contém o código de produção.
2. **Branch `feature-1`**: Modifica o `file1.py` para adicionar uma nova funcionalidade.
3. **Branch `feature-2`**: Modifica o `file2.py` para adicionar outra funcionalidade.
4. **Branch `feature-3`**: Modifica o `file3.py` para adicionar uma terceira funcionalidade.

### Passos Revisados:

1. **Criar e mudar para a branch `feature-1`**:
    ```bash
    git branch feature-1
    git checkout feature-1
    # Modificar file1.py e fazer commit
    git commit -am "Adiciona nova funcionalidade em file1.py"
    ```

2. **Criar e mudar para a branch `feature-2`**:
    ```bash
    git branch feature-2
    git checkout feature-2
    # Modificar file2.py e fazer commit
    git commit -am "Adiciona nova funcionalidade em file2.py"
    ```

3. **Criar e mudar para a branch `feature-3`**:
    ```bash
    git branch feature-3
    git checkout feature-3
    # Modificar file3.py e fazer commit
    git commit -am "Adiciona nova funcionalidade em file3.py"
    ```

### Mesclando as Branches de Funcionalidade no `main`:

Depois que cada funcionalidade estiver pronta, você pode mesclar essas branches de volta ao `main`. Aqui estão os passos:

1. **Mesclar `feature-1` no `main`**:
    ```bash
    git checkout main
    git merge feature-1
    ```

2. **Mesclar `feature-2` no `main`**:
    ```bash
    git checkout main
    git merge feature-2
    ```

3. **Mesclar `feature-3` no `main`**:
    ```bash
    git checkout main
    git merge feature-3
    ```

### Diagrama Mermaid Atualizado:

Aqui está o diagrama que ilustra essas operações, incluindo as etapas de merge:

```mermaid
graph TD;
    subgraph main [Branch: main Production]
        A[Initial Commit] --> B[Main Codebase];
        F[Merge feature-1] --> G[Merge feature-2];
        G --> H[Merge feature-3];
    end

    subgraph feature-1 [Branch: feature-1]
        B --> C[Commit: Modifica file1.py];
    end

    subgraph feature-2 [Branch: feature-2]
        B --> D[Commit: Modifica file2.py];
    end

    subgraph feature-3 [Branch: feature-3]
        B --> E[Commit: Modifica file3.py];
    end

    C --> F;
    D --> G;
    E --> H;
```

### Explicação do Diagrama Atualizado:

- **Branch `main`**: Representa o código de produção. Inicialmente, contém apenas o commit inicial e o código base.
- **Branches de Funcionalidade**:
  - **`feature-1`**: Contém a modificação em `file1.py`.
  - **`feature-2`**: Contém a modificação em `file2.py`.
  - **`feature-3`**: Contém a modificação em `file3.py`.
- **Mesclagens (`Merges`)**:
  - Cada branch de funcionalidade é mesclada de volta ao `main`, integrando as novas funcionalidades no código de produção.

### Comandos de Merge:

- **Mesclar `feature-1` no `main`**:
  ```bash
  git checkout main
  git merge feature-1
  ```

- **Mesclar `feature-2` no `main`**:
  ```bash
  git checkout main
  git merge feature-2
  ```

- **Mesclar `feature-3` no `main`**:
  ```bash
  git checkout main
  git merge feature-3
  ```

### Conclusão:

Esse fluxo permite que cada funcionalidade seja desenvolvida em isolamento, testada individualmente e, quando pronta, integrada ao código de produção sem afetar o `main` até que tudo esteja pronto. Isso torna o processo de desenvolvimento mais seguro e organizado, minimizando conflitos e problemas na integração das funcionalidades.

### O que vamos ver amanhã:

Amanhã, vamos explorar em detalhes o conceito de **Remote Repository** no Git. Até agora, vimos como o **Working Directory**, a **Staging Area**, e o **Local Git Repository** trabalham juntos na sua máquina local para gerenciar as mudanças no seu projeto.

Agora, vamos entender como o **Remote Repository** se encaixa nesse fluxo. O **Remote Repository** é uma versão do seu repositório que fica armazenada em um servidor remoto, como o GitHub, GitLab ou Bitbucket. Ele permite que você:

- **Compartilhe Código com Outros Desenvolvedores**: Enviar (push) seus commits para um repositório remoto permite que outros desenvolvedores acessem e colaborem no seu projeto.
- **Mantenha um Backup Externo**: Ter uma cópia do seu repositório em um servidor remoto fornece uma camada extra de segurança para o seu trabalho.
- **Colabore de Forma Eficiente**: Usar um repositório remoto facilita a colaboração entre times, onde cada membro pode clonar, puxar (pull) e enviar mudanças para o repositório compartilhado.

No diagrama que revisamos, o **Remote Repository** é representado como o destino para onde você envia as mudanças feitas no **Local Git Repository**. Amanhã, veremos como configurar e trabalhar com repositórios remotos, incluindo comandos essenciais como `git push`, `git pull`, e `git clone`, para que você possa colaborar efetivamente em projetos de dados com outras pessoas.

```mermaid
graph TD;

A[Working Directory] --> |"git add"| B[Staging Area];
B --> |"git commit"| C[Commit];
C --> |"Stored in"| D[Local Git Repository];

subgraph Local Machine
    A
    B
    C
    D
end

D --> |"git push"| E[Remote Repository];
```

================================================
File: /Workshop - Git e Github/Aula_02/README.md
================================================
# Introdução ao GitHub e Repositórios Remotos - Aula 02

Bem-vindo à nossa aula sobre GitHub e repositórios remotos! Hoje, vamos explorar como o uso de um repositório remoto, como o GitHub, pode resolver vários desafios que surgem ao trabalhar com um repositório Git local. Veremos como configurar sua conta no GitHub, migrar seu projeto local para lá, e utilizar as principais funcionalidades dessa plataforma.

## 1. Problemas ao Ter Apenas um Git Local

### Desafios de Trabalhar Somente com Git Local

Quando trabalhamos apenas com um repositório Git local, enfrentamos algumas limitações:

- **Colaboração Limitada**: Compartilhar código com outros desenvolvedores exige o uso de métodos manuais, como enviar arquivos por e-mail, o que é ineficiente e propenso a erros.
- **Falta de Backup**: Sem um repositório remoto, todo o código fica armazenado em sua máquina local. Se o disco rígido falhar, você pode perder todo o trabalho.
- **Histórico de Projetos Restrito**: Manter um histórico de versões só em sua máquina impede que outros colaboradores acessem facilmente o progresso do projeto.

**Diagrama Ilustrativo**:

```mermaid
graph TD;

A1[Working Directory - Colaborador 1] --> |"git add"| B1[Staging Area - Colaborador 1];
B1 --> |"git commit"| C1[Commit - Colaborador 1];
C1 --> |"Stored in"| D1[Local Git Repository - Colaborador 1];

A2[Working Directory - Colaborador 2] --> |"git add"| B2[Staging Area - Colaborador 2];
B2 --> |"git commit"| C2[Commit - Colaborador 2];
C2 --> |"Stored in"| D2[Local Git Repository - Colaborador 2];

A3[Working Directory - Colaborador 3] --> |"git add"| B3[Staging Area - Colaborador 3];
B3 --> |"git commit"| C3[Commit - Colaborador 3];
C3 --> |"Stored in"| D3[Local Git Repository - Colaborador 3];

subgraph Local Machine - Colaborador 1
    A1
    B1
    C1
    D1
end

subgraph Local Machine - Colaborador 2
    A2
    B2
    C2
    D2
end

subgraph Local Machine - Colaborador 3
    A3
    B3
    C3
    D3
end

D1 --> |"git push"| E[Remote Repository];
D2 --> |"git push"| E[Remote Repository];
D3 --> |"git push"| E[Remote Repository];

```

## 2. Quais Opções Temos?

### Opções de Repositórios Remotos

Existem várias plataformas que fornecem serviços de repositórios remotos, cada uma com suas características:

- **GitHub**: Popular e amplamente utilizado, com forte integração com ferramentas de CI/CD e uma grande comunidade de desenvolvedores.
- **GitLab**: Focado em DevOps, com recursos avançados de CI/CD e privacidade aprimorada.
- **Bitbucket**: Integrado com a Atlassian (Jira, Confluence), popular em ambientes corporativos.

**Diagrama Ilustrativo**:

```mermaid
graph LR;
    A[Local Git Repository] --> B[GitHub];
    A --> C[GitLab];
    A --> D[Bitbucket];
```

## 3. GitHub: Uma Visão Geral

### O que é GitHub?

O GitHub é uma plataforma de hospedagem de código que oferece controle de versão distribuído e funcionalidades de colaboração para desenvolvedores de software. Ele facilita o gerenciamento de repositórios Git e fornece ferramentas para revisão de código, gerenciamento de projetos, integração contínua, e mais.

**Principais Recursos**:
- **Pull Requests (PRs)**: Facilita a revisão de código e a colaboração.
- **Issues**: Gerenciamento de tarefas e bugs.
- **Actions**: Automação de fluxos de trabalho com CI/CD.

**Diagrama Ilustrativo**:

```mermaid
graph TD;
    A[GitHub] --> B[Pull Requests];
    A --> C[Issues];
    A --> D[Actions];
```

## 4. Criando Nossa Conta e Configurando o GitHub

### Configurando GitHub e SSH

1. **Criando uma Conta no GitHub**:
   - Visite [GitHub.com](https://github.com) e crie uma conta gratuita.

### Configurando Autenticação SSH para GitHub

A autenticação SSH é uma forma segura de conectar seu repositório local ao GitHub, sem precisar inserir seu nome de usuário e senha toda vez que fizer um push ou pull. Aqui está o passo a passo para criar uma chave SSH, adicionar essa chave à sua conta do GitHub, e usá-la para autenticação.

### Passo a Passo para Criar uma Chave SSH

1. **Gerar uma Nova Chave SSH**:

   No terminal, execute o seguinte comando:
   ```bash
   ssh-keygen -t rsa
   ```

   - **O que esse comando faz**:
     - **`ssh-keygen`**: Este comando é usado para gerar um novo par de chaves SSH.
     - **`-t rsa`**: Especifica o tipo de chave a ser gerado, que neste caso é RSA (um dos tipos mais comuns e seguros).

   - **O que ele cria**:
     - Este comando gera dois arquivos:
       - **`id_rsa`**: A chave privada (não compartilhe este arquivo com ninguém).
       - **`id_rsa.pub`**: A chave pública (essa é a chave que você vai adicionar ao GitHub).

2. **Salvar a Chave SSH**:

   Após executar o comando, você verá um prompt pedindo onde salvar o arquivo:

   ```
   Enter file in which to save the key (/Users/username/.ssh/id_rsa):
   ```

   - **O que fazer**: Apenas pressione **Enter** para aceitar o caminho padrão (`~/.ssh/id_rsa`). Isso salva a chave na pasta `.ssh` no diretório home do seu usuário.

3. **Definir uma Senha para a Chave (Opcional)**:

   Em seguida, você será solicitado a definir uma senha para proteger a chave SSH:

   ```
   Enter passphrase (empty for no passphrase):
   ```

   - **O que fazer**: Pressione **Enter** novamente para não definir uma senha. Isso significa que você não precisará digitar uma senha cada vez que usar a chave SSH.

   ```
   Enter same passphrase again:
   ```

   - **O que fazer**: Pressione **Enter** novamente.

   **Nota**: Definir uma senha adiciona uma camada extra de segurança, mas pode ser inconveniente se você precisar digitar a senha frequentemente.

4. **Verificando a Chave SSH Gerada**:

   Para ver o conteúdo da chave pública (aquela que você vai compartilhar com o GitHub), execute o seguinte comando:

   ```bash
   cat ~/.ssh/id_rsa.pub
   ```

   - **O que isso faz**: Mostra o conteúdo do arquivo `id_rsa.pub` no terminal. Este conteúdo é a chave pública que você precisa copiar e adicionar à sua conta do GitHub.
   - **Resultado esperado**: Algo parecido com isso será exibido:
     ```
     ssh-rsa AAAAB3... rest of the key ... your.email@example.com
     ```

5. **Adicionar a Chave SSH ao GitHub**:

   Agora, com a chave pública copiada, vá até sua conta do GitHub e siga os passos:

   - Vá para *Settings* (Configurações).
   - Na seção *SSH and GPG keys*, clique em *New SSH key*.
   - Cole a chave pública que você copiou do terminal no campo *Key*.
   - Dê um nome à chave para identificá-la facilmente (por exemplo, "Meu PC").
   - Clique em *Add SSH key*.

6. **Verificar a Conexão SSH com o GitHub**:

   Para garantir que tudo está configurado corretamente, você pode testar a conexão SSH com o GitHub:

   ```bash
   ssh -T git@github.com
   ```

   - **O que esperar**: Se tudo estiver configurado corretamente, você verá uma mensagem como:
     ```
     Hi username! You've successfully authenticated, but GitHub does not provide shell access.
     ```

### Conclusão

Com esses passos, você configurou uma autenticação SSH para o GitHub. Isso significa que, da próxima vez que fizer um `git push` ou `git pull`, o Git utilizará a chave SSH para autenticação, evitando a necessidade de inserir seu nome de usuário e senha manualmente. Esse método é mais seguro e conveniente, especialmente para desenvolvedores que frequentemente interagem com repositórios Git.

## 5. Migrando Nosso Projeto para o GitHub

### Usando `git remote` e `git push`

1. **Criando um Novo Repositório no GitHub**:
   - No GitHub, clique em "New Repository" e crie um repositório vazio.

2. **Adicionando o Repositório Remoto**:
   - No terminal, vincule seu repositório local ao GitHub:
     ```bash
     git remote add origin https://github.com/usuario/repo.git
     ```

O comando `git remote add origin https://github.com/usuario/repo.git` é usado para vincular seu repositório Git local a um repositório remoto no GitHub (ou em outra plataforma de hospedagem de código). Esse comando essencialmente cria uma referência para o repositório remoto, permitindo que você envie (push) ou traga (pull) alterações entre o repositório local e o remoto.

Aqui está uma explicação mais detalhada:

- **`git remote add`**: Esse é o comando que adiciona um novo repositório remoto ao seu repositório local. Você pode ter múltiplos repositórios remotos associados a um único repositório local, cada um com um nome diferente (por exemplo, `origin`, `upstream`, etc.).

- **`origin`**: Este é o nome padrão dado ao repositório remoto principal. O termo "origin" é apenas um nome que você escolhe para se referir ao repositório remoto. Você pode renomeá-lo, se desejar, mas `origin` é o nome padrão e mais comum.

- **`https://github.com/usuario/repo.git`**: Esse é o URL do repositório remoto no GitHub. Ele indica a localização exata do repositório no qual você deseja enviar seu código. Esse URL pode estar no formato HTTP(S) (como neste caso) ou SSH, dependendo de como você configurou a autenticação.

Depois de executar esse comando, o repositório local agora conhece a localização do repositório remoto e o nome `origin` está associado a esse URL. Isso permite que você use comandos como `git push origin main` para enviar suas alterações para o repositório remoto ou `git pull origin main` para trazer as mudanças mais recentes do remoto para o seu repositório local.

3. **Enviando o Código para o GitHub**:
   - Envie o código para o GitHub:
     ```bash
     git push -u origin main
     ```

Ao executar o comando `git push -u origin main`, o Git tentará enviar os commits do seu repositório local para o repositório remoto no GitHub. **Neste momento**, se você estiver usando HTTPS para acessar o GitHub (o que é indicado pelo URL `https://github.com/usuario/repo.git`), o Git solicitará que você insira seu nome de usuário e senha do GitHub para autenticar a operação.

### O que esperar:

- **Nome de Usuário e Senha**: Quando o Git tenta fazer o push para o repositório remoto pela primeira vez, ele precisa autenticar sua identidade com o GitHub. Você verá uma solicitação no terminal pedindo para inserir seu nome de usuário e senha.
  
- **Autenticação com Tokens**: Desde agosto de 2021, o GitHub não aceita mais senhas para autenticação ao usar HTTPS. Em vez disso, você precisará usar um "Personal Access Token" (token de acesso pessoal) no lugar da senha. Este token pode ser gerado na sua conta do GitHub, na seção de configurações de desenvolvedor. Quando solicitado pela senha, insira o token.

### Explicando em mais detalhes:

- **Nome de Usuário e Token**: 
  - **Nome de Usuário**: O seu nome de usuário do GitHub.
  - **Token**: Um token de acesso pessoal que você deve gerar no GitHub e usar no lugar da senha.

- **Por que isso acontece?**
  - **Autenticação HTTPS**: Quando você usa HTTPS para acessar o repositório, o Git precisa garantir que você tem as permissões necessárias para enviar (push) alterações. Isso é feito pedindo sua autenticação.
  
- **Após a primeira vez**: Se você configurar o cache de credenciais ou usar SSH, o Git pode armazenar essas informações para que você não precise digitá-las novamente para cada push.

Essa autenticação é essencial para garantir que apenas usuários autorizados possam enviar alterações para o repositório remoto.

O parâmetro `-u` no comando `git push -u origin main` é utilizado para definir a branch local (`main` neste caso) como a branch "upstream" padrão para a branch remota associada. Isso significa que, depois de utilizar esse comando uma vez, você pode simplesmente executar `git push` ou `git pull` sem precisar especificar explicitamente o repositório (`origin`) e a branch (`main`) novamente.

### Explicando em mais detalhes:

- **`-u` ou `--set-upstream`**: Este parâmetro faz com que o Git associe a branch local (no exemplo, `main`) com a branch remota correspondente no repositório remoto (`origin`). Após essa associação, o Git "sabe" de onde puxar (`pull`) e para onde empurrar (`push`) as alterações por padrão.

- **Por que isso é útil?** 
  - Facilita comandos futuros: Uma vez que você fez o `git push -u origin main`, você não precisa mais escrever `git push origin main` em futuros pushes; basta usar `git push`.
  - Automatiza o comportamento: Com essa associação configurada, comandos como `git pull` saberão automaticamente de onde trazer as mudanças, simplificando o fluxo de trabalho.

### Exemplo de Uso:

Suponha que você esteja trabalhando em um novo repositório local e ainda não tenha feito nenhum push para o repositório remoto. Você usa o seguinte comando:

```bash
git push -u origin main
```

Isso faz com que:
1. A branch `main` no seu repositório local seja enviada para o repositório `origin`.
2. O Git configure a branch `main` local para "rastrear" a branch `main` no `origin`. Com isso, no futuro, você pode usar apenas `git push` ou `git pull` para enviar ou buscar atualizações, sem precisar especificar `origin` ou `main` novamente.

Essa conveniência é especialmente útil em projetos onde você frequentemente faz push e pull da mesma branch remota.

**Diagrama Ilustrativo**:

```mermaid
graph TD;
    A[Local Git Repository] --> |"git remote add"| B[GitHub Remote];
    B --> |"git push"| C[Remote Repository];
```

## 6. Principais Features do GitHub

### Explorando PRs, Issues, e Actions

- **Pull Requests**: Facilita a revisão de código, permite discussões e aprovação antes da integração ao código principal.
- **Issues**: Ferramenta para rastrear bugs, melhorias e tarefas.
- **Actions**: Automatiza testes, deploys e outras tarefas de CI/CD.

**Diagrama Ilustrativo**:

```mermaid
graph TD;
    A[Developer 1] --> |"Create PR"| B[GitHub Repository];
    B --> |"Review PR"| C[Developer 2];
    C --> |"Merge"| D[Main Branch];
```

Vamos refatorar o exemplo anterior utilizando Pull Requests (PRs) para melhorar o fluxo de trabalho e a colaboração entre os desenvolvedores.

## 7. Estratégias de Pull Requests (PRs)

### Melhorando a Colaboração com PRs

1. **Branch Naming**: Utilize nomes de branches que reflitam a tarefa ou bug a ser resolvido.
2. **PR Review Process**: Estabeleça um processo claro para a revisão de PRs, incluindo revisores designados.
3. **Squash Commits**: Combine múltiplos commits em um único antes de fazer o merge, para manter o histórico de commits limpo.

**Diagrama Ilustrativo**:

```mermaid
graph TD;
    A[Feature Branch] --> |"Squash Commits"| B[Single Commit];
    B --> |"Create PR"| C[Main Branch];
```

### Situação

1. **Branch `main`**: Contém o código de produção, sem as novas funcionalidades que estamos desenvolvendo.
2. **Branch `feature-1`**: Modifica o `file1.py` para adicionar uma nova funcionalidade.
3. **Branch `feature-2`**: Modifica o `file2.py` para adicionar outra funcionalidade.
4. **Branch `feature-3`**: Modifica o `file3.py` para adicionar uma terceira funcionalidade.

### Passos para Implementação com PRs

1. **Criar e Mudar para a Branch `feature-1`**:
    ```bash
    git checkout -b feature-1
    # Modificar file1.py e fazer commit
    git commit -am "Adiciona nova funcionalidade em file1.py"
    # Enviar a branch para o repositório remoto
    git push -u origin feature-1
    ```

2. **Criar um Pull Request para `feature-1`**:
    - No GitHub, crie um Pull Request da branch `feature-1` para a `main`.
    - Aguarde a revisão e aprovação do PR.

3. **Criar e Mudar para a Branch `feature-2`**:
    ```bash
    git checkout -b feature-2
    # Modificar file2.py e fazer commit
    git commit -am "Adiciona nova funcionalidade em file2.py"
    # Enviar a branch para o repositório remoto
    git push -u origin feature-2
    ```

4. **Criar um Pull Request para `feature-2`**:
    - No GitHub, crie um Pull Request da branch `feature-2` para a `main`.
    - Aguarde a revisão e aprovação do PR.

5. **Criar e Mudar para a Branch `feature-3`**:
    ```bash
    git checkout -b feature-3
    # Modificar file3.py e fazer commit
    git commit -am "Adiciona nova funcionalidade em file3.py"
    # Enviar a branch para o repositório remoto
    git push -u origin feature-3
    ```

6. **Criar um Pull Request para `feature-3`**:
    - No GitHub, crie um Pull Request da branch `feature-3` para a `main`.
    - Aguarde a revisão e aprovação do PR.

### Diagrama Mermaid Atualizado com PRs:

Aqui está o diagrama que ilustra o processo de criação de branches e PRs:

```mermaid
graph TD;
    subgraph main [Branch: main Production]
        A[Initial Commit] --> B[Main Codebase];
    end

    subgraph feature-1 [Branch: feature-1]
        B --> C[Commit: Modifica file1.py];
        C --> D[Pull Request];
        D --> E[Merge to Main];
    end

    subgraph feature-2 [Branch: feature-2]
        B --> F[Commit: Modifica file2.py];
        F --> G[Pull Request];
        G --> H[Merge to Main];
    end

    subgraph feature-3 [Branch: feature-3]
        B --> I[Commit: Modifica file3.py];
        I --> J[Pull Request];
        J --> K[Merge to Main];
    end

    A --- B;
```

### Explicação do Diagrama Atualizado:

- **Branch `main`**: Representa o código de produção.
- **Branches de Funcionalidade**:
  - **`feature-1`**: Modifica `file1.py` e é enviada como um PR para a `main`.
  - **`feature-2`**: Modifica `file2.py` e segue o mesmo processo.
  - **`feature-3`**: Modifica `file3.py` e também segue o processo de PR.
- **Pull Requests e Merges**:
  - Cada branch é enviada como um PR. Após revisão e aprovação, ela é mesclada (`merged`) na `main`, garantindo que o código de produção seja atualizado de forma controlada e revisada.

### Conclusão:

Esse fluxo de trabalho com Pull Requests (PRs) promove uma colaboração mais organizada e segura, garantindo que todas as modificações sejam revisadas antes de serem integradas ao código de produção. Cada PR permite a discussão, revisão e validação das mudanças, assegurando a qualidade do código e a minimização de bugs em produção.

Aqui está um ciclo de desenvolvimento típico usando Git, desde a criação de mudanças no diretório de trabalho até a integração dessas mudanças na branch principal (main) após um Pull Request (PR).

### Fluxo de Desenvolvimento

1. **Diretório de Trabalho (Work Directory)**: Onde você faz as modificações nos arquivos.
2. **Staging Area**: Onde você adiciona as mudanças que deseja incluir no próximo commit.
3. **.git (Repositório Local)**: Onde os commits são armazenados localmente.
4. **Push**: Envia os commits do repositório local para o repositório remoto (GitHub).
5. **GitHub Branch**: A branch específica no GitHub onde as mudanças são enviadas.
6. **Pull Request (PR) para Main**: As mudanças na branch específica são revisadas e, se aprovadas, mescladas na branch principal (`main`).
7. **Pull Main**: A branch `main` atualizada é puxada (pull) de volta para o repositório local, sincronizando as mudanças aprovadas.

### Verificação e Alterações

O fluxo descrito está correto, mas vamos detalhar cada etapa no diagrama Mermaid para garantir que todas as etapas estão cobertas.

### Diagrama Mermaid

```mermaid
graph TD;
    A[Work Directory] --> |"git add"| B[Staging Area];
    B --> |"git commit"| C[.git Local Repository];
    C --> |"git push"| D[GitHub Branch feature-branch];
    D --> |"Create Pull Request"| E[Pull Request to Main];
    E --> |"Review and Merge PR"| F[GitHub Branch main];
    F --> |"git pull"| G[.git Local Repository];
    G --> |"Update Work Directory"| A;
```

### Explicação do Diagrama

1. **Work Directory**: Você começa fazendo alterações no código, que estão no diretório de trabalho do seu projeto.
2. **Staging Area**: Com o comando `git add`, você move as alterações para a Staging Area, preparando-as para o commit.
3. **.git (Local Repository)**: Usando `git commit`, as mudanças na Staging Area são registradas no repositório local, criando um snapshot do código naquele momento.
4. **Push para GitHub Branch**: O comando `git push` envia os commits do repositório local para uma branch específica no GitHub, como `feature-branch`.
5. **GitHub Branch (feature-branch)**: Essa branch no GitHub é onde o código modificado reside enquanto aguarda revisão.
6. **Pull Request para Main**: Um PR é criado a partir da `feature-branch` para a `main`. Outros desenvolvedores revisam as mudanças, discutem e sugerem melhorias.
7. **Review e Merge do PR**: Após a revisão, o PR é mesclado na branch principal (`main`) no GitHub.
8. **Pull Main**: Finalmente, você sincroniza o repositório local com o repositório remoto atualizado usando `git pull`, trazendo as mudanças aprovadas na branch `main` de volta para o seu ambiente local.

### Conclusão

Esse fluxo reflete um ciclo de desenvolvimento completo e organizado, promovendo boas práticas de controle de versão, colaboração e integração contínua. Cada etapa garante que as mudanças sejam revisadas antes de serem integradas ao código principal, mantendo a integridade do projeto.

## 8. Fazendo `git clone`

### Como Clonar um Repositório

1. **Clonando um Repositório Existente**:
   - No terminal, clone o repositório:
     ```bash
     git clone https://github.com/usuario/repo.git
     ```

2. **Trabalhando com o Repositório Clonado**:
   - Navegue até o diretório clonado e comece a trabalhar:
     ```bash
     cd repo
     ```

**Diagrama Ilustrativo**:

```mermaid
graph TD;
    A[GitHub Repository] --> |"git clone"| B[Local Machine];
    B --> |"Local Copy"| C[Working Directory];
```

### 9. Trabalhando com o `.gitignore`

### O que é o `.gitignore`?

O arquivo `.gitignore` é um arquivo especial que você pode incluir em seu repositório Git para especificar quais arquivos ou diretórios devem ser ignorados pelo Git. Isso significa que qualquer coisa listada no `.gitignore` não será rastreada, versionada ou enviada ao repositório remoto, mesmo se estiver no diretório de trabalho.

### Por que Usar um `.gitignore`?

- **Arquivos Temporários**: Muitas vezes, projetos geram arquivos temporários ou de build que não precisam ser versionados. Exemplos incluem arquivos `.log`, diretórios `node_modules/`, arquivos de compilação, entre outros.
- **Dados Sensíveis**: Evitar a inclusão de arquivos que contenham informações sensíveis, como senhas, chaves de API, ou configurações locais específicas que não deveriam ser compartilhadas.
- **Configurações de Ambiente**: Arquivos de configuração que variam de acordo com o ambiente (por exemplo, `.env`) e não devem ser incluídos no repositório para evitar conflitos entre diferentes ambientes de desenvolvimento.

### Como Criar e Usar um `.gitignore`?

1. **Criando um Arquivo `.gitignore`**:
   - No diretório raiz do seu projeto, crie um arquivo chamado `.gitignore`.
     ```bash
     touch .gitignore
     ```

2. **Adicionando Padrões ao `.gitignore`**:
   - Especifique os arquivos e diretórios que você deseja ignorar.
     ```plaintext
     # Ignorar todos os arquivos .log
     *.log

     # Ignorar o diretório node_modules/
     node_modules/

     # Ignorar arquivos de configuração de ambiente
     .env
     ```

3. **Aplicando o `.gitignore`**:
   - Após adicionar arquivos ao `.gitignore`, eles não serão mais rastreados pelo Git. Se algum arquivo já estiver sendo rastreado, você precisará removê-lo do índice do Git (sem removê-lo do seu diretório de trabalho):
     ```bash
     git rm --cached nome_do_arquivo
     ```

### Exemplo de `.gitignore` para um Projeto Python

Aqui está um exemplo de um arquivo `.gitignore` típico para um projeto Python:

```plaintext
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# Diretórios de ambiente virtual
venv/
env/
.venv/

# Arquivos de configuração local
.env

# Arquivos de log
*.log

# Arquivos de configuração do IDE
.vscode/
.idea/
```

**Diagrama Ilustrativo**:

```mermaid
graph TD;
    A[Projeto Python] --> B[.gitignore];
    B --> C[Ignora venv/, __pycache__/, *.log];
    B --> D[Ignora .env, .vscode/];
    B --> E[Rastreia apenas arquivos relevantes];
```

### Dicas ao Usar `.gitignore`:

- **Coloque o `.gitignore` no início do projeto**: É uma boa prática configurar o `.gitignore` logo no início do desenvolvimento para evitar que arquivos indesejados sejam adicionados ao repositório.
- **Não ignore demais**: Certifique-se de que apenas arquivos realmente desnecessários sejam ignorados. Ignorar demais pode levar à perda de arquivos importantes.
- **Modelos de `.gitignore`**: Existem modelos prontos de `.gitignore` para diferentes linguagens e frameworks. O GitHub, por exemplo, oferece uma ampla coleção de templates de `.gitignore` [aqui](https://github.com/github/gitignore).

### Conclusão

O uso do `.gitignore` é essencial para manter um repositório Git limpo e organizado, rastreando apenas os arquivos que realmente importam para o desenvolvimento do projeto. Ele ajuda a evitar conflitos e garante que dados sensíveis ou desnecessários não sejam acidentalmente compartilhados ou versionados.

### 10. Importância do `README.md`

### O que é o `README.md`?

O `README.md` é um arquivo de texto, geralmente escrito em formato Markdown, que é incluído na raiz de um repositório Git. Ele serve como a "porta de entrada" do seu projeto, fornecendo informações essenciais e contextuais para qualquer pessoa que acessar o repositório. Um bom `README.md` é fundamental para tornar o projeto acessível, compreensível e útil para outros desenvolvedores, colaboradores ou até mesmo para você no futuro.

### Por que o `README.md` é Importante?

- **Primeira Impressão**: O `README.md` é geralmente a primeira coisa que as pessoas veem quando visitam seu repositório. Um arquivo bem escrito e claro pode atrair e engajar colaboradores, usuários, ou mesmo empregadores potenciais.
- **Documentação Essencial**: Ele fornece uma visão geral do projeto, incluindo seu propósito, como configurá-lo, utilizá-lo, e contribuir com ele. Isso ajuda a diminuir a curva de aprendizado para novos usuários ou desenvolvedores.
- **Facilita a Colaboração**: Um `README.md` bem documentado esclarece como os colaboradores podem contribuir, incluindo diretrizes sobre pull requests, issues, e outros aspectos do desenvolvimento colaborativo.

### O Que Incluir em um `README.md`?

1. **Título do Projeto**:
   - Nome claro e descritivo do projeto.

2. **Descrição**:
   - Uma breve descrição do que o projeto faz e qual problema ele resolve.

3. **Instalação**:
   - Instruções claras sobre como instalar ou configurar o projeto. Isso pode incluir requisitos de sistema, dependências e passos de instalação.

4. **Uso**:
   - Exemplos de como usar o projeto. Isso pode incluir exemplos de código, comandos ou capturas de tela.

5. **Contribuição**:
   - Diretrizes para contribuir com o projeto. Inclua informações sobre como enviar pull requests, reportar bugs ou sugerir melhorias.

6. **Licença**:
   - Especificar a licença sob a qual o projeto é distribuído, como MIT, GPL, Apache, etc.

7. **Referências e Créditos**:
   - Agradecimentos e links para recursos, bibliotecas ou pessoas que contribuíram para o projeto.

