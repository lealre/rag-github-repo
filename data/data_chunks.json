{
    "09-streamlit-dashboard-realtime": [
        "09-streamlit-dashboard-realtime/README.md - Parte (1/2)\n09-streamlit-dashboard-realtime/README.md\n\n# Workshop Streamlit\n\n![imagem](./pic/foto.png)\n\n## Roteiro da Aula\n\n- **9h00** Boas vindas\n- **9h30** Hello World\n- **10h00** Principais comandos\n- **10h30** Projeto Survey\n- **11h30** Intervalo\n- **12h00** Projeto Dashboard Realtime \n\n## O que vamos fazer hoje\n\n```mermaid\ngraph TD\n    subgraph Coleta[Aplicação de Coleta de Dados]\n        A[Iniciar Aplicação de Coleta de Dados] --> B[Conectar ao Banco de Dados]\n        B --> C{Conexão bem-sucedida?}\n        C -- Sim --> D[Verificar Tabela]\n        D --> E{Tabela Existe?}\n        E -- Não --> F[Criar Tabela]\n        E -- Sim --> G[Exibir Formulário]\n        G --> H[Preencher Formulário]\n        H --> I[Submeter Formulário]\n        I --> J[Salvar Dados no Banco de Dados]\n        J --> K[Exibir Mensagem de Sucesso]\n        C -- Não --> L[Exibir Mensagem de Erro]\n    end\n\n    subgraph DB[Banco de Dados PostgreSQL]\n        DB_PostgreSQL[(PostgreSQL)]\n        J --> DB_PostgreSQL\n        P --> DB_PostgreSQL\n    end\n    \n    subgraph Visualização[Aplicação de Visualização de Dados]\n        M[Iniciar Aplicação de Visualização de Dados] --> N[Conectar ao Banco de Dados]\n        N --> O{Conexão bem-sucedida?}\n        O -- Sim --> P[Carregar Dados do Banco]\n        P --> Q[Exibir Tabela de Dados]\n        Q --> R[Exibir Gráficos]\n        R --> S[Exibir Nuvem de Palavras]\n        S --> T[Exibir Mapa]\n        T --> U[Exibir Imagem]\n        O -- Não --> V[Exibir Mensagem de Erro]\n    end\n    \n    Coleta --> DB\n    DB --> Visualização\n```\n\n![coleta](./pic/coleta.png)\n[APP de Coleta](https://workshop-jornada-coleta.streamlit.app/)\n\n![coleta](./pic/dash.png)\n[APP de Dashboard]https://workshop-jornada-dash.streamlit.app/\n\n## A Maneira Mais Simples de Criar um Aplicativo Web com Python\n\nCriar aplicativos web geralmente envolve o uso de frameworks web Python como Django e Flask. Embora esses frameworks sejam poderosos e flexíveis, eles possuem uma curva de aprendizado significativa e podem exigir um investimento substancial de tempo para desenvolver algo funcional.\n\n### Por que usar Streamlit?\n\nDesenvolver com Streamlit torna o processo de criação de aplicativos web muito mais rápido e fácil. Com Streamlit, você pode transformar scripts Python em aplicativos web interativos em questão de minutos. Aqui estão alguns benefícios:\n\n- **Rápida Prototipagem**: Streamlit permite que você crie protótipos rapidamente sem a necessidade de escrever código HTML, CSS ou JavaScript.\n- **Foco em Dados**: Ideal para cientistas de dados e analistas, Streamlit facilita a visualização e a interação com dados diretamente em Python.\n- **Simplicidade**: A API intuitiva do Streamlit permite que você escreva aplicativos web de forma natural, usando construções familiares do Python.\n- **Interatividade**: Adicione widgets interativos como sliders, botões e seletores com facilidade para tornar suas análises de dados mais dinâmicas.\n\n### Comparação com Outros Frameworks\n\n| Característica        | Django & Flask                      | Streamlit                               |\n|-----------------------|-------------------------------------|-----------------------------------------|\n| **Curva de Aprendizado** | Alta                                | Baixa                                   |\n| **Tempo de Desenvolvimento** | Lento (configuração manual de roteamento, templates, etc.) | Rápido (scripts Python para web apps)  |\n| **Conhecimento Necessário**  | Python, HTML, CSS, JavaScript   | Apenas Python                           |\n| **Interatividade**    | Necessário configurar manualmente  | Integrado e fácil de usar               |\n| **Foco Principal**    | Desenvolvimento web geral           | Aplicações de dados e visualizações     |\n\n### Exemplo de Código com Streamlit\n\nAqui está um exemplo simples de como é fácil criar uma aplicação web com Streamlit:\n\n```python\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\n# Título da aplicação\nst.title(\"Exemplo de Aplicação Streamlit\")\n\n# Adiciona um cabeçalho\nst.header(\"Introdução ao Streamlit\")\n\n# Adiciona um texto\nst.text(\"Esta é uma aplicação web criada com Streamlit!\")\n\n# Cria um dataframe\ndata  pd.DataFrame({\n    'Coluna 1': [1, 2, 3, 4],\n    'Coluna 2': [10, 20, 30, 40]\n})\n\n# Exibe o dataframe\nst.dataframe(data)\n\n# Adiciona um gráfico de linha\nst.line_chart(data)\n\n# Adiciona um botão e uma resposta ao clique\nif st.button(\"Clique aqui\"):\n    st.write(\"Botão clicado!\")\n\n# Adiciona um seletor\nopcao  st.selectbox(\"Escolha uma opção\", ['Opção A', 'Opção B', 'Opção C'])\nst.write(\"Você selecionou:\", opcao)\n```\n\n### Iniciando com Streamlit\n\nPara começar a usar Streamlit, siga estes passos:\n\n1. **Instale Streamlit**:\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Crie um script Python** com o conteúdo do exemplo acima e salve-o como `app.py`.\n\n3. **Execute o Streamlit**:\n   ```bash\n   streamlit run app.py\n   ```\n\n4. **Abra o navegador** e acesse `http://localhost:8501` para ver sua aplicação web em ação!\n\nStreamlit é uma ferramenta poderosa e simples que transforma a maneira como você desenvolve e compartilha suas análises de dados. Experimente e veja como pode facilitar o seu trabalho!\n\n## O que é o Streamlit?\n\n- **Streamlit é um Web Framework Open Source em Python** que transforma scripts de dados em aplicativos web interativos e compartilháveis em questão de minutos.\n  \n- **Streamlit não requer experiência em front-end**. Você pode criar aplicações completas usando apenas Python, o que permite que você foque no desenvolvimento do seu modelo e análise de dados, sem se preocupar com HTML, CSS ou JavaScript.\n\n### Principais Vantagens do Streamlit\n\n- **Fácil de Usar**: A simplicidade do Streamlit permite que você desenvolva rapidamente aplicativos web sem a necessidade de aprender tecnologias de front-end. Se você conhece Python, já sabe o suficiente para criar aplicativos web com Streamlit.\n  \n- **Rápido**: Com Streamlit, você pode transformar seus scripts Python em aplicativos web em minutos. A configuração é mínima, e você pode ver as mudanças em tempo real enquanto desenvolve.\n  \n- **Interativo**: Adicionar interatividade às suas análises de dados é simples com Streamlit. Você pode facilmente incluir widgets como sliders, botões, seletores e muito mais para tornar suas aplicações dinâmicas e responsivas.\n  \n- **Open Source**: Streamlit é gratuito e open source, apoiado por uma comunidade ativa. Isso significa que você pode contribuir para o projeto e se beneficiar das contribuições de outros desenvolvedores.\n\n## Iniciando um Projeto Streamlit\n\nComeçar um projeto com Streamlit é simples e rápido. Aqui estão os passos básicos:\n\n1. **Crie um Script Python**: O primeiro passo é criar um arquivo Python, por exemplo, `app.py`.\n\n2. **Importe a Biblioteca Streamlit**: Dentro do seu script, importe a biblioteca Streamlit e use uma série de métodos do Streamlit para criar widgets de entrada e saída.\n\n### Exemplo de Código\n\nAqui está um exemplo simples de como começar:\n\n```python\nimport streamlit as st\n\n# Título da aplicação\nst.title(\"Meu Primeiro App com Streamlit\")\n\n# Adiciona um cabeçalho\nst.header(\"Introdução ao Streamlit\")\n\n# Adiciona um texto\nst.text(\"Streamlit facilita a criação de aplicações web interativas com Python!\")\n\n# Widgets de entrada\nnome  st.text_input(\"Digite seu nome\")\nidade  st.slider(\"Selecione sua idade\", 0, 100, 25)\n\n# Exibe os dados de entrada\nst.write(f\"Nome: {nome}\")\nst.write(f\"Idade: {idade}\")\n\n# Adiciona um gráfico simples\nst.line_chart([1, 2, 3, 4, 5])\n```\n\n### Executando o Aplicativo\n\nPara executar sua aplicação Streamlit, use o comando abaixo no terminal:\n\n```bash\nstreamlit run app.py\n```\n\nAbra seu navegador e acesse `http://localhost:8501` para ver sua aplicação em ação!\n\n# Aula Introdutória ao Streamlit\n\nEste repositório contém um exemplo prático que explora os principais métodos do Streamlit, uma biblioteca em Python que facilita a criação de aplicações web interativas. O objetivo é fornecer uma base sólida para iniciantes aprenderem a utilizar os recursos básicos do Streamlit.\n\n## Objetivo\n\nO objetivo deste exemplo é explorar os principais métodos do Streamlit, abordando diferentes tipos de exibição de texto, dados, métricas, gráficos, mapas, mídia e widgets interativos. \n\n## Estrutura do Projeto\n\n- **exemplo/main.py**: Contém o código principal que demonstra os métodos do Streamlit.\n- **exemplo/main_exercicio.py**: Contém somente os comentários, servindo como exercício para preencher os métodos do Streamlit.\n- **requirements.txt**: Lista de dependências necessárias para executar o projeto.\n- **watch.py**: Script opcional para reiniciar automaticamente o Streamlit ao detectar alterações no código.\n\n## Como Executar o Projeto\n\n1. **Clone o repositório**:\n\n   ```sh\n   git clone https://github.com/seu-usuario/streamlit-intro.git\n   cd streamlit-intro\n   ```\n\n2. **Crie um ambiente virtual e ative-o**:\n\n   ```sh\n   python -m venv venv\n   source venv/bin/activate  # Para Windows: venv\\Scripts\\activate\n   ```\n\n3. **Instale as dependências**:\n\n   ```sh\n   pip install -r requirements.txt\n   ```\n\n4. **Execute o Streamlit**:\n\n   ```sh\n   streamlit run exemplo/main.py\n   ```\n\n   O Streamlit irá automaticamente observar mudanças no arquivo `main.py` e recarregar a aplicação.\n\n# Conteúdo do Exemplo\n\n### 1. Títulos e Texto\n\n- **Título da aplicação**: `st.title(titulo)`\n- **Cabeçalho**: `st.header(cabecalho)`\n- **Subcabeçalho**: `st.subheader(subcabecalho)`\n- **Texto simples**: `st.text(texto)`\n- **Markdown**: `st.markdown(markdown_texto)`\n- **Fórmula LaTeX**: `st.latex(latex_formula)`\n- **Código com destaque de sintaxe**: `st.code(codigo)`\n\n### 2. Exibição de Dados\n\n- **Exibe um DataFrame**: `st.write(df)`\n- **DataFrame com redimensionamento**: `st.dataframe(df)`\n- **Tabela estática**: `st.table(df)`\n- **Objeto JSON**: `st.json(json_obj)`\n- **CSV como string**: `st.write(csv_string)`\n\n\n- **Lista de números**: `st.write(lista)`\n\n### 3. Métricas\n\n- **Métrica com delta**: `st.metric(label, value, delta)`\n- **Métricas diversas**: `st.metric(label, value)`\n\n### 4. Gráficos\n\n- **Gráfico de linha**: `st.line_chart(data)`\n- **Gráfico de área**: `st.area_chart(data)`\n- **Gráfico de barra**: `st.bar_chart(data)`\n- **Gráfico de dispersão**: `st.plotly_chart(scatter_plot)`\n- **Histograma**: `st.plotly_chart(histogram)`\n\n### 5. Mapas\n\n- **Exibe um mapa**: `st.map(map_data)`\n\n### 6. Mídia\n\n- **Imagem com legenda**: `st.image(url, caption)`\n- **Reprodutor de áudio**: `st.audio(url)`\n- **Reprodutor de vídeo**: `st.video(url)`\n\n### 7. Widgets\n\n- **Botão**: `st.button(label)`\n- **Caixa de seleção**: `st.checkbox(label)`\n- **Opções de escolha única**: `st.radio(label, options)`\n- **Menu suspenso**: `st.selectbox(label, options)`\n- **Menu suspenso múltiplo**: `st.multiselect(label, options)`\n- **Barra deslizante**: `st.slider(label, min_value, max_value, value)`\n- **Barra deslizante com opções de texto**: `st.select_slider(label, options, value)`\n- **Caixa de entrada de texto**: `st.text_input(label)`\n- **Caixa de entrada de número**: `st.number_input(label, min_value, max_value)`\n- **Área de texto**: `st.text_area(label)`\n- **Seletor de data**: `st.date_input(label, value)`\n\n### 8. Barra Lateral\n\n- **Título da barra lateral**: `st.sidebar.title(title)`\n- **Botão na barra lateral**: `st.sidebar.button(label)`\n\n### 9. Carregamento de CSV e Downloads\n\n- **Carregamento de arquivo CSV**: `st.file_uploader(label, type)`\n- **Barra de progresso durante o upload**: `st.progress(progress)`\n- **Download de arquivo Parquet**: `st.download_button(label, data, file_name, mime)`\n\n# Setup do nosso projeto\n\nEste projeto utiliza Streamlit para criar aplicações web interativas em Python. Vamos configurar o ambiente de desenvolvimento com as seguintes ferramentas:\n\n- Git e Github (utilizando o Github CLI) para versionamento do código\n- Python 3.12.1 utilizando o Pyenv\n- Poetry para gerenciamento de pacotes e ambiente virtual\n- Streamlit para desenvolver a aplicação\n- Ruff para linting\n- Taskipy para tarefas automatizadas\n- Pytest para testes\n\n## Configurando o ambiente de desenvolvimento\n\n### 1. Criando o repositório no Github\n\nUtilize o Github CLI para criar um novo repositório:\n\n```bash\ngh repo create\n```\n\n### 2. Criando o arquivo .gitignore\n\nUtilize o ignr para criar um arquivo .gitignore específico para projetos Python:\n\n```bash\nignr -n python\n```\n\n### 3. Cri",
        "09-streamlit-dashboard-realtime/README.md - Parte (2/2)\nando o README.md\n\nCrie um arquivo README.md (este arquivo) para documentar o projeto.\n\n```bash\ntouch README.md\n```\n\n### 4. Instalando Python 3.12.1 com Pyenv\n\n```bash\npyenv install 3.12.1\npyenv local 3.12.1\n```\n\n### 5. Configurando o ambiente com Poetry\n\nInicialize um novo projeto com Poetry:\n\n```bash\npoetry init\n```\n\n### 6. Instalando Streamlit\n\nAdicione o Streamlit ao seu projeto:\n\n```bash\npoetry add streamlit\n```\n\n### 7. Instalando e Configurando Ruff\n\nRuff é uma ferramenta de linting rápida para Python, que ajuda a identificar e corrigir problemas no código, como formatação incorreta, erros de sintaxe e práticas de codificação inadequadas. A análise estática de código, como a realizada pelo Ruff, é uma prática essencial para garantir a qualidade e a manutenção do código. Ela verifica o código-fonte sem executá-lo, encontrando erros comuns e garantindo conformidade com padrões de codificação.\n\n#### 7.1. Adicionando Ruff ao grupo de desenvolvimento\n\nPara adicionar Ruff ao grupo de desenvolvimento, execute:\n\n```bash\npoetry add --group dev ruff\n```\n\n#### 7.2. Configurando Ruff no `pyproject.toml`\n\nAdicione a configuração do Ruff no arquivo `pyproject.toml` para personalizar seu comportamento:\n\n```toml\n[tool.ruff]\n# Configurações de linting\nline-length  88  # Comprimento máximo de linha\nselect  [\"E\", \"F\", \"W\"]  # Seleciona as categorias de erros e avisos\nignore  [\"E203\", \"W503\"]  # Ignora regras específicas\n\n# Adicione paths específicos se necessário\nexclude  [\"build\", \"dist\"]\n\n# Para especificar o conjunto de regras do Black para formatação\nextend-select  [\"B\"]\n```\n\n### 7.3. Realizando análise estática\n\nPara realizar a análise estática do código com Ruff, execute o seguinte comando:\n\n```bash\nruff check .\n```\n\n### 7.4. Corrigindo problemas automaticamente\n\nRuff também pode corrigir problemas automaticamente quando possível. Para fazer isso, utilize o comando:\n\n```bash\nruff check . --fix\n```\n\n### Exemplo de Configuração Completa no `pyproject.toml`\n\nAqui está um exemplo de como o `pyproject.toml` deve ficar com a configuração do Ruff:\n\n```toml\n[tool.poetry]\nname  \"projeto-streamlit\"\nversion  \"0.1.0\"\ndescription  \"Projeto Streamlit para criar aplicações web interativas em Python.\"\nauthors  [\"Seu Nome <seu.email@example.com>\"]\n\n[tool.poetry.dependencies]\npython  \"^3.12\"\nstreamlit  \"^1.0.0\"\n\n[tool.poetry.dev-dependencies]\nruff  \"^0.0.255\"\npytest  \"^6.2.4\"\ntaskipy  \"^1.9.0\"\npre-commit  \"^2.14.0\"\n\n[build-system]\nrequires  [\"poetry-core>1.0.0\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n[tool.ruff]\nline-length  88\nselect  [\"E\", \"F\", \"W\", \"C\", \"N\", \"B\", \"I\", \"PL\", \"PT\"]\nignore  [\"E203\", \"W503\"]\nexclude  [\"build\", \"dist\"]\nextend-select  [\"B\"]\n\n[tool.taskipy.tasks]\nlint  \"ruff check .\"\ntest  \"pytest\"\n```\n\nCom essas configurações, você terá um ambiente de desenvolvimento bem configurado, incluindo a análise estática do código com Ruff, garantindo que seu código esteja sempre em conformidade com as melhores práticas de desenvolvimento em Python.\n\n### 8. Instalando Taskipy\n\nAdicione Taskipy ao seu projeto:\n\n```bash\npoetry add --group dev taskipy\n```\n\nAdicione o seguinte ao seu `pyproject.toml` para configurar o Taskipy:\n\n```toml\n[tool.taskipy.tasks]\nlint  \"ruff check .\"\n```\n\n# Integração Streamlit e outras bibliotecas gráficas\n\nStreamlit é uma biblioteca de código aberto em Python que torna extremamente fácil criar e compartilhar aplicativos web de dados. Uma das grandes vantagens do Streamlit é a sua capacidade de integrar diversas bibliotecas gráficas populares do Python, proporcionando uma experiência visual rica e interativa. Aqui estão algumas vantagens e destaques dessas integrações:\n\n### 1. Matplotlib\n\n#### O que é?\nMatplotlib é uma biblioteca de plotagem 2D extremamente popular em Python, usada para criar gráficos estáticos, animados e interativos. É altamente configurável e permite criar visualizações complexas com facilidade.\n\n#### Vantagens no Streamlit\n- **Facilidade de Integração**: Com Streamlit, você pode exibir gráficos Matplotlib de forma simples utilizando `st.pyplot()`.\n- **Interatividade**: Streamlit permite adicionar interatividade aos seus gráficos Matplotlib sem a necessidade de configurar um ambiente web completo.\n- **Visualizações Poderosas**: Combine a flexibilidade do Matplotlib com a simplicidade do Streamlit para criar dashboards poderosos e visualizações de dados detalhadas.\n\n### Comando para Instalação\n```bash\npoetry add matplotlib\n```\n\n### 2. Folium\n\n#### O que é?\nFolium é uma biblioteca que facilita a visualização de dados geoespaciais utilizando Leaflet.js. Com Folium, você pode criar mapas interativos e adicionar marcadores, camadas e outras funcionalidades.\n\n#### Vantagens no Streamlit\n- **Mapas Interativos**: Streamlit e Folium juntos permitem a criação de mapas interativos dentro de aplicativos web, ideais para análises geoespaciais.\n- **Visualização de Dados Geográficos**: Exiba dados geográficos e geolocalizados diretamente no navegador, facilitando a compreensão e análise espacial.\n- **Simplicidade**: Adicione mapas interativos aos seus aplicativos com poucas linhas de código usando `folium_static()`.\n\n### Comando para Instalação\n```bash\npoetry add folium streamlit-folium\n```\n\n### 3. WordCloud\n\n#### O que é?\nWordCloud é uma biblioteca para a geração de nuvens de palavras a partir de texto. As nuvens de\n\n palavras são uma forma visual de representar a frequência ou importância de palavras em um texto, com palavras mais frequentes aparecendo maiores.\n\n#### Vantagens no Streamlit\n- **Visualização de Texto**: Utilize WordCloud com Streamlit para criar representações visuais atraentes de dados textuais.\n- **Facilidade de Uso**: Crie e exiba nuvens de palavras rapidamente usando `WordCloud` e `st.pyplot()`.\n- **Análise Textual**: Ideal para visualizar e explorar grandes volumes de texto de maneira intuitiva.\n\n### Comando para Instalação\n```bash\npoetry add wordcloud\n```\n\n### Conclusão\n\nStreamlit se destaca como uma ferramenta poderosa para criar aplicativos de dados interativos, integrando facilmente bibliotecas gráficas populares como Matplotlib, Folium e WordCloud. Estas integrações permitem aos desenvolvedores focar na análise e visualização de dados, sem a necessidade de se preocupar com a infraestrutura web subjacente. Com Streamlit, você pode transformar scripts de dados em aplicativos web compartilháveis em minutos, facilitando a disseminação e compreensão de insights valiosos.\n\n### Exemplo de Código\n\n```python\nimport streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport folium\nfrom streamlit_folium import folium_static\nfrom wordcloud import WordCloud\n\n# Exemplo de gráfico Matplotlib\nst.header(\"Gráfico Matplotlib\")\nfig, ax  plt.subplots()\nax.plot([1, 2, 3, 4], [10, 20, 25, 30])\nst.pyplot(fig)\n\n# Exemplo de mapa Folium\nst.header(\"Mapa Folium\")\nm  folium.Map(location[45.5236, -122.6750], zoom_start13)\nfolium.Marker([45.5236, -122.6750], popup\"The Waterfront\").add_to(m)\nfolium_static(m)\n\n# Exemplo de nuvem de palavras WordCloud\nst.header(\"Nuvem de Palavras\")\ntext  \"Python Streamlit Matplotlib Folium WordCloud\"\nwordcloud  WordCloud(width800, height400, background_color'white').generate(text)\nfig, ax  plt.subplots()\nax.imshow(wordcloud, interpolation'bilinear')\nax.axis(\"off\")\nst.pyplot(fig)\n```\n\nStreamlit transforma a maneira como você trabalha com dados, tornando a criação de visualizações e aplicativos de dados mais acessível e eficiente.\n\n## Survey App com Streamlit e PostgreSQL\n\n### Descrição\n\nEste projeto demonstra como criar uma aplicação web de enquete utilizando Streamlit para a interface do usuário e PostgreSQL para o armazenamento de dados. O objetivo é coletar dados dos participantes através de um formulário, salvar esses dados em um banco de dados PostgreSQL e posteriormente visualizá-los e analisá-los em uma outra aplicação web.\n\n### Estrutura do Projeto\n\nO projeto está dividido em duas partes principais, cada uma localizada em uma pasta separada:\n\n1. **Aplicação de Coleta de Dados** (`projeto_coleta`):\n   - Coleta os dados dos participantes através de um formulário web.\n   - Salva os dados coletados em um banco de dados PostgreSQL.\n\n2. **Aplicação de Visualização de Dados** (`projeto_dash`):\n   - Carrega os dados do banco de dados PostgreSQL.\n   - Exibe os dados em gráficos interativos e tabelas.\n\n### Arquivos\n\n- `projeto_coleta/main.py`: Contém a implementação da aplicação de coleta de dados.\n- `projeto_dash/visualization.py`: Contém a implementação da aplicação de visualização de dados.\n- `.env`: Arquivo de configuração com as variáveis de ambiente para conexão ao banco de dados PostgreSQL.\n- `requirements.txt`: Lista de dependências do projeto.\n\n### Instruções para Executar\n\n#### 1. Clonar o Repositório\n\nClone o repositório para sua máquina local:\n\n```bash\ngit clone https://github.com/seu-usuario/seu-repositorio.git\ncd seu-repositorio\n```\n\n#### 2. Configurar o Ambiente Virtual\n\nCrie e ative um ambiente virtual:\n\n```bash\npython -m venv venv\nsource venv/bin/activate  # No Windows use `venv\\Scripts\\activate`\n```\n\n#### 3. Instalar Dependências\n\nInstale as dependências necessárias:\n\n```bash\npip install -r requirements.txt\n```\n\n#### 4. Configurar Variáveis de Ambiente\n\nCrie um arquivo `.env` na raiz do projeto e adicione as configurações de conexão ao banco de dados PostgreSQL:\n\n```env\nDB_HOSTseu_host\nDB_DATABASEseu_database\nDB_USERseu_usuario\nDB_PASSWORDsua_senha\n```\n\n#### 5. Executar a Aplicação de Coleta de Dados\n\nEntre na pasta `projeto_coleta` e inicie a aplicação de coleta de dados:\n\n```bash\ncd projeto_coleta\nstreamlit run main.py\n```\n\n#### 6. Executar a Aplicação de Visualização de Dados\n\nEm um novo terminal, entre na pasta `projeto_dash` e inicie a aplicação de visualização de dados:\n\n```bash\ncd projeto_dash\nstreamlit run visualization.py\n```\n\n### Funcionalidades\n\n#### Aplicação de Coleta de Dados (`main.py`)\n\n1. **Conexão ao Banco de Dados**:\n   - Conecta ao banco de dados PostgreSQL utilizando as variáveis de ambiente configuradas.\n\n2. **Criação da Tabela**:\n   - Cria a tabela `survey_data` no banco de dados, caso ainda não exista.\n\n3. **Formulário de Enquete**:\n   - Coleta informações dos participantes, como estado, área de atuação, bibliotecas utilizadas, horas de estudo, conforto com dados e experiência em Python, SQL e Cloud.\n\n4. **Salvar Dados**:\n   - Salva os dados coletados no banco de dados PostgreSQL.\n\n#### Aplicação de Visualização de Dados (`visualization.py`)\n\n1. **Conexão ao Banco de Dados**:\n   - Conecta ao banco de dados PostgreSQL utilizando as variáveis de ambiente configuradas.\n\n2. **Carregar Dados**:\n   - Carrega os dados da tabela `survey_data` do banco de dados.\n\n3. **Exibir Dados**:\n   - Exibe os dados em uma tabela.\n\n4. **Gráficos Interativos**:\n   - Exibe gráficos de área mostrando o nível de conforto com dados versus horas de estudo.\n   - Exibe gráficos de linha mostrando a experiência técnica dos participantes em Python, SQL e Cloud.\n   - Exibe um mapa do Brasil com a distribuição dos participantes por estado.\n   - Exibe uma nuvem de palavras com as bibliotecas utilizadas pelos participantes.\n   - Exibe as top 3 bibliotecas utilizadas por área de atuação.\n\n5. **Exibir Imagem**:\n   - Exibe uma imagem ao final da página.\n\n### Workflow\n\n#### Workflow da Aplicação de Coleta de Dados\n\n```mermaid\ngraph TD\n    A[Iniciar Aplicação de Coleta de Dados] --> B[Conectar ao Banco de Dados]\n    B --> C{Conexão bem-sucedida?}\n    C -- Sim --> D[Verificar Tabela]\n    D --> E{Tabela Existe?}\n    E -- Não --> F[Criar Tabela]\n    E -- Sim --> G[Exibir Formulário]\n    G --> H[Preencher Formulário]\n    H --> I[Submeter Formulário]\n    I --> J[Salvar Dados no Banco de Dados]\n    J --> K[Exibir Mensagem de Sucesso]\n    C -- Não --> L[Exibir Mensagem de Erro]\n```\n\n#### Workflow da Aplicação de Visualização de Dados\n\n```mermaid\ngraph TD\n    A[Iniciar Aplicação de Visualização de Dados] --> B[Conectar ao Banco de Dados]\n    B --> C{Conexão bem-sucedida?}\n    C -- Sim --> D[Carregar Dados do Banco]\n    D --> E[Exibir Tabela de Dados]\n    E --> F[Exibir Gráficos]\n    F --> G[Exibir Nuvem de Palavras]\n    G --> H[Exibir Mapa]\n    H --> I[Exibir Imagem]\n    C -- Não --> J[Exibir Mensagem de Erro]\n```\n\nCom esse guia, você estará preparado para configurar, desenvolver e executar um projeto completo utilizando Streamlit e PostgreSQL, desde a coleta até a visualização de dados.\n\n",
        "09-streamlit-dashboard-realtime/app.py\n\nimport pandas as pd\nimport streamlit as st\n\n# Título da aplicação\nst.title(\"Exemplo de Aplicação Streamlit\")\n\n# Adiciona um cabeçalho\nst.header(\"Introdução ao Streamlit\")\n\n# Adiciona um texto\nst.text(\"Esta é uma aplicação web criada com Streamlit!\")\n\n# Cria um dataframe\ndata  pd.DataFrame({\"Coluna 1\": [1, 2, 3, 4], \"Coluna 2\": [10, 20, 30, 40]})\n\n# Exibe o dataframe\nst.dataframe(data)\n\n# Adiciona um gráfico de linha\nst.line_chart(data)\n\n# Adiciona um botão e uma resposta ao clique\nif st.button(\"Clique aqui\"):\n    st.write(\"Botão clicado!\")\n\n# Adiciona um seletor\nopcao  st.selectbox(\"Escolha uma opção\", [\"Opção A\", \"Opção B\", \"Opção C\"])\nst.write(\"Você selecionou:\", opcao)\n\n\n09-streamlit-dashboard-realtime/pyproject.toml\n\n[tool.poetry]\nname  \"workshop-streamlit\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"lvgalvaofilho <lvgalvaofilho@gmail.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"3.12.1\"\nstreamlit  \"^1.37.0\"\nfolium  \"^0.17.0\"\nstreamlit-folium  \"^0.22.0\"\nmatplotlib  \"^3.9.1\"\nseaborn  \"^0.13.2\"\npydeck  \"^0.9.1\"\nwordcloud  \"^1.9.3\"\npsycopg2-binary  \"^2.9.9\"\npython-dotenv  \"^1.0.1\"\nsqlalchemy  \"^2.0.31\"\nxlsxwriter  \"^3.2.0\"\nplotly  \"^5.23.0\"\n\n[tool.poetry.group.dev.dependencies]\nruff  \"^0.5.5\"\ntaskipy  \"^1.13.0\"\npre-commit  \"^3.7.1\"\npytest  \"^8.3.2\"\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n[tool.ruff]\nline-length  200\nindent-width  4\nexclude  [\"build\",\"exemplo\",\"ruff_erros\"]\n\n\n[tool.ruff.lint]\nselect  [\"E\", \"F\", \"W\", \"C\", \"N\", \"B\", \"I\", \"PL\", \"PT\"]\n\n# Allow fix for all enabled rules (when `--fix`) is provided.\nfixable  [\"ALL\"]\nunfixable  []\n\n[tool.ruff.format]\n# Like Black, use double quotes for strings.\nquote-style  \"double\"\n\n[tool.taskipy.tasks]\nlint  \"ruff check .\"\nformat  \"ruff format .\"\nruff  \"ruff check . --fix && ruff format .\"\n\n09-streamlit-dashboard-realtime/.python-version\n\n3.12.1\n\n\n09-streamlit-dashboard-realtime/exemplo/main.py\n\nimport io\nimport time\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nimport streamlit as st\n\n# 1. Títulos e Texto\ntitulo  \"Aula Introdutória ao Streamlit\"\nst.title(titulo)  # Exibe o título da aplicação\n\ncabecalho  \"Aprendendo os Principais Métodos\"\nst.header(cabecalho)  # Exibe um cabeçalho\n\n# Exibição de Texto\nsubcabecalho_texto  \"Métodos de Exibição de Texto\"\nst.subheader(subcabecalho_texto)  # Exibe um subcabeçalho\n\n# Exibe um texto simples\ntexto_simples  (\n    \"Streamlit facilita a criação de aplicações web interativas com Python.\"\n)\nst.text(texto_simples)  # Exibe um texto simples\n\n# Exibe um texto em markdown\ntexto_markdown  \"### Este é um texto em **markdown**!\"\nst.markdown(texto_markdown)  # Exibe texto formatado usando Markdown\n\n# Exibe uma fórmula em LaTeX\nformula_latex  r\"\"\" e^{i\\pi} + 1  0 \"\"\"\nst.latex(formula_latex)  # Exibe uma fórmula matemática usando LaTeX\n\n# Exibe um código\ncodigo_exemplo  \"x  42\"\nst.code(\n    codigo_exemplo, language\"python\"\n)  # Exibe um trecho de código com destaque de sintaxe\n\n# 2. Exibição de Dados\nsubcabecalho_dados  \"Exibição de Dados\"\nst.subheader(subcabecalho_dados)  # Exibe um subcabeçalho\n\n# Cria um DataFrame e exibe-o de várias formas\ndata  {\"A\": [1, 2, 3, 4], \"B\": [10, 20, 30, 40]}\ndf  pd.DataFrame(data)\n\n# Exibe o DataFrame usando o método write\nst.write(\n    \"Aqui está um dataframe:\", df\n)  # Exibe o DataFrame com formatação padrão\n\n# Exibe o DataFrame usando o método dataframe\nst.dataframe(df)  # Exibe o DataFrame com opções de redimensionamento\n\n# Exibe o DataFrame usando o método table\nst.table(df)  # Exibe o DataFrame como uma tabela estática\n\n# Exibe um JSON\njson_exemplo  {\"name\": \"Streamlit\", \"type\": \"Web Framework\"}\nst.json(json_exemplo)  # Exibe um objeto JSON\n\n# Exibe um CSV como string\ncsv_exemplo  df.to_csv(indexFalse)\nst.write(\"Exibindo CSV:\", csv_exemplo)  # Exibe o DataFrame como CSV\n\n# Exibe uma lista\nlista_exemplo  [1, 2, 3, 4, 5]\nst.write(\"Lista de números:\", lista_exemplo)  # Exibe uma lista\n\n# 3. Métricas\nsubcabecalho_metricas  \"Métricas\"\nst.subheader(subcabecalho_metricas)  # Exibe um subcabeçalho\n\n# Exibe uma métrica com delta (diferença)\nst.metric(\n    label\"Temperatura\", value\"70 °F\", delta\"1.2 °F\"\n)  # Exibe uma métrica com delta (mudança)\n\n# Exibe mais métricas com diferentes valores e deltas\nst.metric(label\"Umidade\", value\"60%\", delta\"-5%\")\nst.metric(label\"Velocidade do Vento\", value\"15 km/h\", delta\"2 km/h\")\nst.metric(label\"Nível de Ruído\", value\"40 dB\", delta\"1.5 dB\")\nst.metric(label\"Pressão Atmosférica\", value\"1013 hPa\", delta\"2 hPa\")\n\n# Exibe uma métrica sem delta\nst.metric(label\"População\", value\"8 bilhões\")\n\n# 4. Gráficos\nsubcabecalho_graficos  \"Gráficos\"\nst.subheader(subcabecalho_graficos)  # Exibe um subcabeçalho\n\n# Cria e exibe gráficos de linha, área e barra\nchart_data  pd.DataFrame(np.random.randn(20, 3), columns[\"a\", \"b\", \"c\"])\nst.line_chart(chart_data)  # Exibe um gráfico de linha\nst.area_chart(chart_data)  # Exibe um gráfico de área\nst.bar_chart(chart_data)  # Exibe um gráfico de barra\n\n# Mais exemplos de gráficos\nmais_graficos  \"Mais Exemplos de Gráficos\"\nst.subheader(mais_graficos)  # Exibe um subcabeçalho\n\n# Gráfico de dispersão\nscatter_data  pd.DataFrame(np.random.randn(100, 2), columns[\"x\", \"y\"])\nst.plotly_chart(\n    {\n        \"data\": [\n            {\n                \"x\": scatter_data[\"x\"],\n                \"y\": scatter_data[\"y\"],\n                \"type\": \"scatter\",\n                \"mode\": \"markers\",\n            }\n        ],\n        \"layout\": {\"title\": \"Gráfico de Dispersão\"},\n    }\n)  # Exibe um gráfico de dispersão\n\n# Histograma\nhist_data  np.random.randn(1000)\nst.plotly_chart(\n    {\n        \"data\": [{\"x\": hist_data, \"type\": \"histogram\"}],\n        \"layout\": {\"title\": \"Histograma\"},\n    }\n)  # Exibe um histograma\n\n# 5. Mapas\nsubcabecalho_mapas  \"Mapas\"\nst.subheader(subcabecalho_mapas)  # Exibe um subcabeçalho\n\n# Cria e exibe um mapa\nmap_data  pd.DataFrame(\n    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],\n    columns[\"lat\", \"lon\"],\n)\nst.map(map_data)  # Exibe um mapa com pontos aleatórios\n\n# 6. Mídia\nsubcabecalho_midia  \"Mídia\"\nst.subheader(subcabecalho_midia)  # Exibe um subcabeçalho\n\n# Exibe uma imagem\nimagem_url  \"https://www.streamlit.io/images/brand/streamlit-logo-primary-colormark-darktext.png\"\nimagem_legenda  \"Streamlit Logo\"\nst.image(imagem_url, captionimagem_legenda)  # Exibe uma imagem com legenda\n\n# Exibe um áudio\naudio_url  \"https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3\"\nst.audio(audio_url)  # Exibe um reprodutor de áudio\n\n# Exibe um vídeo\nvideo_url  \"https://www.youtube.com/watch?vB2iAodr0fOo\"\nst.video(video_url)  # Exibe um reprodutor de vídeo\n\n# 7. Widgets\nsubcabecalho_widgets  \"Widgets\"\nst.subheader(subcabecalho_widgets)  # Exibe um subcabeçalho\n\n# Botão - Exibe um botão que, ao ser clicado, mostra uma mensagem\nif st.button(\"Clique aqui\"):\n    st.write(\"Botão clicado!\")  # Mensagem exibida ao clicar no botão\n\n# Checkbox - Exibe uma caixa de seleção\naceita_termos  st.checkbox(\"Eu aceito os termos e condições\")\nst.write(\"Aceita os termos:\", aceita_termos)  # Exibe o valor selecionado\n\n# Radio - Exibe opções de escolha única\nopcao_radio  st.radio(\"Escolha uma opção\", (\"Opção 1\", \"Opção 2\", \"Opção 3\"))\nst.write(\"Opção escolhida:\", opcao_radio)  # Exibe a opção selecionada\n\n# Selectbox - Exibe um menu suspenso para selecionar uma opção\nopcao_selectbox  st.selectbox(\n    \"Selecione uma opção\", [\"Opção A\", \"Opção B\", \"Opção C\"]\n)\nst.write(\"Opção selecionada:\", opcao_selectbox)  # Exibe a opção selecionada\n\n# Multiselect - Exibe um menu suspenso para selecionar várias opções\nopcoes_multiselect  st.multiselect(\n    \"Selecione múltiplas opções\", [\"Opção 1\", \"Opção 2\", \"Opção 3\"]\n)\nst.write(\n    \"Opções selecionadas:\", opcoes_multiselect\n)  # Exibe as opções selecionadas\n\n# Slider - Exibe uma barra deslizante para selecionar um valor\nvalor_slider  st.slider(\"Selecione um valor\", 0, 100, 50)\nst.write(\"Valor selecionado:\", valor_slider)  # Exibe o valor selecionado\n\n# Select Slider - Exibe uma barra deslizante com opções de texto\nintervalo_slider  st.select_slider(\n    \"Selecione um intervalo\", options[\"a\", \"b\", \"c\", \"d\"], value(\"b\", \"c\")\n)\nst.write(\n    \"Intervalo selecionado:\", intervalo_slider\n)  # Exibe o intervalo selecionado\n\n# Text Input - Exibe uma caixa de entrada de texto\nnome  st.text_input(\"Digite seu nome\")\nst.write(\"Nome digitado:\", nome)  # Exibe o texto digitado\n\n# Number Input - Exibe uma caixa de entrada de número\nnumero  st.number_input(\"Selecione um número\", 0, 100)\nst.write(\"Número selecionado:\", numero)  # Exibe o número selecionado\n\n# Text Area - Exibe uma área de texto\ntexto  st.text_area(\"Escreva um texto\")\nst.write(\"Texto digitado:\", texto)  # Exibe o texto digitado\n\n# Date Input - Exibe um seletor de data\ndata  st.date_input(\"Selecione uma data\", datetime.now())\nst.write(\"Data selecionada:\", data)  # Exibe a data selecionada\n\n# Sidebar\nst.sidebar.title(\"Barra Lateral\")  # Exibe o título da barra lateral\nbotao_sidebar  st.sidebar.button(\"Botão na Barra Lateral\")\nif botao_sidebar:\n    st.sidebar.write(\n        \"Botão na barra lateral clicado!\"\n    )  # Mensagem exibida ao clicar no botão da barra lateral\n\n# Carregar CSV\nsubcabecalho_csv  \"Carregar CSV\"\nst.subheader(subcabecalho_csv)  # Exibe um subcabeçalho\nuploaded_file  st.file_uploader(\"Escolha um arquivo CSV\", type\"csv\")\n\nif uploaded_file is not None:\n    # Barra de progresso durante o upload\n    progress_bar  st.progress(0)\n\n    # Simulação do progresso de carregamento\n    for i in range(100):\n        time.sleep(0.01)\n        progress_bar.progress(i + 1)\n\n    # Lê o CSV\n    csv_data  pd.read_csv(uploaded_file)\n    st.write(\"Dados do CSV:\")\n    st.dataframe(csv_data)  # Exibe o conteúdo do arquivo CSV\n\n    # Soltar balões após o upload\n    st.balloons()\n\n    # Função para converter DataFrame para Parquet\n    @st.cache_data\n    def convert_df_to_parquet(df):\n        output  io.BytesIO()\n        df.to_parquet(output, indexFalse)\n        return output.getvalue()\n\n    st.download_button(\n        label\"Baixar dados como Parquet\",\n        dataconvert_df_to_parquet(csv_data),\n        file_name\"dados.parquet\",\n        mime\"application/octet-stream\",\n    )\n\n\n09-streamlit-dashboard-realtime/exemplo/main_exercicio.py\n\nimport io\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nimport streamlit as st\n\n# 1. Títulos e Texto\ntitulo  \"Aula Introdutória ao Streamlit\"\ncabecalho  \"Aprendendo os Principais Métodos\"\n\n# Exibição de Texto\nsubcabecalho_texto  \"Métodos de Exibição de Texto\"\n\n# Texto simples\ntexto_simples  (\n    \"Streamlit facilita a criação de aplicações web interativas com Python.\"\n)\n\n# Texto em markdown\ntexto_markdown  \"### Este é um texto em **markdown**!\"\n\n# Fórmula em LaTeX\nformula_latex  r\"\"\" e^{i\\pi} + 1  0 \"\"\"\n\n# Código\ncodigo_exemplo  \"x  42\"\n\n# 2. Exibição de Dados\nsubcabecalho_dados  \"Exibição de Dados\"\n\n# Dados do DataFrame\ndata  {\"A\": [1, 2, 3, 4], \"B\": [10, 20, 30, 40]}\ndf  pd.DataFrame(data)\n\n# JSON\njson_exemplo  {\"name\": \"Streamlit\", \"type\": \"Web Framework\"}\n\n# CSV como string\ncsv_exemplo  df.to_csv(indexFalse)\n\n# Lista\nlista_exemplo  [1, 2, 3, 4, 5]\n\n# 3. Métricas\nsubcabecalho_metricas  \"Métricas\"\n\n# Métricas com delta\ntemperatura  {\"label\": \"Temperatura\", \"value\": \"70 °F\", \"delta\": \"1.2 °F\"}\numidade  {\"label\": \"Umidade\", \"value\": \"60%\", \"delta\": \"-5%\"}\nvento  {\"label\": \"Velocidade do Vento\", \"value\": \"15 km/h\", \"delta\": \"2 km/h\"}\nruido  {\"label\": \"Nível de Ruído\", \"value\": \"40 dB\", \"delta\": \"1.5 dB\"}\npressao  {\n    \"label\": \"Pressão Atmosférica\",\n    \"value\": \"1013 hPa\",\n    \"delta\": \"2 hPa\",\n}\n\n# Métrica sem delta\npopulacao  {\"label\": \"População\", \"value\": \"8 bilhões\"}\n\n# 4. Gráficos\nsubcabecalho_graficos  \"Gráficos\"\n\n# Dados dos gráficos\nchart_data  pd.DataFrame(np.random.randn(20, 3), columns[\"a\", \"b\", \"c\"])\n\n# Mais exemplos de gráficos\nmais_graficos  \"Mais Exemplos de Gráficos\"\n\n# Dados do gráfico de dispersão\nscatter_data  pd.DataFrame(np.random.randn(100, 2), columns[\"x\", \"y\"])\n\n# Dados do histograma\nhist_data  np.random.randn(1000)\n\n# 5. Mapas\nsubcabecalho_mapas  \"Mapas\"\n\n# Dados do mapa\nmap_data  pd.DataFrame(\n    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],\n    columns[\"lat\", \"lon\"],\n)\n\n# 6. Mídia\nsubcabecalho_midia  \"Mídia\"\n\n# URL da imagem\nimagem_url  \"https://www.streamlit.io/images/brand/streamlit-logo-primary-colormark-darktext.png\"\nimagem_legenda  \"Streamlit Logo\"\n\n# URL do áudio\naudio_url  \"https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3\"\n\n# URL do vídeo\nvideo_url  \"https://www.youtube.com/watch?vB2iAodr0fOo\"\n\n# 7. Widgets\nsubcabecalho_widgets  \"Widgets\"\n\n# Botão\nbotao  \"Clique aqui\"\nbotao_mensagem  \"Botão clicado!\"\n\n# Checkbox\ncheckbox_label  \"Eu aceito os termos e condições\"\n\n# Radio\nradio_label  \"Escolha uma opção\"\nradio_options  (\"Opção 1\", \"Opção 2\", \"Opção 3\")\n\n# Selectbox\nselectbox_label  \"Selecione uma opção\"\nselectbox_options  [\"Opção A\", \"Opção B\", \"Opção C\"]\n\n# Multiselect\nmultiselect_label  \"Selecione múltiplas opções\"\nmultiselect_options  [\"Opção 1\", \"Opção 2\", \"Opção 3\"]\n\n# Slider\nslider_label  \"Selecione um valor\"\nslider_min  0\nslider_max  100\nslider_default  50\n\n# Select Slider\nselect_slider_label  \"Selecione um intervalo\"\nselect_slider_options  [\"a\", \"b\", \"c\", \"d\"]\nselect_slider_default  (\"b\", \"c\")\n\n# Text Input\ntext_input_label  \"Digite seu nome\"\n\n# Number Input\nnumber_input_label  \"Selecione um número\"\nnumber_input_min  0\nnumber_input_max  100\n\n# Text Area\ntext_area_label  \"Escreva um texto\"\n\n# Date Input\ndate_input_label  \"Selecione uma data\"\ndate_input_default  datetime.now()\n\n# Sidebar\nsidebar_title  \"Barra Lateral\"\nsidebar_button_label  \"Botão na Barra Lateral\"\nsidebar_button_message  \"Botão na barra lateral clicado!\"\n\n# Carregar CSV\nsubcabecalho_csv  \"Carregar CSV\"\nfile_uploader_label  \"Escolha um arquivo CSV\"\nfile_uploader_type  \"csv\"\n\n\n# Função para converter DataFrame para Parquet\n@st.cache_data\ndef convert_df_to_parquet(df):\n    output  io.BytesIO()\n    df.to_parquet(output, indexFalse)\n    return output.getvalue()\n\n\ndownload_button_label_parquet  \"Baixar dados como Parquet\"\ndownload_button_filename_parquet  \"dados.parquet\"\ndownload_button_mime_parquet  \"application/octet-stream\"\n\n\n09-streamlit-dashboard-realtime/exemplo_erros_ruff/erros_ruff.py\n\nimport os  # Importação não utilizada\nimport sys  # Importação não utilizada\n\ndef function_one():\n    pass\n\ndef function_two():\n    pass\n\ndef example_function(a,b):  # Falta espaço após a vírgula\n    \"\"\"Exemplo de função com vários erros\"\"\"\n    print(\"Esta é uma linha de código que é muito longa e ultrapassa o limite de 79 caracteres, o que não está de acordo com o PEP8.\")  # Linha muito longa (E501)\n    result  1 \\\n        + 2  # Quebra de linha desnecessária (W503)\n    myVariable  10  # Nome de variável não está no estilo snake_case (N806)\n    return result + a + b\n\ndef another_function():\n    print(undefined_variable)  # Variável não definida (F821)\n\ndef function_with_trailing_whitespace():  \n    pass  # Espaço em branco à direita (W291)\n\ndef function_without_docstring():\n    pass  # Deveria ter uma docstring explicando a função (D103)\n\ndef function_with_missing_type_annotations(a, b):  # Deveria ter anotação de tipo (ANN001)\n    return a + b\n\ndef unused_function_example():  # Função definida mas não utilizada (F841)\n    pass\n\nx  5\ny   10  # Dois espaços antes da atribuição (E222)\n\ndef function_with_no_space_in_def(param1,param2):  # Falta de espaço após a vírgula (E231)\n    return param1 + param2\n\ndef inconsistent_indentation():\n  a  5  # Indentação inconsistente (E111)\n    b  6\n    return a + b\n\ndef mixed_tabs_and_spaces():\n    a  10  # Mistura de espaços e tabulações (E101)\n\tb  20\n\treturn a + b\n\ndef incorrect_default_argument(value[]):  # Uso de lista mutável como valor padrão (B006)\n    value.append(1)\n    return value\n\ndef function_with_multiple_returns(a):  # Múltiplos retornos (C901)\n    if a > 10:\n        return True\n    elif a  10:\n        return False\n    else:\n        return None\n\nclass ExampleClass:\n    def __init__(self):\n        self.value  10  # Atributo não documentado (D107)\n\n    def get_value(self):\n        return self.value\n\n    def set_value(self, value):\n        self.value  value  # Método setter sem validação (B008)\n\n\n09-streamlit-dashboard-realtime/exemplo_libs_grafico/exemplo.py\n\nimport folium\nimport matplotlib.pyplot as plt\nimport streamlit as st\nfrom streamlit_folium import folium_static\nfrom wordcloud import WordCloud\n\n# Exemplo de gráfico Matplotlib\nst.header(\"Gráfico Matplotlib\")\nfig, ax  plt.subplots()\nax.plot([1, 2, 3, 4], [10, 20, 25, 30])\nst.pyplot(fig)\n\n# Exemplo de mapa Folium\nst.header(\"Mapa Folium\")\nm  folium.Map(location[45.5236, -122.6750], zoom_start13)\nfolium.Marker([45.5236, -122.6750], popup\"The Waterfront\").add_to(m)\nfolium_static(m)\n\n# Exemplo de nuvem de palavras WordCloud\nst.header(\"Nuvem de Palavras\")\ntext  \"Python Streamlit Matplotlib Folium WordCloud\"\nwordcloud  WordCloud(\n    width800, height400, background_color\"white\"\n).generate(text)\nfig, ax  plt.subplots()\nax.imshow(wordcloud, interpolation\"bilinear\")\nax.axis(\"off\")\nst.pyplot(fig)\n\n\n",
        "09-streamlit-dashboard-realtime/projeto_coleta/coleta.py\n\nimport streamlit as st\nimport pandas as pd\nimport os\n\n# Nome do arquivo CSV onde os dados serão armazenados\ndata_file  \"survey_data.csv\"\n\n# Opções de estados\nestados  [\n    \"Acre\", \"Alagoas\", \"Amapá\", \"Amazonas\", \"Bahia\", \"Ceará\",\n    \"Distrito Federal\", \"Espírito Santo\", \"Goiás\", \"Maranhão\",\n    \"Mato Grosso\", \"Mato Grosso do Sul\", \"Minas Gerais\", \"Pará\",\n    \"Paraíba\", \"Paraná\", \"Pernambuco\", \"Piauí\", \"Rio de Janeiro\",\n    \"Rio Grande do Norte\", \"Rio Grande do Sul\", \"Rondônia\", \"Roraima\",\n    \"Santa Catarina\", \"São Paulo\", \"Sergipe\", \"Tocantins\"\n]\n\n# Opções de áreas de atuação\nareas_atuacao  [\"Analista de Dados\", \"Cientista de Dados\", \"Engenheiro de Dados\"]\n\n# Opções de bibliotecas\nbibliotecas  [\n    \"Pandas\", \"Pydantic\", \"scikit-learn\", \"Git\", \"Pandera\", \"streamlit\",\n    \"postgres\", \"databricks\", \"AWS\", \"Azure\", \"airflow\", \"dbt\",\n    \"Pyspark\", \"Polars\", \"Kafka\", \"Duckdb\", \"PowerBI\", \"Excel\", \"Tableau\", \"storm\"\n]\n\n# Opções de horas codando\nhoras_codando  [\"Menos de 5\", \"5-10\", \"10-20\", \"Mais de 20\"]\n\n# Opções de conforto com dados\nconforto_dados  [\"Desconfortável\", \"Neutro\", \"Confortável\", \"Muito Confortável\"]\n\n# Criação do formulário\nwith st.form(\"dados_enquete\"):\n    estado  st.selectbox(\"Estado\", estados)\n    area_atuacao  st.selectbox(\"Área de Atuação\", areas_atuacao)\n    bibliotecas_selecionadas  st.multiselect(\"Bibliotecas e ferramentas mais utilizadas\", bibliotecas)\n    horas_codando  st.selectbox(\"Horas Codando ao longo da semana\", horas_codando)\n    conforto_dados  st.selectbox(\"Conforto ao programar e trabalhar com dados\", conforto_dados)\n    experiencia_python  st.slider(\"Experiência de Python\", 0, 10)\n    experiencia_sql  st.slider(\"Experiência de SQL\", 0, 10)\n    experiencia_cloud  st.slider(\"Experiência em Cloud\", 0, 10)\n\n    # Botão para submeter o formulário\n    submit_button  st.form_submit_button(\"Enviar\")\n\n# Se o botão foi clicado, salvar os dados no DataFrame e no CSV\nif submit_button:\n    novo_dado  {\n        \"Estado\": estado,\n        \"Bibliotecas e ferramentas\": \", \".join(bibliotecas_selecionadas),\n        \"Área de Atuação\": area_atuacao,\n        \"Horas de Estudo\": horas_codando,\n        \"Conforto com Dados\": conforto_dados,\n        \"Experiência de Python\": experiencia_python,\n        \"Experiência de SQL\": experiencia_sql,\n        \"Experiência em Cloud\": experiencia_cloud,\n    }\n    new_data  pd.DataFrame([novo_dado])\n\n    # Verificar se o arquivo existe antes de tentar ler\n    if os.path.exists(data_file):\n        existing_data  pd.read_csv(data_file)\n        updated_data  existing_data.append(new_data, ignore_indexTrue)\n    else:\n        updated_data  new_data\n    \n    # Salvar os dados no arquivo CSV\n    updated_data.to_csv(data_file, indexFalse)\n    st.success(\"Dados enviados com sucesso!\")\n\nst.write(\"Outside the form\")\n\n\n09-streamlit-dashboard-realtime/projeto_coleta/coleta_postgres_psycopg2.py\n\nimport streamlit as st\nimport pandas as pd\nimport psycopg2\nimport os\nfrom psycopg2 import sql\nfrom dotenv import load_dotenv\n\n# Carregar variáveis de ambiente\nload_dotenv()\n\n# Configuração do banco de dados\nDB_HOST  os.getenv(\"DB_HOST\")\nDB_DATABASE  os.getenv(\"DB_DATABASE\")\nDB_USER  os.getenv(\"DB_USER\")\nDB_PASSWORD  os.getenv(\"DB_PASSWORD\")\n\n# Função para conectar ao banco de dados\ndef conectar_banco():\n    try:\n        conn  psycopg2.connect(\n            hostDB_HOST,\n            databaseDB_DATABASE,\n            userDB_USER,\n            passwordDB_PASSWORD,\n        )\n        return conn\n    except Exception as e:\n        st.error(f\"Erro ao conectar ao banco de dados: {e}\")\n        return None\n\n# Função para criar a tabela caso não exista\ndef criar_tabela_se_nao_existir(conn):\n    try:\n        with conn.cursor() as cur:\n            create_table_query  \"\"\"\n            CREATE TABLE IF NOT EXISTS survey_data (\n                id SERIAL PRIMARY KEY,\n                estado VARCHAR(50),\n                bibliotecas TEXT,\n                area_atuacao VARCHAR(50),\n                horas_estudo VARCHAR(20),\n                conforto_dados VARCHAR(50),\n                experiencia_python INTEGER,\n                experiencia_sql INTEGER,\n                experiencia_cloud INTEGER\n            )\n            \"\"\"\n            cur.execute(create_table_query)\n            conn.commit()\n    except Exception as e:\n        st.error(f\"Erro ao criar a tabela: {e}\")\n\n# Função para salvar dados no banco de dados\ndef salvar_dados_banco(conn, dados):\n    try:\n        with conn.cursor() as cur:\n            insert_query  sql.SQL(\"\"\"\n                INSERT INTO survey_data (estado, bibliotecas, area_atuacao, horas_estudo, conforto_dados, experiencia_python, experiencia_sql, experiencia_cloud)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n            \"\"\")\n            cur.execute(\n                insert_query,\n                (\n                    dados[\"Estado\"],\n                    dados[\"Bibliotecas e ferramentas\"],\n                    dados[\"Área de Atuação\"],\n                    dados[\"Horas de Estudo\"],\n                    dados[\"Conforto com Dados\"],\n                    dados[\"Experiência de Python\"],\n                    dados[\"Experiência de SQL\"],\n                    dados[\"Experiência de Cloud\"],\n                ),\n            )\n            conn.commit()\n    except Exception as e:\n        st.error(f\"Erro ao salvar os dados no banco de dados: {e}\")\n        conn.rollback()\n\n# Conectando ao banco de dados e criando a tabela se necessário\nconn  conectar_banco()\nif conn is not None:\n    criar_tabela_se_nao_existir(conn)\n\n# Opções de estados, áreas de atuação, bibliotecas, horas codando e conforto com dados\nestados  [\"Acre\", \"Alagoas\", \"Amapá\", \"Amazonas\", \"Bahia\", \"Ceará\",\n           \"Distrito Federal\", \"Espírito Santo\", \"Goiás\", \"Maranhão\",\n           \"Mato Grosso\", \"Mato Grosso do Sul\", \"Minas Gerais\", \"Pará\",\n           \"Paraíba\", \"Paraná\", \"Pernambuco\", \"Piauí\", \"Rio de Janeiro\",\n           \"Rio Grande do Norte\", \"Rio Grande do Sul\", \"Rondônia\", \"Roraima\",\n           \"Santa Catarina\", \"São Paulo\", \"Sergipe\", \"Tocantins\"]\n\nareas_atuacao  [\"Analista de Dados\", \"Cientista de Dados\", \"Engenheiro de Dados\"]\n\nbibliotecas  [\"Pandas\", \"Pydantic\", \"scikit-learn\", \"Git\", \"Pandera\", \"streamlit\",\n               \"postgres\", \"databricks\", \"AWS\", \"Azure\", \"airflow\", \"dbt\",\n               \"Pyspark\", \"Polars\", \"Kafka\", \"Duckdb\", \"PowerBI\", \"Excel\", \"Tableau\", \"storm\"]\n\nhoras_codando  [\"Menos de 5\", \"5-10\", \"10-20\", \"Mais de 20\"]\n\nconforto_dados  [\"Desconfortável\", \"Neutro\", \"Confortável\", \"Muito Confortável\"]\n\n# Criação do formulário\nwith st.form(\"dados_enquete\"):\n    estado  st.selectbox(\"Estado\", estados)\n    area_atuacao  st.selectbox(\"Área de Atuação\", areas_atuacao)\n    bibliotecas_selecionadas  st.multiselect(\"Bibliotecas e ferramentas mais utilizadas\", bibliotecas)\n    horas_codando  st.selectbox(\"Horas Codando ao longo da semana\", horas_codando)\n    conforto_dados  st.selectbox(\"Conforto ao programar e trabalhar com dados\", conforto_dados)\n    experiencia_python  st.slider(\"Experiência de Python\", 0, 10)\n    experiencia_sql  st.slider(\"Experiência de SQL\", 0, 10)\n    experiencia_cloud  st.slider(\"Experiência em Cloud\", 0, 10)\n\n    # Botão para submeter o formulário\n    submit_button  st.form_submit_button(\"Enviar\")\n\n# Se o botão foi clicado, salvar os dados no banco de dados\nif submit_button:\n    novo_dado  {\n        \"Estado\": estado,\n        \"Bibliotecas e ferramentas\": \", \".join(bibliotecas_selecionadas),\n        \"Área de Atuação\": area_atuacao,\n        \"Horas de Estudo\": horas_codando,\n        \"Conforto com Dados\": conforto_dados,\n        \"Experiência de Python\": experiencia_python,\n        \"Experiência de SQL\": experiencia_sql,\n        \"Experiência de Cloud\": experiencia_cloud,\n    }\n    salvar_dados_banco(conn, novo_dado)\n    st.success(\"Dados enviados com sucesso!\")\n\nst.write(\"Outside the form\")\n\n\n09-streamlit-dashboard-realtime/projeto_coleta/coleta_postgres_sqlalchemy.py\n\nimport streamlit as st\nimport pandas as pd\nimport os\nfrom sqlalchemy import create_engine, Column, Integer, String, Text\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom sqlalchemy.orm import declarative_base, sessionmaker\nfrom dotenv import load_dotenv\n\n# Carregar variáveis de ambiente\nload_dotenv()\n\n# Construir a URL do banco de dados\nDB_HOST  os.getenv(\"DB_HOST\")\nDB_DATABASE  os.getenv(\"DB_DATABASE\")\nDB_USER  os.getenv(\"DB_USER\")\nDB_PASSWORD  os.getenv(\"DB_PASSWORD\")\nDATABASE_URL  f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_DATABASE}\"\n\n# Definindo a base e a tabela usando SQLAlchemy ORM\nBase  declarative_base()\n\nclass SurveyData(Base):\n    __tablename__  'survey_data'\n    id  Column(Integer, primary_keyTrue, autoincrementTrue)\n    estado  Column(String(50))\n    bibliotecas  Column(Text)\n    area_atuacao  Column(String(50))\n    horas_estudo  Column(String(20))\n    conforto_dados  Column(String(50))\n    experiencia_python  Column(Integer)\n    experiencia_sql  Column(Integer)\n    experiencia_cloud  Column(Integer)\n\ndef get_engine():\n    try:\n        engine  create_engine(DATABASE_URL)\n        return engine\n    except SQLAlchemyError as e:\n        st.error(f\"Erro ao conectar ao banco de dados: {e}\")\n        return None\n\n# Função para criar a tabela caso não exista\ndef criar_tabela_se_nao_existir(engine):\n    try:\n        Base.metadata.create_all(engine)\n    except SQLAlchemyError as e:\n        st.error(f\"Erro ao criar a tabela: {e}\")\n\n# Função para salvar dados no banco de dados\ndef salvar_dados_banco(session, dados):\n    try:\n        novo_dado  SurveyData(\n            estadodados[\"Estado\"],\n            bibliotecasdados[\"Bibliotecas e ferramentas\"],\n            area_atuacaodados[\"Área de Atuação\"],\n            horas_estudodados[\"Horas de Estudo\"],\n            conforto_dadosdados[\"Conforto com Dados\"],\n            experiencia_pythondados[\"Experiência de Python\"],\n            experiencia_sqldados[\"Experiência de SQL\"],\n            experiencia_clouddados[\"Experiência de Cloud\"],\n        )\n        session.add(novo_dado)\n        session.commit()\n    except SQLAlchemyError as e:\n        st.error(f\"Erro ao salvar os dados no banco de dados: {e}\")\n        session.rollback()\n\n# Obter a instância do engine e criar a tabela se necessário\nengine  get_engine()\nif engine is not None:\n    criar_tabela_se_nao_existir(engine)\n\n# Configurar a sessão do SQLAlchemy\nSession  sessionmaker(bindengine)\n\n# Opções de estados, áreas de atuação, bibliotecas, horas codando e conforto com dados\nestados  [\"Acre\", \"Alagoas\", \"Amapá\", \"Amazonas\", \"Bahia\", \"Ceará\",\n           \"Distrito Federal\", \"Espírito Santo\", \"Goiás\", \"Maranhão\",\n           \"Mato Grosso\", \"Mato Grosso do Sul\", \"Minas Gerais\", \"Pará\",\n           \"Paraíba\", \"Paraná\", \"Pernambuco\", \"Piauí\", \"Rio de Janeiro\",\n           \"Rio Grande do Norte\", \"Rio Grande do Sul\", \"Rondônia\", \"Roraima\",\n           \"Santa Catarina\", \"São Paulo\", \"Sergipe\", \"Tocantins\"]\n\nareas_atuacao  [\"Analista de Dados\", \"Cientista de Dados\", \"Engenheiro de Dados\"]\n\nbibliotecas  [\"Pandas\", \"Pydantic\", \"scikit-learn\", \"Git\", \"Pandera\", \"streamlit\",\n               \"postgres\", \"databricks\", \"AWS\", \"Azure\", \"airflow\", \"dbt\",\n               \"Pyspark\", \"Polars\", \"Kafka\", \"Duckdb\", \"PowerBI\", \"Excel\", \"Tableau\", \"storm\"]\n\nhoras_codando  [\"Menos de 5\", \"5-10\", \"10-20\", \"Mais de 20\"]\n\nconforto_dados  [\"Desconfortável\", \"Neutro\", \"Confortável\", \"Muito Confortável\"]\n\n# Criação do formulário\nwith st.form(\"dados_enquete\"):\n    estado  st.selectbox(\"Estado\", estados)\n    area_atuacao  st.selectbox(\"Área de Atuação\", areas_atuacao)\n    bibliotecas_selecionadas  st.multiselect(\"Bibliotecas e ferramentas mais utilizadas\", bibliotecas)\n    horas_codando  st.selectbox(\"Horas Codando ao longo da semana\", horas_codando)\n    conforto_dados  st.selectbox(\"Conforto ao programar e trabalhar com dados\", conforto_dados)\n    experiencia_python  st.slider(\"Experiência de Python\", 0, 10)\n    experiencia_sql  st.slider(\"Experiência de SQL\", 0, 10)\n    experiencia_cloud  st.slider(\"Experiência em Cloud\", 0, 10)\n\n    # Botão para submeter o formulário\n    submit_button  st.form_submit_button(\"Enviar\")\n\n# Se o botão foi clicado, salvar os dados no banco de dados\nif submit_button:\n    novo_dado  {\n        \"Estado\": estado,\n        \"Bibliotecas e ferramentas\": \", \".join(bibliotecas_selecionadas),\n        \"Área de Atuação\": area_atuacao,\n        \"Horas de Estudo\": horas_codando,\n        \"Conforto com Dados\": conforto_dados,\n        \"Experiência de Python\": experiencia_python,\n        \"Experiência de SQL\": experiencia_sql,\n        \"Experiência de Cloud\": experiencia_cloud,\n    }\n    session  Session()\n    salvar_dados_banco(session, novo_dado)\n    st.success(\"Dados enviados com sucesso!\")\n\nst.write(\"Outside the form\")\n\n\n09-streamlit-dashboard-realtime/projeto_dash/dash.py\n\nfrom collections import Counter\n\nimport folium\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport streamlit as st\nfrom streamlit_folium import folium_static\nfrom wordcloud import WordCloud\n\n# Caminho para o arquivo CSV\nDATA_PATH  \"survey_data.csv\"\nIMAGE_PATH  \"foto.png\"\n\n# Configurações iniciais\ndata  pd.read_csv(DATA_PATH)\n\n# Listas e constantes\nCOMFORT_ORDER  [\n    \"Muito Desconfortável\",\n    \"Desconfortável\",\n    \"Neutro\",\n    \"Confortável\",\n    \"Muito Confortável\",\n]\n\nSTATES_COORDS  {\n    \"São Paulo\": [-23.5505, -46.6333],\n    \"Rio de Janeiro\": [-22.9068, -43.1729],\n    \"Minas Gerais\": [-19.9167, -43.9345],\n    \"Bahia\": [-12.9714, -38.5014],\n    \"Paraná\": [-25.4284, -49.2733],\n    \"Rio Grande do Sul\": [-30.0346, -51.2177],\n    \"Santa Catarina\": [-27.5954, -48.5480],\n    \"Ceará\": [-3.7172, -38.5434],\n    \"Distrito Federal\": [-15.8267, -47.9218],\n    \"Pernambuco\": [-8.0476, -34.8770],\n    \"Goiás\": [-16.6869, -49.2648],\n    \"Pará\": [-1.4558, -48.4902],\n    \"Mato Grosso\": [-15.6014, -56.0979],\n    \"Amazonas\": [-3.1190, -60.0217],\n    \"Espírito Santo\": [-20.3155, -40.3128],\n    \"Paraíba\": [-7.1195, -34.8450],\n    \"Acre\": [-9.97499, -67.8243],\n    # Adicione outras coordenadas dos estados aqui\n}\n\n# Funções de análise e visualização\ndef top_bibliotecas_por_area(data):\n    st.header(\"Top 3 Bibliotecas por Área de Atuação\")\n    areas  data[\"Área de Atuação\"].unique()\n    area_selecionada  st.selectbox(\n        \"Selecione a Área de Atuação\",\n        [\"Nenhuma área selecionada\"] + list(areas),\n    )\n\n    if area_selecionada ! \"Nenhuma área selecionada\":\n        st.subheader(area_selecionada)\n        bibliotecas_area  (\n            data[data[\"Área de Atuação\"]  area_selecionada][\"Bibliotecas\"]\n            .str.cat(sep\",\")\n            .split(\",\")\n        )\n        bibliotecas_contagem  Counter(\n            [biblioteca.strip() for biblioteca in bibliotecas_area]\n        )\n        top_3_bibliotecas  bibliotecas_contagem.most_common(3)\n\n        col1, col2, col3  st.columns(3)\n        colunas  [col1, col2, col3]\n        for i, (biblioteca, count) in enumerate(top_3_bibliotecas):\n            with colunas[i]:\n                st.metric(labelbiblioteca, valuef\"{count} vezes\")\n\n\ndef plotar_grafico_area(data):\n    data[\"Conforto com Dados\"]  pd.Categorical(\n        data[\"Conforto com Dados\"], categoriesCOMFORT_ORDER, orderedTrue\n    )\n    comfort_vs_study_hours  (\n        data.groupby([\"Conforto com Dados\", \"Horas de Estudo\"], observedTrue)\n        .size()\n        .unstack(fill_value0)\n    )\n    comfort_vs_study_hours  comfort_vs_study_hours.reindex(\n        columns[\"Menos de 5\", \"5-10\", \"10-20\", \"Mais de 20\"], fill_value0\n    )\n    colors  [\n        \"#00008B\",\n        \"#87CEEB\",\n        \"#FF6347\",\n        \"#FF0000\",\n    ]\n    st.header(\"Nível de Conforto com Dados vs. Horas de Estudo por Semana\")\n    st.area_chart(comfort_vs_study_hours, colorcolors)\n\n\ndef plotar_graficos_experiencia(data):\n    st.header(\"Experiência Técnica dos Participantes\")\n    col1, col2, col3  st.columns(3)\n\n    with col1:\n        st.subheader(\"Experiência de Python\")\n        experiencia_python_count  (\n            data[\"Experiência de Python\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_python_count)\n\n    with col2:\n        st.subheader(\"Experiência de SQL\")\n        experiencia_sql_count  (\n            data[\"Experiência de SQL\"].value_counts().sort_index()\n        )\n        st.bar_chart(experiencia_sql_count)\n\n    with col3:\n        st.subheader(\"Experiência em Cloud\")\n        experiencia_cloud_count  (\n            data[\"Experiência em Cloud\"].value_counts().sort_index()\n        )\n        st.area_chart(experiencia_cloud_count)\n\n\ndef plotar_mapa(data):\n    st.header(\"Mapa do Brasil com Distribuição dos Participantes\")\n    state_participants  Counter(data[\"Estado\"])\n    map_data  pd.DataFrame(\n        [\n            {\n                \"Estado\": state,\n                \"lat\": coord[0],\n                \"lon\": coord[1],\n                \"Participantes\": state_participants[state],\n            }\n            for state, coord in STATES_COORDS.items()\n            if state_participants[state] > 0  # Filtrar apenas estados com participantes\n        ]\n    )\n    m  folium.Map(location[-15.7801, -47.9292], zoom_start4)\n    for _, row in map_data.iterrows():\n        folium.CircleMarker(\n            location[row[\"lat\"], row[\"lon\"]],\n            radiusrow[\"Participantes\"] * 3,\n            popupf\"{row['Estado']}: {row['Participantes']} participantes\",\n            color\"crimson\",\n            fillTrue,\n            fill_color\"crimson\",\n            weight1,\n        ).add_to(m)\n    folium_static(m)\n\n\ndef plotar_nuvem_palavras(data):\n    st.header(\"Nuvem de Palavras das Bibliotecas Utilizadas\")\n    all_libs  \" \".join(data[\"Bibliotecas\"].dropna().str.replace(\",\", \" \"))\n    wordcloud  WordCloud(\n        width800, height400, background_color\"white\"\n    ).generate(all_libs)\n    fig, ax  plt.subplots(figsize(10, 5))\n    ax.imshow(wordcloud)\n    ax.axis(\"off\")\n    st.pyplot(fig)\n\n\ndef exibir_imagem_final(image_path):\n    st.header(\"Foto da Jornada\")\n    st.image(image_path, use_column_widthTrue)\n\n\n# Execução das funções\nst.title(\"Dados dos Participantes\")\ntop_bibliotecas_por_area(data)\nplotar_grafico_area(data)\nplotar_graficos_experiencia(data)\nplotar_mapa(data)\nplotar_nuvem_palavras(data)\nexibir_imagem_final(IMAGE_PATH)\n\n\n",
        "09-streamlit-dashboard-realtime/projeto_dash/dash_postgres.py\n\nfrom collections import Counter\n\nimport folium\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport streamlit as st\nfrom streamlit_folium import folium_static\nfrom wordcloud import WordCloud\nfrom sqlalchemy import create_engine\nfrom dotenv import load_dotenv\nimport os\n\n# Carregar variáveis de ambiente\nload_dotenv()\n\n# Configuração do banco de dados\nDB_HOST  os.getenv(\"DB_HOST\")\nDB_DATABASE  os.getenv(\"DB_DATABASE\")\nDB_USER  os.getenv(\"DB_USER\")\nDB_PASSWORD  os.getenv(\"DB_PASSWORD\")\n\n# URL do banco de dados\nDATABASE_URL  f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_DATABASE}\"\n\n# Função para conectar ao banco de dados PostgreSQL usando SQLAlchemy\ndef conectar_banco():\n    try:\n        engine  create_engine(DATABASE_URL)\n        return engine\n    except Exception as e:\n        st.error(f\"Erro ao conectar ao banco de dados: {e}\")\n        return None\n\n# Função para carregar dados do banco de dados\ndef carregar_dados(engine):\n    if engine:\n        try:\n            query  \"\"\"\n            SELECT\n                estado AS \"Estado\",\n                bibliotecas AS \"Bibliotecas\",\n                area_atuacao AS \"Área de Atuação\",\n                horas_estudo AS \"Horas de Estudo\",\n                conforto_dados AS \"Conforto com Dados\",\n                experiencia_python AS \"Experiência de Python\",\n                experiencia_sql AS \"Experiência de SQL\",\n                experiencia_cloud AS \"Experiência em Cloud\"\n            FROM\n                survey_data\n            \"\"\"\n            data  pd.read_sql(query, engine)\n            return data\n        except Exception as e:\n            st.error(f\"Erro ao carregar os dados do banco de dados: {e}\")\n            return pd.DataFrame()\n    return pd.DataFrame()\n\n# Conectar ao banco de dados e carregar os dados\nengine  conectar_banco()\ndata  carregar_dados(engine)\n\nIMAGE_PATH  \"foto.png\"\n\n# Listas e constantes\nCOMFORT_ORDER  [\n    \"Muito Desconfortável\",\n    \"Desconfortável\",\n    \"Neutro\",\n    \"Confortável\",\n    \"Muito Confortável\",\n]\n\nSTATES_COORDS  {\n    \"São Paulo\": [-23.5505, -46.6333],\n    \"Rio de Janeiro\": [-22.9068, -43.1729],\n    \"Minas Gerais\": [-19.9167, -43.9345],\n    \"Bahia\": [-12.9714, -38.5014],\n    \"Paraná\": [-25.4284, -49.2733],\n    \"Rio Grande do Sul\": [-30.0346, -51.2177],\n    \"Santa Catarina\": [-27.5954, -48.5480],\n    \"Ceará\": [-3.7172, -38.5434],\n    \"Distrito Federal\": [-15.8267, -47.9218],\n    \"Pernambuco\": [-8.0476, -34.8770],\n    \"Goiás\": [-16.6869, -49.2648],\n    \"Pará\": [-1.4558, -48.4902],\n    \"Mato Grosso\": [-15.6014, -56.0979],\n    \"Amazonas\": [-3.1190, -60.0217],\n    \"Espírito Santo\": [-20.3155, -40.3128],\n    \"Paraíba\": [-7.1195, -34.8450],\n    \"Acre\": [-9.97499, -67.8243],\n    # Adicione outras coordenadas dos estados aqui\n}\n\n# Funções de análise e visualização\ndef top_bibliotecas_por_area(data):\n    st.header(\"Top 3 Bibliotecas por Área de Atuação\")\n    areas  data[\"Área de Atuação\"].unique()\n    area_selecionada  st.selectbox(\n        \"Selecione a Área de Atuação\",\n        [\"Nenhuma área selecionada\"] + list(areas),\n    )\n\n    if area_selecionada ! \"Nenhuma área selecionada\":\n        st.subheader(area_selecionada)\n        bibliotecas_area  (\n            data[data[\"Área de Atuação\"]  area_selecionada][\"Bibliotecas\"]\n            .str.cat(sep\",\")\n            .split(\",\")\n        )\n        bibliotecas_contagem  Counter(\n            [biblioteca.strip() for biblioteca in bibliotecas_area]\n        )\n        top_3_bibliotecas  bibliotecas_contagem.most_common(3)\n\n        col1, col2, col3  st.columns(3)\n        colunas  [col1, col2, col3]\n        for i, (biblioteca, count) in enumerate(top_3_bibliotecas):\n            with colunas[i]:\n                st.metric(labelbiblioteca, valuef\"{count} vezes\")\n\n\ndef plotar_grafico_area(data):\n    data[\"Conforto com Dados\"]  pd.Categorical(\n        data[\"Conforto com Dados\"], categoriesCOMFORT_ORDER, orderedTrue\n    )\n    comfort_vs_study_hours  (\n        data.groupby([\"Conforto com Dados\", \"Horas de Estudo\"], observedTrue)\n        .size()\n        .unstack(fill_value0)\n    )\n    comfort_vs_study_hours  comfort_vs_study_hours.reindex(\n        columns[\"Menos de 5\", \"5-10\", \"10-20\", \"Mais de 20\"], fill_value0\n    )\n    colors  [\n        \"#00008B\",\n        \"#87CEEB\",\n        \"#FF6347\",\n        \"#FF0000\",\n    ]\n    st.header(\"Nível de Conforto com Dados vs. Horas de Estudo por Semana\")\n    st.area_chart(comfort_vs_study_hours, colorcolors)\n\n\ndef plotar_graficos_experiencia(data):\n    st.header(\"Experiência Técnica dos Participantes\")\n    col1, col2, col3  st.columns(3)\n\n    with col1:\n        st.subheader(\"Experiência de Python\")\n        experiencia_python_count  (\n            data[\"Experiência de Python\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_python_count)\n\n    with col2:\n        st.subheader(\"Experiência de SQL\")\n        experiencia_sql_count  (\n            data[\"Experiência de SQL\"].value_counts().sort_index()\n        )\n        st.bar_chart(experiencia_sql_count)\n\n    with col3:\n        st.subheader(\"Experiência em Cloud\")\n        experiencia_cloud_count  (\n            data[\"Experiência em Cloud\"].value_counts().sort_index()\n        )\n        st.area_chart(experiencia_cloud_count)\n\n\ndef plotar_mapa(data):\n    st.header(\"Mapa do Brasil com Distribuição dos Participantes\")\n    state_participants  Counter(data[\"Estado\"])\n    map_data  pd.DataFrame(\n        [\n            {\n                \"Estado\": state,\n                \"lat\": coord[0],\n                \"lon\": coord[1],\n                \"Participantes\": state_participants[state],\n            }\n            for state, coord in STATES_COORDS.items()\n            if state_participants[state] > 0  # Filtrar apenas estados com participantes\n        ]\n    )\n    m  folium.Map(location[-15.7801, -47.9292], zoom_start4)\n    for _, row in map_data.iterrows():\n        folium.CircleMarker(\n            location[row[\"lat\"], row[\"lon\"]],\n            radiusrow[\"Participantes\"] * 3,\n            popupf\"{row['Estado']}: {row['Participantes']} participantes\",\n            color\"crimson\",\n            fillTrue,\n            fill_color\"crimson\",\n            weight1,\n        ).add_to(m)\n    folium_static(m)\n\n\ndef plotar_nuvem_palavras(data):\n    st.header(\"Nuvem de Palavras das Bibliotecas Utilizadas\")\n    all_libs  \" \".join(data[\"Bibliotecas\"].dropna().str.replace(\",\", \" \"))\n    wordcloud  WordCloud(\n        width800, height400, background_color\"white\"\n    ).generate(all_libs)\n    fig, ax  plt.subplots(figsize(10, 5))\n    ax.imshow(wordcloud)\n    ax.axis(\"off\")\n    st.pyplot(fig)\n\n\ndef exibir_imagem_final(image_path):\n    st.header(\"Foto da Jornada\")\n    st.image(image_path, use_column_widthTrue)\n\n\n# Execução das funções\nst.title(\"Dados dos Participantes\")\ntop_bibliotecas_por_area(data)\nplotar_grafico_area(data)\nplotar_graficos_experiencia(data)\nplotar_mapa(data)\nplotar_nuvem_palavras(data)\n#exibir_imagem_final(IMAGE_PATH)\n\n\n09-streamlit-dashboard-realtime/projeto_dash/dash_postgres_cache.py\n\nimport os\nfrom collections import Counter\n\nimport folium\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom sqlalchemy import create_engine\nfrom streamlit_folium import folium_static\nfrom wordcloud import WordCloud\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n\n# Função para conectar ao banco de dados PostgreSQL usando SQLAlchemy\n@st.cache_resource\ndef conectar_banco():\n    try:\n        engine  create_engine(\n            f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_DATABASE')}\"\n        )\n        return engine\n    except Exception as e:\n        st.error(f\"Erro ao conectar ao banco de dados: {e}\")\n        return None\n\n\n# Função para carregar dados do banco de dados\n@st.cache_data(ttl300)\ndef carregar_dados(_engine):\n    if _engine:\n        try:\n            query  \"\"\"\n            SELECT\n                estado AS \"Estado\",\n                bibliotecas AS \"Bibliotecas\",\n                area_atuacao AS \"Área de Atuação\",\n                horas_estudo AS \"Horas de Estudo\",\n                conforto_dados AS \"Conforto com Dados\",\n                experiencia_python AS \"Experiência de Python\",\n                experiencia_sql AS \"Experiência de SQL\",\n                experiencia_cloud AS \"Experiência em Cloud\"\n            FROM\n                survey_data\n            \"\"\"\n            data  pd.read_sql(query, _engine)\n            return data\n        except Exception as e:\n            st.error(f\"Erro ao carregar os dados do banco de dados: {e}\")\n            return pd.DataFrame()\n    return pd.DataFrame()\n\n\n# Função para exibir os dados\ndef exibir_dados(data):\n    st.header(\"Dados dos Participantes\")\n    st.dataframe(data)\n\n\n# Função para plotar gráfico de área\ndef plotar_grafico_area(data):\n    comfort_order  [\n        \"Muito Desconfortável\",\n        \"Desconfortável\",\n        \"Neutro\",\n        \"Confortável\",\n        \"Muito Confortável\",\n    ]\n    data[\"Conforto com Dados\"]  pd.Categorical(\n        data[\"Conforto com Dados\"], categoriescomfort_order, orderedTrue\n    )\n    comfort_vs_study_hours  (\n        data.groupby([\"Conforto com Dados\", \"Horas de Estudo\"], observedTrue)\n        .size()\n        .unstack(fill_value0)\n    )\n    comfort_vs_study_hours  comfort_vs_study_hours.reindex(\n        columns[\"Menos de 5\", \"5-10\", \"10-20\", \"Mais de 20\"], fill_value0\n    )\n    colors  [\n        \"#00008B\",\n        \"#87CEEB\",\n        \"#FF6347\",\n        \"#FF0000\",\n    ]  # Dark Blue, Light Blue, Light Red, Red\n    st.header(\"Nível de Conforto com Dados vs. Horas de Estudo por Semana\")\n    st.area_chart(comfort_vs_study_hours, colorcolors)\n\n\n# Função para plotar gráficos de experiência técnica\ndef plotar_graficos_experiencia(data):\n    st.header(\"Experiência Técnica dos Participantes\")\n    col1, col2, col3  st.columns(3)\n\n    with col1:\n        st.subheader(\"Experiência de Python\")\n        experiencia_python_count  (\n            data[\"Experiência de Python\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_python_count)\n\n    with col2:\n        st.subheader(\"Experiência de SQL\")\n        experiencia_sql_count  (\n            data[\"Experiência de SQL\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_sql_count)\n\n    with col3:\n        st.subheader(\"Experiência em Cloud\")\n        experiencia_cloud_count  (\n            data[\"Experiência em Cloud\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_cloud_count)\n\n\n# Função para plotar o mapa do Brasil\ndef plotar_mapa(data):\n    st.header(\"Mapa do Brasil com Distribuição dos Participantes\")\n    state_coords  {\n        \"Acre\": [-9.97499, -67.8243],\n        \"Alagoas\": [-9.5713, -36.7820],\n        \"Amapá\": [0.0370, -51.0504],\n        \"Amazonas\": [-3.1190, -60.0217],\n        \"Bahia\": [-12.9714, -38.5014],\n        \"Ceará\": [-3.7172, -38.5434],\n        \"Distrito Federal\": [-15.8267, -47.9218],\n        \"Espírito Santo\": [-20.3155, -40.3128],\n        \"Goiás\": [-16.6869, -49.2648],\n        \"Maranhão\": [-2.5307, -44.3068],\n        \"Mato Grosso\": [-15.6014, -56.0979],\n        \"Mato Grosso do Sul\": [-20.4697, -54.6201],\n        \"Minas Gerais\": [-19.9167, -43.9345],\n        \"Pará\": [-1.4558, -48.4902],\n        \"Paraíba\": [-7.1195, -34.8450],\n        \"Paraná\": [-25.4284, -49.2733],\n        \"Pernambuco\": [-8.0476, -34.8770],\n        \"Piauí\": [-5.0892, -42.8016],\n        \"Rio de Janeiro\": [-22.9068, -43.1729],\n        \"Rio Grande do Norte\": [-5.7945, -35.2110],\n        \"Rio Grande do Sul\": [-30.0346, -51.2177],\n        \"Rondônia\": [-8.7612, -63.9039],\n        \"Roraima\": [2.8235, -60.6758],\n        \"Santa Catarina\": [-27.5954, -48.5480],\n        \"São Paulo\": [-23.5505, -46.6333],\n        \"Sergipe\": [-10.9472, -37.0731],\n        \"Tocantins\": [-10.1844, -48.3336],\n    }\n    state_participants  Counter(data[\"Estado\"])\n    map_data  pd.DataFrame(\n        [\n            {\n                \"Estado\": state,\n                \"lat\": coord[0],\n                \"lon\": coord[1],\n                \"Participantes\": state_participants[state],\n            }\n            for state, coord in state_coords.items()\n            if state_participants[state]\n            > 0  # Filtrar apenas estados com participantes\n        ]\n    )\n    m  folium.Map(location[-15.7801, -47.9292], zoom_start4)\n    for _, row in map_data.iterrows():\n        folium.CircleMarker(\n            location[row[\"lat\"], row[\"lon\"]],\n            radiusrow[\"Participantes\"]\n            * 10,  # Ajustar o raio proporcionalmente\n            popupf\"{row['Estado']}: {row['Participantes']} participantes\",\n            color\"crimson\",\n            fillTrue,\n            fill_color\"crimson\",\n            weight1,  # Ajustar a espessura do contorno do círculo\n        ).add_to(m)\n    folium_static(m)\n\n\n# Função para plotar nuvem de palavras\ndef plotar_nuvem_palavras(data):\n    st.header(\"Nuvem de Palavras das Bibliotecas Utilizadas\")\n    all_libs  \" \".join(data[\"Bibliotecas\"].dropna().str.replace(\",\", \" \"))\n    wordcloud  WordCloud(\n        width800, height400, background_color\"white\"\n    ).generate(all_libs)\n    fig, ax  plt.subplots(figsize(10, 5))\n    ax.imshow(wordcloud, interpolation\"bilinear\")\n    ax.axis(\"off\")  # remove o quadro em volta\n    st.pyplot(fig)  # gera a imagem no streamlit\n    plt.close(fig)\n\n\ndef top_bibliotecas_por_area(data):\n    st.header(\"Top 3 Bibliotecas por Área de Atuação\")\n    areas  data[\"Área de Atuação\"].unique()\n    area_selecionada  st.selectbox(\n        \"Selecione a Área de Atuação\",\n        [\"Nenhuma área selecionada\"] + list(areas),\n    )\n\n    if area_selecionada ! \"Nenhuma área selecionada\":\n        st.subheader(area_selecionada)\n        bibliotecas_area  (\n            data[data[\"Área de Atuação\"]  area_selecionada][\"Bibliotecas\"]\n            .str.cat(sep\",\")\n            .split(\",\")\n        )\n        bibliotecas_contagem  Counter(bibliotecas_area)\n        top_3_bibliotecas  bibliotecas_contagem.most_common(3)\n        col1, col2, col3  st.columns(3)\n        colunas  [col1, col2, col3]\n        for col, (biblioteca, count) in zip(colunas, top_3_bibliotecas):\n            with col:\n                st.metric(labelbiblioteca.strip(), valuef\"{count} vezes\")\n\n\n# Função para exibir uma imagem no final\ndef exibir_imagem_final(image_path):\n    st.header(\"Foto da Jornada\")\n    st.image(image_path, use_column_widthTrue)\n\n\n# Carregar os dados do banco de dados\nengine  conectar_banco()\ndata  carregar_dados(engine)\n\n# Chamar as funções para exibir os dados e gráficos\nst.title(\"Geração de Gráficos da Enquete\")\nexibir_dados(data)\nplotar_grafico_area(data)\nplotar_graficos_experiencia(data)\nplotar_mapa(data)\nplotar_nuvem_palavras(data)\ntop_bibliotecas_por_area(data)\nexibir_imagem_final(\"foto.png\")\n\n\n09-streamlit-dashboard-realtime/survey/README.md\n\n## Vantagens de Utilizar o Streamlit\n\nStreamlit é uma biblioteca de código aberto em Python que torna extremamente fácil criar e compartilhar aplicativos web de dados. Uma das grandes vantagens do Streamlit é a sua capacidade de integrar diversas bibliotecas gráficas populares do Python, proporcionando uma experiência visual rica e interativa. Aqui estão algumas vantagens e destaques dessas integrações:\n\n### 1. Matplotlib\n\n#### O que é?\nMatplotlib é uma biblioteca de plotagem 2D extremamente popular em Python, usada para criar gráficos estáticos, animados e interativos. É altamente configurável e permite criar visualizações complexas com facilidade.\n\n#### Vantagens no Streamlit\n- **Facilidade de Integração**: Com Streamlit, você pode exibir gráficos Matplotlib de forma simples utilizando `st.pyplot()`.\n- **Interatividade**: Streamlit permite adicionar interatividade aos seus gráficos Matplotlib sem a necessidade de configurar um ambiente web completo.\n- **Visualizações Poderosas**: Combine a flexibilidade do Matplotlib com a simplicidade do Streamlit para criar dashboards poderosos e visualizações de dados detalhadas.\n\n### 2. Folium\n\n#### O que é?\nFolium é uma biblioteca que facilita a visualização de dados geoespaciais utilizando Leaflet.js. Com Folium, você pode criar mapas interativos e adicionar marcadores, camadas e outras funcionalidades.\n\n#### Vantagens no Streamlit\n- **Mapas Interativos**: Streamlit e Folium juntos permitem a criação de mapas interativos dentro de aplicativos web, ideais para análises geoespaciais.\n- **Visualização de Dados Geográficos**: Exiba dados geográficos e geolocalizados diretamente no navegador, facilitando a compreensão e análise espacial.\n- **Simplicidade**: Adicione mapas interativos aos seus aplicativos com poucas linhas de código usando `folium_static()`.\n\n### 3. WordCloud\n\n#### O que é?\nWordCloud é uma biblioteca para a geração de nuvens de palavras a partir de texto. As nuvens de palavras são uma forma visual de representar a frequência ou importância de palavras em um texto, com palavras mais frequentes aparecendo maiores.\n\n#### Vantagens no Streamlit\n- **Visualização de Texto**: Utilize WordCloud com Streamlit para criar representações visuais atraentes de dados textuais.\n- **Facilidade de Uso**: Crie e exiba nuvens de palavras rapidamente usando `WordCloud` e `st.pyplot()`.\n- **Análise Textual**: Ideal para visualizar e explorar grandes volumes de texto de maneira intuitiva.\n\n### Conclusão\n\nStreamlit se destaca como uma ferramenta poderosa para criar aplicativos de dados interativos, integrando facilmente bibliotecas gráficas populares como Matplotlib, Folium e WordCloud. Estas integrações permitem aos desenvolvedores focar na análise e visualização de dados, sem a necessidade de se preocupar com a infraestrutura web subjacente. Com Streamlit, você pode transformar scripts de dados em aplicativos web compartilháveis em minutos, facilitando a disseminação e compreensão de insights valiosos.\n\n### Exemplo de Código\n\n```python\nimport streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport folium\nfrom streamlit_folium import folium_static\nfrom wordcloud import WordCloud\n\n# Exemplo de gráfico Matplotlib\nst.header(\"Gráfico Matplotlib\")\nfig, ax  plt.subplots()\nax.plot([1, 2, 3, 4], [10, 20, 25, 30])\nst.pyplot(fig)\n\n# Exemplo de mapa Folium\nst.header(\"Mapa Folium\")\nm  folium.Map(location[45.5236, -122.6750], zoom_start13)\nfolium.Marker([45.5236, -122.6750], popup\"The Waterfront\").add_to(m)\nfolium_static(m)\n\n# Exemplo de nuvem de palavras WordCloud\nst.header(\"Nuvem de Palavras\")\ntext  \"Python Streamlit Matplotlib Folium WordCloud\"\nwordcloud  WordCloud(width800, height400, background_color'white').generate(text)\nfig, ax  plt.subplots()\nax.imshow(wordcloud, interpolation'bilinear')\nax.axis(\"off\")\nst.pyplot(fig)\n```\n\nStreamlit transforma a maneira como você trabalha com dados, tornando a criação de visualizações e aplicativos de dados mais acessível e eficiente.\n\n",
        "09-streamlit-dashboard-realtime/survey/dash.py\n\nfrom collections import Counter\n\nimport folium\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport streamlit as st\nfrom streamlit_folium import folium_static\nfrom wordcloud import WordCloud\n\n# Carregar dados do CSV\ndata_path  \"survey_data.csv\"  # caminho ajustado para o seu arquivo CSV\ndata  pd.read_csv(data_path)\n\n\ndef top_bibliotecas_por_area(data):\n    st.header(\"Top 3 Bibliotecas por Área de Atuação\")\n    areas  data[\"Área de Atuação\"].unique()\n    area_selecionada  st.selectbox(\n        \"Selecione a Área de Atuação\",\n        [\"Nenhuma área selecionada\"] + list(areas),\n    )\n\n    if area_selecionada ! \"Nenhuma área selecionada\":\n        st.subheader(area_selecionada)\n        bibliotecas_area  (\n            data[data[\"Área de Atuação\"]  area_selecionada][\"Bibliotecas\"]\n            .str.cat(sep\",\")\n            .split(\",\")\n        )\n        bibliotecas_contagem  Counter(\n            [biblioteca.strip() for biblioteca in bibliotecas_area]\n        )\n        top_3_bibliotecas  bibliotecas_contagem.most_common(3)\n\n        col1, col2, col3  st.columns(3)\n        colunas  [col1, col2, col3]\n        for i, (biblioteca, count) in enumerate(top_3_bibliotecas):\n            with colunas[i]:\n                st.metric(labelbiblioteca, valuef\"{count} vezes\")\n\n\n# Função para plotar gráfico de área\ndef plotar_grafico_area(data):\n    comfort_order  [\n        \"Muito Desconfortável\",\n        \"Desconfortável\",\n        \"Neutro\",\n        \"Confortável\",\n        \"Muito Confortável\",\n    ]\n    data[\"Conforto com Dados\"]  pd.Categorical(\n        data[\"Conforto com Dados\"], categoriescomfort_order, orderedTrue\n    )\n    comfort_vs_study_hours  (\n        data.groupby([\"Conforto com Dados\", \"Horas de Estudo\"], observedTrue)\n        .size()\n        .unstack(fill_value0)\n    )\n    comfort_vs_study_hours  comfort_vs_study_hours.reindex(\n        columns[\"Menos de 5\", \"5-10\", \"10-20\", \"Mais de 20\"], fill_value0\n    )\n    colors  [\n        \"#00008B\",\n        \"#87CEEB\",\n        \"#FF6347\",\n        \"#FF0000\",\n    ]  # Dark Blue, Light Blue, Light Red, Red\n    st.header(\"Nível de Conforto com Dados vs. Horas de Estudo por Semana\")\n    st.area_chart(comfort_vs_study_hours, colorcolors)\n\n\n# Função para plotar gráficos de experiência técnica\ndef plotar_graficos_experiencia(data):\n    st.header(\"Experiência Técnica dos Participantes\")\n    col1, col2, col3  st.columns(3)\n\n    with col1:\n        st.subheader(\"Experiência de Python\")\n        experiencia_python_count  (\n            data[\"Experiência de Python\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_python_count)\n\n    with col2:\n        st.subheader(\"Experiência de SQL\")\n        experiencia_sql_count  (\n            data[\"Experiência de SQL\"].value_counts().sort_index()\n        )\n        st.bar_chart(experiencia_sql_count)\n\n    with col3:\n        st.subheader(\"Experiência em Cloud\")\n        experiencia_cloud_count  (\n            data[\"Experiência em Cloud\"].value_counts().sort_index()\n        )\n        st.area_chart(experiencia_cloud_count)\n\n\n# Função para plotar o mapa do Brasil\ndef plotar_mapa(data):\n    st.header(\"Mapa do Brasil com Distribuição dos Participantes\")\n    state_coords  {\n        \"São Paulo\": [-23.5505, -46.6333],\n        \"Rio de Janeiro\": [-22.9068, -43.1729],\n        \"Minas Gerais\": [-19.9167, -43.9345],\n        \"Bahia\": [-12.9714, -38.5014],\n        \"Paraná\": [-25.4284, -49.2733],\n        \"Rio Grande do Sul\": [-30.0346, -51.2177],\n        \"Santa Catarina\": [-27.5954, -48.5480],\n        \"Ceará\": [-3.7172, -38.5434],\n        \"Distrito Federal\": [-15.8267, -47.9218],\n        \"Pernambuco\": [-8.0476, -34.8770],\n        \"Goiás\": [-16.6869, -49.2648],\n        \"Pará\": [-1.4558, -48.4902],\n        \"Mato Grosso\": [-15.6014, -56.0979],\n        \"Amazonas\": [-3.1190, -60.0217],\n        \"Espírito Santo\": [-20.3155, -40.3128],\n        \"Paraíba\": [-7.1195, -34.8450],\n        \"Acre\": [-9.97499, -67.8243],\n        # Adicione outras coordenadas dos estados aqui\n    }\n    state_participants  Counter(data[\"Estado\"])\n    map_data  pd.DataFrame(\n        [\n            {\n                \"Estado\": state,\n                \"lat\": coord[0],\n                \"lon\": coord[1],\n                \"Participantes\": state_participants[state],\n            }\n            for state, coord in state_coords.items()\n        ]\n    )\n    m  folium.Map(location[-15.7801, -47.9292], zoom_start4)\n    for _, row in map_data.iterrows():\n        folium.CircleMarker(\n            location[row[\"lat\"], row[\"lon\"]],\n            radiusrow[\"Participantes\"]\n            * 3,  # Ajustar o raio proporcionalmente\n            popupf\"{row['Estado']}: {row['Participantes']} participantes\",\n            color\"crimson\",\n            fillTrue,\n            fill_color\"crimson\",\n            weight1,  # Ajustar a espessura do contorno do círculo\n        ).add_to(m)\n    folium_static(m)\n\n\n# Função para plotar nuvem de palavras\ndef plotar_nuvem_palavras(data):\n    st.header(\"Nuvem de Palavras das Bibliotecas Utilizadas\")\n    all_libs  \" \".join(data[\"Bibliotecas\"].dropna().str.replace(\",\", \" \"))\n    wordcloud  WordCloud(\n        width800, height400, background_color\"white\"\n    ).generate(all_libs)\n    fig, ax  plt.subplots(figsize(10, 5))\n    print(ax)\n    ax.imshow(wordcloud)\n    ax.axis(\"off\")  # remove o quadro em volta\n    st.pyplot(fig)  # gera a imagem no streamlit\n\n\n# Função para exibir uma imagem no final\ndef exibir_imagem_final(image_path):\n    st.header(\"Foto da Jornada\")\n    st.image(image_path, use_column_widthTrue)\n\n\n# Chamar as funções\nst.title(\"Dados dos Participantes\")\ntop_bibliotecas_por_area(data)\nplotar_grafico_area(data)\nplotar_graficos_experiencia(data)\nplotar_mapa(data)\nplotar_nuvem_palavras(data)\nexibir_imagem_final(\"foto.png\")\n\n\n09-streamlit-dashboard-realtime/survey/dash_postgres.py\n\nimport os\nfrom collections import Counter\n\nimport folium\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom sqlalchemy import create_engine\nfrom streamlit_folium import folium_static\nfrom wordcloud import WordCloud\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n\n# Função para conectar ao banco de dados PostgreSQL usando SQLAlchemy\ndef conectar_banco():\n    try:\n        engine  create_engine(\n            f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_DATABASE')}\"\n        )\n        return engine\n    except Exception as e:\n        st.error(f\"Erro ao conectar ao banco de dados: {e}\")\n        return None\n\n\n# Função para carregar dados do banco de dados\ndef carregar_dados(engine):\n    if engine:\n        try:\n            query  \"\"\"\n            SELECT\n                estado AS \"Estado\",\n                bibliotecas AS \"Bibliotecas\",\n                area_atuacao AS \"Área de Atuação\",\n                horas_estudo AS \"Horas de Estudo\",\n                conforto_dados AS \"Conforto com Dados\",\n                experiencia_python AS \"Experiência de Python\",\n                experiencia_sql AS \"Experiência de SQL\",\n                experiencia_cloud AS \"Experiência em Cloud\"\n            FROM\n                survey_data\n            \"\"\"\n            data  pd.read_sql(query, engine)\n            return data\n        except Exception as e:\n            st.error(f\"Erro ao carregar os dados do banco de dados: {e}\")\n            return pd.DataFrame()\n    return pd.DataFrame()\n\n\n# Função para exibir os dados\ndef exibir_dados(data):\n    st.header(\"Dados dos Participantes\")\n    st.dataframe(data)\n\n\n# Função para plotar gráfico de área\ndef plotar_grafico_area(data):\n    comfort_order  [\n        \"Muito Desconfortável\",\n        \"Desconfortável\",\n        \"Neutro\",\n        \"Confortável\",\n        \"Muito Confortável\",\n    ]\n    data[\"Conforto com Dados\"]  pd.Categorical(\n        data[\"Conforto com Dados\"], categoriescomfort_order, orderedTrue\n    )\n    comfort_vs_study_hours  (\n        data.groupby([\"Conforto com Dados\", \"Horas de Estudo\"], observedTrue)\n        .size()\n        .unstack(fill_value0)\n    )\n    comfort_vs_study_hours  comfort_vs_study_hours.reindex(\n        columns[\"Menos de 5\", \"5-10\", \"10-20\", \"Mais de 20\"], fill_value0\n    )\n    colors  [\n        \"#00008B\",\n        \"#87CEEB\",\n        \"#FF6347\",\n        \"#FF0000\",\n    ]  # Dark Blue, Light Blue, Light Red, Red\n    st.header(\"Nível de Conforto com Dados vs. Horas de Estudo por Semana\")\n    st.area_chart(comfort_vs_study_hours, colorcolors)\n\n\n# Função para plotar gráficos de experiência técnica\ndef plotar_graficos_experiencia(data):\n    st.header(\"Experiência Técnica dos Participantes\")\n    col1, col2, col3  st.columns(3)\n\n    with col1:\n        st.subheader(\"Experiência de Python\")\n        experiencia_python_count  (\n            data[\"Experiência de Python\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_python_count)\n\n    with col2:\n        st.subheader(\"Experiência de SQL\")\n        experiencia_sql_count  (\n            data[\"Experiência de SQL\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_sql_count)\n\n    with col3:\n        st.subheader(\"Experiência em Cloud\")\n        experiencia_cloud_count  (\n            data[\"Experiência em Cloud\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_cloud_count)\n\n\n# Função para plotar o mapa do Brasil\ndef plotar_mapa(data):\n    st.header(\"Mapa do Brasil com Distribuição dos Participantes\")\n    state_coords  {\n        \"São Paulo\": [-23.5505, -46.6333],\n        \"Rio de Janeiro\": [-22.9068, -43.1729],\n        \"Minas Gerais\": [-19.9167, -43.9345],\n        \"Bahia\": [-12.9714, -38.5014],\n        \"Paraná\": [-25.4284, -49.2733],\n        \"Rio Grande do Sul\": [-30.0346, -51.2177],\n        \"Santa Catarina\": [-27.5954, -48.5480],\n        \"Ceará\": [-3.7172, -38.5434],\n        \"Distrito Federal\": [-15.8267, -47.9218],\n        \"Pernambuco\": [-8.0476, -34.8770],\n        \"Goiás\": [-16.6869, -49.2648],\n        \"Pará\": [-1.4558, -48.4902],\n        \"Mato Grosso\": [-15.6014, -56.0979],\n        \"Amazonas\": [-3.1190, -60.0217],\n        \"Espírito Santo\": [-20.3155, -40.3128],\n        \"Paraíba\": [-7.1195, -34.8450],\n        \"Acre\": [-9.97499, -67.8243],\n        # Adicione outras coordenadas dos estados aqui\n    }\n    state_participants  Counter(data[\"Estado\"])\n    map_data  pd.DataFrame(\n        [\n            {\n                \"Estado\": state,\n                \"lat\": coord[0],\n                \"lon\": coord[1],\n                \"Participantes\": state_participants[state],\n            }\n            for state, coord in state_coords.items()\n        ]\n    )\n    m  folium.Map(location[-15.7801, -47.9292], zoom_start4)\n    for _, row in map_data.iterrows():\n        folium.CircleMarker(\n            location[row[\"lat\"], row[\"lon\"]],\n            radiusrow[\"Participantes\"]\n            * 3,  # Ajustar o raio proporcionalmente\n            popupf\"{row['Estado']}: {row['Participantes']} participantes\",\n            color\"crimson\",\n            fillTrue,\n            fill_color\"crimson\",\n            weight1,  # Ajustar a espessura do contorno do círculo\n        ).add_to(m)\n    folium_static(m)\n\n\n# Função para plotar nuvem de palavras\ndef plotar_nuvem_palavras(data):\n    st.header(\"Nuvem de Palavras das Bibliotecas Utilizadas\")\n    all_libs  \" \".join(data[\"Bibliotecas\"].dropna().str.replace(\",\", \" \"))\n    wordcloud  WordCloud(\n        width800, height400, background_color\"white\"\n    ).generate(all_libs)\n    fig, ax  plt.subplots(figsize(10, 5))\n    ax.imshow(wordcloud, interpolation\"bilinear\")\n    ax.axis(\"off\")  # remove o quadro em volta\n    st.pyplot(fig)  # gera a imagem no streamlit\n\n\n# Função para exibir o top 3 bibliotecas por área de atuação\ndef top_bibliotecas_por_area(data):\n    st.header(\"Top 3 Bibliotecas por Área de Atuação\")\n    areas  data[\"Área de Atuação\"].unique()\n    area_selecionada  st.selectbox(\n        \"Selecione a Área de Atuação\",\n        [\"Nenhuma área selecionada\"] + list(areas),\n    )\n\n    if area_selecionada ! \"Nenhuma área selecionada\":\n        st.subheader(area_selecionada)\n        bibliotecas_area  (\n            data[data[\"Área de Atuação\"]  area_selecionada][\"Bibliotecas\"]\n            .str.cat(sep\",\")\n            .split(\",\")\n        )\n        bibliotecas_contagem  Counter(bibliotecas_area)\n        top_3_bibliotecas  bibliotecas_contagem.most_common(3)\n        col1, col2, col3  st.columns(3)\n        colunas  [col1, col2, col3]\n        for col, (biblioteca, count) in zip(colunas, top_3_bibliotecas):\n            with col:\n                st.metric(labelbiblioteca.strip(), valuef\"{count} vezes\")\n\n\n# Função para exibir uma imagem no final\ndef exibir_imagem_final(image_path):\n    st.header(\"Foto da Jornada\")\n    st.image(image_path, use_column_widthTrue)\n\n\n# Carregar os dados do banco de dados\nengine  conectar_banco()\ndata  carregar_dados(engine)\n\n# Chamar as funções para exibir os dados e gráficos\nst.title(\"Geração de Gráficos da Enquete\")\nexibir_dados(data)\nplotar_grafico_area(data)\nplotar_graficos_experiencia(data)\nplotar_mapa(data)\nplotar_nuvem_palavras(data)\ntop_bibliotecas_por_area(data)\nexibir_imagem_final(\"foto.png\")\n\n\n09-streamlit-dashboard-realtime/survey/dash_postgres_cache.py\n\nimport os\nfrom collections import Counter\n\nimport folium\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom sqlalchemy import create_engine\nfrom streamlit_folium import folium_static\nfrom wordcloud import WordCloud\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n\n# Função para conectar ao banco de dados PostgreSQL usando SQLAlchemy\n@st.cache_resource\ndef conectar_banco():\n    try:\n        engine  create_engine(\n            f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_DATABASE')}\"\n        )\n        return engine\n    except Exception as e:\n        st.error(f\"Erro ao conectar ao banco de dados: {e}\")\n        return None\n\n\n# Função para carregar dados do banco de dados\n@st.cache_data(ttl300)\ndef carregar_dados(_engine):\n    if _engine:\n        try:\n            query  \"\"\"\n            SELECT\n                estado AS \"Estado\",\n                bibliotecas AS \"Bibliotecas\",\n                area_atuacao AS \"Área de Atuação\",\n                horas_estudo AS \"Horas de Estudo\",\n                conforto_dados AS \"Conforto com Dados\",\n                experiencia_python AS \"Experiência de Python\",\n                experiencia_sql AS \"Experiência de SQL\",\n                experiencia_cloud AS \"Experiência em Cloud\"\n            FROM\n                survey_data\n            \"\"\"\n            data  pd.read_sql(query, _engine)\n            return data\n        except Exception as e:\n            st.error(f\"Erro ao carregar os dados do banco de dados: {e}\")\n            return pd.DataFrame()\n    return pd.DataFrame()\n\n\n# Função para exibir os dados\ndef exibir_dados(data):\n    st.header(\"Dados dos Participantes\")\n    st.dataframe(data)\n\n\n# Função para plotar gráfico de área\ndef plotar_grafico_area(data):\n    comfort_order  [\n        \"Muito Desconfortável\",\n        \"Desconfortável\",\n        \"Neutro\",\n        \"Confortável\",\n        \"Muito Confortável\",\n    ]\n    data[\"Conforto com Dados\"]  pd.Categorical(\n        data[\"Conforto com Dados\"], categoriescomfort_order, orderedTrue\n    )\n    comfort_vs_study_hours  (\n        data.groupby([\"Conforto com Dados\", \"Horas de Estudo\"], observedTrue)\n        .size()\n        .unstack(fill_value0)\n    )\n    comfort_vs_study_hours  comfort_vs_study_hours.reindex(\n        columns[\"Menos de 5\", \"5-10\", \"10-20\", \"Mais de 20\"], fill_value0\n    )\n    colors  [\n        \"#00008B\",\n        \"#87CEEB\",\n        \"#FF6347\",\n        \"#FF0000\",\n    ]  # Dark Blue, Light Blue, Light Red, Red\n    st.header(\"Nível de Conforto com Dados vs. Horas de Estudo por Semana\")\n    st.area_chart(comfort_vs_study_hours, colorcolors)\n\n\n# Função para plotar gráficos de experiência técnica\ndef plotar_graficos_experiencia(data):\n    st.header(\"Experiência Técnica dos Participantes\")\n    col1, col2, col3  st.columns(3)\n\n    with col1:\n        st.subheader(\"Experiência de Python\")\n        experiencia_python_count  (\n            data[\"Experiência de Python\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_python_count)\n\n    with col2:\n        st.subheader(\"Experiência de SQL\")\n        experiencia_sql_count  (\n            data[\"Experiência de SQL\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_sql_count)\n\n    with col3:\n        st.subheader(\"Experiência em Cloud\")\n        experiencia_cloud_count  (\n            data[\"Experiência em Cloud\"].value_counts().sort_index()\n        )\n        st.line_chart(experiencia_cloud_count)\n\n\n# Função para plotar o mapa do Brasil\ndef plotar_mapa(data):\n    st.header(\"Mapa do Brasil com Distribuição dos Participantes\")\n    state_coords  {\n        \"Acre\": [-9.97499, -67.8243],\n        \"Alagoas\": [-9.5713, -36.7820],\n        \"Amapá\": [0.0370, -51.0504],\n        \"Amazonas\": [-3.1190, -60.0217],\n        \"Bahia\": [-12.9714, -38.5014],\n        \"Ceará\": [-3.7172, -38.5434],\n        \"Distrito Federal\": [-15.8267, -47.9218],\n        \"Espírito Santo\": [-20.3155, -40.3128],\n        \"Goiás\": [-16.6869, -49.2648],\n        \"Maranhão\": [-2.5307, -44.3068],\n        \"Mato Grosso\": [-15.6014, -56.0979],\n        \"Mato Grosso do Sul\": [-20.4697, -54.6201],\n        \"Minas Gerais\": [-19.9167, -43.9345],\n        \"Pará\": [-1.4558, -48.4902],\n        \"Paraíba\": [-7.1195, -34.8450],\n        \"Paraná\": [-25.4284, -49.2733],\n        \"Pernambuco\": [-8.0476, -34.8770],\n        \"Piauí\": [-5.0892, -42.8016],\n        \"Rio de Janeiro\": [-22.9068, -43.1729],\n        \"Rio Grande do Norte\": [-5.7945, -35.2110],\n        \"Rio Grande do Sul\": [-30.0346, -51.2177],\n        \"Rondônia\": [-8.7612, -63.9039],\n        \"Roraima\": [2.8235, -60.6758],\n        \"Santa Catarina\": [-27.5954, -48.5480],\n        \"São Paulo\": [-23.5505, -46.6333],\n        \"Sergipe\": [-10.9472, -37.0731],\n        \"Tocantins\": [-10.1844, -48.3336],\n    }\n    state_participants  Counter(data[\"Estado\"])\n    map_data  pd.DataFrame(\n        [\n            {\n                \"Estado\": state,\n                \"lat\": coord[0],\n                \"lon\": coord[1],\n                \"Participantes\": state_participants[state],\n            }\n            for state, coord in state_coords.items()\n            if state_participants[state]\n            > 0  # Filtrar apenas estados com participantes\n        ]\n    )\n    m  folium.Map(location[-15.7801, -47.9292], zoom_start4)\n    for _, row in map_data.iterrows():\n        folium.CircleMarker(\n            location[row[\"lat\"], row[\"lon\"]],\n            radiusrow[\"Participantes\"]\n            * 10,  # Ajustar o raio proporcionalmente\n            popupf\"{row['Estado']}: {row['Participantes']} participantes\",\n            color\"crimson\",\n            fillTrue,\n            fill_color\"crimson\",\n            weight1,  # Ajustar a espessura do contorno do círculo\n        ).add_to(m)\n    folium_static(m)\n\n\n# Função para plotar nuvem de palavras\ndef plotar_nuvem_palavras(data):\n    st.header(\"Nuvem de Palavras das Bibliotecas Utilizadas\")\n    all_libs  \" \".join(data[\"Bibliotecas\"].dropna().str.replace(\",\", \" \"))\n    wordcloud  WordCloud(\n        width800, height400, background_color\"white\"\n    ).generate(all_libs)\n    fig, ax  plt.subplots(figsize(10, 5))\n    ax.imshow(wordcloud, interpolation\"bilinear\")\n    ax.axis(\"off\")  # remove o quadro em volta\n    st.pyplot(fig)  # gera a imagem no streamlit\n    plt.close(fig)\n\n\ndef top_bibliotecas_por_area(data):\n    st.header(\"Top 3 Bibliotecas por Área de Atuação\")\n    areas  data[\"Área de Atuação\"].unique()\n    area_selecionada  st.selectbox(\n        \"Selecione a Área de Atuação\",\n        [\"Nenhuma área selecionada\"] + list(areas),\n    )\n\n    if area_selecionada ! \"Nenhuma área selecionada\":\n        st.subheader(area_selecionada)\n        bibliotecas_area  (\n            data[data[\"Área de Atuação\"]  area_selecionada][\"Bibliotecas\"]\n            .str.cat(sep\",\")\n            .split(\",\")\n        )\n        bibliotecas_contagem  Counter(bibliotecas_area)\n        top_3_bibliotecas  bibliotecas_contagem.most_common(3)\n        col1, col2, col3  st.columns(3)\n        colunas  [col1, col2, col3]\n        for col, (biblioteca, count) in zip(colunas, top_3_bibliotecas):\n            with col:\n                st.metric(labelbiblioteca.strip(), valuef\"{count} vezes\")\n\n\n# Função para exibir uma imagem no final\ndef exibir_imagem_final(image_path):\n    st.header(\"Foto da Jornada\")\n    st.image(image_path, use_column_widthTrue)\n\n\n# Carregar os dados do banco de dados\nengine  conectar_banco()\ndata  carregar_dados(engine)\n\n# Chamar as funções para exibir os dados e gráficos\nst.title(\"Geração de Gráficos da Enquete\")\nexibir_dados(data)\nplotar_grafico_area(data)\nplotar_graficos_experiencia(data)\nplotar_mapa(data)\nplotar_nuvem_palavras(data)\ntop_bibliotecas_por_area(data)\nexibir_imagem_final(\"foto.png\")\n\n\n09-streamlit-dashboard-realtime/survey/exemplo.env\n\n# .env\nDB_HOSTseu_host\nDB_DATABASEseu_banco\nDB_USERseu_usuario\nDB_PASSWORDsua_senha\n\n"
    ],
    "08-kafka-pubsub-streaming": [
        "08-kafka-pubsub-streaming/README.md\n\n# kafka-workshop\n\nBem-vindo ao projeto de monitoramento de transações financeiras em tempo real com Apache Kafka! Este projeto faz parte de um workshop prático exclusivo para os alunos da **Escola Jornada de Dados**.\n\n[Excalidraw](https://link.excalidraw.com/l/8pvW6zbNUnD/1FWXp1AaTy3)\n\n## Sobre o Workshop\n\n* **Data:** Sábado, 13 de julho\n* **Horário:** 9:00 da manhã\n* **Duração:** 4 horas\n* **Público:** Exclusivo para alunos da Jornada de Dados\n\n### Overview Kafka\n\nKafka-demo\n\nKafka-Topics-Partitions\n\nKafka-Brokers-Replications\n\nKafka-partitions\n\nKafka-producers\n\nKafka-connect\n\nSchema-Registry\n\n\n### Descrição do Workshop\n\n## Escopo do Projeto: Simulação de 50 Refrigeradores Espalhados pelo Brasil\n\nClaro! Aqui está o README atualizado com o comando para criar o tópico `marketing-project`.\n\n## Projeto: Monitoramento de Refrigeração no Brasil\n\n### Objetivo\n\nEste projeto tem como objetivo simular 50 refrigeradores espalhados pelo Brasil. Cada refrigerador irá reportar a temperatura a cada segundo. Queremos entender como os refrigeradores estão operando em diferentes regiões e monitorar temperaturas anômalas que podem indicar falhas ou problemas.\n\n### Componentes do Projeto\n\n1. **Producers**:\n    - 50 produtores simulando refrigeradores.\n    - Cada produtor envia dados de temperatura a cada segundo.\n    - As temperaturas são geradas com base em dois intervalos: \n        - 45 refrigeradores com temperaturas entre 0°C e 5°C.\n        - 5 refrigeradores com temperaturas entre 20°C e 40°C.\n    - Cada produtor tem um ID único gerado com UUID.\n\n2. **Consumer**:\n    - Um consumidor que lê os dados em tempo real e exibe as temperaturas usando Streamlit.\n\n3. **Kafka Topic**:\n    - Tópico chamado `marketing-project`.\n\n### Passos para Configuração\n\n#### 1. Configurar o Confluent CLI\n\nCertifique-se de que você tenha configurado o Confluent CLI e esteja autenticado:\n\n```bash\nconfluent login\n```\n\n#### 2. Selecionar o Ambiente e o Cluster\n\n```bash\nconfluent environment use <environment_id>\nconfluent kafka cluster use <cluster_id>\n```\n\n#### 3. Criar o Tópico `marketing-project`\n\n```bash\nconfluent kafka topic create marketing-project --partitions 6\n```\n\n### Objetivo\n\nO objetivo deste projeto é simular 50 refrigeradores espalhados pelo Brasil, gerando dados de temperatura em tempo real. Essa simulação visa monitorar e analisar as variações de temperatura, permitindo uma melhor compreensão do comportamento térmico dos refrigeradores em diferentes regiões e condições climáticas. Com este projeto, pretendemos:\n\n1. **Monitorar Temperaturas em Tempo Real**: Capturar e exibir dados de temperatura de 50 refrigeradores espalhados pelo Brasil, atualizados a cada segundo.\n2. **Analisar Distribuição de Temperaturas**: Identificar padrões de temperatura e comportamentos anômalos entre os refrigeradores.\n3. **Avaliar Eficiência Térmica**: Verificar a eficiência térmica dos refrigeradores em manter a temperatura dentro de uma faixa desejada.\n4. **Identificar Problemas Potenciais**: Detectar refrigeradores que possam estar operando fora da faixa de temperatura esperada, sinalizando possíveis falhas ou necessidade de manutenção.\n\n### Descrição do Projeto\n\n1. **Simulação dos Refrigeradores**:\n    - **Quantidade**: 50 refrigeradores.\n    - **Localização**: Cada refrigerador terá uma latitude e longitude fixas, representando diferentes regiões do Brasil.\n    - **Distribuição de Temperatura**:\n        - 45 refrigeradores manterão temperaturas entre 0 e 5 graus Celsius (95%).\n        - 5 refrigeradores terão temperaturas entre 20 e 40 graus Celsius (5%), representando possíveis falhas ou condições extremas.\n\n2. **Produção de Dados**:\n    - **Intervalo de Produção**: Cada refrigerador gerará uma leitura de temperatura a cada segundo.\n    - **Formato dos Dados**: Os dados serão enviados em formato JSON, contendo a latitude, longitude, temperatura e identificador do refrigerador.\n\n3. **Tópico Kafka**:\n    - **Nome do Tópico**: `marketing-project`.\n    - **Produção e Consumo**: Os dados gerados pelos 50 refrigeradores serão publicados neste tópico Kafka.\n\n4. **Consumidor de Dados**:\n    - **Visualização em Tempo Real**: Um consumidor será implementado usando Streamlit para exibir as últimas leituras de temperatura de cada refrigerador em tempo real.\n    - **Monitoramento e Análise**: A aplicação Streamlit permitirá monitorar os dados e identificar rapidamente qualquer comportamento anômalo.\n\n### Componentes do Projeto\n\n1. **Producers**:\n    - 50 instâncias de produtores, cada uma simulando um refrigerador.\n    - Utilização da biblioteca `Faker` para gerar dados de localização e temperatura.\n    - Publicação dos dados no tópico `marketing-project` no Kafka.\n\n2. **Consumer**:\n    - Uma aplicação Streamlit que consome os dados do tópico `marketing-project`.\n    - Exibição dos dados em um dashboard, mostrando a última leitura de temperatura de cada refrigerador.\n\n### Ferramentas e Tecnologias\n\n1. **Apache Kafka**: Para transmissão e ingestão de dados em tempo real.\n2. **Confluent Cloud**: Plataforma gerenciada de Kafka para simplificar a configuração e o gerenciamento do cluster.\n3. **Faker**: Biblioteca para geração de dados fictícios.\n4. **Docker**: Para containerização dos produtores e consumidor.\n5. **Streamlit**: Para visualização dos dados em tempo real.\n\n### Implementação\n\n1. **Configuração do Tópico Kafka**:\n    - Criar o tópico `marketing-project` no Confluent Cloud.\n\n2. **Desenvolvimento dos Producers**:\n    - Implementar o script de produção utilizando `Faker` para gerar os dados de temperatura e localização.\n    - Containerizar os produtores usando Docker.\n\n3. **Desenvolvimento do Consumer**:\n    - Implementar a aplicação Streamlit para consumir e exibir os dados em tempo real.\n    - Containerizar o consumidor usando Docker.\n\n4. **Execução**:\n    - Iniciar os 50 produtores e o consumidor utilizando `docker-compose`.\n\n## Executar os Containers\nCertifique-se de que o arquivo docker-compose.yml está configurado corretamente e execute os seguintes comandos no terminal para construir a imagem Docker e iniciar os containers:\n\n```bash\ndocker-compose build\ndocker-compose up -d\n```\n\n### Resultado Esperado\n\nAo final do projeto, espera-se ter uma simulação funcional de 50 refrigeradores espalhados pelo Brasil, com dados de temperatura sendo gerados e exibidos em tempo real. A aplicação Streamlit fornecerá uma interface intuitiva para monitorar e analisar esses dados, possibilitando a identificação de padrões e anomalias de temperatura, além de fornecer insights sobre a eficiência térmica dos refrigeradores em diferentes condições climáticas.\n\n### Bizu para apagar\n\nconfluent kafka topic update marketing-project --config retention.ms1000\n\nconfluent kafka topic delete marketing-project\n\n\n###\n\n```bash\nconfluent kafka topic consume marketing-project --from-beginning\n```\n\n\n\n08-kafka-pubsub-streaming/docker-compose.yml\n\n---\nversion: \"3.9\"\n\nservices:\n  zookeeper:\n    platform: linux/amd64\n    image: confluentinc/cp-zookeeper:${CONFLUENT_PLATFORM_VERSION:-7.4.0}\n    container_name: zookeeper\n    restart: unless-stopped\n    ports:\n      - '32181:32181'\n      - '2888:2888'\n      - '3888:3888'\n    environment:\n      ZOOKEEPER_SERVER_ID: 1\n      ZOOKEEPER_CLIENT_PORT: 32181\n      ZOOKEEPER_TICK_TIME: 2000\n      ZOOKEEPER_INIT_LIMIT: 5\n      ZOOKEEPER_SYNC_LIMIT: 2\n      ZOOKEEPER_SERVERS: zookeeper:2888:3888\n    healthcheck:\n      test: echo stat | nc localhost 32181\n      interval: 10s\n      timeout: 10s\n      retries: 3\n    networks:\n      - kafka\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"1m\"\n\n  kafka-ui: # https://github.com/provectus/kafka-ui/blob/master/documentation/compose/kafka-ui.yaml\n    container_name: kafka-ui \n    image: provectuslabs/kafka-ui:latest\n    ports:\n      - 8087:8080\n    depends_on:\n      - broker-1\n      - broker-2\n      - broker-3\n    environment:\n      KAFKA_CLUSTERS_0_NAME: broker-1\n      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker-1:29091\n      KAFKA_CLUSTERS_0_METRICS_PORT: 19101\n      #KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schemaregistry0:8085\n      #KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: first\n      #KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect0:8083\n      KAFKA_CLUSTERS_1_NAME: broker-2\n      KAFKA_CLUSTERS_1_BOOTSTRAPSERVERS: broker-2:29092\n      KAFKA_CLUSTERS_1_METRICS_PORT: 19102\n      #KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schemaregistry0:8085\n      KAFKA_CLUSTERS_2_NAME: broker-3\n      KAFKA_CLUSTERS_2_BOOTSTRAPSERVERS: broker-3:29093\n      KAFKA_CLUSTERS_2_METRICS_PORT: 19103\n      #KAFKA_CLUSTERS_1_SCHEMAREGISTRY: http://schemaregistry1:8085\n      DYNAMIC_CONFIG_ENABLED: 'true'\n    networks:\n      - kafka\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"1m\"\n\n  broker-1:\n    platform: linux/amd64\n    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}\n    container_name: broker-1\n    restart: unless-stopped\n    ports:\n      - '9091:9091'\n    depends_on:\n      - zookeeper\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\n      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-1:29091,EXTERNAL://localhost:9091\n      KAFKA_DEFAULT_REPLICATION_FACTOR: 3\n      KAFKA_NUM_PARTITIONS: 3\n      #KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      #KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n      #KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n      #KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_JMX_PORT: 19101\n      KAFKA_JMX_HOSTNAME: localhost\n    healthcheck:\n      test: nc -vz localhost 9091\n      interval: 10s\n      timeout: 10s\n      retries: 3\n    networks:\n      - kafka\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"1m\"\n\n  broker-2:\n    platform: linux/amd64\n    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}\n    container_name: broker-2\n    restart: unless-stopped\n    ports:\n      - '9092:9092'\n    depends_on:\n      - zookeeper\n    environment:\n      KAFKA_BROKER_ID: 2\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\n      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-2:29092,EXTERNAL://localhost:9092\n      KAFKA_DEFAULT_REPLICATION_FACTOR: 3\n      KAFKA_NUM_PARTITIONS: 3\n      #KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      #KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n      #KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n      #KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_JMX_PORT: 19102\n      KAFKA_JMX_HOSTNAME: localhost\n    healthcheck:\n      test: nc -vz localhost 9092\n      interval: 10s\n      timeout: 10s\n      retries: 3\n    networks:\n      - kafka\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"1m\"\n\n  broker-3:\n    platform: linux/amd64\n    image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}\n    container_name: broker-3\n    restart: unless-stopped\n    ports:\n      - '9093:9093'\n    depends_on:\n      - zookeeper\n    environment:\n      KAFKA_BROKER_ID: 3\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\n      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-3:29093,EXTERNAL://localhost:9093\n      KAFKA_DEFAULT_REPLICATION_FACTOR: 3\n      KAFKA_NUM_PARTITIONS: 3\n      #KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      #KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n      #KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n      #KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_JMX_PORT: 19103\n      KAFKA_JMX_HOSTNAME: localhost\n    healthcheck:\n      test: nc -vz localhost 9093\n      interval: 10s\n      timeout: 10s\n      retries: 3\n    networks:\n      - kafka\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"1m\"\n  \n  # kafka-shell:\n  #   image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.4.0}\n  #   container_name: kafka-shell\n  #   restart: unless-stopped\n  #   depends_on:\n  #     - zookeeper\n  #     - broker-1\n  #     - broker-2\n  #     - broker-3\n  #   entrypoint: \"\"\n  #   command: \"sleep 10000000\"\n  #   networks:\n  #     - kafka\n\n  producer:\n    platform: linux/amd64\n    container_name: producer\n    build: ./python-client/\n    restart: always\n    environment:\n      - ACTIONproducer\n      - BOOTSTRAP_SERVERSbroker-1:29091,broker-2:29092,broker-3:29093\n      - TOPICmy-topic\n      - PYTHONUNBUFFERED1 # https://github.com/docker/compose/issues/4837#issuecomment-302765592\n    networks:\n      - kafka\n    depends_on:\n      - zookeeper\n      - broker-1\n      - broker-2\n      - broker-3\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"1m\"\n\n  consumer:\n    platform: linux/amd64\n    container_name: consumer\n    build: ./python-client/\n    restart: always\n    environment:\n      - ACTIONconsumer\n      - BOOTSTRAP_SERVERSbroker-1:29091,broker-2:29092,broker-3:29093\n      - TOPICmy-topic\n      - CONSUMER_GROUPcg-group-id\n      - PYTHONUNBUFFERED1 # https://github.com/docker/compose/issues/4837#issuecomment-302765592\n    networks:\n      - kafka\n    depends_on:\n      - zookeeper\n      - broker-1\n      - broker-2\n      - broker-3\n      - producer\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"1m\"\n\n  # shell:\n  #   container_name: shell\n  #   build: ./python-client/\n  #   restart: always\n  #   environment:\n  #     - ACTIONshell\n  #     - BOOTSTRAP_SERVERSbroker-1:29091,broker-2:29092,broker-3:29093\n  #     - TOPICmy-topic\n  #   networks:\n  #     - kafka\n  #   logging:\n  #     driver: \"json-file\"\n  #     options:\n  #       max-size: \"1m\"\n\nnetworks:\n  kafka:\n    name: kafka\n\n08-kafka-pubsub-streaming/pyproject.toml\n\n[tool.poetry]\nname  \"kafka-workshop\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"lvgalvaofilho <lvgalvaofilho@gmail.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"3.12.1\"\nkafka-python  \"^2.0.2\"\nsix  \"^1.16.0\"\nconfluent-kafka  \"^2.5.0\"\npython-dotenv  \"^1.0.1\"\n\n\n[tool.poetry.group.dev.dependencies]\nruff  \"^0.5.1\"\ntaskipy  \"^1.13.0\"\n\n[tool.taskipy.tasks]\n\nlint  'ruff check . && ruff check . --diff'\nformat  'ruff check . --fix && ruff format . '\n\n[tool.ruff]\nline-length  79\n\n[tool.ruff.lint]\npreview  true\nselect  ['I', 'F', 'E', 'W', 'PL', 'PT']\nignore  ['E402', 'F811', 'E501']\n\n[tool.ruff.format]\npreview  true\nquote-style  'single'\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\n08-kafka-pubsub-streaming/.python-version\n\n3.12.1\n\n\n",
        "08-kafka-pubsub-streaming/kafka-brokers/README.md\n\n## Resumo: Kafka Brokers e Replicação\n\n### Kafka Brokers\n\n**Conceito Básico:**\n- Kafka é um sistema distribuído composto por várias máquinas chamadas brokers.\n- Cada broker hospeda um conjunto de partições do Kafka e lida com solicitações de leitura e escrita de eventos.\n\n**Infraestrutura Física:**\n- Os brokers podem ser servidores físicos, instâncias de nuvem, contêineres em pods, ou servidores virtualizados.\n- Independentemente da forma de implementação, cada broker executa o processo do Kafka broker.\n\n**Funções dos Brokers:**\n- Cada broker gerencia algumas partições do Kafka, aceitando eventos novos para essas partições ou lendo eventos delas.\n- Brokers também gerenciam a replicação de partições entre si.\n\n**Simplicidade:**\n- Brokers são projetados para serem simples, garantindo que eles sejam facilmente escaláveis, compreensíveis, modificáveis e extensíveis.\n\n### Kafka Replication\n\n**Necessidade de Replicação:**\n- Armazenar cada partição em um único broker seria arriscado devido à suscetibilidade a falhas dos brokers e seus sistemas de armazenamento.\n- Portanto, os dados das partições são copiados para vários brokers para garantir a segurança dos dados.\n\n**Tipos de Réplicas:**\n- **Líder**: A principal réplica da partição.\n- **Seguidoras**: Réplicas adicionais que seguem a líder.\n\n**Processo de Replicação:**\n- Cada partição replicada tem uma réplica líder e várias seguidoras.\n- Escrita e leitura de dados são feitas principalmente na réplica líder.\n- Após a escrita na líder, a líder e as seguidoras trabalham juntas para replicar os dados.\n\n### Introdução\n\nA replicação é um aspecto crucial do Apache Kafka que garante a durabilidade e disponibilidade dos dados. Neste exercício, vamos entender como a replicação funciona e por que ela é essencial para um sistema de mensageria distribuído como o Kafka.\n\n### Conceitos Básicos\n\n- **Partition**: Uma partição é uma unidade de paralelismo em Kafka. Cada tópico pode ter várias partições.\n- **Leader Replica**: A réplica líder é a partição principal onde todas as gravações ocorrem.\n- **Follower Replicas**: As réplicas seguidoras são cópias da partição que residem em outros brokers para garantir a durabilidade e a disponibilidade dos dados.\n\n### Funcionamento da Replicação\n\n1. **Escrita no Leader**:\n   - Quando os dados são produzidos para uma partição, eles são inicialmente escritos na réplica líder.\n\n2. **Sincronização com Followers**:\n   - Após a escrita, a réplica líder replica os dados para as réplicas seguidoras. Este processo garante que todas as réplicas tenham os mesmos dados.\n\n3. **Falha e Recuperação**:\n   - Se um broker que hospeda a réplica líder falhar, uma das réplicas seguidoras é promovida a líder. Isso garante a continuidade das operações sem perda de dados.\n\n### Diagrama de Replicação\n\n```mermaid\ngraph TD;\n    Producer -->|Escreve| Leader;\n    Leader -->|Replica Dados| Follower1;\n    Leader -->|Replica Dados| Follower2;\n    Follower1 -->|Sincroniza| Leader;\n    Follower2 -->|Sincroniza| Leader;\n    Leader -->|Failover| Follower1;\n    Follower1 -->|Promovido a Líder| Leader;\n```\n\n### Passos para Configurar e Verificar a Replicação\n\n#### 1. Criar um Tópico com Réplicas\nPara criar um tópico com múltiplas réplicas, use o comando:\n\n```bash\nconfluent kafka topic create temperatura --partitions 3 --config min.insync.replicas2 --config replication.factor2\n```\n\n#### 2. Produzir Dados para o Tópico\nProduza mensagens para o tópico `temperatura` com valores aleatórios entre 35 e 40:\n\n```bash\necho \"1:$(shuf -i 35-40 -n 1)\" | confluent kafka topic produce temperatura\necho \"2:$(shuf -i 35-40 -n 1)\" | confluent kafka topic produce temperatura\necho \"3:$(shuf -i 35-40 -n 1)\" | confluent kafka topic produce temperatura\n```\n\n#### 3. Verificar a Replicação\nPara verificar a replicação, use o comando `describe` no tópico:\n\n```bash\nconfluent kafka topic describe temperatura\n```\n\nIsso mostrará informações sobre as partições e suas réplicas.\n\n### Número Ideal de Réplicas no Kafka\n\nDeterminar o número ideal de réplicas para suas partições no Kafka é uma decisão que envolve considerar trade-offs entre durabilidade, disponibilidade e custo. Aqui estão algumas diretrizes gerais e como calcular a quantidade adequada de réplicas:\n\n### Diretrizes Gerais\n\n1. **Durabilidade e Alta Disponibilidade**:\n   - Mais réplicas aumentam a durabilidade e a disponibilidade dos dados.\n   - Em um cenário comum, ter 2 réplicas (um líder e um seguidor) pode ser suficiente para muitas aplicações, pois garante que os dados permaneçam disponíveis se um broker falhar.\n\n2. **Custo**:\n   - Cada réplica adicional aumenta o uso de armazenamento e recursos de rede, o que pode aumentar os custos operacionais.\n   - Avalie o trade-off entre a necessidade de alta disponibilidade e os custos adicionais.\n\n3. **Requisitos de Tolerância a Falhas**:\n   - Se a aplicação exige uma alta tolerância a falhas (por exemplo, sistemas financeiros ou críticos), considere usar 3 réplicas ou mais.\n   - Para ambientes menos críticos, 2 réplicas podem ser adequadas.\n\n### Como Calcular o Número de Réplicas\n\nPara calcular o número ideal de réplicas, você pode seguir estas etapas:\n\n1. **Analisar a Criticidade dos Dados**:\n   - Determine o impacto da perda temporária de acesso aos dados.\n   - Quanto mais críticos os dados, mais réplicas você deve considerar.\n\n2. **Avaliar a Infraestrutura**:\n   - Considere o número de brokers no cluster. O número de réplicas não deve exceder o número de brokers.\n   - Para garantir alta disponibilidade, o número de réplicas deve ser menor que o número total de brokers.\n\n3. **Calcular o Fator de Replicação**:\n   - Um fator de replicação de 2 (1 líder + 1 seguidor) é o mínimo recomendado para garantir alta disponibilidade.\n   - Um fator de replicação de 3 (1 líder + 2 seguidores) oferece uma boa combinação de durabilidade e custo.\n\n### Exemplo Prático\n\nVamos criar um tópico com 3 partições e 2 réplicas (1 líder e 1 seguidor) no Confluent Cloud.\n\n#### 1. Criar um Tópico com Replicação\n\n```bash\nconfluent kafka topic create temperatura --partitions 3 --config replication.factor2\n```\n\n#### 2. Produzir Mensagens para o Tópico\n\nProduza mensagens para o tópico `temperatura` com valores aleatórios entre 35 e 40:\n\n```bash\nfor i in {1..8}; do echo \"$i:$(shuf -i 35-40 -n 1)\" | confluent kafka topic produce temperatura; done\n```\n\n#### 3. Verificar a Replicação\n\nUse o comando `describe` no tópico para verificar a configuração das partições e réplicas:\n\n```bash\nconfluent kafka topic describe temperatura\n```\n\n#### 4. Monitorar e Ajustar\n\n- **Verificação na Interface Web**:\n  - Navegue até a interface do Confluent Cloud e selecione seu cluster.\n  - Na guia \"Topics\", clique no tópico `temperatura` para ver detalhes das partições e réplicas.\n\n- **Monitoramento de Métricas**:\n  - Use as ferramentas de monitoramento do Confluent Cloud para observar o comportamento do cluster, latências e disponibilidade.\n\n### Conclusão\n\nO número ideal de réplicas depende dos requisitos específicos de durabilidade, disponibilidade e custo da sua aplicação. Embora 2 réplicas possam ser suficientes para muitas aplicações, ambientes mais críticos podem exigir 3 réplicas ou mais. Use as diretrizes acima para tomar uma decisão informada.\n\n### Diferença entre Partição e Replicação no Apache Kafka\n\n### Partição (Partition)\n\n#### Conceito\n- **Definição**: Uma partição é uma unidade de paralelismo em Kafka. Cada tópico pode ter várias partições.\n- **Objetivo**: Aumentar a capacidade de processamento e a paralelização das operações de leitura e escrita.\n- **Distribuição**: As mensagens são distribuídas entre as partições de um tópico.\n- **Ordenação**: Mensagens dentro de uma partição são ordenadas de forma estrita, mas não há garantia de ordenação entre partições diferentes do mesmo tópico.\n\n#### Como Funciona\n- Quando um produtor envia mensagens para um tópico, essas mensagens são distribuídas entre as partições.\n- Cada partição é tratada de forma independente e pode ser armazenada em diferentes brokers no cluster.\n- Consumidores podem ler mensagens de várias partições em paralelo, melhorando o throughput.\n\n#### Exemplo\n- Suponha um tópico `temperatura` com 3 partições:\n    - Partição 0: Contém mensagens de sensor A\n    - Partição 1: Contém mensagens de sensor B\n    - Partição 2: Contém mensagens de sensor C\n\n```mermaid\ngraph TD;\n    Producer -->|Particiona| Partition0;\n    Producer -->|Particiona| Partition1;\n    Producer -->|Particiona| Partition2;\n```\n\n### Replicação (Replication)\n\n#### Conceito\n- **Definição**: Replicação é o processo de criar cópias de uma partição em diferentes brokers para garantir a durabilidade e a disponibilidade dos dados.\n- **Objetivo**: Garantir a disponibilidade e a integridade dos dados em caso de falha de um broker.\n- **Tipos de Réplicas**: \n    - **Leader Replica**: A réplica principal que recebe todas as operações de escrita e leitura.\n    - **Follower Replicas**: Réplicas secundárias que mantêm uma cópia dos dados da réplica líder.\n\n#### Como Funciona\n- Quando uma mensagem é escrita em uma partição líder, essa mensagem é replicada para as partições seguidoras.\n- Se o broker que hospeda a réplica líder falhar, uma das réplicas seguidoras é promovida a líder.\n- Isso garante que os dados estejam disponíveis mesmo se um ou mais brokers falharem.\n\n#### Exemplo\n- Suponha um tópico `temperatura` com 3 partições e cada partição replicada em 3 brokers (1 líder e 2 seguidores).\n\n```mermaid\ngraph TD;\n    Producer -->|Escreve| Leader0;\n    Leader0 -->|Replica| Follower0-1;\n    Leader0 -->|Replica| Follower0-2;\n\n    Producer -->|Escreve| Leader1;\n    Leader1 -->|Replica| Follower1-1;\n    Leader1 -->|Replica| Follower1-2;\n\n    Producer -->|Escreve| Leader2;\n    Leader2 -->|Replica| Follower2-1;\n    Leader2 -->|Replica| Follower2-2;\n```\n\n### Comparação\n\n| Aspecto       | Partição                          | Replicação                            |\n|---------------|-----------------------------------|---------------------------------------|\n| **Definição** | Unidade de paralelismo            | Cópia de dados para durabilidade      |\n| **Objetivo**  | Aumentar a capacidade de processamento | Garantir disponibilidade e integridade dos dados |\n| **Operação**  | Mensagens distribuídas entre partições | Mensagens replicadas entre réplicas  |\n| **Falha**     | Se uma partição falhar, apenas uma parte dos dados é afetada | Se o líder falhar, um seguidor é promovido a líder |\n\n### Conclusão\n\n- **Partições**: São usadas para melhorar a escalabilidade e a paralelização no Kafka. Elas permitem que várias operações de leitura e escrita ocorram simultaneamente, aumentando o throughput.\n- **Replicação**: Garante a alta disponibilidade e a durabilidade dos dados, criando cópias das partições em múltiplos brokers. Isso assegura que os dados estejam sempre acessíveis, mesmo em caso de falha de um broker.\n\nCompreender a diferença entre partições e replicação é crucial para projetar sistemas Kafka escaláveis e resilientes. Se precisar de mais detalhes ou ajuda, estou à disposição!\n\n08-kafka-pubsub-streaming/kafka-consumers/README.md\n\n# Consumidores com Confluent Kafka Python no Confluent Cloud\n\n## Introdução\n\nNessa parte, vamos explorar como criar e configurar um consumidor utilizando a biblioteca Python `confluent_kafka` para ler mensagens de um broker Kafka hospedado no Confluent Cloud. O objetivo é consumir mensagens de um tópico que simula leituras de temperatura de um sensor de geladeira.\n\n## Conceitos Básicos\n\n- **Consumidor (Consumer)**: Aplicações que leem (consomem) dados do Kafka.\n\n## Diagrama de Arquitetura\n\n```mermaid\ngraph TD;\n    A[Sensor de Geladeira] -->|Produz Dados| B[Kafka Producer];\n    B -->|Envia Dados| C[Confluent Cloud Broker];\n    C -->|Distribui Dados| D[Partições de Tópicos];\n    D --> E[Kafka Consumer];\n    E --> F[Armazenamento em CSV];\n```\n\n## Código do Consumidor\n\nCrie um arquivo chamado `kafka_consumers.py` com o seguinte conteúdo:\n\n```python\nfrom confluent_kafka import Consumer\nfrom dotenv import load_dotenv\nimport os\nimport csv\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Configurações do consumidor\nconf  {\n    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n    'sasl.mechanisms': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': os.getenv('SASL_USERNAME'),\n    'sasl.password': os.getenv('SASL_PASSWORD'),\n    'group.id': 'python-consumer-group',\n    'auto.offset.reset': 'earliest'\n}\n\n# Criação do consumidor\nconsumer  Consumer(**conf)\n\n# Inscrição no tópico\ntopic  os.getenv('TOPIC_PRODUCER')\nconsumer.subscribe([topic])\n\n# Função para processar mensagens\ndef consume_messages():\n    # Nome do arquivo CSV\n    script_dir  os.path.dirname(os.path.abspath(__file__))  # Diretório do script atual\n    csv_file  os.path.join(script_dir, 'messages.csv')  # Caminho completo para o arquivo CSV\n    \n    # Abrir o arquivo CSV em modo de escrita\n    with open(csv_file, mode'w', newline'') as file:\n        writer  csv.writer(file)\n        writer.writerow(['Key', 'Value'])  # Escrever cabeçalho no CSV\n        \n        try:\n            while True:\n                msg  consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem\n                # Exibe a mensagem recebida\n                key  msg.key().decode('utf-8')\n                value  msg.value().decode('utf-8')\n                print(f\"Received message: Key: {key}, Value: {value}\")\n                # Escreve a mensagem no CSV\n                writer.writerow([key, value])\n        except KeyboardInterrupt:\n            pass\n        finally:\n            print(\"Closing consumer.\")\n            consumer.close()\n\nif __name__  \"__main__\":\n    consume_messages()\n```\n\n## Executando o Consumidor\n\nPara executar o consumidor e começar a ler mensagens do tópico, use o seguinte comando:\n\n```bash\npython kafka_consumers.py\n```\n\nAs mensagens lidas serão exibidas no console e salvas em um arquivo `messages.csv` no mesmo diretório do script Python.\n\n## Conclusão\n\nCom este guia, você configurou e executou um consumidor Kafka utilizando Python e a biblioteca `confluent_kafka`. Isso permite que você leia e processe mensagens de um tópico Kafka hospedado no Confluent Cloud, armazenando os dados consumidos em um arquivo CSV para análise posterior.\n\n08-kafka-pubsub-streaming/kafka-consumers/kafka_consumers.py\n\nfrom confluent_kafka import Consumer, KafkaError\nfrom dotenv import load_dotenv\nimport os\nimport csv\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Configurações do consumidor\nconf  {\n    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n    'sasl.mechanisms': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': os.getenv('SASL_USERNAME'),\n    'sasl.password': os.getenv('SASL_PASSWORD'),\n    'group.id': 'novo-python-consumer-group',\n    'auto.offset.reset': 'earliest'\n}\n\n# Criação do consumidor\nconsumer  Consumer(**conf)\n\n# Inscrição no tópico\ntopic  os.getenv('TOPIC_PRODUCER')\nconsumer.subscribe([topic])\n\n# Função para processar mensagens\ndef consume_messages():\n    # Nome do arquivo CSV\n    script_dir  os.path.dirname(os.path.abspath(__file__))  # Diretório do script atual\n    csv_file  os.path.join(script_dir, 'messages.csv')  # Caminho completo para o arquivo CSV\n    \n    # Abrir o arquivo CSV em modo de escrita\n    with open(csv_file, mode'w', newline'') as file:\n        writer  csv.writer(file)\n        writer.writerow(['Key', 'Value'])  # Escrever cabeçalho no CSV\n        \n        try:\n            while True:\n                msg  consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem\n                # Exibe a mensagem recebida\n                if msg is None:  # Se nenhuma mensagem foi recebida\n                    continue\n                if msg.error():  # Se houve um erro ao receber a mensagem\n                    continue\n                key  msg.key().decode('utf-8')\n                value  msg.value().decode('utf-8')\n                print(f\"Received message: Key: {key}, Value: {value}\")\n                # Escreve a mensagem no CSV\n                writer.writerow([key, value])\n        except KeyboardInterrupt:\n            pass\n        finally:\n            print(\"Closing consumer.\")\n            consumer.close()\n\nif __name__  \"__main__\":\n    consume_messages()\n\n\n08-kafka-pubsub-streaming/kafka-consumers/kafka_consumers_02.py\n\nfrom confluent_kafka import Consumer, KafkaError\nfrom dotenv import load_dotenv\nimport os\nimport csv\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Configurações do consumidor\nconf  {\n    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n    'sasl.mechanisms': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': os.getenv('SASL_USERNAME'),\n    'sasl.password': os.getenv('SASL_PASSWORD'),\n    'group.id': 'outro-python-consumer-group',\n    'auto.offset.reset': 'earliest'\n}\n\n# Criação do consumidor\nconsumer  Consumer(**conf)\n\n# Inscrição no tópico\ntopic  os.getenv('TOPIC_PRODUCER')\nconsumer.subscribe([topic])\n\n# Função para processar mensagens\ndef consume_messages():\n    # Nome do arquivo CSV\n    script_dir  os.path.dirname(os.path.abspath(__file__))  # Diretório do script atual\n    csv_file  os.path.join(script_dir, 'messages_02.csv')  # Caminho completo para o arquivo CSV\n    \n    # Abrir o arquivo CSV em modo de escrita\n    with open(csv_file, mode'w', newline'') as file:\n        writer  csv.writer(file)\n        writer.writerow(['Key', 'Value'])  # Escrever cabeçalho no CSV\n        \n        try:\n            while True:\n                msg  consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem\n                # Exibe a mensagem recebida\n                if msg is None:  # Se nenhuma mensagem foi recebida\n                    continue\n                if msg.error():  # Se houve um erro ao receber a mensagem\n                    continue\n                key  msg.key().decode('utf-8')\n                value  msg.value().decode('utf-8')\n                print(f\"Received message: Key: {key}, Value: {value}\")\n                # Escreve a mensagem no CSV\n                writer.writerow([key, value])\n        except KeyboardInterrupt:\n            pass\n        finally:\n            print(\"Closing consumer.\")\n            consumer.close()\n\nif __name__  \"__main__\":\n    consume_messages()\n\n\n",
        "08-kafka-pubsub-streaming/kafka-demo/README.md\n\n# Workshop de Kafka - Exercício de Configuração e Produção de Mensagens\n\n## Pré-requisitos\n\n1. **Confluent Cloud Cluster:** Certifique-se de ter configurado o Confluent Cloud Cluster e o CLI conforme os exercícios anteriores.\n2. **Kafka CLI:** Tenha o Kafka CLI instalado e configurado para se conectar ao seu cluster Confluent Cloud.\n\n## Passos para o Exercício\n\n### 1. Criar uma Conta no Confluent Cloud\n- Vá para a [URL do Confluent Cloud](https://confluent.cloud).\n- Insira seu nome, email e senha.\n- Clique no botão \"Start Free\" e espere receber um email de confirmação.\n- Confirme seu email para prosseguir com a criação do cluster.\n\n### 2. Configurar o Cluster\n- Após confirmar o email, siga as instruções para configurar seu cluster.\n- Escolha entre um cluster básico, padrão ou dedicado. Para este exercício, escolha o **cluster básico**.\n\n### 3. Aplicar Código Promocional\n- Navegue até \"Settings\" > \"Billing and Payment\".\n- Insira o código promocional `kafka101` para obter $101 adicionais de uso gratuito.\n\n### 4. Criar um Tópico\n\n#### Criar um Tópico Usando a Interface Web\n- Na página inicial do Confluent Cloud, selecione a guia \"Topics\".\n- Clique em \"Create Topic\" e nomeie o tópico como `tecnologias`.\n- Mantenha o número padrão de partições (6) e crie o tópico.\n\n#### Criar um Tópico Usando o CLI\n- No terminal, após configurar o CLI conforme os passos seguintes, crie um tópico:\n\n    ```bash\n    confluent kafka topic create tecnologias --partitions 6\n    ```\n\n### 5. Produzir Mensagens Usando a Interface Web\n- Navegue até a guia \"Messages\" para visualizar a produção e o consumo de mensagens em tempo real.\n- Clique em \"Produce a new message to this topic\".\n- Insira `1` como chave e `Python` como valor, e clique em \"Produce\".\n\n### 6. Configurar a Interface de Linha de Comando (CLI)\n- Na página do Confluent Cloud, vá até \"CLI and Tools\" para baixar e configurar as ferramentas de linha de comando.\n- No terminal, faça login no Confluent Cloud:\n\n    ```bash\n    confluent login --save\n    ```\n\n- Use o mesmo email e senha que você usou para criar sua conta.\n\n### 7. Selecionar Ambiente e Cluster\n- Liste os ambientes disponíveis:\n\n    ```bash\n    confluent environment list\n    ```\n\n- Use o ID do ambiente padrão:\n\n    ```bash\n    confluent environment use <environment_id>\n    ```\n\n- Liste os clusters Kafka disponíveis:\n\n    ```bash\n    confluent kafka cluster list\n    ```\n\n- Use o ID do cluster:\n\n    ```bash\n    confluent kafka cluster use <cluster_id>\n    ```\n\n### 8. Criar e Configurar Chave da API\n- Crie uma chave da API:\n\n    ```bash\n    confluent api-key create --resource <cluster_id>\n    ```\n\n- Salve a chave da API e o segredo fornecidos.\n- Use a chave da API:\n\n    ```bash\n    confluent api-key use <api_key> --resource <cluster_id>\n    ```\n\n### 9. Produzir Mensagens Usando o CLI\n- Liste os tópicos disponíveis:\n\n    ```bash\n    confluent kafka topic list\n    ```\n\n- Para produzir mensagens para o tópico `tecnologias`, abra um terminal CLI e execute:\n\n    ```bash\n    confluent kafka topic produce tecnologias\n    ```\n\n- No prompt, insira uma mensagem de cada vez e pressione Enter:\n\n    ```plaintext\n    1:Python\n    2:SQL\n    3:Kafka\n    4:Spark\n    5:Airflow\n    6:Kubernetes\n    7:Terraform\n    8:Docker\n    ```\n\n### 10. Consumir Mensagens Usando o CLI\n- Abra outro terminal CLI para consumir as mensagens desde o início do tópico:\n\n    ```bash\n    confluent kafka topic consume tecnologias --from-beginning\n    ```\n\n### 11. Verificar Mensagens no Console Web\n- Volte para a interface web do Confluent Cloud e verifique as mensagens produzidas.\n- Para ver mensagens na interface web, defina o deslocamento (offset) para zero e verifique cada partição.\n\n### Conclusão\n\nSe você seguiu todos os passos, realizou várias atividades importantes:\n- Criou uma conta no Confluent Cloud e configurou seu primeiro cluster Kafka.\n- Criou um tópico e produziu mensagens usando a interface web.\n- Instalou o CLI, criou uma chave da API e produziu/consumiu mensagens usando o CLI.\n\n### Gráfico Mermaid\n\n```mermaid\ngraph TD;\n    A[Criar Conta no Confluent Cloud] --> B[Configurar Cluster]\n    B --> C[Aplicar Código Promocional]\n    C --> D[Criar Tópico]\n    D --> E[Produzir Mensagens (Web)]\n    E --> F[Configurar CLI]\n    F --> G[Selecionar Ambiente e Cluster]\n    G --> H[Criar e Configurar Chave da API]\n    H --> I[Produzir Mensagens (CLI)]\n    I --> J[Consumir Mensagens (CLI)]\n    J --> K[Verificar Mensagens (Web)]\n```\n\n### Referências\n- [Confluent Cloud Documentation](https://docs.confluent.io/cloud/current/get-started/index.html)\n- [Kafka Documentation](https://kafka.apache.org/documentation/)\n\n---\n\nEspero que este README e o gráfico ajudem no entendimento e execução do exercício sobre configuração e produção de mensagens no Kafka. Se precisar de mais detalhes ou ajuda, estou à disposição!\n\n08-kafka-pubsub-streaming/kafka-ecosystem/README.md\n\nAqui está um README melhorado com a explicação sobre o ecossistema Kafka e um diagrama Mermaid mais detalhado e sem erros:\n\n# Ecossistema Kafka\n\n### Introdução\n\nNessa parte, vamos explorar o ecossistema mais amplo do Kafka. Mesmo que os brokers gerenciem tópicos particionados e replicados, e que tenhamos uma coleção crescente de produtores e consumidores escrevendo e lendo eventos, certos padrões surgem na forma como esses componentes interagem. Esses padrões incentivam os desenvolvedores a construir funcionalidades recorrentes em torno do Kafka.\n\n### Ecosistema Kafka\n\nO ecossistema Kafka inclui várias ferramentas e componentes que resolvem problemas comuns e aumentam as capacidades do Kafka. Alguns dos principais componentes são:\n\n- **Kafka Connect**: Uma ferramenta para mover dados entre Kafka e outros sistemas de forma robusta e escalável.\n- **Confluent Schema Registry**: Um serviço para gerenciar e aplicar versões de esquemas de dados para Kafka.\n- **Kafka Streams**: Uma biblioteca para processar fluxos de dados em tempo real.\n- **ksqlDB**: Um banco de dados de streaming que oferece uma interface SQL para processar dados em Kafka.\n\n### Diagrama do Ecossistema Kafka\n\n![imagem](./kafka_eco.webp)\n\n### Componentes do Ecossistema Kafka\n\n#### Kafka Connect\n\nKafka Connect é uma ferramenta para mover grandes coleções de dados dentro e fora do Kafka. Ele é usado para integrar Kafka com bancos de dados, sistemas de armazenamento e outros sistemas de dados.\n\n#### Confluent Schema Registry\n\nO Schema Registry gerencia e aplica versões de esquemas de dados, garantindo que os dados enviados para o Kafka estejam em conformidade com os esquemas esperados. Ele ajuda a manter a compatibilidade e a integridade dos dados.\n\n#### Kafka Streams\n\nKafka Streams é uma biblioteca cliente para construir aplicações de streaming que processam e analisam dados em Kafka. Ele permite o processamento em tempo real de fluxos de dados.\n\n#### ksqlDB\n\nksqlDB é um banco de dados de streaming que fornece uma interface SQL para processar e analisar dados em Kafka. Ele permite consultas ad-hoc e análise contínua de fluxos de dados.\n\n### Conclusão\n\nEntender o ecossistema Kafka é essencial para aproveitar ao máximo suas capacidades. Além dos produtores e consumidores básicos, ferramentas como Kafka Connect, Schema Registry, Kafka Streams e ksqlDB fornecem funcionalidades adicionais que simplificam e ampliam o uso do Kafka.\n\nContinue explorando esses componentes para construir soluções robustas e escaláveis com Kafka.\n\n08-kafka-pubsub-streaming/kafka-ecosystem/kafka-connect/README.md\n\n# Kafka Connect com Confluent Cloud: Geração e Consumo de Dados\n\n## Visão Geral\n\nNeste exercício, vamos criar um conector de origem para o Kafka Connect no Confluent Cloud, que produzirá dados para um tópico Kafka. Uma vez configurado o conector, consumiremos esses dados a partir da linha de comando.\n\n## Passos\n\n1. **Criar um Tópico**\n2. **Configurar o DataGen Connector**\n3. **Consumir os Dados**\n\n## Pré-requisitos\n\n- Uma conta no Confluent Cloud\n- CLI do Confluent instalado e autenticado\n\n## Passo 1: Criar um Tópico\n\nPrimeiro, precisamos fornecer um tópico para o conector DataGen produzir dados. Podemos criar esse tópico usando a interface do Confluent Cloud ou o Confluent CLI.\n\n### Usando a Interface do Confluent Cloud\n\n1. Acesse o painel do Confluent Cloud.\n2. Navegue até a seção \"Tópicos\".\n3. Clique em \"Criar Tópico\".\n4. Nomeie o tópico como `inventory` e use as configurações padrão.\n5. Clique em \"Criar\" para finalizar.\n\n### Usando o Confluent CLI\n\n```bash\nconfluent kafka topic create inventory --partitions 1 --cluster <cluster-id>\n```\n\nSubstitua `<cluster-id>` pelo ID real do seu cluster Kafka.\n\n## Passo 2: Configurar o DataGen Connector\n\nAgora, precisamos configurar o conector DataGen para gerar dados de amostra de acordo com um esquema predefinido.\n\n### Usando a Interface do Confluent Cloud\n\n1. Navegue até a seção \"Connect\".\n2. Clique em \"Adicionar Conector\".\n3. Selecione \"DataGen\" da lista de conectores disponíveis.\n4. Configure o conector:\n    - **Nome do Conector**: DataGen-Inventory\n    - **Tópico**: `inventory`\n    - **Formato da Mensagem de Saída**: JSON\n    - **API Key**: Crie ou selecione uma API Key e um segredo existentes.\n    - **Modelo de Dados**: Selecione \"Inventory\" dos modelos disponíveis.\n5. Clique em \"Continuar\" e revise a configuração.\n6. Clique em \"Lançar\" para iniciar o conector.\n\n### Exemplo de Configuração\n\n```json\n{\n  \"name\": \"DataGen-Inventory\",\n  \"config\": {\n    \"connector.class\": \"io.confluent.kafka.connect.datagen.DatagenConnector\",\n    \"kafka.topic\": \"inventory\",\n    \"quickstart\": \"Inventory\",\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"value.converter.schemas.enable\": \"false\",\n    \"max.interval\": 1000,\n    \"iterations\": -1,\n    \"tasks.max\": 1\n  }\n}\n```\n\nVocê pode usar a interface do Confluent Cloud para inserir essa configuração.\n\n## Passo 3: Consumir os Dados\n\nUma vez que o conector DataGen estiver em execução, podemos consumir os dados gerados usando o Confluent CLI.\n\n### Usando o Confluent CLI\n\n```bash\nconfluent kafka topic consume inventory --from-beginning --cluster <cluster-id>\n```\n\nSubstitua `<cluster-id>` pelo ID real do seu cluster Kafka.\n\n## Conclusão\n\nAo longo deste exercício, aprendemos como iniciar rapidamente com o Kafka Connect criando um conector de origem simples que gera dados. Isso é apenas o começo do que o Kafka Connect tem a oferecer. Incentivamos você a explorar mais recursos para entender todas as capacidades do Kafka Connect.\n\n## Recursos Adicionais\n\n- [Documentação do Confluent Kafka Connect](https://docs.confluent.io/platform/current/connect/index.html)\n- [Guia de Início Rápido do Confluent Cloud](https://docs.confluent.io/cloud/current/get-started/index.html)\n- [Deep Dive no Kafka Connect](https://docs.confluent.io/platform/current/connect/concepts.html)\n\nSeguindo esses passos, você pode configurar facilmente um conector de origem do Kafka Connect, gerar dados de amostra e consumir esses dados usando o Confluent Cloud.\n\n08-kafka-pubsub-streaming/kafka-ecosystem/kafka-connect/kafka_consumer_connect.py\n\nfrom confluent_kafka import Consumer, KafkaError\nfrom dotenv import load_dotenv\nimport os\nimport csv\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Configurações do consumidor\nconf  {\n    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n    'sasl.mechanisms': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': os.getenv('SASL_USERNAME'),\n    'sasl.password': os.getenv('SASL_PASSWORD'),\n    'group.id': 'python-consumer-group',\n    'auto.offset.reset': 'earliest'\n}\n\n# Criação do consumidor\nconsumer  Consumer(**conf)\n\n# Inscrição no tópico\ntopic  os.getenv('GET_POSTGRES')\nconsumer.subscribe([topic])\n\n# Função para processar mensagens\ndef consume_messages():\n    # Nome do arquivo CSV\n    script_dir  os.path.dirname(os.path.abspath(__file__))  # Diretório do script atual\n    csv_file  os.path.join(script_dir, 'messages.csv')  # Caminho completo para o arquivo CSV\n    \n    # Abrir o arquivo CSV em modo de escrita\n    with open(csv_file, mode'w', newline'') as file:\n        writer  csv.writer(file)\n        writer.writerow(['Key', 'Value'])  # Escrever cabeçalho no CSV\n        \n        try:\n            while True:\n                msg  consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem\n                # Exibe a mensagem recebida\n                if msg is None:  # Se nenhuma mensagem foi recebida\n                    continue\n                if msg.error():  # Se houve um erro ao receber a mensagem\n                    continue\n                key  msg.key()\n                value  msg.value()\n                print(f\"Received message: Key: {key}, Value: {value}\")\n                # Escreve a mensagem no CSV\n                writer.writerow([key, value])\n        except KeyboardInterrupt:\n            pass\n        finally:\n            print(\"Closing consumer.\")\n            consumer.close()\n\nif __name__  \"__main__\":\n    consume_messages()\n\n\n08-kafka-pubsub-streaming/kafka-ecosystem/ksqlDB/README.md\n\nClaro! Vou explicar cada comando e passo detalhadamente para que você entenda o objetivo de cada um deles.\n\n# Hands On: ksqlDB\n\nNeste exercício, você aprenderá como manipular seus dados usando o ksqlDB. Até agora, estivemos produzindo dados e lendo dados de um tópico do Apache Kafka sem quaisquer etapas intermediárias. Com a transformação e agregação de dados, podemos fazer muito mais!\n\nNo último exercício, criamos um Conector Datagen Source para produzir um fluxo de dados de pedidos para um tópico Kafka, serializando-o em Avro usando o Confluent Schema Registry. Este exercício depende dos dados que nosso conector está produzindo, então, se você não completou o exercício anterior, encorajamos você a fazê-lo antes de prosseguir.\n\nAntes de começarmos, certifique-se de que seu Conector Datagen Source ainda está ativo e em execução.\n\n## Passo a Passo\n\n### 1. Criar Cluster ksqlDB\nNa página inicial do cluster no Confluent Cloud Console, selecione ksqlDB no menu à esquerda. \n- **Objetivo**: Iniciar o ksqlDB, que é a plataforma de streaming SQL para Apache Kafka, permitindo consultas e transformações em tempo real dos dados no Kafka.\n\n### 2. Configurar Acesso\nEscolha \"Global access\" na página de controle de acesso e continue para dar um nome ao cluster ksqlDB e depois inicie o cluster.\n- **Objetivo**: Configurar o acesso ao cluster ksqlDB para garantir que todos os serviços e aplicações tenham acesso ao ksqlDB.\n\n### 3. Registrar Tópico\nApós a provisão do cluster ksqlDB, você será levado ao editor ksqlDB. Adicione o tópico `orders` do exercício anterior à aplicação ksqlDB. Registre-o como um stream executando:\n```sql\nCREATE STREAM orders_stream WITH (\n  KAFKA_TOPIC'orders', \n  VALUE_FORMAT'AVRO',\n  PARTITIONS6,\n  TIMESTAMP'ordertime');\n```\n- **Objetivo**: Criar um stream no ksqlDB a partir do tópico `orders` para que possamos realizar consultas SQL sobre os dados em tempo real.\n\n### 4. Visualizar Stream\nNavegue até a aba Streams e selecione `orders_stream` para visualizar mais detalhes sobre o stream.\n- **Objetivo**: Inspecionar os detalhes do stream, incluindo os campos e o formato dos dados, para entender melhor a estrutura do fluxo de dados.\n\n### 5. Transformar Dados\nExecute a seguinte consulta para transformar o campo `ordertime` em um formato mais legível e extrair os dados aninhados do struct `address`:\n```sql\nSELECT \n    TIMESTAMPTOSTRING(ORDERTIME, 'yyyy-MM-dd HH:mm:ss.SSS') AS ORDERTIME_FORMATTED,\n    orderid,\n    itemid,\n    orderunits,\n    address->city, \n    address->state,\n    address->zipcode\nFROM ORDERS_STREAM;\n```\n- **Objetivo**: Transformar os dados no stream `orders_stream` para melhorar a legibilidade do campo `ordertime` e extrair campos aninhados do endereço.\n\n### 6. Persistir Resultados\nPara persistir os resultados em um novo stream, adicione uma linha `CREATE STREAM` no início da consulta:\n```sql\nCREATE STREAM ORDERS_STREAM_TS AS\nSELECT \n    TIMESTAMPTOSTRING(ORDERTIME, 'yyyy-MM-dd HH:mm:ss.SSS') AS ORDERTIME_FORMATTED,\n    orderid,\n    itemid,\n    orderunits,\n    address->city, \n    address->state,\n    address->zipcode \nFROM ORDERS_STREAM;\n```\n- **Objetivo**: Criar um novo stream `ORDERS_STREAM_TS` que contém os dados transformados do stream original `orders_stream`.\n\n### 7. Agregação de Dados\nCrie uma tabela para contar quantos pedidos são feitos por estado. O ksqlDB facilita a segmentação dos dados em janelas de tempo:\n```sql\nCREATE TABLE STATE_COUNTS AS \nSELECT \n  address->state,\n  COUNT_DISTINCT(ORDERID) AS DISTINCT_ORDERS\nFROM ORDERS_STREAM\nWINDOW TUMBLING (SIZE 7 DAYS) \nGROUP BY address->state;\n```\n- **Objetivo**: Agregar os dados no stream `ORDERS_STREAM` para contar o número distinto de pedidos por estado em uma janela de 7 dias, criando uma tabela `STATE_COUNTS`.\n\n### 8. Consultar Tabela\nNavegue até a aba Tables para ver os dados relacionados à tabela.\n- **Objetivo**: Visualizar e consultar os dados agregados na tabela `STATE_COUNTS`.\n\n### 9. Consulta de Pull\nPara ver apenas os valores mais recentes de uma tabela, execute uma consulta de pull. Veja um instantâneo atual contendo os estados que tiveram mais de dois pedidos por período de uma semana:\n```sql\nSELECT\n    *\nFROM STATE_COUNTS\nWHERE DISTINCT_ORDERS > 2;\n```\n- **Objetivo**: Executar uma consulta de pull para obter um snapshot dos dados na tabela `STATE_COUNTS`, filtrando os estados que tiveram mais de dois pedidos na última semana.\n\nEsses passos fornecem uma visão detalhada de como configurar, transformar e consultar dados em tempo real usando o ksqlDB com Apache Kafka.\n\n",
        "08-kafka-pubsub-streaming/kafka-ecosystem/schema_registry/README.md\n\n# Confluent Schema Registry\n\nQuando aplicações estão produzindo e consumindo mensagens do Kafka, duas coisas acontecem:\n\n1. **Novos Consumidores Emergentes**: Novas aplicações, potencialmente desenvolvidas por diferentes equipes, precisarão entender o formato das mensagens nos tópicos existentes.\n2. **Evolução do Formato das Mensagens**: O formato das mensagens vai evoluir à medida que o negócio evolui. Por exemplo, objetos de pedido podem ganhar novos campos de status ou nomes de usuário podem ser divididos em nome e sobrenome.\n\n## O Problema\n\nO formato dos objetos de domínio é um alvo em constante movimento. Precisamos de uma maneira de concordar sobre o esquema das mensagens nos tópicos. É aí que entra o Confluent Schema Registry.\n\n## O que é o Schema Registry?\n\nO Schema Registry é um processo de servidor independente que roda em uma máquina externa aos brokers do Kafka. Ele mantém um banco de dados de todos os esquemas que foram escritos nos tópicos no cluster. Esse banco de dados é persistido em um tópico interno do Kafka e é armazenado em cache no Schema Registry para acesso de baixa latência.\n\n### Funcionalidades\n\n- **Manutenção de Esquemas**: O Schema Registry mantém um banco de dados de esquemas persistido em um tópico do Kafka.\n- **API REST**: Produtores e consumidores podem usar a API REST do Schema Registry para verificar a compatibilidade dos esquemas.\n- **Suporte a Alta Disponibilidade**: Pode ser configurado em uma configuração redundante para alta disponibilidade.\n- **Compatibilidade de Esquemas**: Permite verificar a compatibilidade dos esquemas em tempo de produção e consumo.\n\n## Exemplo de Uso\n\n### Configurando um Produtor para Usar o Schema Registry\n\nQuando um produtor é configurado para usar o Schema Registry, ele chama a API do Schema Registry no momento da produção de uma mensagem. Se o esquema da mensagem for compatível com as regras de compatibilidade definidas para o tópico, a produção terá sucesso. Caso contrário, a produção falhará, permitindo que o código da aplicação trate essa condição.\n\n### Suporte a Diferentes Formatos de Serialização\n\nO Schema Registry suporta três formatos de serialização:\n- **JSON Schema**\n- **Avro**\n- **Protobuf**\n\n## Vantagens\n\n- **Gestão de Evolução de Esquemas**: Facilita a gestão da evolução de esquemas, evitando falhas de runtime quando possível.\n- **Colaboração e Controle de Versão**: Permite a colaboração em torno das mudanças de esquema, utilizando ferramentas de controle de versão como arquivos IDL (Interface Description Language).\n\n## Conclusão\n\nEm sistemas não triviais, o uso do Schema Registry é essencial. Ele oferece uma maneira padronizada e automatizada de aprender sobre os esquemas e gerenciar suas evoluções internamente, facilitando a vida dos desenvolvedores e garantindo a compatibilidade das mensagens.\n\n## Recursos Adicionais\n\n- [Documentação do Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/index.html)\n- [Guia de Início Rápido do Confluent Cloud](https://docs.confluent.io/cloud/current/get-started/index.html)\n- [Deep Dive no Schema Registry](https://docs.confluent.io/platform/current/schema-registry/concepts.html)\n\n## Diagrama de Arquitetura\n\n```mermaid\ngraph TD;\n    A[Aplicação Produtora] -->|Produz Dados| B[Kafka Topic];\n    B -->|Esquemas| C[Schema Registry];\n    D[Aplicação Consumidora] -->|Consome Dados| B;\n    C -->|Verifica Esquemas| D;\n    E[Confluent CLI] -->|Gerencia| C;\n```\n\n## Exemplo de Configuração de Esquema\n\n```json\n{\n  \"type\": \"record\",\n  \"namespace\": \"com.mycorp.mynamespace\",\n  \"name\": \"vendas\",\n  \"fields\": [\n    { \"name\": \"id\", \"type\": \"int\", \"doc\": \"Identificador da venda.\" },\n    { \"name\": \"data_venda\", \"type\": \"string\", \"doc\": \"Data da venda.\" },\n    { \"name\": \"produto_id\", \"type\": \"int\", \"doc\": \"Identificador do produto.\" },\n    { \"name\": \"quantidade\", \"type\": \"int\", \"doc\": \"Quantidade vendida.\" },\n    { \"name\": \"valor_total\", \"type\": \"float\", \"doc\": \"Valor total da venda.\" }\n  ]\n}\n```\n\nUse o Schema Registry para garantir que as mensagens produzidas e consumidas estejam de acordo com este esquema, permitindo uma gestão eficiente e robusta da evolução dos dados.\n\n## Hands-on: Confluent Schema Registry\n\n### Configuração e Uso do Schema Registry\n\n1. **Habilitar o Schema Registry**: No Confluent Cloud Console, selecione Schema Registry no canto inferior esquerdo.\n2. **Configurar Credenciais de API**: Crie uma chave de API e um segredo na seção \"API credentials\" da página do Schema Registry.\n3. **Criar Conector Datagen**: Navegue até \"Data integration\" e selecione \"Connectors\". Adicione um conector Datagen Source para gerar dados de exemplo.\n4. **Configurar o Tópico e o Conector**: Crie um novo tópico chamado `orders` e configure o conector para usar o formato Avro.\n5. **Consumir Mensagens**: Use o comando abaixo para consumir mensagens do tópico `orders`.\n\nPasso 1: Armazenar a Chave da API\nArmazene a chave da API e o segredo:\n\nÓtimo! Agora que você tem a chave da API e o segredo, vamos seguir os passos para armazenar a chave da API e consumir mensagens do tópico `orders`.\n\n### Passo 0: Criar uma nova chave\n\n```bash\nconfluent api-key create --resource lkc-7zoqpw\n```\n\n### Passo 1: Armazenar a Chave da API\n\nArmazene a chave da API e o segredo:\n\n```bash\nconfluent api-key store CKTUPV72A52DME6K mVETdpD41gScm+njQJcljcm5M7IcFWWjcE3TXvfxTenxmzBHbM3syzxRii3iQoWC --resource lkc-7zoqpw\n```\n\n### Passo 2: Selecionar a Chave da API\n\nDepois de armazenar a chave da API, selecione-a:\n\n```bash\nconfluent api-key use CKTUPV72A52DME6K --resource lkc-7zoqpw\n```\n\n### Passo 3: Consumir Mensagens do Tópico\n\nFinalmente, consuma as mensagens do tópico `orders`:\n\n```bash\nconfluent kafka topic consume orders\n```\n\n### Código Completo\n\n1. Armazene a chave da API:\n\n    ```bash\n    confluent api-key store CKTUPV72A52DME6K mVETdpD41gScm+njQJcljcm5M7IcFWWjcE3TXvfxTenxmzBHbM3syzxRii3iQoWC --resource lkc-7zoqpw\n    ```\n\n2. Selecione a chave da API:\n\n    ```bash\n    confluent api-key use CKTUPV72A52DME6K --resource lkc-7zoqpw\n    ```\n\n3. Consuma mensagens do tópico:\n\n    ```bash\n    confluent kafka topic consume orders\n    ```\n\nSeguindo esses passos, você deve conseguir consumir as mensagens do tópico `orders` usando o Confluent CLI.\n\n![schemas_img](/kafka-ecosystem/schema_registry/schemas_key.png)\n```bash\nconfluent kafka topic consume --value-format avro --schema-registry-api-key N7CUKKPO4QRVRIXW --schema-registry-api-secret k4PPzQTYqMBDIV4es9P8rww5Kb48XlLWFwvP+nH1TPDkf6XqTYM3qmhOXHevoMYA orders\n```\n\n08-kafka-pubsub-streaming/kafka-marketing-project/README.md\n\n## Escopo do Projeto: Simulação de 50 Refrigeradores Espalhados pelo Brasil\n\nClaro! Aqui está o README atualizado com o comando para criar o tópico `marketing-project`.\n\n## Projeto: Monitoramento de Refrigeração no Brasil\n\n### Objetivo\n\nEste projeto tem como objetivo simular 50 refrigeradores espalhados pelo Brasil. Cada refrigerador irá reportar a temperatura a cada segundo. Queremos entender como os refrigeradores estão operando em diferentes regiões e monitorar temperaturas anômalas que podem indicar falhas ou problemas.\n\n### Componentes do Projeto\n\n1. **Producers**:\n    - 50 produtores simulando refrigeradores.\n    - Cada produtor envia dados de temperatura a cada segundo.\n    - As temperaturas são geradas com base em dois intervalos: \n        - 45 refrigeradores com temperaturas entre 0°C e 5°C.\n        - 5 refrigeradores com temperaturas entre 20°C e 40°C.\n    - Cada produtor tem um ID único gerado com UUID.\n\n2. **Consumer**:\n    - Um consumidor que lê os dados em tempo real e exibe as temperaturas usando Streamlit.\n\n3. **Kafka Topic**:\n    - Tópico chamado `marketing-project`.\n\n### Passos para Configuração\n\n#### 1. Configurar o Confluent CLI\n\nCertifique-se de que você tenha configurado o Confluent CLI e esteja autenticado:\n\n```bash\nconfluent login\n```\n\n#### 2. Selecionar o Ambiente e o Cluster\n\n```bash\nconfluent environment use <environment_id>\nconfluent kafka cluster use <cluster_id>\n```\n\n#### 3. Criar o Tópico `marketing-project`\n\n```bash\nconfluent kafka topic create marketing-project --partitions 6\n```\n\n### Objetivo\n\nO objetivo deste projeto é simular 50 refrigeradores espalhados pelo Brasil, gerando dados de temperatura em tempo real. Essa simulação visa monitorar e analisar as variações de temperatura, permitindo uma melhor compreensão do comportamento térmico dos refrigeradores em diferentes regiões e condições climáticas. Com este projeto, pretendemos:\n\n1. **Monitorar Temperaturas em Tempo Real**: Capturar e exibir dados de temperatura de 50 refrigeradores espalhados pelo Brasil, atualizados a cada segundo.\n2. **Analisar Distribuição de Temperaturas**: Identificar padrões de temperatura e comportamentos anômalos entre os refrigeradores.\n3. **Avaliar Eficiência Térmica**: Verificar a eficiência térmica dos refrigeradores em manter a temperatura dentro de uma faixa desejada.\n4. **Identificar Problemas Potenciais**: Detectar refrigeradores que possam estar operando fora da faixa de temperatura esperada, sinalizando possíveis falhas ou necessidade de manutenção.\n\n### Descrição do Projeto\n\n1. **Simulação dos Refrigeradores**:\n    - **Quantidade**: 50 refrigeradores.\n    - **Localização**: Cada refrigerador terá uma latitude e longitude fixas, representando diferentes regiões do Brasil.\n    - **Distribuição de Temperatura**:\n        - 45 refrigeradores manterão temperaturas entre 0 e 5 graus Celsius (95%).\n        - 5 refrigeradores terão temperaturas entre 20 e 40 graus Celsius (5%), representando possíveis falhas ou condições extremas.\n\n2. **Produção de Dados**:\n    - **Intervalo de Produção**: Cada refrigerador gerará uma leitura de temperatura a cada segundo.\n    - **Formato dos Dados**: Os dados serão enviados em formato JSON, contendo a latitude, longitude, temperatura e identificador do refrigerador.\n\n3. **Tópico Kafka**:\n    - **Nome do Tópico**: `marketing-project`.\n    - **Produção e Consumo**: Os dados gerados pelos 50 refrigeradores serão publicados neste tópico Kafka.\n\n4. **Consumidor de Dados**:\n    - **Visualização em Tempo Real**: Um consumidor será implementado usando Streamlit para exibir as últimas leituras de temperatura de cada refrigerador em tempo real.\n    - **Monitoramento e Análise**: A aplicação Streamlit permitirá monitorar os dados e identificar rapidamente qualquer comportamento anômalo.\n\n### Componentes do Projeto\n\n1. **Producers**:\n    - 50 instâncias de produtores, cada uma simulando um refrigerador.\n    - Utilização da biblioteca `Faker` para gerar dados de localização e temperatura.\n    - Publicação dos dados no tópico `marketing-project` no Kafka.\n\n2. **Consumer**:\n    - Uma aplicação Streamlit que consome os dados do tópico `marketing-project`.\n    - Exibição dos dados em um dashboard, mostrando a última leitura de temperatura de cada refrigerador.\n\n### Ferramentas e Tecnologias\n\n1. **Apache Kafka**: Para transmissão e ingestão de dados em tempo real.\n2. **Confluent Cloud**: Plataforma gerenciada de Kafka para simplificar a configuração e o gerenciamento do cluster.\n3. **Faker**: Biblioteca para geração de dados fictícios.\n4. **Docker**: Para containerização dos produtores e consumidor.\n5. **Streamlit**: Para visualização dos dados em tempo real.\n\n### Implementação\n\n1. **Configuração do Tópico Kafka**:\n    - Criar o tópico `marketing-project` no Confluent Cloud.\n\n2. **Desenvolvimento dos Producers**:\n    - Implementar o script de produção utilizando `Faker` para gerar os dados de temperatura e localização.\n    - Containerizar os produtores usando Docker.\n\n3. **Desenvolvimento do Consumer**:\n    - Implementar a aplicação Streamlit para consumir e exibir os dados em tempo real.\n    - Containerizar o consumidor usando Docker.\n\n4. **Execução**:\n    - Iniciar os 50 produtores e o consumidor utilizando `docker-compose`.\n\n## Executar os Containers\nCertifique-se de que o arquivo docker-compose.yml está configurado corretamente e execute os seguintes comandos no terminal para construir a imagem Docker e iniciar os containers:\n\n```bash\ndocker-compose build\ndocker-compose up -d\n```\n\n### Resultado Esperado\n\nAo final do projeto, espera-se ter uma simulação funcional de 50 refrigeradores espalhados pelo Brasil, com dados de temperatura sendo gerados e exibidos em tempo real. A aplicação Streamlit fornecerá uma interface intuitiva para monitorar e analisar esses dados, possibilitando a identificação de padrões e anomalias de temperatura, além de fornecer insights sobre a eficiência térmica dos refrigeradores em diferentes condições climáticas.\n\n### Bizu para apagar\n\nconfluent kafka topic update marketing-project --config retention.ms1000\n\nconfluent kafka topic delete marketing-project\n\n\n###\n\n```bash\nconfluent kafka topic consume marketing-project --from-beginning\n```\n\n\n\n08-kafka-pubsub-streaming/kafka-marketing-project/docker-compose.yml\n\nversion: '3.8'\n\nservices:\n  producer:\n    build: ./producer\n    environment:\n      - BOOTSTRAP_SERVERS${BOOTSTRAP_SERVERS}\n      - SASL_USERNAME${SASL_USERNAME}\n      - SASL_PASSWORD${SASL_PASSWORD}\n      - TOPIC_NAME${TOPIC_NAME}\n      - PRODUCER_ID${PRODUCER_ID}\n    deploy:\n      replicas: 50\n\n  consumer:\n    build: ./consumer\n    environment:\n      - BOOTSTRAP_SERVERS${BOOTSTRAP_SERVERS}\n      - SASL_USERNAME${SASL_USERNAME}\n      - SASL_PASSWORD${SASL_PASSWORD}\n      - TOPIC_NAME${TOPIC_NAME}\n    volumes:\n      - ./data:/app/data\n\n  dashboard:\n    build: ./dashboard\n    environment:\n      - BOOTSTRAP_SERVERS${BOOTSTRAP_SERVERS}\n      - SASL_USERNAME${SASL_USERNAME}\n      - SASL_PASSWORD${SASL_PASSWORD}\n      - TOPIC_NAME${TOPIC_NAME}\n    ports:\n      - \"8501:8501\"\n    volumes:\n      - ./data:/app/data\n\n\n08-kafka-pubsub-streaming/kafka-marketing-project/consumer/Dockerfile\n\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY consumer.py .\n\nRUN pip install confluent_kafka sqlalchemy pandas psycopg2-binary\n\nCMD [\"python\", \"consumer.py\"]\n\n\n08-kafka-pubsub-streaming/kafka-marketing-project/consumer/consumer.py\n\nimport time\nimport random\nfrom confluent_kafka import Consumer, KafkaError\nimport json\nimport os\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom datetime import datetime\n\n# Configuração do Consumer\nconsumer_conf  {\n    'bootstrap.servers': os.environ['BOOTSTRAP_SERVERS'],\n    'security.protocol': 'SASL_SSL',\n    'sasl.mechanisms': 'PLAIN',\n    'sasl.username': os.environ['SASL_USERNAME'],\n    'sasl.password': os.environ['SASL_PASSWORD'],\n    'group.id': 'temperature-consumer-group',\n    'auto.offset.reset': 'earliest'\n}\n\nconsumer  Consumer(consumer_conf)\nconsumer.subscribe(['marketing-project'])\n\n# Configuração do PostgreSQL\ndb_url  \"postgresql://postgres_kafka_user:UQ4PAxSQbukeWVcFEDpdNquS6zhbt8zs@dpg-cq8vi35ds78s7396a4og-a.oregon-postgres.render.com/postgres_kafka_sink\"\nengine  create_engine(db_url)\n\ndef consume_messages():\n    batch_size  100  # Número de mensagens para processar por batch\n    batch_interval  5  # Intervalo de tempo (em segundos) entre os batches\n    messages  []\n    start_time  time.time()  # Inicializa start_time dentro da função\n\n    try:\n        while True:\n            msg  consumer.poll(1.0)  # Espera até 1 segundo por uma nova mensagem\n            if msg is None:\n                continue\n            if msg.error():\n                continue\n\n            value  json.loads(msg.value().decode('utf-8'))  # Converte o valor para JSON\n            # Desnormalizar a mensagem\n            data  {\n                'id': value['id'],\n                'latitude': value['latitude'],\n                'longitude': value['longitude'],\n                'temperature': value['temperature'],\n                'time': datetime.now()  # Captura o tempo atual da mensagem\n            }\n            messages.append(data)\n\n            # Processar batch quando atingir o tamanho ou o intervalo de tempo\n            if len(messages) > batch_size or (time.time() - start_time) > batch_interval:\n                df  pd.DataFrame(messages)\n                df.to_sql('temperature_data', engine, if_exists'append', indexFalse)\n                messages  []  # Limpar lista de mensagens após salvar no banco\n                start_time  time.time()  # Resetar o contador de tempo\n\n    except KeyboardInterrupt:\n        pass\n    finally:\n        print(\"Closing consumer.\")\n        consumer.close()\n\nif __name__  \"__main__\":\n    start_time  time.time()\n    consume_messages()\n\n\n08-kafka-pubsub-streaming/kafka-marketing-project/consumer/requirements.txt\n\nconfluent_kafka\nfaker\nstreamlit\nsqlalchemy\n\n08-kafka-pubsub-streaming/kafka-marketing-project/dashboard/Dockerfile\n\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY dashboard.py .\n\nRUN pip install streamlit sqlalchemy pandas psycopg2-binary python-dotenv plotly\n\nCMD [\"streamlit\", \"run\", \"dashboard.py\"]\n\n\n08-kafka-pubsub-streaming/kafka-marketing-project/dashboard/dashboard.py\n\nimport streamlit as st\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom dotenv import load_dotenv\nimport os\nimport time\nimport plotly.express as px\n\n# Carregar variáveis de ambiente\nload_dotenv()\n\n# Configuração do PostgreSQL\ndb_url  'postgresql://postgres_kafka_user:UQ4PAxSQbukeWVcFEDpdNquS6zhbt8zs@dpg-cq8vi35ds78s7396a4og-a.oregon-postgres.render.com/postgres_kafka_sink'\nengine  create_engine(db_url)\n\n# Função para carregar dados do banco de dados\ndef load_data():\n    query  \"\"\"\n    SELECT DISTINCT ON (id) *\n    FROM temperature_data\n    ORDER BY id, time DESC;\n    \"\"\"\n    try:\n        df  pd.read_sql(query, engine)\n        return df\n    except Exception as e:\n        st.error(f\"Erro ao carregar dados: {e}\")\n        return pd.DataFrame()  # Retorna um DataFrame vazio em caso de erro\n\n# Função para exibir o mapa\ndef display_map(df):\n    df['color']  df['temperature'].apply(lambda x: 'red' if x > 5 else 'blue')\n    fig  px.scatter_mapbox(\n        df,\n        lat\"latitude\",\n        lon\"longitude\",\n        color\"color\",\n        color_discrete_map{\"red\": \"red\", \"blue\": \"blue\"},\n        hover_name\"id\",\n        hover_data{\"temperature\": True, \"latitude\": False, \"longitude\": False},\n        zoom3,\n        height600,\n    )\n    fig.update_layout(mapbox_style\"open-street-map\")\n    st.plotly_chart(fig)\n\n# Loop para atualizar a página a cada 5 segundos\nwhile True:\n    st.title('Dashboard de Temperaturas')\n    data  load_data()\n    if not data.empty:\n        st.write(data)\n        display_map(data)\n    else:\n        st.write('A tabela \"temperature_data\" ainda não existe ou não foi possível carregar os dados.')\n    \n    time.sleep(10)\n    st.experimental_rerun\n\n\n08-kafka-pubsub-streaming/kafka-marketing-project/producer/Dockerfile\n\nFROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY producer.py .\n\nRUN pip install faker confluent_kafka\n\nCMD [\"python\", \"producer.py\"]\n\n\n08-kafka-pubsub-streaming/kafka-marketing-project/producer/producer.py\n\nimport time\nimport random\nfrom confluent_kafka import Producer\nimport uuid\nfrom faker import Faker\nimport json\nimport os\n\nfake  Faker('pt_BR')\n\ndef delivery_report(err, msg):\n    if err is not None:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n\n# Configuração do Producer\nproducer_conf  {\n    'bootstrap.servers': os.environ['BOOTSTRAP_SERVERS'],\n    'security.protocol': 'SASL_SSL',\n    'sasl.mechanisms': 'PLAIN',\n    'sasl.username': os.environ['SASL_USERNAME'],\n    'sasl.password': os.environ['SASL_PASSWORD']\n}\n\ndef generate_unique_id():\n    return int(str(uuid.uuid4().int)[:8])\n\nproducer  Producer(producer_conf)\n\ndef generate_initial_data(id):\n    lat  random.uniform(-33.0, 5.0)\n    lon  random.uniform(-73.0, -34.0)\n    address  fake.address()\n    temperature_range  'low' if random.random() < 0.95 else 'high'\n    data  {\n        'id': id,\n        'latitude': lat,\n        'longitude': lon,\n        'address': address,\n        'temperature_range': temperature_range\n    }\n    return data\n\ndef generate_temperature(temperature_range):\n    if temperature_range  'low':\n        temperature  random.gauss(2.5, 1.0)\n    else:\n        temperature  random.uniform(20.0, 40.0)\n    return round(temperature, 2)\n\nif __name__  '__main__':\n    producer_id  generate_unique_id()\n    initial_data  generate_initial_data(producer_id)\n    while True:\n        temperature  generate_temperature(initial_data['temperature_range'])\n        data  initial_data.copy()\n        data['temperature']  temperature\n        producer.produce('marketing-project', keystr(producer_id), valuejson.dumps(data), callbackdelivery_report)\n        producer.poll(1)\n        time.sleep(5)\n\n\n08-kafka-pubsub-streaming/kafka-marketing-project/producer/requirements.txt\n\nconfluent_kafka\nfaker\n\n",
        "08-kafka-pubsub-streaming/kafka-producers/README-producers.md\n\n### Produtores com Confluent Kafka Python no Confluent Cloud\n\n### Introdução\n\nNessa parte, vamos explorar como criar e configurar um produtor utilizando a biblioteca Python `confluent_kafka` para enviar mensagens para um broker Kafka hospedado no Confluent Cloud. Nosso objetivo é criar um sensor de geladeira para a Ambev, permitindo monitorar a temperatura do freezer e garantir que ele esteja na temperatura ideal.\n\n### Conceitos Básicos\n\n- **Produtor (Producer)**: Aplicações que enviam (produzem) dados para o Kafka.\n\n### Diagrama de Arquitetura\n\n```mermaid\ngraph TD;\n    A[Sensor de Geladeira] -->|Produz Dados| B[Kafka Producer];\n    B -->|Envia Dados| C[Confluent Cloud Broker];\n    C -->|Distribui Dados| D[Partições de Tópicos];\n```\n\n### Estrutura do Produtor\n\nNo Kafka, os produtores são responsáveis por enviar mensagens para tópicos específicos. Usaremos a biblioteca `confluent_kafka` para criar e configurar um produtor em Python.\n\n### Instalação da Biblioteca\n\nPrimeiro, instale a biblioteca `confluent_kafka`:\n\n```bash\npip install confluent_kafka python-dotenv\n```\n\n### Configuração do Produtor para Confluent Cloud\n\n#### Passos para Obter Configurações Necessárias\n\n1. **Obter o Bootstrap Server**:\n   - No Confluent Cloud Console, vá para seu cluster Kafka e encontre o endereço do bootstrap server.\n\n2. **Obter as Credenciais de API (API Key e Secret)**:\n   - No Confluent Cloud Console, vá para \"API keys\" e crie uma nova chave API. Anote a API Key e o Secret.\n\n3. **Criar o Tópico Usando a CLI do Confluent Cloud**:\n\n```bash\nconfluent kafka topic create freezer-temperatura --partitions 3 --cluster lkc-k08pop\n```\n\n### Arquivo `.env`\n\nCrie um arquivo chamado `.env` no mesmo diretório do seu script Python e adicione as seguintes linhas:\n\n```plaintext\nBOOTSTRAP_SERVERSpkc-12576z.us-west2.gcp.confluent.cloud:9092\nSASL_USERNAMEYOUR_API_KEY  # Substitua pela sua API Key\nSASL_PASSWORDYOUR_API_SECRET  # Substitua pelo seu API Secret\nCLIENT_IDpython-producer\nTOPIC_PRODUCERfreezer-temperatura\n```\n\n### Exemplo de Configuração\n\nAqui está um exemplo completo de como configurar e usar o produtor em Python com a biblioteca `confluent_kafka` para o Confluent Cloud.\n\n```python\nfrom confluent_kafka import Producer\nfrom dotenv import load_dotenv\nimport os\nimport random\nimport time\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Configurações do produtor\nconf  {\n    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n    'sasl.mechanisms': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': os.getenv('SASL_USERNAME'),\n    'sasl.password': os.getenv('SASL_PASSWORD'),\n    'client.id': os.getenv('CLIENT_ID')\n}\n\n# Criação do produtor\nproducer  Producer(**conf)\n\n# Função de callback para entrega de mensagens\ndef delivery_report(err, msg):\n    if err is not None:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n\n# Produção de mensagens simulando um sensor de geladeira\ntopic  os.getenv('TOPIC_PRODUCER')\nfor i in range(10):\n    temperature  random.uniform(-5, 5)  # Temperatura aleatória entre -5 e 5 graus Celsius\n    key  f\"sensor{i % 3}\"  # Usar diferentes chaves para distribuir entre partições\n    producer.produce(topic, keykey, valuef\"{temperature:.2f}\", callbackdelivery_report)\n    producer.poll(0)\n    time.sleep(1)  # Simula leitura de temperatura a cada segundo\n\n# Espera até todas as mensagens serem entregues\nproducer.flush()\n```\n\n### Diagrama de Processo de Produção\n\n```mermaid\nsequenceDiagram\n    participant S as Sensor de Geladeira\n    participant P as Kafka Producer\n    participant B as Confluent Cloud Broker\n    participant T as Tópico: freezer-temperatura\n    S->>P: Gera Leitura de Temperatura\n    P->>B: Envia Mensagem\n    B->>T: Armazena na Partição Correspondente\n    T->>C: Consumidores Analisam Dados\n```\n\n### Como o Produtor Escolhe a Partição\n\nO produtor decide a qual partição enviar cada mensagem, seja de forma round-robin (sem chave) ou computando a partição destino através do hash da chave.\n\n1. **Round-Robin**:\n   - Mensagens sem chave são distribuídas de forma round-robin entre as partições.\n\n2. **Hash da Chave**:\n   - Mensagens com chave têm a partição destino calculada através do hash da chave.\n\n### Funcionamento Interno do Produtor\n\n- **Gerenciamento de Pools de Conexão**: O produtor gerencia pools de conexão para otimizar a comunicação com o cluster.\n- **Bufferização de Rede**: Mensagens são bufferizadas antes de serem enviadas para os brokers.\n- **Acknowledge de Mensagens**: O produtor espera os acknowledgements dos brokers para liberar espaço no buffer.\n- **Retransmissão de Mensagens**: Retransmissão de mensagens ocorre quando necessário.\n\n### Recomendações\n\nPara realmente entender como os produtores funcionam, é altamente recomendável que você escreva e execute algum código. Veja a API em ação, digite os comandos você mesmo e observe o comportamento do Kafka.\n\n### Conclusão\n\nOs produtores são uma parte essencial do Kafka, permitindo que você envie dados para o cluster de forma eficiente e escalável. Compreender como configurá-los e como eles interagem com as partições é crucial para aproveitar ao máximo o Apache Kafka.\n\n### Execução do Script\n\nPara executar o script, certifique-se de que o arquivo `.env` está no mesmo diretório e execute o seguinte comando:\n\n```bash\npython kafka_producers.py\n```\n\nIsso iniciará o produtor Kafka que enviará leituras de temperatura simuladas para o tópico `freezer-temperatura` no Confluent Cloud.\n\n### Lendo os valores gerados\n\n```bash\nconfluent kafka topic consume freezer-temperatura --cluster lkc-k08pop --from-beginning\n```\n\n08-kafka-pubsub-streaming/kafka-producers/kafka_producers.py\n\nfrom confluent_kafka import Producer\nfrom dotenv import load_dotenv\nimport os\nimport random\nimport time\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Configurações do produtor\nconf  {\n    'bootstrap.servers': os.getenv('BOOTSTRAP_SERVERS'),\n    'sasl.mechanisms': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': os.getenv('SASL_USERNAME'),\n    'sasl.password': os.getenv('SASL_PASSWORD'),\n    'client.id': os.getenv('CLIENT_ID')\n}\n\n# Criação do produtor\nproducer  Producer(**conf)\n\n# Função de callback para entrega de mensagens\ndef delivery_report(err, msg):\n    if err is not None:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n\n# Produção de mensagens simulando um sensor de geladeira\ntopic  os.getenv('TOPIC_PRODUCER')\nfor i in range(10):\n    temperature  random.uniform(-5, 5)  # Temperatura aleatória entre -5 e 5 graus Celsius\n    key  f\"sensor{(i % 3) + 1}\"  # Usar diferentes chaves para distribuir entre partições\n    producer.produce(topic, keykey, valuef\"{temperature:.2f}\", callbackdelivery_report)\n    producer.poll(0)\n    time.sleep(1)  # Simula leitura de temperatura a cada segundo\n\n# Espera até todas as mensagens serem entregues\nproducer.flush()\n\n\n08-kafka-pubsub-streaming/kafka-topics-partitions/README.md\n\n# Exercício de Particionamento de Tópicos\n\n## Pré-requisitos\n\n1. **Confluent Cloud Cluster:** Certifique-se de ter configurado o Confluent Cloud Cluster e o CLI conforme os exercícios anteriores.\n2. **Kafka CLI:** Tenha o Kafka CLI instalado e configurado para se conectar ao seu cluster Confluent Cloud.\n\n## Passos para o Exercício\n\n### 1. Listar Tópicos Existentes\nPara ver uma lista de tópicos no seu cluster, execute o comando abaixo no terminal:\n\n```bash\nconfluent kafka topic list\n```\n\n### 2. Descrever o Tópico Existente\nPara ver mais detalhes sobre a configuração do tópico `tecnologias`, use o comando:\n\n```bash\nconfluent kafka topic describe tecnologias\n```\n\n### Principais Pontos do Comando `describe tecnologias`\n\nAqui estão os detalhes atualizados com os valores convertidos para segundos, megabytes (MB), e dias, conforme aplicável:\n\n1. **cleanup.policy**: \n   - Valor: `delete`\n   - Descrição: Define a política de limpeza de logs. `delete` significa que os logs antigos serão excluídos quando o tempo de retenção expirar.\n\n2. **compression.type**: \n   - Valor: `producer`\n   - Descrição: Especifica o tipo de compressão usada. `producer` indica que a compressão é definida pelo produtor.\n\n3. **delete.retention.ms**: \n   - Valor: `86400000` (1 dia)\n   - Descrição: Tempo de retenção em milissegundos para mensagens deletadas. (1 dia)\n\n4. **file.delete.delay.ms**: \n   - Valor: `60000` (60 segundos)\n   - Descrição: Atraso antes que um segmento de log deletado seja realmente removido do disco. (60 segundos)\n\n5. **flush.messages**: \n   - Valor: `9223372036854775807`\n   - Descrição: Número de mensagens que devem ser escritas antes que os dados sejam forçados a serem liberados no disco (influencia a durabilidade).\n\n6. **flush.ms**: \n   - Valor: `9223372036854775807`\n   - Descrição: Tempo em milissegundos antes que os dados sejam forçados a serem liberados no disco (influencia a durabilidade).\n\n7. **index.interval.bytes**: \n   - Valor: `4096` (4 KB)\n   - Descrição: Número de bytes entre entradas de índice. (4 KB)\n\n8. **max.message.bytes**: \n   - Valor: `2097164` (2 MB)\n   - Descrição: Tamanho máximo de uma mensagem (em bytes) que o tópico pode aceitar. (2 MB)\n\n9. **message.downconversion.enable**: \n   - Valor: `true`\n   - Descrição: Permite a conversão descendente de mensagens para versões anteriores de formato.\n\n10. **message.format.version**: \n    - Valor: `3.0-IV1`\n    - Descrição: Versão do formato da mensagem usada pelo tópico.\n\n11. **message.timestamp.type**: \n    - Valor: `CreateTime`\n    - Descrição: Tipo de timestamp usado para mensagens (tempo de criação da mensagem).\n\n12. **min.cleanable.dirty.ratio**: \n    - Valor: `0.5`\n    - Descrição: Proporção mínima de dados \"sujos\" em um log de segmento que aciona a limpeza de compactação.\n\n13. **min.insync.replicas**: \n    - Valor: `2`\n    - Descrição: Número mínimo de réplicas que devem estar sincronizadas para produzir mensagens com sucesso.\n\n14. **num.partitions**: \n    - Valor: `6`\n    - Descrição: Número de partições no tópico.\n\n15. **retention.bytes**: \n    - Valor: `-1`\n    - Descrição: Limite de retenção de bytes para o tópico. `-1` indica que não há limite.\n\n16. **retention.ms**: \n    - Valor: `604800000` (7 dias)\n    - Descrição: Tempo de retenção em milissegundos para mensagens no tópico. (7 dias)\n\n17. **segment.bytes**: \n    - Valor: `104857600` (100 MB)\n    - Descrição: Tamanho do segmento de log em bytes. (100 MB)\n\n18. **segment.ms**: \n    - Valor: `604800000` (7 dias)\n    - Descrição: Tempo em milissegundos antes que um novo segmento de log seja criado. (7 dias)\n\n19. **unclean.leader.election.enable**: \n    - Valor: `false`\n    - Descrição: Se `false`, desativa a eleição de líderes que não estão completamente sincronizados, melhorando a durabilidade à custa da disponibilidade.\n\n### 3. Criar Novos Tópicos com Diferentes Partições\nVamos criar dois novos tópicos chamados `tecnologias1` e `tecnologias4` com 1 e 4 partições, respectivamente.\n\n#### Criar o Tópico `tecnologias1` com 1 Partição\n```bash\nconfluent kafka topic create tecnologias1 --partitions 1\n```\n\n#### Criar o Tópico `tecnologias4` com 4 Partições\n```bash\nconfluent kafka topic create tecnologias4 --partitions 4\n```\n\n### 4. Verificar a Contagem de Partições dos Novos Tópicos\nPara verificar a contagem de partições dos novos tópicos, use o comando `describe` novamente:\n\n```bash\nconfluent kafka topic describe tecnologias1\nconfluent kafka topic describe tecnologias4\n```\n\n### 5. Produzir Dados para os Tópicos\n\n#### Produzir Dados para o Tópico `tecnologias1`\n```bash\nconfluent kafka topic produce tecnologias1\n```\n\nNo prompt, insira uma mensagem de cada vez e pressione Enter:\n\n```plaintext\n1:Python\n2:SQL\n3:Kafka\n4:Spark\n5:Airflow\n6:Kubernetes\n7:Terraform\n8:Docker\n```\n\n#### Produzir Dados para o Tópico `tecnologias4`\n```bash\nconfluent kafka topic produce tecnologias4\n```\n\nNo prompt, insira uma mensagem de cada vez e pressione Enter:\n\n```plaintext\n1:Python\n2:SQL\n3:Kafka\n4:Spark\n5:Airflow\n6:Kubernetes\n7:Terraform\n8:Docker\n```\n\n### 6. Verificar as Mensagens Produzidas\nNo console web do Confluent Cloud, visualize as mensagens produzidas. Observe que o tópico `tecnologias1` tem todas as 8 mensagens em uma única partição, enquanto o tópico `tecnologias4` tem uma distribuição ligeiramente diferente.\n\n### Conclusão\nCom isso, você deve ter uma boa ideia de como a contagem de partições pode afetar a distribuição de dados nos seus tópicos. No próximo exercício, continuaremos explorando mais recursos do Kafka.\n\n### Gráfico Mermaid\n\n```mermaid\ngraph TD;\n    A[Configurar CLI] --> B[Listar Tópicos]\n    B --> C[Descrever Tópico]\n    C --> D[Criar Tópicos tecnologias1 e tecnologias4]\n    D --> E[Verificar Partições]\n    E --> F[Produzir Dados para Tópicos]\n    F --> G[Verificar Mensagens no Console Web]\n```\n\n### Vantagens e Desvantagens de Trabalhar com um Número de Partições\n\n## Vantagens\n\n### 1. **Paralelismo**\n   - **Vantagem**: Aumentar o número de partições permite maior paralelismo. Cada partição pode ser processada por um consumidor diferente, aumentando a capacidade de processamento.\n   - **Exemplo**: Em um cenário de alta carga, múltiplos consumidores podem processar mensagens simultaneamente, reduzindo o tempo de processamento.\n\n### 2. **Escalabilidade**\n   - **Vantagem**: Maior número de partições facilita a escalabilidade horizontal, permitindo adicionar mais consumidores conforme a demanda aumenta.\n   - **Exemplo**: Em sistemas distribuídos, adicionar novas instâncias de consumidores para lidar com picos de carga sem reconfigurar o cluster.\n\n### 3. **Balanceamento de Carga**\n   - **Vantagem**: Mensagens são distribuídas entre várias partições, o que ajuda a balancear a carga de trabalho entre diferentes consumidores.\n   - **Exemplo**: Diferentes consumidores podem trabalhar em paralelo, cada um processando uma partição específica, evitando gargalos.\n\n### 4. **Melhoria no Desempenho de Escrita**\n   - **Vantagem**: Várias partições podem melhorar a taxa de transferência de escrita, já que múltiplos produtores podem escrever em diferentes partições simultaneamente.\n   - **Exemplo**: Em aplicações com alta taxa de escrita, a capacidade de distribuir escritas entre múltiplas partições pode aumentar a eficiência geral.\n\n## Desvantagens\n\n### 1. **Complexidade na Gerência de Partições**\n   - **Desvantagem**: Mais partições podem aumentar a complexidade da gerência, incluindo monitoramento, manutenção e configuração.\n   - **Exemplo**: Necessidade de ferramentas avançadas para gerenciar grandes quantidades de partições, aumentando a carga administrativa.\n\n### 2. **Rebalanceamento**\n   - **Desvantagem**: Rebalancear partições entre consumidores pode ser custoso em termos de desempenho, especialmente em clusters grandes.\n   - **Exemplo**: Durante o rebalanceamento, pode haver um tempo de inatividade ou degradação de desempenho à medida que as partições são redistribuídas.\n\n### 3. **Overhead de Recursos**\n   - **Desvantagem**: Mais partições exigem mais recursos do Kafka broker, incluindo memória, armazenamento e processamento.\n   - **Exemplo**: Cada partição tem uma sobrecarga associada, e um número excessivo pode levar ao consumo excessivo de recursos do servidor.\n\n### 4. **Problemas de Ordenação**\n   - **Desvantagem**: Garantir a ordenação das mensagens pode ser mais difícil com múltiplas partições, pois cada partição mantém sua própria ordem.\n   - **Exemplo**: Para manter a ordenação global de mensagens, pode ser necessário um processamento adicional ou uma lógica complexa no consumidor.\n\n### 5. **Latência de Rebalanceamento**\n   - **Desvantagem**: Quando um novo consumidor é adicionado, ou um consumidor existente falha, o tempo para rebalancear as partições pode adicionar latência.\n   - **Exemplo**: Em um sistema com alta disponibilidade, a latência de rebalanceamento pode afetar a entrega de mensagens em tempo real.\n\n### Conclusão\nEscolher o número adequado de partições é uma decisão crítica que deve equilibrar as necessidades de desempenho e paralelismo com a complexidade de gerenciamento e os recursos disponíveis. Avaliar cuidadosamente os requisitos do sistema e realizar testes de carga podem ajudar a determinar a configuração ideal para um cenário específico.\n\nNão há um número \"ideal\" de partições que funcione para todos os cenários, pois o número apropriado de partições para um tópico Kafka depende de diversos fatores, incluindo a taxa de produção e consumo de mensagens, o tamanho das mensagens, e a arquitetura geral do seu sistema. No entanto, aqui estão algumas diretrizes e considerações que podem ajudar a determinar um número adequado de partições:\n\n## Fatores para Determinar o Número de Partições\n\n### 1. **Taxa de Produção e Consumo**\n   - **Alto Volume de Dados**: Se você está lidando com um alto volume de dados, mais partições podem ajudar a distribuir a carga entre vários consumidores.\n   - **Concorrência de Consumidores**: O número de consumidores em um grupo deve idealmente corresponder ou ser menor que o número de partições para garantir que cada partição seja consumida eficientemente.\n\n### 2. **Tamanho das Mensagens**\n   - **Mensagens Grandes**: Se suas mensagens são grandes, mais partições podem ajudar a evitar que uma única partição se torne um gargalo.\n\n### 3. **Desempenho e Latência**\n   - **Paralelismo**: Mais partições permitem maior paralelismo, aumentando o throughput geral do sistema.\n   - **Latência**: Muito poucas partições podem levar a gargalos, enquanto muitas partições podem introduzir sobrecarga administrativa.\n\n### 4. **Escalabilidade**\n   - **Crescimento Futuro**: Considere o crescimento futuro do seu sistema. Pode ser mais fácil começar com mais partições do que necessário e crescer nelas, do que aumentar o número de partições posteriormente.\n\n### 5. **Replicação e Tolerância a Falhas**\n   - **Número de Réplicas**: Partições adicionais podem ajudar a garantir maior resiliência e disponibilidade.\n\n### 6. **Limitações Operacionais**\n   - **Limites do Kafka**: Tenha em mente que cada partição adiciona uma certa quantidade de sobrecarga administrativa ao Kafka e ao Zookeeper.\n   - **Recursos de Hardware**: Certifique-se de que seus brokers têm recursos suficientes (CPU, memória, disco) para lidar com o número de partições desejado.\n\n## Implementação em Instâncias EC2\n\n### 1. **Distribuição de Partições**\n   - **Balanceamento de Carga**: As partições podem ser distribuídas entre várias instâncias EC2 para garantir balanceamento de carga e alta disponibilidade.\n   - **Dimensionamento**: Dependendo do tamanho e da carga de trabalho, você pode alocar várias partições em uma única instância EC2, mas isso deve ser feito com cautela para evitar gargalos.\n\n### 2. **Considerações de Deploy**\n   - **Tipo de Instância**: Escolha instâncias EC2 que correspondam aos requisitos de desempenho do Kafka. Instâncias com alto I/O, como as séries I3, podem ser vantajosas.\n   - **Storage**: Utilize armazenamento de alta performance, como SSDs, para partições Kafka.\n   - **Rede**: Garanta que a latência da rede entre instâncias EC2 seja baixa, especialmente se elas estão em diferentes zonas de disponibilidade.\n\n### 3. **Escalabilidade e Gerenciamento**\n   - **Auto Scaling**: Configure grupos de auto scaling para instâncias EC2 para lidar com variações na carga de trabalho.\n   - **Monitoramento e Alertas**: Use ferramentas de monitoramento para observar o desempenho das partições e instâncias, e configure alertas para identificar problemas rapidamente.\n\n## Exemplo de Configuração\n\n### Criar Tópico com Partições via CLI\n\n```bash\nconfluent kafka topic create tecnologias --partitions 10\n```\n\n### Produzir e Consumir Mensagens\n\n#### Produzir Mensagens\n\n```bash\n# Abra um terminal para produzir mensagens\nconfluent kafka topic produce tecnologias\n\n# Insira mensagens manualmente\n1:Python\n2:Kafka\n3:AWS\n4:Docker\n5:Spark\n6:Hadoop\n7:Kubernetes\n8:TensorFlow\n9:Pandas\n10:SQL\n```\n\n#### Consumir Mensagens\n\n```bash\n# Abra um segundo terminal para consumir mensagens\nconfluent kafka topic consume tecnologias --from-beginning\n```\n\n## Conclusão\n\nO número ideal de partições depende das necessidades específicas do seu sistema, incluindo a taxa de produção e consumo, o tamanho das mensagens e a necessidade de escalabilidade futura. Distribuir partições entre várias instâncias EC2 pode ajudar a garantir alta disponibilidade e desempenho, mas deve ser feito com planejamento cuidadoso para evitar gargalos e garantir eficiência operacional.\n\n\n### Referências\n- [Confluent Cloud Documentation](https://docs.confluent.io/cloud/current/get-started/index.html)\n- [Kafka Documentation](https://kafka.apache.org/documentation/)\n\n\n08-kafka-pubsub-streaming/python-client/Dockerfile\n\nFROM --platformamd64 python:3.8-alpine\nRUN pip install kafka-python Faker\nENV ACTION produce\nENV BOOTSTRAP_SERVERS \"broker-1:29091,broker-2:29092,broker-3:29093\"\nADD consume.py /src/consume.py\nADD produce.py /src/produce.py\nADD run.sh /src/run.sh\nCMD [\"sh\", \"/src/run.sh\", \"$ACTION\"]\n\n\n",
        "08-kafka-pubsub-streaming/python-client/consume.py\n\n# https://kafka-python.readthedocs.io/en/master/#kafkaproducer\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nfrom kafka import KafkaConsumer\n\nTOPIC  os.environ.get('TOPIC', 'foobar')\nCONSUMER_GROUP  os.environ.get('CONSUMER_GROUP', 'cg-group-id')\nBOOTSTRAP_SERVERS  os.environ.get(\n    'BOOTSTRAP_SERVERS', 'localhost:9091,localhost:9092,localhost:9093'\n).split(',')\n\nprint('iniciando')\n\n\ndef configurar_consumidor():\n    try:\n        consumidor  KafkaConsumer(\n            TOPIC,\n            bootstrap_serversBOOTSTRAP_SERVERS,\n            auto_offset_reset'latest',\n            enable_auto_commitTrue,\n            group_idCONSUMER_GROUP,\n            value_deserializerlambda x: json.loads(x.decode('utf-8')),\n        )\n        return consumidor\n\n    except Exception as e:\n        if e  'NoBrokersAvailable':\n            print('aguardando os brokers ficarem disponíveis')\n        return 'nao-disponivel'\n\n\ndef diferenca_tempo(received_time):\n    agora  datetime.now().strftime('%s')\n    return int(agora) - received_time\n\n\nprint('iniciando consumidor, verificando se os brokers estão disponíveis')\nconsumidor  'nao-disponivel'\n\nwhile consumidor  'nao-disponivel':\n    print('brokers ainda não disponíveis')\n    time.sleep(5)\n    consumidor  configurar_consumidor()\n\nprint('brokers estão disponíveis e prontos para consumir mensagens')\n\nfor mensagem in consumidor:\n    try:\n        print(mensagem.value)\n        # print(f\"Mensagem recebida em: {mensagem.timestamp}\")\n        # agora  datetime.now().strftime(\"%s\")\n        # print(f\"Timestamp atual {agora}\")\n    except Exception as e:\n        print('ocorreu uma exceção no consumo')\n        print(e)\n\n# Fechar o consumidor\nprint('fechando consumidor')\nconsumidor.close()\n\n\n08-kafka-pubsub-streaming/python-client/produce.py\n\n# https://kafka-python.readthedocs.io/en/master/#kafkaproducer\nimport json\nimport os\nimport random\nimport time\nimport uuid\n\nfrom faker import Faker\nfrom kafka import KafkaProducer\n\nfake  Faker()\n\nTOPIC  os.environ.get('TOPIC', 'foobar')\nBOOTSTRAP_SERVERS  os.environ.get(\n    'BOOTSTRAP_SERVERS', 'localhost:9091,localhost:9092,localhost:9093'\n).split(',')\n\n\ndef criar_transacao(contador):\n    mensagem  {\n        'id_sequencia': contador,\n        'id_usuario': str(fake.random_int(min20000, max100000)),\n        'id_transacao': str(uuid.uuid4()),\n        'id_produto': str(uuid.uuid4().fields[-1])[:5],\n        'endereco': str(\n            fake.street_address()\n            + ' | '\n            + fake.city()\n            + ' | '\n            + fake.country_code()\n        ),\n        'cadastro_em': str(fake.date_time_this_month()),\n        'id_plataforma': str(random.choice(['Mobile', 'Laptop', 'Tablet'])),\n        'mensagem': 'transacao feita pelo usuario {}'.format(\n            str(uuid.uuid4().fields[-1])\n        ),\n    }\n    return mensagem\n\n\ndef configurar_produtor():\n    try:\n        produtor  KafkaProducer(\n            bootstrap_serversBOOTSTRAP_SERVERS,\n            value_serializerlambda v: json.dumps(v).encode('utf-8'),\n        )\n        return produtor\n    except Exception as e:\n        if e  'NoBrokersAvailable':\n            print('aguardando os brokers ficarem disponíveis')\n        return 'nao-disponivel'\n\n\nprint('configurando produtor, verificando se os brokers estão disponíveis')\nprodutor  'nao-disponivel'\n\nwhile produtor  'nao-disponivel':\n    print('brokers ainda não disponíveis')\n    time.sleep(5)\n    produtor  configurar_produtor()\n\nprint('brokers estão disponíveis e prontos para produzir mensagens')\ncontador  0\n\nwhile True:\n    contador + 1\n    mensagem_json  criar_transacao(contador)\n    produtor.send(TOPIC, mensagem_json)\n    print(\n        'mensagem enviada para o kafka com id de sequência {}'.format(contador)\n    )\n    time.sleep(2)\n\nprodutor.close()\n\n\n08-kafka-pubsub-streaming/python-client/requirements.txt\n\nkafka-python\nFaker\n\n08-kafka-pubsub-streaming/python-client/run.sh\n\n#!/usr/bin/env sh\nset -x\n\nif [ \"$ACTION\"  \"producer\" ] \nthen\n  echo \"starting $ACTION\"\n  env | grep BOOTSTRAP\n  python3 /src/produce.py\nfi\n\nif [ \"$ACTION\"  \"consumer\" ]\nthen\n  echo \"starting $ACTION\"\n  env | grep BOOTSTRAP\n  python3 /src/consume.py\nfi\n\nif [ \"$ACTION\"  \"shell\" ]\nthen\n  sleep 10000000\nfi\n\n"
    ],
    "WSL": [
        "WSL/readme.md\n\n### **Passo a passo: Configurando VS Code com WSL e acessando a pasta \"projetos\"**\n\n---\n\n### **1. Baixar e instalar a extensão WSL no VS Code**\n1. Abra o **Visual Studio Code** no Windows.\n2. Vá para a aba de **Extensões** (ou use `Ctrl+Shift+X`).\n3. Na barra de pesquisa, digite:  \n   ```\n   Remote - WSL\n   ```\n4. Clique em **\"Install\"** para instalar a extensão **Remote - WSL**.\n5. Após a instalação, reinicie o VS Code, se necessário.\n\n---\n\n### **2. Criar uma pasta chamada \"projetos\" na `home` usando o WSL**\n\n1. Abra o **WSL** (pesquise \"WSL\" ou \"Ubuntu\" no menu Iniciar).\n2. No terminal, digite:\n   ```bash\n   cd ~\n   mkdir projetos\n   cd projetos\n   ```\n3. Verifique se a pasta foi criada com:\n   ```bash\n   ls\n   ```\n   - O comando deve mostrar a pasta \"projetos\".\n\n---\n\n### **3. Acessar a pasta usando o Git Bash**\n\n1. Abra o **Git Bash** no Windows.\n2. Navegue até a pasta \"projetos\" com:\n   ```bash\n   cd //wsl$/Ubuntu/home/projetos/\n   ```\n3. Digite:\n   ```bash\n   ls\n   ```\n   - Se tudo estiver correto, você verá a pasta \"projetos\".\n\n---\n\n### **4. Acessar a pasta \"projetos\" pelo Explorador de Arquivos do Windows**\n\n1. Abra o **Explorador de Arquivos**.\n2. Na barra de endereços, digite:\n   ```\n   \\\\wsl$\\Ubuntu\\home\\projetos\\\n   ```\n3. Pressione **Enter**.  \n   Agora você pode ver e gerenciar os arquivos dentro dessa pasta diretamente pelo Windows.\n\n---\n\n### **Dica Extra: Abrir a pasta diretamente no VS Code**\n\n1. No terminal do **WSL**, navegue até a pasta:\n   ```bash\n   cd ~/projetos\n   ```\n2. Abra a pasta no VS Code com:\n   ```bash\n   code .\n   ```\n   - Isso abrirá o VS Code diretamente com a pasta \"projetos\" como seu workspace.\n\n---\n\nAgora você tem a pasta configurada, pode navegar por ela tanto no WSL quanto no Git Bash e Explorador de Arquivos, e integrá-la facilmente com o VS Code!\n\n"
    ],
    "Bootcamp - SQL e Analytics": [
        "Bootcamp - SQL e Analytics/README.md\n\n# Bootcamp de SQL e Analytics\n\nBem-vindos ao nosso Bootcamp de SQL e Analytics, um programa intensivo projetado para equipar você com habilidades de engenharia de dados e análise de dados. \n\nNosso bootcamp é repleto de recursos e atividades interativas para garantir uma experiência de aprendizado prática. Aqui estão os principais componentes do nosso programa:\n\n#### 1. **Repositório no GitHub**\n\n* **Link:** [Data Engineering Roadmap](https://github.com/lvgalvao/data-engineering-roadmap)\n* **Descrição:** Este repositório é a sua porta de entrada para os recursos de conteúdos da jornada de dados. Ele contém todos os materiais didáticos, exercícios práticos, datasets para análise e scripts que você utilizará durante o bootcamp. Cada módulo está detalhadamente documentado para que você possa seguir o caminho de aprendizagem com clareza e estrutura.\n\n#### 2. **Plataforma Alpaclass**\n\n* **Link:** [Jornada de Dados](https://jornadadedados.alpaclass.com/s/conteudos)\n* **Descrição:** A Alpaclass é a nossa plataforma de aprendizado dedicada, onde você encontrará os vídeos das aulas. O conteúdo está organizado de forma sequencial para facilitar o seu progresso de um tópico para o próximo, com marcos claros para monitorar seu desenvolvimento ao longo do curso. Caso tenha alguma dúvida, deixar no comentário do vídeo incluindo também o tempo.\n\n#### 3. **Aulas ao Vivo**\n\n* **Horário:** Diariamente ao meio-dia\n* **Plataforma:** Google Meet\n* **Descrição:** Nossas aulas ao vivo são uma oportunidade para interagir com os instrutores e colegas. Durante estas sessões, você pode esclarecer dúvidas, explorar conceitos complexos e participar de discussões em tempo real. \n\n#### 4. **Grupo do WhatsApp**\n\n* **Descrição:** Um espaço dedicado para suporte contínuo, o grupo do WhatsApp permite que você discuta desafios, compartilhe insights e peça ajuda tanto aos seus colegas quanto aos instrutores. Este canal está sempre ativo, assegurando que você nunca fique sem suporte.\n\n* **Foto do Grupo:** \n\n![Foto grupo](https://github.com/lvgalvao/data-engineering-roadmap/blob/main/Bootcamp%20-%20SQL%20e%20Analytics/pics/grupo_alunos.png)\n\n\n#### 5. **Repositório no Excalidraw**\n\n* **Link:** [Excalidraw](https://link.excalidraw.com/l/8pvW6zbNUnD/3ktnOgfFeRK)\n* **Objetivo:** Conceituar e expor graficamente estruturas de dados, fluxos de trabalho e arquiteturas de sistemas.\n\n* **Descrição:** O repositório no Excalidraw serve como uma ferramenta visual para compreender melhor os conceitos abstratos e complexos que você encontrará no curso. Aqui, você terá acesso a diagramas e esquemas que ilustram de forma clara e eficaz os tópicos de engenharia e análise de dados, facilitando a compreensão e o aprendizado.\n\n#### 6. **Conteúdo**\n\n- *[Aula-01](./Aula-01)* - **Visão Geral e Preparação do ambiente SQL**\n- *[Aula-02](./Aula-02)* - **SQL para Analytics: Nossas primeiras consultas**\n- *[Aula-03](./Aula-03)* - **SQL para Analytics: Join and Having in SQL**\n- *[Aula-04](./Aula-04)* - **Windows Function**\n- *[Aula-05](./Aula-05)* - **Projeto de Análise de dados**\n- *[Aula-06](./Aula-06)* - **CTE vs Subqueries vs Views vs Temporary Tables vs Materialized Views**\n- *[Aula-07](./Aula-07)* - **Stored Procedures**\n- *[Aula-08](./Aula-08)* - **CTE vs Subqueries vs Views vs Temporary Tables vs Materialized Views**\n- *[Aula-09](./Aula-09)* - **Triggers (Gatilhos) e Projeto Prático II**\n- *[Aula-10](./Aula-10)* - **Transação**\n- *[Aula-11](./Aula-11)* - **Ordem de consulta**\n- *[Aula-12](./Aula-12)* - **Database Indexing**\n- *[Aula-13](./Aula-13)* - **Database Partition**\n\n\n## Por Que Participar?\nParticipar do nosso Bootcamp de SQL e Analytics não apenas fortalece suas habilidades técnicas, mas também expande sua rede profissional através de interações com colegas e especialistas da indústria. Ao final do programa, você estará bem equipado para enfrentar desafios complexos de dados e fazer contribuições significativas no campo da engenharia e análise de dados.\n\n\nBootcamp - SQL e Analytics/Aula-01/README.md\n\n# Aula 01 - Visão Geral e Preparação do ambiente SQL\n\n## Introdução\n\nBem-vindos ao nosso workshop sobre SQL e PostgreSQL. Hoje, vamos mergulhar nos conceitos básicos de bancos de dados e como o PostgreSQL pode ser utilizado para gerenciar dados de forma eficiente. Nosso objetivo é garantir que todos vocês tenham uma boa base para explorar mais sobre SQL e operações de banco de dados nos próximos dias.\n\n## Por que Postgres?\n\nPostgreSQL é um sistema de gerenciamento de banco de dados relacional (RDBMS) desenvolvido no Departamento de Ciência da Computação da Universidade da Califórnia em Berkeley. POSTGRES foi pioneiro em muitos conceitos que só se tornaram disponíveis em alguns sistemas de banco de dados comerciais muito mais tarde:\n\n* complex queries\n* foreign keys\n* triggers\n* updatable views\n* transactional integrity\n\nAlém disso, o PostgreSQL pode ser estendido pelo usuário de várias maneiras, por exemplo, adicionando novos\n\n* data types\n* functions\n* operators\n* aggregate functions\n* index methods\n* procedural languages\n\n## Informações Adicionais \n\nAlém do conteúdo do curso, recomendo alguns outros lugares para estudo.\n\n[Documentação](https://www.postgresql.org/docs/current/index.html) Documentação oficial do Postgres, todas as features estão aqui.\n\n\n[Wiki](https://wiki.postgresql.org/wiki/Main_Page) A wiki do PostgreSQL contém a lista de Perguntas Frequentes (FAQ), lista de tarefas pendentes (TODO) e informações detalhadas sobre muitos outros tópicos.\n\n[Site](https://www.postgresql.org/) na Web O site do PostgreSQL fornece detalhes sobre a última versão e outras informações para tornar seu trabalho ou lazer com o PostgreSQL mais produtivo.\n\n[Comunidade](https://github.com/postgres/postgres) O código O PostgreSQL é um projeto de código aberto. Como tal, depende da comunidade de usuários para suporte contínuo. À medida que você começa a usar o PostgreSQL, dependerá de outros para obter ajuda, seja através da documentação ou através das listas de discussão. Considere devolver o seu conhecimento. Leia as listas de discussão e responda às perguntas. Se você aprender algo que não está na documentação, escreva e contribua com isso. Se você adicionar recursos ao código, contribua com eles.\n\n## Instalação\n\nAntes de poder usar o PostgreSQL, você precisa instalá-lo, é claro. É possível que o PostgreSQL já esteja instalado em seu local, seja porque foi incluído na distribuição do seu sistema operacional ou porque o administrador do sistema já o instalou.\n\nSe você não tem certeza se o PostgreSQL já está disponível ou se você pode usá-lo para suas experimentações, então você pode instalá-lo por conta própria. Fazer isso não é difícil e pode ser um bom exercício.\n\n- Instalando o postgres Local\n\n## Fundamentos da Arquitetura\n\nAntes de prosseguirmos, é importante que você entenda a arquitetura básica do sistema PostgreSQL. Compreender como as partes do PostgreSQL interagem tornará tudo mais fácil.\n\nNo jargão de tecnologia, o PostgreSQL utiliza um modelo cliente/servidor. \n\nUm processo servidor, que gerencia os arquivos de banco de dados, aceita conexões com o banco de dados de aplicações cliente e executa ações no banco de dados em nome dos clientes. O programa do servidor de banco de dados é chamado de postgres.\n\nA aplicação cliente do usuário (frontend) que deseja realizar operações de banco de dados. As aplicações cliente podem ser muito diversas em natureza: um cliente pode ser uma ferramenta orientada a texto, uma aplicação gráfica, um servidor web que acessa o banco de dados para exibir páginas web ou uma ferramenta especializada de manutenção de banco de dados. Algumas aplicações cliente são fornecidas com a distribuição do PostgreSQL; a maioria é desenvolvida pelos usuários.\n\nComo é típico em aplicações cliente/servidor, o cliente e o servidor podem estar em hosts diferentes. Nesse caso, eles se comunicam por uma conexão de rede TCP/IP. Você deve ter isso em mente, porque os arquivos que podem ser acessados em uma máquina cliente podem não ser acessíveis (ou podem ser acessíveis apenas com um nome de arquivo diferente) na máquina do servidor de banco de dados.\n\nO servidor PostgreSQL pode lidar com múltiplas conexões simultâneas de clientes. Para alcançar isso, ele inicia (“forks”) um novo processo para cada conexão. A partir desse ponto, o cliente e o novo processo servidor se comunicam sem intervenção do processo postgres original. Assim, o processo servidor supervisor está sempre em execução, aguardando conexões de clientes, enquanto os processos de clientes e servidores associados vêm e vão. (Tudo isso, é claro, é invisível para o usuário. Só mencionamos isso aqui para completude.)\n\n## Criando um Banco de Dados\n\nO primeiro teste para verificar se você pode acessar o servidor de banco de dados é tentar criar um banco de dados. Um servidor PostgreSQL em execução pode gerenciar vários bancos de dados. Tipicamente, um banco de dados separado é usado para cada projeto ou usuário.\n\nPara isso vamos entrar dentro do nosso cliente `pgAdmin 4`\n\nTambém podemos nos conectar em servidores remoto, ex: `Render`\n\n## Criando nosso Schema\n\n![Northwind database](https://github.com/pthom/northwind_psql/raw/master/ER.png)\n\nPara este projeto, vamos utilizar um script SQL simples que preencherá um banco de dados com o famoso exemplo [Northwind](https://github.com/pthom/northwind_psql), adaptado para o PostgreSQL. Esse script configurará o banco de dados Northwind no PostgreSQL, criando todas as tabelas necessárias e inserindo dados de exemplo para que você possa começar a trabalhar imediatamente com consultas e análises SQL em um contexto prático. Este banco de dados de exemplo é uma ótima ferramenta para aprender e praticar as operações e técnicas de SQL, especialmente útil para entender como manipular dados relacionais em um ambiente realista.\n\n## Primeiros comandos\n\nVamos agora para um guia introdutório para operações básicas de SQL utilizando o banco de dados Northwind. Cada comando SQL será explicado com uma breve introdução para ajudar no entendimento e aplicação prática.\n\n#### Exemplo de Seleção Completa\n\nPara selecionar todos os dados de uma tabela:\n\n```sql\n-- Exibe todos os registros da tabela Customers\nSELECT * FROM customers;\n```\n\n#### Seleção de Colunas Específicas\n\nPara selecionar colunas específicas:\n\n```sql\n-- Exibe o nome de contato e a cidade dos clientes\nSELECT contact_name, city FROM customers;\n```\n\n#### Utilizando DISTINCT\n\nPara selecionar valores distintos:\n\n```sql\n-- Lista todos os países dos clientes\nSELECT country FROM customers;\n-- Lista os países sem repetição\nSELECT DISTINCT country FROM customers;\n-- Conta quantos países únicos existem\nSELECT COUNT(DISTINCT country) FROM customers;\n```\n\n#### Cláusula WHERE\n\nPara filtrar registros:\n\n```sql\n-- Seleciona todos os clientes do México\nSELECT * FROM customers WHERE country'Mexico';\n-- Seleciona clientes com ID específico\nSELECT * FROM customers WHERE customer_id'ANATR';\n-- Utiliza AND para múltiplos critérios\nSELECT * FROM customers WHERE country'Germany' AND city'Berlin';\n-- Utiliza OR para mais de uma cidade\nSELECT * FROM customers WHERE city'Berlin' OR city'Aachen';\n-- Utiliza NOT para excluir a Alemanha\nSELECT * FROM customers WHERE country<>'Germany';\n-- Combina AND, OR e NOT\nSELECT * FROM customers WHERE country'Germany' AND (city'Berlin' OR city'Aachen');\n-- Exclui clientes da Alemanha e EUA\nSELECT * FROM customers WHERE country<>'Germany' AND country<>'USA';\n```\n\n#### ORDER BY\n\nPara ordenar os resultados:\n\n```sql\n-- Ordena clientes pelo país\nSELECT * FROM customers ORDER BY country;\n-- Ordena por país em ordem descendente\nSELECT * FROM customers ORDER BY country DESC;\n-- Ordena por país e nome do contato\nSELECT * FROM customers ORDER BY country, contact_name;\n-- Ordena por país em ordem ascendente e nome em ordem descendente\nSELECT * FROM customers ORDER BY country ASC, contact_name DESC;\n```\n\n#### Utilizando LIKE e IN\nPara busca por padrões e listas de valores:\n\n```sql\n-- Clientes com nome de contato começando por \"a\"\nSELECT * FROM customers WHERE contact_name LIKE 'a%';\n-- Clientes com nome de contato não começando por \"a\"\nSELECT * FROM customers WHERE contact_name NOT LIKE 'a%';\n-- Clientes de países específicos\nSELECT * FROM customers WHERE country IN ('Germany', 'France', 'UK');\n-- Clientes não localizados em 'Germany', 'France', 'UK'\nSELECT * FROM customers WHERE country NOT IN ('Germany', 'France', 'UK');\n```\n\n#### Desafio\n\n- Instalar o Postgres\n- Criar o projeto Northwind local\n- Realizar todos os comandos acima\n\n\n",
        "Bootcamp - SQL e Analytics/Aula-02/README.md\n\n# Aula 02 - SQL para Analytics: Nossas primeiras consultas\n\n## Objetivo\n\nRealizar nossas primeiras consultas no banco Northwind\n\n## Uma tangente antes de realizarmos nossas primeiras consultas\n\nSQL, ou Structured Query Language, é uma linguagem de programação projetada para gerenciar dados armazenados em um sistema de gerenciamento de banco de dados relacional (RDBMS). SQL possui vários componentes, cada um responsável por diferentes tipos de tarefas e operações que podem ser executadas em um banco de dados. Esses componentes incluem DDL, DML, DCL, e DQL, entre outros. Aqui está um resumo de cada um deles:\n\nCada componente da linguagem SQL tem um papel fundamental na gestão e no uso de bancos de dados, e diferentes tipos de profissionais de tecnologia podem utilizar esses comandos para desempenhar suas funções específicas. Vamos detalhar quem geralmente é responsável por cada tipo de comando e qual o objetivo de cada um dos componentes mencionados (DDL, DML, DQL, DCL, TCL):\n\n### 1. DDL (Data Definition Language)\n\nO DDL ou Linguagem de Definição de Dados é usado para definir e modificar a estrutura do banco de dados e seus objetos, como tabelas, índices, restrições, esquemas, entre outros. Comandos DDL incluem:\n\n* **CREATE**: Usado para criar novos objetos no banco de dados, como tabelas, índices, funções, vistas, triggers, etc.\n* **ALTER**: Modifica a estrutura de um objeto existente no banco de dados, por exemplo, adicionando uma coluna a uma tabela ou alterando características de uma coluna existente.\n* **DROP**: Remove objetos do banco de dados.\n* **TRUNCATE**: Remove todos os registros de uma tabela, liberando o espaço ocupado por esses registros.\n\n* **Responsável**: Administradores de banco de dados (DBAs) e desenvolvedores de banco de dados.\n* **Objetivo**: O DDL é usado para criar e modificar a estrutura do banco de dados e de seus objetos. Esses comandos ajudam a definir como os dados são organizados, armazenados, e como as relações entre eles são estabelecidas. Eles são essenciais durante a fase de design do banco de dados e quando são necessárias mudanças na estrutura.\n\n### 2. DML (Data Manipulation Language)\n\nO DML ou Linguagem de Manipulação de Dados é usado para gerenciar dados dentro dos objetos (como tabelas). Inclui comandos para inserir, modificar e deletar dados:\n\n* **INSERT**: Insere dados em uma tabela.\n* **UPDATE**: Altera dados existentes em uma tabela.\n* **DELETE**: Remove dados de uma tabela.\n* **MERGE**: Uma operação que permite inserir, atualizar ou deletar registros em uma tabela com base em um conjunto de condições determinadas.\n\n* **Responsável**: Desenvolvedores de software, analistas de dados e, ocasionalmente, usuários finais através de interfaces que executam comandos DML por trás dos panos.\n* **Objetivo**: O DML é crucial para o gerenciamento dos dados dentro das tabelas. Ele é utilizado para inserir, atualizar, deletar e manipular dados armazenados. Analistas de dados podem usar DML para preparar conjuntos de dados para análise, enquanto os desenvolvedores o utilizam para implementar a lógica de negócios.\n\n### 3. DQL (Data Query Language)\n\nO DQL ou Linguagem de Consulta de Dados é fundamentalmente usado para realizar consultas nos dados. O comando mais conhecido na DQL é o **SELECT**, que é utilizado para recuperar dados de uma ou mais tabelas.\n\n* **Responsável**: Analistas de dados, cientistas de dados, e qualquer usuário que necessite extrair informações do banco de dados.\n* **Objetivo**: O DQL é usado para consultar e recuperar dados. É fundamental para gerar relatórios, realizar análises, e fornecer dados que ajudem na tomada de decisões. O comando `SELECT`, parte do DQL, é um dos mais usados e é essencial para qualquer tarefa que requer visualização ou análise de dados.\n\n### 4. DCL (Data Control Language)\n\nO DCL ou Linguagem de Controle de Dados inclui comandos relacionados à segurança na acessibilidade dos dados no banco de dados. Isso envolve comandos para conceder e revogar permissões de acesso:\n\n* **GRANT**: Concede permissões de acesso aos usuários.\n* **REVOKE**: Remove permissões de acesso.\n\n* **Responsável**: Administradores de banco de dados.\n* **Objetivo**: O DCL é usado para configurar permissões em um banco de dados, garantindo que apenas usuários autorizados possam acessar, modificar, ou administrar os dados. Isso é crucial para a segurança e a governança de dados, protegendo informações sensíveis e mantendo a integridade do sistema.\n\n### 5. TCL (Transaction Control Language)\n\nO TCL ou Linguagem de Controle de Transação é usado para gerenciar transações no banco de dados. Transações são importantes para manter a integridade dos dados e garantir que operações múltiplas sejam concluídas com sucesso ou não sejam realizadas de todo:\n\n* **COMMIT**: Confirma uma transação, tornando todas as mudanças permanentes no banco de dados.\n* **ROLLBACK**: Desfaz todas as mudanças feitas durante a transação atual.\n* **SAVEPOINT**: Define um ponto na transação que pode ser usado para um rollback parcial.\n\n* **Responsável**: Desenvolvedores de software e administradores de banco de dados.\n* **Objetivo**: O TCL é usado para gerenciar transações no banco de dados, garantindo que as operações sejam completadas com sucesso ou revertidas em caso de erro. Isso é essencial para manter a consistência e integridade dos dados, especialmente em ambientes onde múltiplas transações ocorrem simultaneamente.\n\nEssa separação de responsabilidades ajuda a manter a organização e eficiência das operações do banco de dados, além de garantir que as ações executadas em um ambiente de banco de dados sejam seguras e alinhadas com as necessidades da organização.\n\n## Se olharmos os comandos que fizemos ontem...\n\n1) Esse comando é de qual subconjunto?\n\n```sql\nSELECT * FROM customers WHERE country'Mexico';\n```\n\n2) Esse comando é de qual subconjunto?\n\n```sql\nINSERT INTO customers VALUES ('ALFKI', 'Alfreds Futterkiste', 'Maria Anders', 'Sales Representative', 'Obere Str. 57', 'Berlin', NULL, '12209', 'Germany', '030-0074321', '030-0076545');\nINSERT INTO customers VALUES ('ANATR', 'Ana Trujillo Emparedados y helados', 'Ana Trujillo', 'Owner', 'Avda. de la Constitución 2222', 'México D.F.', NULL, '05021', 'Mexico', '(5) 555-4729', '(5) 555-3745');\nINSERT INTO customers VALUES ('ANTON', 'Antonio Moreno Taquería', 'Antonio Moreno', 'Owner', 'Mataderos  2312', 'México D.F.', NULL, '05023', 'Mexico', '(5) 555-3932', NULL);\nINSERT INTO customers VALUES ('AROUT', 'Around the Horn', 'Thomas Hardy', 'Sales Representative', '120 Hanover Sq.', 'London', NULL, 'WA1 1DP', 'UK', '(171) 555-7788', '(171) 555-6750');\nINSERT INTO customers VALUES ('BERGS', 'Berglunds snabbköp', 'Christina Berglund', 'Order Administrator', 'Berguvsvägen  8', 'Luleå', NULL, 'S-958 22', 'Sweden', '0921-12 34 65', '0921-12 34 67');\n```\n\n3) Esse comando é de qual subconjunto?\n\n```sql\nCREATE TABLE suppliers (\n    supplier_id smallint NOT NULL,\n    company_name character varying(40) NOT NULL,\n    contact_name character varying(30),\n    contact_title character varying(30),\n    address character varying(60),\n    city character varying(15),\n    region character varying(15),\n    postal_code character varying(10),\n    country character varying(15),\n    phone character varying(24),\n    fax character varying(24),\n    homepage text\n);\n```\n\n4) Esse comando é de qual subconjunto?\n\n```sql\nSET statement_timeout  0;\nSET lock_timeout  0;\nSET client_encoding  'UTF8';\nSET standard_conforming_strings  on;\nSET check_function_bodies  false;\nSET client_min_messages  warning;\n```\n\n## Agora vamos para nossas primeiras QUERY? (Data Query Language)\n\nData Query Language (DQL) é um subconjunto da linguagem SQL (Structured Query Language) utilizado especificamente para consultar dados em bancos de dados. DQL é fundamental para extrair informações, realizar análises e gerar relatórios a partir dos dados armazenados em um sistema de gerenciamento de banco de dados relacional (RDBMS). O principal comando em DQL é o `SELECT`, que é amplamente utilizado para selecionar dados de uma ou mais tabelas.\n\n**Objetivos da DQL**\n\nO principal objetivo da DQL é permitir que usuários e aplicações recuperem dados de forma eficiente e precisa de um banco de dados. DQL proporciona a flexibilidade para especificar exatamente quais dados são necessários, como devem ser filtrados, agrupados, ordenados e transformados. Isso torna a DQL uma ferramenta essencial para:\n\n* **Análise de dados**: Extrair conjuntos de dados para análise e tomada de decisão baseada em evidências.\n* **Geração de relatórios**: Criar relatórios detalhados que ajudam as organizações a entender o desempenho operacional e estratégico.\n* **Visualização de dados**: Alimentar ferramentas de visualização com dados que ajudam a representar informações complexas de maneira compreensível.\n* **Auditoria e monitoramento**: Acompanhar e revisar operações e transações para conformidade e segurança.\n\n**Como começar com DQL**\n\nPara começar a usar DQL, é essencial ter um conhecimento básico de SQL e entender a estrutura dos dados dentro do banco de dados com o qual você está trabalhando. Aqui estão alguns passos para começar:\n\n1. **Entenda o esquema do banco de dados**: Conheça as tabelas, colunas, tipos de dados e relações entre as tabelas.\n2. **Aprenda os fundamentos do comando `SELECT`**: Comece com consultas simples para selecionar colunas específicas de uma tabela.\n3. **Use cláusulas para refinar suas consultas**:\n    * **WHERE**: Para filtrar registros.\n    * **GROUP BY**: Para agrupar registros.\n    * **HAVING**: Para filtrar grupos.\n    * **ORDER BY**: Para ordenar os resultados.\n4. **Pratique com dados de exemplo**: Use um banco de dados de exemplo para praticar suas consultas e testar diferentes cenários.\n\n**Principais comandos da DQL**\n\n* **SELECT**: O comando mais fundamental em DQL, usado para selecionar dados de uma ou mais tabelas.\n    \n    ```sql\n    SELECT * FROM customers;\n    select contact_name, city from customers;\n    ```\n    \n* **DISTINCT**: Usado com `SELECT` para retornar apenas valores distintos.\n    \n    ```sql\n    select country from customers;\n    select distinct country from customers;\n    select count(distinct country) from customers;\n    ```\n\n* **WHERE**: Usado para filtrar.\n\n```sql\n-- Seleciona todos os clientes do México\nSELECT * FROM customers WHERE country'Mexico';\n-- Seleciona clientes com ID específico\nSELECT * FROM customers WHERE customer_id'ANATR';\n-- Utiliza AND para múltiplos critérios\nSELECT * FROM customers WHERE country'Germany' AND city'Berlin';\n-- Utiliza OR para mais de uma cidade\nSELECT * FROM customers WHERE city'Berlin' OR city'Aachen';\n-- Utiliza NOT para excluir a Alemanha\nSELECT * FROM customers WHERE country<>'Germany';\n-- Combina AND, OR e NOT\nSELECT * FROM customers WHERE country'Germany' AND (city'Berlin' OR city'Aachen');\n-- Exclui clientes da Alemanha e EUA\nSELECT * FROM customers WHERE country<>'Germany' AND country<>'USA';\n```\n\n### Mais operadores\n\nOs operadores de comparação no SQL são essenciais para filtrar registros em consultas com base em condições específicas. Vamos examinar cada um dos operadores que você mencionou (`<`, `>`, `<`, `>`, `<>`) com exemplos práticos. Suponhamos que temos uma tabela chamada `products` com uma coluna `unit_price` para o preço dos produtos e uma coluna `units_in_stock` para o número de itens em estoque.\n\n### Operador `<` (Menor que)\n\n```sql\n-- Seleciona todos os produtos com preço menor que 20\nSELECT * FROM products\nWHERE unit_price < 20;\n```\n\n### Operador `>` (Maior que)\n\n```sql\n-- Seleciona todos os produtos com preço maior que 100\nSELECT * FROM products\nWHERE unit_price > 100;\n```\n\n### Operador `<` (Menor ou igual a)\n\n```sql\n-- Seleciona todos os produtos com preço menor ou igual a 50\nSELECT * FROM products\nWHERE unit_price < 50;\n```\n\n### Operador `>` (Maior ou igual a)\n\n```sql\n-- Seleciona todos os produtos com quantidade em estoque maior ou igual a 10\nSELECT * FROM products\nWHERE units_in_stock > 10;\n```\n\n### Operador `<>` (Diferente de)\n\n```sql\n-- Seleciona todos os produtos cujo preço não é 30\nSELECT * FROM products\nWHERE unit_price <> 30;\n```\n\n### Combinação de Operadores\n\nVocê também pode combinar vários operadores em uma única consulta para criar condições mais específicas:\n\n```sql\n-- Seleciona todos os produtos com preço entre 50 e 100 (exclusive)\nSELECT * FROM products\nWHERE unit_price > 50 AND unit_price < 100;\n```\n\n```sql\n-- Seleciona todos os produtos com preço fora do intervalo 20 a 40\nSELECT * FROM products\nWHERE unit_price < 20 OR unit_price > 40;\n```\n\n* **Is null and is not null**: Usado em conjunto com o `where` para criar regras mais complexas de filtro nos registros.\n\n```sql\nSELECT * FROM customers\nWHERE contact_name is Null;\n\nSELECT * FROM customers\nWHERE contact_name is not null;\n```\n\n* **LIKE**\n\n```SQL\n-- Nome do cliente começando com \"a\":\nSELECT * FROM customers\nWHERE contact_name LIKE 'a%';\n```\n\nPara tratar as strings como maiúsculas ou minúsculas em uma consulta SQL, você pode usar as funções `UPPER()` ou `LOWER()`, respectivamente. Essas funções convertem todas as letras em uma string para maiúsculas ou minúsculas, permitindo que você faça comparações de forma mais flexível, ignorando a diferença entre maiúsculas e minúsculas.\n\nAqui está como você pode modificar a consulta para encontrar todos os clientes cujo nome começa com a letra \"a\", independentemente de ser maiúscula ou minúscula:\n\n### Para encontrar nomes que começam com \"a\" em maiúscula ou minúscula:\n\n```sql\nSELECT * FROM customers\nWHERE LOWER(contact_name) LIKE 'a%';\n```\n\nEssa consulta converte todo o `contact_name` para minúsculas antes de fazer a comparação, o que torna a busca insensível a maiúsculas e minúsculas.\n\n### Para encontrar nomes que começam com \"A\" em maiúscula:\n\n```sql\nSELECT * FROM customers\nWHERE UPPER(contact_name) LIKE 'A%';\n```\n\nEssa consulta converte todo o `contact_name` para maiúsculas antes de fazer a comparação, garantindo que apenas os nomes que começam com \"A\" maiúscula sejam selecionados.\n\nUsar `UPPER()` ou `LOWER()` é uma prática comum para garantir que as condições aplicadas em campos de texto não sejam afetadas por diferenças de capitalização nas entradas de dados.\n\n```sql\n-- Nome do cliente terminando com \"a\":\nSELECT * FROM customers\nWHERE contact_name LIKE '%a';\n\n-- Nome do cliente que possui \"or\" em qualquer posição:\nSELECT * FROM customers\nWHERE contact_name LIKE '%or%';\n\n-- Nome do cliente com \"r\" na segunda posição:\nSELECT * FROM customers\nWHERE contact_name LIKE '_r%';\n\n-- Nome do cliente que começa com \"A\" e tem pelo menos 3 caracteres de comprimento:\nSELECT * FROM customers\nWHERE contact_name LIKE 'A_%_%';\n\n-- Nome do contato que começa com \"A\" e termina com \"o\":\nSELECT * FROM customers\nWHERE contact_name LIKE 'A%o';\n\n-- Nome do cliente que NÃO começa com \"a\":\nSELECT * FROM customers\nWHERE contact_name NOT LIKE 'A%';\n\n-- Usando o curinga [charlist] (SQL server)\nSELECT * FROM customers\nWHERE city LIKE '[BSP]%';\n\n-- Usando o curinga Similar To (Postgres)\nSELECT * FROM customers\nWHERE city SIMILAR TO '(B|S|P)%';\n\n-- Usando o MySQL (coitado, tem nada)\nSELECT * FROM customers\nWHERE (city LIKE 'B%' OR city LIKE 'S%' OR city LIKE 'P%');\n```\n\n* **Operador IN**\n\n```sql\n-- localizado na \"Alemanha\", \"França\" ou \"Reino Unido\":\nSELECT * FROM customers\nWHERE country IN ('Germany', 'France', 'UK');\n\n-- NÃO localizado na \"Alemanha\", \"França\" ou \"Reino Unido\":\nSELECT * FROM customers\nWHERE country NOT IN ('Germany', 'France', 'UK');\n\n-- Só para dar um gostinho de uma subqueyr... Seleciona todos os clientes que são dos mesmos países que os fornecedores:\n\nSELECT * FROM customers\nWHERE country IN (SELECT country FROM suppliers);\n\n-- Exemplo com BETWEEN\nSELECT * FROM products\nWHERE unit_price BETWEEN 10 AND 20;\n\n-- Exemplo com NOT BETWEEN\nSELECT * FROM products\nWHERE unit_price NOT BETWEEN 10 AND 20;\n\n-- Seleciona todos os produtos com preço ENTRE 10 e 20. Adicionalmente, não mostra produtos com CategoryID de 1, 2 ou 3:\nSELECT * FROM products\nWHERE (unit_price BETWEEN 10 AND 20) AND category_id NOT IN (1, 2, 3);\n```\n\n```sql\n--selects todos os produtos entre 'Carnarvon Tigers' e 'Mozzarella di Giovanni':\nselect * from products\nwhere product_name between 'Carnarvon Tigers' and 'Mozzarella di Giovanni'\norder by product_name;\n\n--Selecione todas as ordens BETWEEN '04-July-1996' e '09-July-1996':\nselect * from orders\nwhere order_date between '07/04/1996' and '07/09/1996';\n```\n\n* **Tangente sobre diferentes bancos**\n\nO comando SQL que você mencionou é específico para PostgreSQL e não necessariamente padrão em todos os SGBDs (Sistemas de Gerenciamento de Banco de Dados). Cada SGBD pode ter funções e formatos de data ligeiramente diferentes. No entanto, a estrutura básica do comando `SELECT` e a cláusula `WHERE` usando `BETWEEN` são bastante universais.\n\nAqui estão algumas variantes para outros SGBDs populares:\n\n### SQL Server\n\nPara formatar datas em SQL Server, você usaria a função `CONVERT` ou `FORMAT` (a partir do SQL Server 2012):\n\n```sql\n-- Usando CONVERT\nSELECT CONVERT(VARCHAR, order_date, 120) FROM orders\nWHERE order_date BETWEEN '1996-04-07' AND '1996-09-07';\n\n-- Usando FORMAT\nSELECT FORMAT(order_date, 'yyyy-MM-dd') FROM orders\nWHERE order_date BETWEEN '1996-04-07' AND '1996-09-07';\n```\n\n### MySQL\n\nMySQL utiliza a função `DATE_FORMAT` para formatar datas:\n\n```sql\nSELECT DATE_FORMAT(order_date, '%Y-%m-%d') FROM orders\nWHERE order_date BETWEEN '1996-04-07' AND '1996-09-07';\n```\n\n### Oracle\n\nOracle também usa a função `TO_CHAR` como PostgreSQL para formatação de datas:\n\n```sql\nSELECT TO_CHAR(order_date, 'YYYY-MM-DD') FROM orders\nWHERE order_date BETWEEN TO_DATE('1996-04-07', 'YYYY-MM-DD') AND TO_DATE('1996-09-07', 'YYYY-MM-DD');\n```\n\n### SQLite\n\nSQLite não tem uma função dedicada para formatar datas, mas você pode usar funções de string para manipular formatos de data padrão:\n\n```sql\nSELECT strftime('%Y-%m-%d', order_date) FROM orders\nWHERE order_date BETWEEN '1996-04-07' AND '1996-09-07';\n```\n\n* **Funções Agregadas** (COUNT, MAX, MIN, SUM, AVG): Usadas para realizar cálculos em um conjunto de valores.\n\nAs funções agregadas são uma ferramenta fundamental na linguagem SQL, utilizadas para realizar cálculos sobre um conjunto de valores e retornar um único valor resultante. Essas funções são especialmente úteis em operações que envolvem a análise estatística de dados, como a obtenção de médias, somas, valores máximos e mínimos, entre outros. Ao operar em conjuntos de dados, as funções agregadas permitem extrair insights significativos, suportar decisões de negócios, e simplificar dados complexos em informações gerenciáveis.\n    \nAs funções agregadas geralmente são usadas em consultas SQL com a cláusula GROUP BY, que agrupa linhas que têm os mesmos valores em colunas especificadas. No entanto, podem ser usadas sem GROUP BY para resumir todos os dados de uma tabela. Aqui estão as principais funções agregadas e como são aplicadas:\n\n```sql\n-- Exemplo de MIN()\nSELECT MIN(unit_price) AS preco_minimo\nFROM products;\n\n-- Exemplo de MAX()\nSELECT MAX(unit_price) AS preco_maximo\nFROM products;\n\n-- Exemplo de COUNT()\nSELECT COUNT(*) AS total_de_produtos\nFROM products;\n\n-- Exemplo de AVG()\nSELECT AVG(unit_price) AS preco_medio\nFROM products;\n\n-- Exemplo de SUM()\nSELECT SUM(quantity) AS quantidade_total_de_order_details\nFROM order_details;\n```\n\n### Práticas Recomendadas\n\n* **Precisão de dados**: Ao usar `AVG()` e `SUM()`, esteja ciente do tipo de dados da coluna para evitar imprecisões, especialmente com dados flutuantes.\n* **NULLs**: Lembre-se de que a maioria das funções agregadas ignora valores `NULL`, exceto `COUNT(*)`, que conta todas as linhas, incluindo aquelas com valores `NULL`.\n* **Performance**: Em tabelas muito grandes, operações agregadas podem ser custosas em termos de desempenho. Considere usar índices adequados ou realizar pré-agregações quando aplicável.\n* **Clareza**: Ao usar `GROUP BY`, assegure-se de que todas as colunas não agregadas na sua cláusula `SELECT` estejam incluídas na cláusula `GROUP BY`.\n\n### Exemplo de MIN() com GROUP BY\n\n```sql\n-- Calcula o menor preço unitário de produtos em cada categoria\nSELECT category_id, MIN(unit_price) AS preco_minimo\nFROM products\nGROUP BY category_id;\n```\n\n### Exemplo de MAX() com GROUP BY\n\n```sql\n-- Calcula o maior preço unitário de produtos em cada categoria\nSELECT category_id, MAX(unit_price) AS preco_maximo\nFROM products\nGROUP BY category_id;\n```\n\n### Exemplo de COUNT() com GROUP BY\n\n```sql\n-- Conta o número total de produtos em cada categoria\nSELECT category_id, COUNT(*) AS total_de_produtos\nFROM products\nGROUP BY category_id;\n```\n\n### Exemplo de AVG() com GROUP BY\n\n```sql\n-- Calcula o preço médio unitário de produtos em cada categoria\nSELECT category_id, AVG(unit_price) AS preco_medio\nFROM products\nGROUP BY category_id;\n```\n\n### Exemplo de SUM() com GROUP BY\n\n```sql\n-- Calcula a quantidade total de produtos pedidos por pedido\nSELECT order_id, SUM(quantity) AS quantidade_total_por_pedido\nFROM order_details\nGROUP BY order_id;\n```\n\n* **Desafio**\n\n1. Obter todas as colunas das tabelas Clientes, Pedidos e Fornecedores\n\n```sql\nSELECT * FROM customers;\nSELECT * FROM orders;\nSELECT * FROM suppliers;\n```\n\n2. Obter todos os Clientes em ordem alfabética por país e nome\n\n```sql\nSELECT *\nFROM customers\nORDER BY country, contact_name;\n```\n\n3. Obter os 5 pedidos mais antigos\n\n```sql\nSELECT * \nFROM orders \nORDER BY order_date\nLIMIT 5;\n```\n\n4. Obter a contagem de todos os Pedidos feitos durante 1997\n\n```sql\nSELECT COUNT(*) AS \"Number of Orders During 1997\"\nFROM orders\nWHERE order_date BETWEEN '1997-1-1' AND '1997-12-31';\n```\n\n5. Obter os nomes de todas as pessoas de contato onde a pessoa é um gerente, em ordem alfabética\n\n```sql\nSELECT contact_name\nFROM customers\nWHERE contact_title LIKE '%Manager%'\nORDER BY contact_name;\n```\n\n6. Obter todos os pedidos feitos em 19 de maio de 1997\n\n```sql\nSELECT *\nFROM orders\nWHERE order_date  '1997-05-19';\n```\n\n",
        "Bootcamp - SQL e Analytics/Aula-02/desafio.sql\n\n-- 1. Obter todas as colunas das tabelas Clientes, Pedidos e Fornecedores\n\n-- 2. Obter todos os Clientes em ordem alfabética por país e nome\n\n-- 3. Obter os 5 pedidos mais antigos\n\n-- 4. Obter a contagem de todos os Pedidos feitos durante 1997\n\n-- 5. Obter os nomes de todas as pessoas de contato onde a pessoa é um gerente, em ordem alfabética\n\n-- 6. Obter todos os pedidos feitos em 19 de maio de 1997\n\n\nBootcamp - SQL e Analytics/Aula-03/README.md\n\n# Aula 03 - SQL para Analytics: Join and Having in SQL\n\n## Introdução aos Joins em SQL\n\nJoins em SQL são fundamentais para combinar registros de duas ou mais tabelas em um banco de dados com base em uma condição comum, geralmente uma chave estrangeira. Essa técnica permite que dados relacionados, que são armazenados em tabelas separadas, sejam consultados juntos de forma eficiente e coerente. \n\nOs joins são essenciais para consultar dados complexos e para aplicações em que a normalização do banco de dados resulta em distribuição de informações por diversas tabelas.\n\nExistem vários tipos de joins, cada um com seu uso específico dependendo das necessidades da consulta:\n\n1. **Inner Join**: Retorna registros que têm correspondência em ambas as tabelas.\n2. **Left Join (ou Left Outer Join)**: Retorna todos os registros da tabela esquerda e os registros correspondentes da tabela direita. Se não houver correspondência, os resultados da tabela direita terão valores `NULL`.\n3. **Right Join (ou Right Outer Join)**: Retorna todos os registros da tabela direita e os registros correspondentes da tabela esquerda. Se não houver correspondência, os resultados da tabela esquerda terão valores `NULL`.\n4. **Full Join (ou Full Outer Join)**: Retorna registros quando há uma correspondência em uma das tabelas. Se não houver correspondência, ainda assim, o resultado aparecerá com `NULL` nos campos da tabela sem correspondência.\n\n### 1. Criar um relatório para todos os pedidos de 1996 e seus clientes\n\n**Inner Join**\n\n**Uso**: Utilizado quando você precisa de registros que têm correspondência exata em ambas as tabelas. \n\n**Exemplo Prático**: Se quisermos encontrar todos os pedidos de 1996 e os detalhes dos clientes que fizeram esses pedidos, usamos um Inner Join. Isso garante que só obteremos os pedidos que possuem um cliente correspondente e que foram feitos em 1996.\n\n```sql\n-- Cria um relatório para todos os pedidos de 1996 e seus clientes (152 linhas)\nSELECT *\nFROM orders o\nINNER JOIN customers c ON o.customer_id  c.customer_id\nWHERE EXTRACT(YEAR FROM o.order_date)  1996; -- EXTRACT(part FROM date) part pode ser YEAR, MONTH, DAY, etc\n```\n\nGustavo trouxe tambem essa implementacao\n\n'WHERE DATE_PART('YEAR', o.order_date)  1996'\n\nNo SQL server pode usar o \n\n'WHERE YEAR(o.order_date)  1996'\n\nNo contexto da sua consulta, o uso de EXTRACT na cláusula WHERE serve especificamente para aplicar um filtro nos dados retornados pelo SELECT, garantindo que apenas registros do ano de 1996 sejam incluídos no conjunto de resultados. Este é um uso legítimo e comum de EXTRACT para manipulação de condições baseadas em datas dentro de cláusulas WHERE.\n\n### 2. Criar um relatório que mostra o número de funcionários e clientes de cada cidade que tem funcionários\n\n**Left Join**\n\n**Uso**: Usado quando você quer todos os registros da primeira (esquerda) tabela, com os correspondentes da segunda (direita) tabela. Se não houver correspondência, a segunda tabela terá campos `NULL`. \n\n**Exemplo Prático**: Se precisarmos listar todas as cidades onde temos funcionários, e também queremos saber quantos clientes temos nessas cidades, mesmo que não haja clientes, usamos um Left Join.\n\n```sql\n-- Cria um relatório que mostra o número de funcionários e clientes de cada cidade que tem funcionários (5 linhas)\nSELECT e.city AS cidade, \n       COUNT(DISTINCT e.employee_id) AS numero_de_funcionarios, \n       COUNT(DISTINCT c.customer_id) AS numero_de_clientes\nFROM employees e \nLEFT JOIN customers c ON e.city  c.city\nGROUP BY e.city\nORDER BY cidade;\n```\n\n### Exemplo de Resultados da Consulta\n\n| cidade | numero_de_funcionarios | numero_de_clientes |\n| --- | --- | --- |\n| Kirkland | 1 | 1 |\n| London | 4 | 6 |\n| Redmond | 1 | 0 |\n| Seattle | 2 | 1 |\n| Tacoma | 1 | 0 |\n\n### Descrição da Tabela\n\n* **cidade**: O nome da cidade onde os funcionários e clientes estão localizados.\n* **numero_de_funcionarios**: Contagem dos funcionários distintos nessa cidade. Este número vem diretamente da tabela `employees`.\n* **numero_de_clientes**: Contagem dos clientes distintos que têm a mesma cidade que os funcionários. Se não houver clientes em uma cidade onde há funcionários, o número será 0.\n\n### Explicação Detalhada\n\n* **Kirkland**: Tem um equilíbrio entre o número de funcionários e clientes, com ambos os valores sendo 1. Isso indica uma correspondência direta entre locais de funcionários e clientes.\n* **London**: Apresenta uma maior concentração tanto de funcionários quanto de clientes, com mais clientes (6) do que funcionários (4), indicando uma forte presença de ambos na cidade.\n* **Redmond**: Tem 1 funcionário, mas nenhum cliente registrado nesta cidade, sugerindo que, embora a empresa tenha presença laboral aqui, não há clientes registrados.\n* **Seattle**: Tem 2 funcionários e apenas 1 cliente, mostrando uma presença menor de clientes em relação aos funcionários.\n* **Tacoma**: Similar a Redmond, tem funcionários (1) mas nenhum cliente, o que pode indicar uma área onde a empresa opera, mas ainda não estabeleceu uma base de clientes.\n\nEssa análise é particularmente útil para entender como os recursos humanos da empresa (funcionários) estão distribuídos em relação à sua base de clientes em diferentes locais. Isso pode ajudar a identificar cidades onde a empresa pode precisar intensificar esforços de aquisição de clientes ou avaliar a eficácia de suas operações e estratégias de mercado locais.\n\n\n### 3. Criar um relatório que mostra o número de funcionários e clientes de cada cidade que tem clientes\n\n**Right Join**\n\n**Uso**: É o inverso do Left Join e é menos comum. Usado quando queremos todos os registros da segunda (direita) tabela e os correspondentes da primeira (esquerda) tabela. \n\n**Exemplo Prático**: Para listar todas as cidades onde temos clientes, e também contar quantos funcionários temos nessas cidades, usamos um Right Join.\n\n```sql\n-- Cria um relatório que mostra o número de funcionários e clientes de cada cidade que tem clientes (69 linhas)\nSELECT c.city AS cidade, \n       COUNT(DISTINCT c.customer_id) AS numero_de_clientes, \n       COUNT(DISTINCT e.employee_id) AS numero_de_funcionarios\nFROM employees e \nRIGHT JOIN customers c ON e.city  c.city\nGROUP BY c.city\nORDER BY cidade;\n```\n\n### Diferenças Principais do `RIGHT JOIN`\n\n1. **Foco na Tabela à Direita**: Ao contrário do `LEFT JOIN` que foca na tabela à esquerda, o `RIGHT JOIN` garante que todos os registros da tabela à direita (neste caso, `customers`) estejam presentes no resultado. Se não houver correspondência na tabela à esquerda (`employees`), as colunas relacionadas desta tabela aparecerão como `NULL`.\n    \n2. **Exibição de Dados Não Correspondentes**: Como mostrado, o `RIGHT JOIN` pode exibir linhas onde não há correspondência na tabela à esquerda, o que é útil para identificar dados que estão apenas na tabela à direita. No contexto de um negócio, isso pode destacar áreas (ou dados) que requerem atenção, como clientes em locais onde a empresa não tem funcionários representados.\n    \n3. **Utilização Estratégica para Análise de Dados**: O `RIGHT JOIN` é menos comum que o `LEFT JOIN` porque muitas vezes as tabelas são organizadas de modo que a tabela mais importante (ou abrangente) seja colocada à esquerda da consulta. No entanto, o `RIGHT JOIN` é útil quando a tabela à direita é prioritária e queremos garantir que todos os seus registros sejam analisados.\n\n\n### 4. Criar um relatório que mostra o número de funcionários e clientes de cada cidade\n\n* **Análise Completa de Dados**: O `FULL JOIN` é útil quando você precisa de uma visão completa dos dados em duas tabelas relacionadas, especialmente para identificar onde os dados estão faltando em uma ou ambas as tabelas.\n* **Relatórios Abrangentes**: Permite criar relatórios que mostram todas as possíveis relações entre duas tabelas, incluindo onde as relações não existem.\n* **Análise de Lacunas de Dados**: Ajuda a identificar lacunas nos dados de ambas as tabelas simultaneamente, facilitando análises de cobertura e consistência entre conjuntos de dados.\n\n**Full Join**\n\n**Uso**: Utilizado quando queremos a união de Left Join e Right Join, mostrando todos os registros de ambas as tabelas, e preenchendo com `NULL` onde não há correspondência. \n\n**Exemplo Prático**: Para listar todas as cidades onde temos clientes ou funcionários, e contar ambos em cada cidade, usamos um Full Join.\n\n```sql\n-- Cria um relatório que mostra o número de funcionários e clientes de cada cidade (71 linhas)\nSELECT\n\tCOALESCE(e.city, c.city) AS cidade,\n\tCOUNT(DISTINCT e.employee_id) AS numero_de_funcionarios,\n\tCOUNT(DISTINCT c.customer_id) AS numero_de_clientes\nFROM employees e \nFULL JOIN customers c ON e.city  c.city\nGROUP BY e.city, c.city\nORDER BY cidade;\n```\n\nEsta consulta retorna uma lista de todas as cidades conhecidas por ambas as tabelas, junto com a contagem de funcionários e clientes em cada cidade. Aqui estão alguns cenários possíveis no resultado:\n\n### Análise do Resultado\n\nO resultado do `FULL JOIN` mostra:\n\n* A maioria das cidades listadas tem clientes, mas não funcionários (indicado por \"0\" no número de funcionários).\n* Em algumas cidades, como \"Kirkland\", \"Redmond\", \"Seattle\", e \"Tacoma\", há funcionários e/ou clientes, mostrando correspondência direta entre as tabelas.\n* Notavelmente, em cidades como \"London\" e \"Madrid\", o número de clientes é significativamente maior do que o de funcionários, o que pode indicar centros de alta atividade de clientes sem uma proporção correspondente de suporte de funcionários.\n\n### Observações Importantes\n\n1. **Cidades com apenas Clientes**: A maioria das cidades no resultado possui clientes, mas não funcionários. Isso pode sugerir que a empresa tem uma ampla base de clientes geograficamente, mas uma distribuição mais limitada de sua força de trabalho.\n    \n2. **Cidades com Funcionários e sem Clientes**: Cidades como \"Redmond\" e \"Tacoma\" têm funcionários, mas nenhuma contagem de clientes listada, indicando que há operações da empresa sem correspondente atividade de clientes registrada nesses locais.\n    \n3. **Concentrações de Clientes e Funcionários**: Em cidades como \"London\", \"Seattle\", e \"Sao Paulo\", há uma concentração significativa de clientes e alguma presença de funcionários, sugerindo centros operacionais ou mercados importantes para a empresa.\n    \n4. **Ausência de Dados em Algumas Cidades**: Algumas cidades têm zero funcionários e clientes, indicando que pode haver um erro de dados, cidades listadas incorretamente, ou simplesmente que não há atividade de funcionários ou clientes registrados nesses locais.\n    \n\n### Implicações Estratégicas\n\nA partir desses dados, a empresa poderia considerar várias ações estratégicas:\n\n* **Expansão de Funcionários**: Investir em recursos humanos nas cidades com altos números de clientes, mas baixa presença de funcionários, para melhorar o suporte e a satisfação do cliente.\n    \n* **Análise de Mercado**: Realizar uma análise mais aprofundada sobre por que certas cidades têm alta atividade de clientes e ajustar as estratégias de marketing e vendas conforme necessário.\n    \n* **Revisão de Dados**: Verificar a precisão dos dados para entender melhor as discrepâncias ou ausências nas contagens de funcionários e clientes.\n    \nEste exemplo realça o valor de usar `FULL JOIN` para obter uma visão completa da relação entre duas variáveis críticas (funcionários e clientes) e como essa informação pode ser usada para insights estratégicos.\n\n## Having\n\n### 1. Criar um relatório que mostra a quantidade total de produtos (da tabela order_details)\n\n```sql\n-- Cria um relatório que mostra a quantidade total de produtos encomendados.\n-- Mostra apenas registros para produtos para os quais a quantidade encomendada é menor que 200 (5 linhas)\nSELECT o.product_id, p.product_name, SUM(o.quantity) AS quantidade_total\nFROM order_details o\nJOIN products p ON p.product_id  o.product_id\nGROUP BY o.product_id, p.product_name\nHAVING SUM(o.quantity) < 200\nORDER BY quantidade_total DESC;\n```\n\n### 2. Criar um relatório que mostra o total de pedidos por cliente desde 31 de dezembro de 1996\n\n```sql\n-- Cria um relatório que mostra o total de pedidos por cliente desde 31 de dezembro de 1996.\n-- O relatório deve retornar apenas linhas para as quais o total de pedidos é maior que 15 (5 linhas)\nSELECT customer_id, COUNT(order_id) AS total_de_pedidos\nFROM orders\nWHERE order_date > '1996-12-31'\nGROUP BY customer_id\nHAVING COUNT(order_id) > 15\nORDER BY total_de_pedidos;\n```\n\n### Explicação das Consultas Convertidas\n\n**Consulta 1:**\n\n* **Seleção e Junção**: A consulta seleciona o `product_id` e `product_name` da tabela `products` e junta com a tabela `order_details` pelo `product_id`.\n* **Agrupamento e Filtragem**: Os dados são agrupados por `product_id` e `product_name`, e a função agregada `SUM(o.quantity)` calcula a quantidade total de cada produto encomendado. A cláusula `HAVING` é usada para filtrar produtos cuja quantidade total encomendada é menor que 200.\n\n**Consulta 2:**\n\n* **Filtragem de Data**: A consulta filtra os pedidos realizados após 31 de dezembro de 1996.\n* **Agrupamento e Contagem**: Agrupa os pedidos pelo `customer_id` e conta o número de pedidos feitos por cada cliente usando `COUNT(order_id)`.\n* **Filtragem de Resultados**: Utiliza a cláusula `HAVING` para incluir apenas os clientes que fizeram mais de 15 pedidos desde a data especificada.\n\nBootcamp - SQL e Analytics/Aula-03/desafio.sql\n\n-- 1. Cria um relatório para todos os pedidos de 1996 e seus clientes (152 linhas)\n\n-- 2. Cria um relatório que mostra o número de funcionários e clientes de cada cidade que tem funcionários (5 linhas)\n\n-- 3. Cria um relatório que mostra o número de funcionários e clientes de cada cidade que tem clientes (69 linhas)\n\n-- 4.Cria um relatório que mostra o número de funcionários e clientes de cada cidade (71 linhas)\n\n-- 5. Cria um relatório que mostra a quantidade total de produtos encomendados.\n-- Mostra apenas registros para produtos para os quais a quantidade encomendada é menor que 200 (5 linhas)\n\n-- 6. Cria um relatório que mostra o total de pedidos por cliente desde 31 de dezembro de 1996.\n-- O relatório deve retornar apenas linhas para as quais o total de pedidos é maior que 15 (5 linhas)\n\n\n",
        "Bootcamp - SQL e Analytics/Aula-04/README.md\n\n# Aula 04 - Windows Function\n\nDocumentação Postgres:  https://www.postgresql.org/docs/current/functions-window.html\n\n## Com o que vimos até aqui: Group By\n\nCom SQL que vimos até agora conseguimos dois tipos de resultado: todas as linhas (com ou sem filtro/where) ou linhas agrupadas (group by)\n\nCálcular:\nQuantos produtos únicos existem?\nQuantos produtos no total?\nQual é o valor total pago?\n\n```sql\nSELECT order_id,\n       COUNT(order_id) AS unique_product,\n       SUM(quantity) AS total_quantity,\n       SUM(unit_price * quantity) AS total_price\nFROM order_details\nGROUP BY order_id\nORDER BY order_id;\n```\n\n## Com Windows Function\n\nAs `Windows Function` permitem uma análise de dados eficiente e precisa, ao possibilitar cálculos dentro de `partições ou linhas específicas`. Elas são cruciais para tarefas como classificação, agregação e análise de tendências em consultas SQL.\n\nEssas funções são aplicadas a cada linha de um conjunto de resultados, e utilizam uma cláusula `OVER()` para determinar como cada linha é processada dentro de uma \"janela\", permitindo controle sobre o comportamento da função dentro de um grupo de dados ordenados.\n\nWindow Functions Syntax componentes\n```sql\nwindow_function_name(arg1, arg2, ...) OVER (\n  [PARTITION BY partition_expression, ...]\n  [ORDER BY sort_expression [ASC | DESC], ...]\n)\n```\n\n* **window_function_name**: Este é o nome da função de janela que você deseja usar, como SUM, RANK, LEAD, etc.\n\n* **arg1, arg2, ...:** Estes são os argumentos que você passa para a função de janela, se ela exigir algum. Por exemplo, para a função SUM, você especificaria a coluna que deseja somar.\n\n* **OVER**: Principal conceito das windows functions, ele que cria essa \"Janela\" onde fazem nossos cálculos\n\n* **PARTITION BY:** Esta cláusula opcional divide o conjunto de resultados em partições ou grupos. A função de janela opera independentemente dentro de cada partição.\n\n* **ORDER BY:** Esta cláusula opcional especifica a ordem em que as linhas são processadas dentro de cada partição. Você pode especificar a ordem ascendente (ASC) ou descendente (DESC).\n\n```sql\nSELECT DISTINCT order_id,\n   COUNT(order_id) OVER (PARTITION BY order_id) AS unique_product,\n   SUM(quantity) OVER (PARTITION BY order_id) AS total_quantity,\n   SUM(unit_price * quantity) OVER (PARTITION BY order_id) AS total_price\nFROM order_details\nORDER BY order_id;\n```\n\n## MIN (), MAX (), AVG ()\n\nQuais são os valores mínimo, máximo e médio de frete pago por cada cliente? (tabela orders)\n\n### Usando Group by\n\n```sql\nSELECT customer_id,\n   MIN(freight) AS min_freight,\n   MAX(freight) AS max_freight,\n   AVG(freight) AS avg_freight\nFROM orders\nGROUP BY customer_id\nORDER BY customer_id;\n```\n\n### Detalhes da Consulta Ajustada:\n\n* **`customer_id`**: Seleciona o identificador único do cliente da tabela `orders`.\n* **`MIN(freight) AS min_freight`**: Calcula o valor mínimo de frete para cada cliente.\n* **`MAX(freight) AS max_freight`**: Calcula o valor máximo de frete para cada cliente.\n* **`AVG(freight) AS avg_freight`**: Calcula o valor médio de frete para cada cliente.\n\n### Explicação:\n\n* A função `MIN` extrai o menor valor de frete registrado para cada cliente.\n* A função `MAX` obtém o maior valor de frete registrado para cada cliente.\n* A função `AVG` fornece o valor médio de frete por cliente, útil para entender o custo médio de envio associado a cada um.\n* `GROUP BY customer_id` agrupa os registros por `customer_id`, permitindo que as funções agregadas calculem seus resultados para cada grupo de cliente.\n* `ORDER BY customer_id` garante que os resultados sejam apresentados em ordem crescente de `customer_id`, facilitando a leitura e a análise dos dados.\n\n### Usando Windows Function\n\n```sql\nSELECT DISTINCT customer_id,\n   MIN(freight) OVER (PARTITION BY customer_id) AS min_freight,\n   MAX(freight) OVER (PARTITION BY customer_id) AS max_freight,\n   AVG(freight) OVER (PARTITION BY customer_id) AS avg_freight\nFROM orders\nORDER BY customer_id;\n```\n\n### Explicação da Consulta Ajustada:\n\n* **`customer_id`**: Seleciona o identificador único do cliente da tabela `orders`.\n* **`MIN(freight) OVER (PARTITION BY customer_id)`**: Utiliza a função de janela `MIN` para calcular o valor mínimo de frete para cada grupo de registros que têm o mesmo `customer_id`.\n* **`MAX(freight) OVER (PARTITION BY customer_id)`**: Utiliza a função de janela `MAX` para calcular o valor máximo de frete para cada `customer_id`.\n* **`AVG(freight) OVER (PARTITION BY customer_id)`**: Utiliza a função de janela `AVG` para calcular o valor médio de frete para cada `customer_id`.\n\n### Características das Funções de Janela:\n\n* **Funções de Janela (`OVER`)**: As funções de janela permitem que você execute cálculos sobre um conjunto de linhas relacionadas a cada entrada. Ao usar o `PARTITION BY customer_id`, a função de janela é reiniciada para cada novo `customer_id`. Isso significa que cada cálculo de `MIN`, `MAX`, e `AVG` é confinado ao conjunto de ordens de cada cliente individualmente.\n* **`DISTINCT`**: A cláusula `DISTINCT` é utilizada para garantir que cada `customer_id` apareça apenas uma vez nos resultados finais, juntamente com seus respectivos valores de frete mínimo, máximo e médio. Isso é necessário porque as funções de janela calculam valores para cada linha, e sem `DISTINCT`, cada `customer_id` poderia aparecer múltiplas vezes se houver várias ordens por cliente.\n\n## Colapso\n\nPara ilustrar como a cláusula `GROUP BY` influencia os resultados de uma consulta SQL e por que ela pode \"colapsar\" as linhas para uma única linha por grupo, vou dar um exemplo baseado nas funções de agregação `MIN`, `MAX`, e `AVG` que discutimos anteriormente. Essas funções são frequentemente usadas para calcular estatísticas resumidas dentro de cada grupo especificado por `GROUP BY`.\n\n### Exemplo sem GROUP BY\n\nConsidere a seguinte consulta sem usar `GROUP BY`:\n\n```sql \n-- 830 linhas\nSELECT customer_id, freight\nFROM orders;\n```\n\nEssa consulta simplesmente seleciona o `customer_id` e o `freight` de cada ordem. Se houver múltiplas ordens para cada cliente, cada ordem aparecerá como uma linha separada no conjunto de resultados.\n\n### Exemplo com GROUP BY\n\nAgora, vamos adicionar `GROUP BY` e funções de agregação:\n\n```sql\n-- 89 linhas\nSELECT customer_id,\n       MIN(freight) AS min_freight,\n       MAX(freight) AS max_freight,\n       AVG(freight) AS avg_freight\nFROM orders\nGROUP BY customer_id\nORDER BY customer_id;\n```\n\n### O que acontece aqui:\n\n* **`GROUP BY customer_id`**: Esta cláusula agrupa todas as entradas na tabela `orders` que têm o mesmo `customer_id`. Para cada grupo, a consulta calcula os valores mínimo, máximo e médio de `freight`.\n    \n* **Agregações (`MIN`, `MAX`, `AVG`)**: Cada uma dessas funções de agregação opera sobre o conjunto de `freight` dentro do grupo especificado pelo `customer_id`. Apenas um valor para cada função de agregação é retornado por grupo.\n    \n\n### Por que \"colapsa\" as linhas:\n\n* Quando usamos `GROUP BY`, a consulta não retorna mais uma linha para cada entrada na tabela `orders`. Em vez disso, ela retorna uma linha para cada grupo de `customer_id`, onde cada linha contém o `customer_id` e os valores agregados de `freight` para esse grupo. Isso significa que se um cliente tem várias ordens, você não verá cada ordem individualmente; em vez disso, você verá uma linha resumida com as estatísticas de frete para todas as ordens desse cliente.\n\n### Limitação do SELECT com GROUP BY:\n\n* Se você tentar selecionar uma coluna que não está incluída na cláusula `GROUP BY` e que não é uma expressão agregada, a consulta falhará. Por exemplo, a consulta a seguir resultará em erro porque `order_date` não está em uma função agregada nem no `GROUP BY`:\n\n```sql\nSELECT customer_id, order_date, AVG(freight) AS avg_freight\nFROM orders\nGROUP BY customer_id;\n```\n\n### Mensagem de Erro Típica:\n\n* Em muitos sistemas de gerenciamento de banco de dados, como PostgreSQL ou MySQL, essa consulta resultaria em um erro como: \"column \"orders.order_date\" must appear in the GROUP BY clause or be used in an aggregate function\".\n\nEste exemplo mostra claramente como o `GROUP BY` \"colapsa\" as linhas em grupos, permitindo cálculos resumidos, mas também impõe restrições sobre quais colunas podem ser selecionadas diretamente.\n\nPara ilustrar como evitar o \"colapso\" das linhas utilizando funções de janela (window functions) em vez de `GROUP BY`, vamos utilizar as mesmas estatísticas de frete (mínimo, máximo e médio) por cliente, mas manter todas as linhas de pedidos individuais visíveis no conjunto de resultados. As funções de janela permitem calcular agregações enquanto ainda se mantém cada linha distinta na saída.\n\n### Consulta com Funções de Janela\n\nAqui está como você pode escrever uma consulta que utiliza funções de janela para calcular o frete mínimo, máximo e médio para cada cliente sem colapsar as linhas:\n\n```sql\nSELECT \n    customer_id,\n    order_id,  -- Mantendo a visibilidade de cada pedido\n    freight,\n    MIN(freight) OVER (PARTITION BY customer_id) AS min_freight,\n    MAX(freight) OVER (PARTITION BY customer_id) AS max_freight,\n    AVG(freight) OVER (PARTITION BY customer_id) AS avg_freight\nFROM orders\nORDER BY customer_id, order_id;\n```\n\n### Explicação da Consulta\n\n* **Seleção de Colunas**: `customer_id`, `order_id`, e `freight` são selecionados diretamente, o que mantém cada linha de pedido individual visível no resultado.\n* **Funções de Janela**: `MIN(freight) OVER`, `MAX(freight) OVER`, e `AVG(freight) OVER` são aplicadas com a cláusula `PARTITION BY customer_id`. Isso significa que as estatísticas de frete são calculadas para cada grupo de `customer_id`, mas a aplicação é feita sem agrupar as linhas em um único resultado por cliente. Cada linha no conjunto de resultados original mantém sua identidade única.\n* **`PARTITION BY customer_id`**: Assegura que as funções de janela são recalculadas para cada cliente. Cada pedido mantém sua linha, mas agora também inclui as informações agregadas de frete específicas para o cliente ao qual o pedido pertence.\n* **`ORDER BY customer_id, order_id`**: Ordena os resultados primeiro por `customer_id` e depois por `order_id`, facilitando a leitura dos dados.\n\n### Vantagens das Funções de Janela\n\n* **Preservação de Dados Detalhados**: Ao contrário do `GROUP BY`, que agrega e reduz os dados a uma linha por grupo, as funções de janela mantêm cada linha individual do conjunto de dados original visível. Isso é útil para análises detalhadas onde você precisa ver tanto os valores agregados quanto os dados de linha individual.\n* **Flexibilidade**: Você pode calcular múltiplas métricas de agregação em diferentes partições dentro da mesma consulta sem múltiplas passagens pelos dados ou subconsultas complexas.\n\nEste método é especialmente útil em relatórios e análises detalhadas onde tanto o contexto agregado quanto os detalhes de cada evento individual (neste caso, cada pedido) são importantes para uma compreensão completa dos dados.\n\nFunções de classificação de janela no SQL são um conjunto de ferramentas valiosas usadas para atribuir classificações, posições ou números sequenciais às linhas dentro de um conjunto de resultados com base em critérios específicos.\n\nElas são aplicadas em vários cenários, como criar leaderboards, classificar produtos por vendas, identificar os melhores desempenhos ou acompanhar mudanças ao longo do tempo. Essas funções são ferramentas poderosas para obter insights e tomar decisões informadas na análise de dados.\n\n### 2.1 RANK(), DENSE_RANK() e ROW_NUMBER()\n\n* **RANK()**: Atribui um rank único a cada linha, deixando lacunas em caso de empates.\n* **DENSE_RANK()**: Atribui um rank único a cada linha, com ranks contínuos para linhas empatadas.\n* **ROW_NUMBER()**: Atribui um número inteiro sequencial único a cada linha, independentemente de empates, sem lacunas.\n\n### Exemplo: Classificação dos produtos mais venvidos POR order ID\n\nex: o mesmo produto pode ficar em primeiro por ter vendido muito por ORDER e depois ficar em segundo por ter vendido muito por ORDER\n\n```sql\nSELECT  \n  o.order_id, \n  p.product_name, \n  (o.unit_price * o.quantity) AS total_sale,\n  ROW_NUMBER() OVER (ORDER BY (o.unit_price * o.quantity) DESC) AS order_rn, \n  RANK() OVER (ORDER BY (o.unit_price * o.quantity) DESC) AS order_rank, \n  DENSE_RANK() OVER (ORDER BY (o.unit_price * o.quantity) DESC) AS order_dense\nFROM  \n  order_details o\nJOIN \n  products p ON p.product_id  o.product_id;\n```\n\n### Explicação da Consulta\n\n* **Seleção de Dados**: A consulta seleciona o `order_id`, `product_name` da tabela `products`, e calcula `total_sale` como o produto de `unit_price` e `quantity` da tabela `order_details`.\n    \n* **Funções de Classificação**:\n    \n    * **`ROW_NUMBER()`**: Atribui um número sequencial a cada linha baseada no total de vendas (`total_sale`), ordenado do maior para o menor. Cada linha recebe um número único dentro do conjunto de resultados inteiro.\n    * **`RANK()`**: Atribui um rank a cada linha baseado no `total_sale`, onde linhas com valores iguais recebem o mesmo rank, e o próximo rank disponível considera os empates (por exemplo, se dois itens compartilham o primeiro lugar, o próximo item será o terceiro).\n    * **`DENSE_RANK()`**: Funciona de forma similar ao `RANK()`, mas os ranks subsequentes não têm lacunas. Se dois itens estão empatados no primeiro lugar, o próximo item será o segundo.\n* **`JOIN`**: A junção entre `order_details` e `products` é feita pelo `product_id`, permitindo que o nome do produto seja incluído nos resultados baseados nos IDs correspondentes em ambas as tabelas.\n    \n## Este relatório apresenta o ID de cada pedido juntamente com o total de vendas e a classificação percentual e a distribuição cumulativa do valor de cada venda em relação ao valor total das vendas para o mesmo pedido. Esses cálculos são realizados com base no preço unitário e na quantidade de produtos vendidos em cada pedido.\n\n### Exemplo: Classificação dos produtos mais venvidos usnado SUB QUERY\n\n```sql\nSELECT  \n  sales.product_name, \n  total_sale,\n  ROW_NUMBER() OVER (ORDER BY total_sale DESC) AS order_rn, \n  RANK() OVER (ORDER BY total_sale DESC) AS order_rank, \n  DENSE_RANK() OVER (ORDER BY total_sale DESC) AS order_dense\nFROM (\n  SELECT \n    p.product_name, \n    SUM(o.unit_price * o.quantity) AS total_sale\n  FROM  \n    order_details o\n  JOIN \n    products p ON p.product_id  o.product_id\n  GROUP BY p.product_name\n) AS sales\nORDER BY sales.product_name;\n```\n\n### Utilidade da Consulta\n\nEsta consulta é útil para análises de vendas, onde é necessário identificar os produtos mais vendidos, bem como sua classificação em termos de receita gerada. Ela permite que os analistas vejam rapidamente quais produtos geram mais receita e como eles se classificam em relação uns aos outros, facilitando decisões estratégicas relacionadas a estoque, promoções e planejamento de vendas.\n\n### Funções PERCENT_RANK() e CUME_DIST()\n\nAmbos retornam um valor entre 0 e 1\n\n* **PERCENT_RANK()**: Calcula o rank relativo de uma linha específica dentro do conjunto de resultados como uma porcentagem. É computado usando a seguinte fórmula:\n    * RANK é o rank da linha dentro do conjunto de resultados.\n    * N é o número total de linhas no conjunto de resultados.\n    * PERCENT_RANK  (RANK - 1) / (N - 1)\n* **CUME_DIST()**: Calcula a distribuição acumulada de um valor no conjunto de resultados. Representa a proporção de linhas que são menores ou iguais à linha atual. A fórmula é a seguinte:\n    * CUME_DIST  (Número de linhas com valores < linha atual) / (Número total de linhas)\n\nAmbas as funções PERCENT_RANK() e CUME_DIST() são valiosas para entender a distribuição e posição de pontos de dados dentro de um conjunto de dados, particularmente em cenários onde você deseja comparar a posição de um valor específico com a distribuição geral de dados.\n\n```sql\nSELECT  \n  order_id, \n  unit_price * quantity AS total_sale,\n  ROUND(CAST(PERCENT_RANK() OVER (PARTITION BY order_id \n    ORDER BY (unit_price * quantity) DESC) AS numeric), 2) AS order_percent_rank,\n  ROUND(CAST(CUME_DIST() OVER (PARTITION BY order_id \n    ORDER BY (unit_price * quantity) DESC) AS numeric), 2) AS order_cume_dist\nFROM  \n  order_details;\n```\n\n### Explicação da Consulta Ajustada:\n\n* **Seleção de Dados**: A consulta seleciona o `order_id` e calcula `total_sale` como o produto de `unit_price` e `quantity`.\n* **Funções de Janela**:\n    * **`PERCENT_RANK()`**: Aplicada com uma partição por `order_id` e ordenada pelo `total_sale` de forma descendente, calcula a posição percentual de cada venda em relação a todas as outras no mesmo pedido.\n    * **`CUME_DIST()`**: Similarmente, calcula a distribuição acumulada das vendas, indicando a proporção de vendas que não excedem o `total_sale` da linha atual dentro de cada pedido.\n* **Arredondamento**: Os resultados de `PERCENT_RANK()` e `CUME_DIST()` são arredondados para duas casas decimais para facilitar a interpretação.\n\nEsta consulta é útil para análises detalhadas de desempenho de vendas dentro de pedidos, permitindo que gestores e analistas identifiquem rapidamente quais itens contribuem mais\n\nA função NTILE() no SQL é usada para dividir o conjunto de resultados em um número especificado de partes aproximadamente iguais ou \"faixas\" e atribuir um número de grupo ou \"bucket\" a cada linha com base em sua posição dentro do conjunto de resultados ordenado.\n\n```sql\nNTILE(n) OVER (ORDER BY coluna)\n```\n\n* **n**: O número de faixas ou grupos que você deseja criar.\n* **ORDER BY coluna**: A coluna pela qual você deseja ordenar o conjunto de resultados antes de aplicar a função NTILE().\n\n### Exemplo: Listar funcionários dividindo-os em 3 grupos\n\n```sql\nSELECT first_name, last_name, title,\n   NTILE(3) OVER (ORDER BY first_name) AS group_number\nFROM employees;\n```\n\n### Explicação da Consulta Ajustada:\n\n* **Seleção de Dados**: A consulta seleciona `first_name`, `last_name` e `title` da tabela `employees`.\n* **NTILE(3) OVER (ORDER BY first_name)**: Aplica a função NTILE para dividir os funcionários em 3 grupos baseados na ordem alfabética de seus primeiros nomes. Cada funcionário receberá um número de grupo (`group_number`) que indica a qual dos três grupos ele pertence.\n\nEsta consulta é útil para análises que requerem a distribuição equitativa dos dados em grupos especificados, como para balanceamento de cargas de trabalho, análises segmentadas, ou mesmo para fins de relatórios onde a divisão em grupos facilita a visualização e o entendimento dos dados.\n\nLAG(), LEAD()\n\n* **LAG()**: Permite acessar o valor da linha anterior dentro de um conjunto de resultados. Isso é particularmente útil para fazer comparações com a linha atual ou identificar tendências ao longo do tempo.\n* **LEAD()**: Permite acessar o valor da próxima linha dentro de um conjunto de resultados, possibilitando comparações com a linha subsequente.\n\n### Exemplo: Ordenando os custos de envio pagos pelos clientes de acordo com suas datas de pedido:\n\n```sql\nSELECT \n  customer_id, \n  TO_CHAR(order_date, 'YYYY-MM-DD') AS order_date, \n  shippers.company_name AS shipper_name, \n  LAG(freight) OVER (PARTITION BY customer_id ORDER BY order_date DESC) AS previous_order_freight, \n  freight AS order_freight, \n  LEAD(freight) OVER (PARTITION BY customer_id ORDER BY order_date DESC) AS next_order_freight\nFROM \n  orders\nJOIN \n  shippers ON shippers.shipper_id  orders.ship_via;\n```\n\n* **LEAD() e LAG(): Estas funções de janela são usadas para acessar dados de linhas anteriores ou subsequentes dentro de uma partição definida, muito úteis para comparar o valor de frete entre ordens consecutivas de um mesmo cliente.\n\n\nBootcamp - SQL e Analytics/Aula-04/desafio.sql\n\n-- Faça a classificação dos produtos mais venvidos usando usando RANK(), DENSE_RANK() e ROW_NUMBER()\n-- Essa questão tem 2 implementações, veja uma que utiliza subquery e uma que não utiliza.\n-- Tabelas utilizadasFROM order_details o JOIN products p ON p.product_id  o.product_id;\n\n-- Listar funcionários dividindo-os em 3 grupos usando NTILE\n-- FROM employees;\n\n-- Ordenando os custos de envio pagos pelos clientes de acordo \n-- com suas datas de pedido, mostrando o custo anterior e o custo posterior usando LAG e LEAD:\n-- FROM orders JOIN shippers ON shippers.shipper_id  orders.ship_via;\n\n-- Desafio extra: questão intrevista Google\n-- https://medium.com/@aggarwalakshima/interview-question-asked-by-google-and-difference-among-row-number-rank-and-dense-rank-4ca08f888486#:~:textROW_NUMBER()%20always%20provides%20unique,a%20continuous%20sequence%20of%20ranks.\n-- https://platform.stratascratch.com/coding/10351-activity-rank?code_type3\n-- https://www.youtube.com/watch?vdb-qdlp8u3o\n\nBootcamp - SQL e Analytics/Aula-05/README.md\n\n## Aula 05 - Projeto de Análise de dados\n\nVerificar o projeto completo no Github\n\nhttps://github.com/lvgalvao/Northwind-SQL-Analytics/tree/main\n\n\n",
        "Bootcamp - SQL e Analytics/Aula-06/README.md\n\n## Aula 06 - CTE vs Subqueries vs Views vs Temporary Tables vs Materialized Views\n\n[Apostila completa](https://www.linkedin.com/feed/update/urn:li:activity:7190722950499577856/)\n\n1. **CTE (Common Table Expressions):**\n    \n    * **Onde usar:** As CTEs são úteis quando você precisa dividir uma consulta em partes mais gerenciáveis ou quando deseja reutilizar uma subconsulta várias vezes na mesma consulta principal.\n    * **Vantagens:**\n        * Permitem escrever consultas mais legíveis e organizadas, dividindo a lógica em partes distintas.\n        * Podem ser referenciadas várias vezes na mesma consulta.\n    * **Desvantagens:**\n        * Podem não ser tão eficientes quanto outras técnicas, especialmente se a CTE for referenciada várias vezes ou se a consulta for muito complexa.\n\n    ```sql\n    WITH TotalRevenues AS (\n        SELECT \n            customers.company_name, \n            SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total\n        FROM customers\n        INNER JOIN orders ON customers.customer_id  orders.customer_id\n        INNER JOIN order_details ON order_details.order_id  orders.order_id\n        GROUP BY customers.company_name\n    )\n    SELECT * FROM TotalRevenues;\n    ```\n\n2. **Subqueries:**\n    \n    * **Onde usar:** Subqueries são úteis quando você precisa de resultados intermediários para filtrar ou agregar dados em uma consulta principal.\n    * **Vantagens:**\n        * São simples de escrever e entender, especialmente para consultas simples.\n        * Podem ser aninhadas dentro de outras subqueries ou consultas principais.\n    * **Desvantagens:**\n        * Pode tornar consultas complexas difíceis de entender e manter.\n        * Em algumas situações, podem não ser tão eficientes quanto outras técnicas, especialmente se as subqueries forem executadas várias vezes.\n\n    ```sql\n    SELECT product_id FROM (\n\tSELECT product_id \n\tFROM (\n\t\tSELECT product_id, rank\n\t\tFROM (SELECT \n\t\t\t\tproduct_id,\n\t\t\t\tSUM( det.quantity * det.unit_price * ( 1 - det.discount )) sold_value,\n\t\t\t\tRANK() OVER (ORDER BY SUM( det.quantity * det.unit_price * ( 1 - det.discount )) DESC) rank -- WINDOWS FUNCTION\n\t\t\tFROM order_details det\n\t\t\tGROUP BY det.product_id\n\t\t\tORDER BY rank)\n\t\tWHERE rank < 5 )\n\tWHERE product_id BETWEEN 35 and 65 )\n    ORDER BY product_id DESC\n    ```\n\n    Refatorando a subquery acima para CTEs\n\n    ```sql\n    WITH CalculatedValues AS (\n    -- Calcula o valor vendido e o rank para cada produto\n    SELECT \n        product_id,\n        SUM(det.quantity * det.unit_price * (1 - det.discount)) AS sold_value,\n        RANK() OVER (ORDER BY SUM(det.quantity * det.unit_price * (1 - det.discount)) DESC) AS rank\n    FROM order_details det\n    GROUP BY product_id\n    ),\n    TopRankedProducts AS (\n        -- Seleciona apenas os produtos com rank entre os top 5\n        SELECT \n            product_id\n        FROM CalculatedValues\n        WHERE rank < 5\n    ),\n    FilteredProducts AS (\n        -- Filtra os produtos com IDs entre 35 e 65\n        SELECT \n            product_id\n        FROM TopRankedProducts\n        WHERE product_id BETWEEN 35 AND 65\n    )\n    -- Seleciona e ordena os produtos finais\n    SELECT product_id\n    FROM FilteredProducts\n    ORDER BY product_id DESC;\n    ```\n\n3. **Views:**\n    \n    * **Onde usar:** As views são úteis quando você precisa reutilizar uma consulta em várias consultas ou quando deseja simplificar consultas complexas dividindo-as em partes menores.\n    * **Vantagens:**\n        * Permitem abstrair a lógica de consulta complexa em um objeto de banco de dados reutilizável.\n        * Facilitam a segurança, pois você pode conceder permissões de acesso à view em vez das tabelas subjacentes.\n    * **Desvantagens:**\n        * As views não armazenam dados fisicamente, então elas precisam ser reavaliadas sempre que são consultadas, o que pode impactar o desempenho.\n        * Se uma view depende de outras views ou tabelas, a complexidade pode aumentar.\n\n    ```sql\n    CREATE VIEW TotalRevenues AS\n    SELECT \n        customers.company_name, \n        SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total\n    FROM customers\n    INNER JOIN orders ON customers.customer_id  orders.customer_id\n    INNER JOIN order_details ON order_details.order_id  orders.order_id\n    GROUP BY customers.company_name;\n    \n    SELECT * FROM TotalRevenues;\n    ```\n\n    ```sql\n    GRANT SELECT ON TotalRevenues TO user1;\n    ```\n\n4. **Temporary Tables / Staging / Testes ETL :**\n    \n    * **Onde usar:** Tabelas temporárias são úteis quando você precisa armazenar dados temporários para uso em uma sessão de banco de dados ou em uma consulta específica.\n    * **Vantagens:**\n        * Permitem armazenar resultados intermediários de uma consulta complexa para reutilização posterior.\n        * Podem ser indexadas para melhorar o desempenho em consultas subsequentes.\n    * **Desvantagens:**\n        * Podem consumir recursos do banco de dados, especialmente se forem grandes.\n        * Exigem gerenciamento explícito para limpar os dados após o uso.\n\n    ```sql\n    CREATE TEMP TABLE TempTotalRevenues AS\n    SELECT \n        customers.company_name, \n        SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total\n    FROM customers\n    INNER JOIN orders ON customers.customer_id  orders.customer_id\n    INNER JOIN order_details ON order_details.order_id  orders.order_id\n    GROUP BY customers.company_name;\n    \n    SELECT * FROM TempTotalRevenues;\n    ```\n\n5. **Materialized Views / Snapshot:**\n\n    Definição da Oracle: https://oracle-base.com/articles/misc/materialized-views\n    \n    * **Onde usar:** Materialized views são úteis quando você precisa pré-calcular e armazenar resultados de consultas complexas para consultas frequentes ou análises de desempenho.\n    * **Vantagens:**\n        * Permitem armazenar fisicamente os resultados de uma consulta, melhorando significativamente o desempenho em consultas subsequentes.\n        * Reduzem a carga no banco de dados, já que os resultados são pré-calculados e armazenados.\n    * **Desvantagens:**\n        * **Precisam ser atualizadas** regularmente para manter os dados atualizados, o que pode consumir recursos do sistema.\n        * A introdução de dados redundantes pode aumentar os requisitos de armazenamento.\n\n    ```sql\n    CREATE MATERIALIZED VIEW MaterializedTotalRevenues AS\n    SELECT \n        customers.company_name, \n        SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total\n    FROM customers\n    INNER JOIN orders ON customers.customer_id  orders.customer_id\n    INNER JOIN order_details ON order_details.order_id  orders.order_id\n    GROUP BY customers.company_name;\n    \n    SELECT * FROM MaterializedTotalRevenues;\n    ```\n\n    ```sql\n    REFRESH MATERIALIZED VIEW MaterializedTotalRevenues;\n    ```\n\n    ```sql\n    REFRESH MATERIALIZED VIEW CONCURRENTLY MaterializedTotalRevenues\n    ```\n\n* **Performance**\n\n```sql\nWITH TotalRevenues AS (\n    SELECT \n        customers.company_name, \n        SUM(order_details.unit_price * order_details.quantity * (1.0 - order_details.discount)) AS total\n    FROM customers\n    INNER JOIN orders ON customers.customer_id  orders.customer_id\n    INNER JOIN order_details ON order_details.order_id  orders.order_id\n    CROSS JOIN products -- Junção cruzada com a tabela de produtos para aumentar a carga da consulta\n    GROUP BY customers.company_name\n)\nSELECT * FROM TotalRevenues;\n```\n\n```sql\n-- Criação da tabela temporária\nCREATE TEMP TABLE TotalRevenues AS\nSELECT \n    *\nFROM customers\nINNER JOIN orders ON customers.customer_id  orders.customer_id\nINNER JOIN order_details ON order_details.order_id  orders.order_id\nCROSS JOIN products; -- Junção cruzada com a tabela de produtos para aumentar a carga da consulta\n\n-- Consulta na tabela temporária\nSELECT * FROM TotalRevenues;\n```\n\n\nEm resumo, cada técnica tem seu lugar e uso apropriado, dependendo dos requisitos específicos de cada situação. As CTEs e subqueries são úteis para consultas simples ou interações temporárias com os dados, enquanto as views e as tabelas temporárias são mais adequadas para consultas e manipulações de dados mais complexas. As materialized views são ideais para consultas frequentes ou análises de desempenho, onde o desempenho é crucial e os dados podem ser pré-calculados e armazenados fisicamente.\n\n* **Materialized view vs Table**\n\n1. Armazenamento de Dados:\n    \n    * Tabela Normal: Armazena dados fisicamente no banco de dados.\n    * Materialized View: Armazena os resultados de uma consulta como uma tabela física.\n2. Atualização Automática:\n    \n    * Tabela Normal: Os dados são atualizados manual ou automaticamente através de operações de inserção, atualização e exclusão.\n    * Materialized View: Os dados não são atualizados automaticamente. Eles precisam ser atualizados manualmente usando o comando `REFRESH MATERIALIZED VIEW`.\n3. Desempenho:\n    \n    * Tabela Normal: As consultas são executadas diretamente nos dados armazenados na tabela.\n    * Materialized View: As consultas são executadas nos dados armazenados na materialized view, o que pode melhorar o desempenho de consultas complexas ou frequentemente usadas.\n4. Uso de Espaço em Disco:\n    \n    * Tabela Normal: Pode ocupar mais espaço em disco devido ao armazenamento físico de dados.\n    * Materialized View: Pode ocupar menos espaço em disco, pois armazena apenas os resultados da consulta, não os dados brutos.\n5. Flexibilidade:\n    \n    * Tabela Normal: Os dados são atualizados em tempo real e podem ser manipulados diretamente.\n    * Materialized View: Os resultados da consulta são estáticos e precisam ser atualizados manualmente. Eles são usados principalmente para armazenar resultados de consultas complexas que não mudam com frequência.\n\n\n\n* **DROP TEMP TABLE**\n\n    https://www.youtube.com/watch?vvKvnIa6S-nQ&t3682s\n\n    https://www.reddit.com/r/SQL/comments/tg4hei/sql_server_temporary_tables/\n\n    https://www.theknowledgeacademy.com/blog/how-to-create-temp-table-in-sql/#:~:textTo%20add%20data%20to%20a%20Temp%20Table%2C%20you'll%20use,%2C%20column2%2C%20...)\n\n    https://www.reddit.com/r/SQL/comments/tg4hei/sql_server_temporary_tables/\n\nBootcamp - SQL e Analytics/Aula-06/table.sql\n\nSELECT product_id,\n    sum((((quantity)::double precision * unit_price) * ((1)::double precision - discount))) AS sold_value,\n    rank() OVER (ORDER BY (sum((((quantity)::double precision * unit_price) * ((1)::double precision - discount)))) DESC) AS rank\n   FROM order_details det\n  GROUP BY product_id\n\n",
        "Bootcamp - SQL e Analytics/Aula-07/README.md\n\n## Aula 07 - Stored Procedures\n\n* **Stored Procedures vs Views**\n\nAs Views e Stored Procedures são ambos recursos poderosos em bancos de dados relacionais, mas têm propósitos e funcionalidades distintas.\n\n**Views:**\n\n* As Views são abstrações de consulta que permitem aos usuários definir consultas complexas e frequentemente usadas como uma única entidade.\n* Elas são essencialmente consultas SQL pré-definidas que são armazenadas no banco de dados e tratadas como tabelas virtuais.\n* As Views simplificam o acesso aos dados, ocultando a complexidade das consultas subjacentes e fornecendo uma interface consistente para os usuários.\n* Elas são úteis para simplificar consultas frequentes, segmentar permissões de acesso aos dados e abstrair a complexidade do modelo de dados subjacente.\n\n**Stored Procedures:**\n\n* As Stored Procedures são abstrações de transações que consistem em um conjunto de instruções SQL pré-compiladas e armazenadas no banco de dados.\n* Elas são usadas para encapsular operações de banco de dados mais complexas, como atualizações, inserções, exclusões e outras transações.\n* As Stored Procedures podem aceitar parâmetros de entrada e retornar valores de saída, o que as torna altamente flexíveis e reutilizáveis em diferentes partes de um aplicativo.\n* Elas oferecem maior controle sobre as operações de banco de dados e permitem a execução de lógica de negócios no lado do servidor.\n\n## Criando um novo Banco de dados\n\n\nPara ilustrar o processo, vamos estabelecer um novo banco de dados que simula um ambiente bancário convencional.\n\nComeçaremos criando um novo banco de dados. Em seguida, empregaremos os comandos `CREATE TABLE` e `INSERT INTO`.\n\n```sql\nCREATE TABLE IF NOT EXISTS clients (\n    id SERIAL PRIMARY KEY NOT NULL,\n    limite INTEGER NOT NULL,\n    saldo INTEGER NOT NULL\n);\n\nCREATE TABLE IF NOT EXISTS transactions (\n    id SERIAL PRIMARY KEY NOT NULL,\n    tipo CHAR(1) NOT NULL,\n    descricao VARCHAR(10) NOT NULL,\n    valor INTEGER NOT NULL,\n    cliente_id INTEGER NOT NULL,\n    realizada_em TIMESTAMP NOT NULL DEFAULT NOW()\n);\n```\n\nCaso queira usar UUID (para cenários de produção)\n\n```\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\nCREATE TABLE IF NOT EXISTS clients (\n    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    limite INTEGER NOT NULL,\n    saldo INTEGER NOT NULL\n);\n```\n\n\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\nCREATE TABLE IF NOT EXISTS clients (\n    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    limite INTEGER NOT NULL,\n    saldo INTEGER NOT NULL,\n    CHECK (saldo > limite)\n);\n\n\n\n**CREATE TABLE:**\n\n* O comando `CREATE TABLE` é usado para criar uma nova tabela no banco de dados.\n* O `IF NOT EXISTS` é uma cláusula opcional que garante que a tabela só será criada se ainda não existir no banco de dados, evitando erros caso a tabela já exista.\n* Em seguida, é especificada o nome da tabela (`clients` e `transactions` neste caso), seguido por uma lista de colunas e suas definições.\n* Cada coluna é definida com um nome, um tipo de dado e opcionalmente outras restrições, como a definição de uma chave primária (`PRIMARY KEY`) e a obrigatoriedade de não ser nula (`NOT NULL`).\n\n```sql\nINSERT INTO clients (limite, saldo)\nVALUES\n    (10000, 0),\n    (80000, 0),\n    (1000000, 0),\n    (10000000, 0),\n    (500000, 0);\n```\n\n**INSERT INTO:**\n\n* O comando `INSERT INTO` é usado para adicionar novos registros a uma tabela existente.\n* Na cláusula `INSERT INTO`, é especificado o nome da tabela (`clients` neste caso) seguido da lista de colunas entre parênteses, se necessário.\n* Em seguida, a cláusula `VALUES` é usada para especificar os valores a serem inseridos nas colunas correspondentes.\n* Cada linha de valores corresponde a um novo registro a ser inserido na tabela, com os valores na mesma ordem que as colunas foram listadas.\n\nEm resumo, esses comandos são fundamentais para definir a estrutura e inserir dados em tabelas no banco de dados, criando assim a base para armazenar e manipular informações de forma organizada e eficiente.\n\n## Vamos agora simular uma transação bancaria\n\nPara realizar a compra de um Carro, de 80 mil reais, no cliente 1.\n\nVamos realizar esse processo em 2 etapas.\n\nA primeira será um comando de `INSERT INTO` e depois um comando de `UPDATE`\n\n```sql\nINSERT INTO transactions (tipo, descricao, valor, cliente_id)\nVALUES ('d', 'Compra de carro', 80000, 1)\n```\n\n```sql\nUPDATE clients\nSET saldo  saldo + CASE WHEN 'd'  'd' THEN -80000 ELSE 80000 END\nWHERE id  1; -- Substitua pelo ID do cliente desejado\n```\n\n* **Vamos olhar a situação do cliente 1 agora**\n\n```sql\nSELECT saldo, limite \nFROM clients\nWHERE id  1\n```\n\n## Vamos precisar corrigir isso\n\nO comando `DELETE` é uma instrução do SQL usada para remover registros de uma tabela com base em uma condição específica. Ele permite que você exclua dados de uma tabela de banco de dados de forma controlada e precisa. Aqui estão alguns pontos-chave sobre o comando `DELETE`:\n\n1. **Sintaxe Básica**: A sintaxe básica do comando `DELETE` é a seguinte:\n    \n    ```sql\n    DELETE FROM nome_da_tabela\n    WHERE condição;\n    ```\n    \n2. **Cláusula WHERE**: A cláusula `WHERE` é opcional, mas geralmente é usada para especificar quais registros devem ser excluídos. Se não for especificada, todos os registros da tabela serão excluídos.\n    \n3. **Remoção Condicional**: Você pode usar a cláusula `WHERE` para definir uma condição para determinar quais registros serão excluídos. Apenas os registros que atendem a essa condição serão removidos.\n    \n4. **Impacto da Exclusão**: O comando `DELETE` remove permanentemente os registros da tabela, o que significa que os dados excluídos não podem ser recuperados.\n    \n5. **Uso Cauteloso**: É importante usar o comando `DELETE` com cuidado, especialmente sem uma cláusula `WHERE` específica, pois ele pode resultar na exclusão de todos os registros da tabela.\n    \n6. **Transações**: Assim como outros comandos SQL de modificação de dados, como `INSERT` e `UPDATE`, o comando `DELETE` pode ser usado dentro de transações para garantir a consistência e a integridade dos dados.\n    \n\nNo exemplo que você forneceu:\n\n```sql\nDELETE FROM transactions\nWHERE id  1;\n```\n\nEste comando remove o registro da tabela `transactions` onde o `id` é igual a `1`. Isso resultará na exclusão permanente desse registro específico da tabela. Certifique-se sempre de usar o comando `DELETE` com cuidado e de verificar duas vezes a condição antes de executá-lo para evitar a exclusão acidental de dados importantes.\n\n```sql\nDELETE FROM transactions\nWHERE id  1;\n```\n\nVamos precisar voltar também com o saldo atual do cliente, que era de 0.\n\n```sql\nUPDATE clients\nSET saldo  0\nWHERE id  1;\n```\n\n## Como evitar isso? Stored Procedures\n\nStored Procedures são rotinas armazenadas no banco de dados que contêm uma série de instruções SQL e podem ser executadas por aplicativos ou usuários conectados ao banco de dados. Elas oferecem várias vantagens, como:\n\n1. **Reutilização de código:** As stored procedures permitem que blocos de código SQL sejam escritos uma vez e reutilizados em várias partes do aplicativo.\n    \n2. **Desempenho:** Por serem compiladas e armazenadas no banco de dados, as stored procedures podem ser executadas de forma mais eficiente do que várias consultas SQL enviadas separadamente pelo aplicativo.\n    \n3. **Segurança:** As stored procedures podem ajudar a proteger o banco de dados, pois os aplicativos só precisam de permissão para executar a stored procedure, não para acessar diretamente as tabelas.\n    \n4. **Abstração de dados:** Elas podem ser usadas para ocultar a complexidade do modelo de dados subjacente, fornecendo uma interface simplificada para os usuários ou aplicativos.\n    \n5. **Controle de transações:** As stored procedures podem incluir instruções de controle de transações para garantir a integridade dos dados durante operações complexas.\n\nVamos entender cada parte da stored procedure `realizar_transacao`:\n\n1. **Criação da Procedure:**\n    \n    ```sql\n    CREATE OR REPLACE PROCEDURE realizar_transacao(\n        IN p_tipo CHAR(1),\n        IN p_descricao VARCHAR(10),\n        IN p_valor INTEGER,\n        IN p_cliente_id INTEGER\n    )\n    ```\n    \n    * Esta declaração cria ou substitui uma stored procedure chamada `realizar_transacao`.\n    * A procedure tem quatro parâmetros de entrada: `p_tipo`, `p_descricao`, `p_valor` e `p_cliente_id`, cada um com seu tipo de dado especificado.\n2. **Definição da Linguagem:**\n    \n    Sobre a languages na documentação do postgresql tem 4 linguagens padrões  disponíveis: PL/pgSQL (Chapter 43), PL/Tcl (Chapter 44), PL/Perl (Chapter 45), and PL/Python (Chapter 46)\n\n    ```sql\n    LANGUAGE plpgsql\n    ```\n    \n    * Define a linguagem da stored procedure como PL/pgSQL, que é uma linguagem procedural para o PostgreSQL.\n3. **Corpo da Procedure:**\n    \n    ```sql\n    AS $$\n    DECLARE\n        saldo_atual INTEGER;\n        limite_cliente INTEGER;\n    BEGIN\n        -- Corpo da procedure...\n    END;\n    $$;\n    ```\n    \n    * O corpo da stored procedure é definido entre `AS $$` e `$$;`.\n    * Dentro do corpo, declaramos variáveis locais usando `DECLARE`.\n    * A execução da procedure ocorre entre `BEGIN` e `END;`.\n4. **Obtenção de Dados:**\n    \n    ```sql\n    -- Obtém o saldo atual e o limite do cliente\n    SELECT saldo, limite INTO saldo_atual, limite_cliente\n    FROM clients\n    WHERE id  p_cliente_id;\n    ```\n    \n    * Esta parte do código executa uma consulta para obter o saldo atual e o limite do cliente com o ID fornecido.\n5. **Verificação da Transação:**\n    \n    ```sql\n    -- Verifica se a transação é válida com base no saldo e no limite\n    IF p_tipo  'd' AND saldo_atual - p_valor < -limite_cliente THEN\n        RAISE EXCEPTION 'Saldo insuficiente para realizar a transação';\n    END IF;\n    ```\n    \n    * Aqui, é feita uma verificação para garantir que a transação seja válida com base no tipo de transação ('d' para débito) e se o saldo atual menos o valor da transação é menor que o limite de crédito do cliente. Se a condição for verdadeira, uma exceção é lançada.\n6. **Atualização do Saldo:**\n    \n    ```sql\n    -- Atualiza o saldo do cliente\n    UPDATE clients\n    SET saldo  saldo + CASE WHEN p_tipo  'd' THEN -p_valor ELSE p_valor END\n    WHERE id  p_cliente_id;\n    ```\n    \n    * Nesta parte, o saldo do cliente é atualizado com base no tipo de transação. Se for um débito ('d'), o valor é subtraído do saldo atual; caso contrário, é adicionado.\n7. **Inserção da Transação:**\n    \n    ```sql\n    -- Insere uma nova transação\n    INSERT INTO transactions (tipo, descricao, valor, cliente_id)\n    VALUES (p_tipo, p_descricao, p_valor, p_cliente_id);\n    ```\n    \n    * Por fim, uma nova transação é inserida na tabela `transactions` com os detalhes fornecidos.\n\nEssa stored procedure encapsula todo o processo de realização de uma transação bancária, desde a validação do saldo e limite do cliente até a atualização do saldo e a inserção da transação. Ela oferece uma maneira conveniente e segura de executar essas operações de forma consistente e controlada.\n\nPara chamar a stored procedure `realizar_transacao` com os parâmetros fornecidos, você pode executar o seguinte comando SQL no PostgreSQL:\n\n```sql\nCALL realizar_transacao('d', 'carro', 80000, 1);\n```\n\nIsso invocará a procedure `realizar_transacao` com os parâmetros fornecidos:\n\n* `p_tipo`: 'd'\n* `p_descricao`: 'carro'\n* `p_valor`: 80000\n* `p_cliente_id`: 1\n\nCertifique-se de executar esse comando em um ambiente onde a stored procedure `realizar_transacao` esteja definida e acessível.\n\n## Desafio\n\nCriar uma stored procedure \"ver_extrato\" para fornecer uma visão detalhada do extrato bancário de um cliente, incluindo seu saldo atual e as informações das últimas 10 transações realizadas. Esta operação recebe como entrada o ID do cliente e retorna uma mensagem com o saldo atual do cliente e uma lista das últimas 10 transações, contendo o ID da transação, o tipo de transação (depósito ou retirada), uma breve descrição, o valor da transação e a data em que foi realizada.\n\n**Explicação Detalhada:**\n\n1. **Entrada de Parâmetros:**\n    \n    * A stored procedure recebe o ID do cliente como parâmetro de entrada.\n2. **Obtenção do Saldo Atual:**\n    \n    * É realizada uma consulta na tabela \"clients\" para obter o saldo atual do cliente com base no ID fornecido.\n3. **Exibição do Saldo Atual:**\n    \n    * O saldo atual do cliente é exibido por meio de uma mensagem de aviso.\n4. **Obtenção das Últimas 10 Transações:**\n    \n    * É realizada uma consulta na tabela \"transactions\" para obter as últimas 10 transações do cliente, ordenadas pela data de realização em ordem decrescente.\n5. **Exibição das Transações:**\n    \n    * Utilizando um loop `FOR`, cada transação é iterada e suas informações são exibidas por meio de mensagens de aviso.\n    * Para cada transação, são exibidos o ID da transação, o tipo de transação (depósito ou retirada), uma breve descrição da transação, o valor da transação e a data em que foi realizada.\n    * O loop é interrompido após exibir as informações das últimas 10 transações.\n\n```sql\nCREATE OR REPLACE PROCEDURE ver_extrato(\n    IN p_cliente_id INTEGER\n)\nLANGUAGE plpgsql\nAS $$\nDECLARE\n    saldo_atual INTEGER;\n    transacao RECORD;\n    contador INTEGER : 0;\nBEGIN\n    -- Obtém o saldo atual do cliente\n    SELECT saldo INTO saldo_atual\n    FROM clients\n    WHERE id  p_cliente_id;\n\n    -- Retorna o saldo atual do cliente\n    RAISE NOTICE 'Saldo atual do cliente: %', saldo_atual;\n\n    -- Retorna as 10 últimas transações do cliente\n    RAISE NOTICE 'Últimas 10 transações do cliente:';\n    FOR transacao IN\n        SELECT *\n        FROM transactions\n        WHERE cliente_id  p_cliente_id\n        ORDER BY realizada_em DESC\n        LIMIT 10\n    LOOP\n        contador : contador + 1;\n        RAISE NOTICE 'ID: %, Tipo: %, Descrição: %, Valor: %, Data: %', transacao.id, transacao.tipo, transacao.descricao, transacao.valor, transacao.realizada_em;\n        EXIT WHEN contador > 10;\n    END LOOP;\nEND;\n$$;\n```\n\nBootcamp - SQL e Analytics/Aula-07/create_table.sql\n\nCREATE TABLE IF NOT EXISTS clients (\n    id SERIAL PRIMARY KEY NOT NULL,\n    limite INTEGER NOT NULL,\n    saldo INTEGER NOT NULL\n);\n\nCREATE TABLE IF NOT EXISTS transactions (\n    id SERIAL PRIMARY KEY NOT NULL,\n    tipo CHAR(1) NOT NULL,\n    descricao VARCHAR(10) NOT NULL,\n    valor INTEGER NOT NULL,\n    cliente_id INTEGER NOT NULL,\n    realizada_em TIMESTAMP NOT NULL DEFAULT NOW()\n);\n\nINSERT INTO clients (limite, saldo)\nVALUES\n    (100000, 0),\n    (80000, 0),\n    (1000000, 0),\n    (10000000, 0),\n    (500000, 0);\n\n\n\nBootcamp - SQL e Analytics/Aula-07/python.py\n\n@app.post(\"/clientes/{cliente_id}/transacoes\", response_modelschemas.ClienteResponse)\nasync def post_transacao(cliente_id: int, \n                         transacao: schemas.TransactionCreateRequest, \n                         session: AsyncSession  Depends(get_session)):\n    \n    result  await session.execute(\" CALL realizar_transacao(transacao.type, transacao.description, transacao.value, cliente_id)\")\n\n    return cliente\n\nBootcamp - SQL e Analytics/Aula-07/store_procedure.sql\n\nCREATE OR REPLACE PROCEDURE ver_extrato(\n    IN p_cliente_id INTEGER\n)\nLANGUAGE plpgsql\nAS $$\nDECLARE\n    saldo_atual INTEGER;\n    transacao RECORD;\n    contador INTEGER : 0;\nBEGIN\n    -- Obtém o saldo atual do cliente\n    SELECT saldo INTO saldo_atual\n    FROM clients\n    WHERE id  p_cliente_id;\n\n    -- Retorna o saldo atual do cliente\n    RAISE NOTICE 'Saldo atual do cliente: %', saldo_atual;\n\n    -- Retorna as 10 últimas transações do cliente\n    RAISE NOTICE 'Últimas 10 transações do cliente:';\n    FOR transacao IN\n        SELECT *\n        FROM transactions\n        WHERE cliente_id  p_cliente_id\n        ORDER BY realizada_em DESC\n        LIMIT 10\n    LOOP\n        contador : contador + 1;\n        RAISE NOTICE 'ID: %, Tipo: %, Descrição: %, Valor: %, Data: %', transacao.id, transacao.tipo, transacao.descricao, transacao.valor, transacao.realizada_em;\n        EXIT WHEN contador > 10;\n    END LOOP;\nEND;\n$$;\n\n\nBootcamp - SQL e Analytics/Aula-07/store_procedure_com_saldo_transacao.sql\n\nCREATE OR REPLACE PROCEDURE realizar_transacao(\nIN p_tipo CHAT(1),\nIN p_descricao VARCHAR(10),\nIN p_valor INTEGER,\nIN p_cliente_id UUID\n)\nLANGUAGE plpgsql\nAS $$\nDECLARE\n    saldo_atual INTEGER;\n    limite_cliente INTEGER;\n\tsaldo_apos_transacao INTEGER;\nBEGIN\n    SELECT saldo, limite INTO saldo_atual, limite_cliente\n\tFROM clients\n\tWHERE id  p_cliente_id;\n\t\n\tRAISE NOTICE 'Saldo atual do cliente: %', saldo_atual;\n\tRAISE NOTICE 'Limite atual do cliente: %', limite_cliente;\n\t\n\tIF p_tipo  'd' AND saldo_atual - p_valor < - limite_cliente THEN\n\t\tRAISE EXCEPTION 'Limite inferior ao necessario da transacao'\n\tEND IF;\n\t\n\tUPDATE clients\n    SET saldo  saldo + CASE WHEN p_tipo  'd' THEN -p_valor ELSE p_valor END\n    WHERE id  p_cliente_id;\n\n    INSERT INTO transactions (tipo, descricao, valor, cliente_id)\n    VALUES (p_tipo, p_descricao, p_valor, p_cliente_id);\n\t\n\tSELECT saldo INTO saldo_apos_transacao\n\tFROM clients\n\tWHERE id  p_cliente_id;\n\t\n\tRAISE NOTICE 'Saldo cliente apos transacao: %', saldo_apos_transacao;\nEND;\n$$;\n\n",
        "Bootcamp - SQL e Analytics/Aula-08/README.md\n\n## Aula 07 - Stored Procedures\n\n* **Stored Procedures vs Views**\n\nAs Views e Stored Procedures são ambos recursos poderosos em bancos de dados relacionais, mas têm propósitos e funcionalidades distintas.\n\n**Views:**\n\n* As Views são abstrações de consulta que permitem aos usuários definir consultas complexas e frequentemente usadas como uma única entidade.\n* Elas são essencialmente consultas SQL pré-definidas que são armazenadas no banco de dados e tratadas como tabelas virtuais.\n* As Views simplificam o acesso aos dados, ocultando a complexidade das consultas subjacentes e fornecendo uma interface consistente para os usuários.\n* Elas são úteis para simplificar consultas frequentes, segmentar permissões de acesso aos dados e abstrair a complexidade do modelo de dados subjacente.\n\n**Stored Procedures:**\n\n* As Stored Procedures são abstrações de transações que consistem em um conjunto de instruções SQL pré-compiladas e armazenadas no banco de dados.\n* Elas são usadas para encapsular operações de banco de dados mais complexas, como atualizações, inserções, exclusões e outras transações.\n* As Stored Procedures podem aceitar parâmetros de entrada e retornar valores de saída, o que as torna altamente flexíveis e reutilizáveis em diferentes partes de um aplicativo.\n* Elas oferecem maior controle sobre as operações de banco de dados e permitem a execução de lógica de negócios no lado do servidor.\n\n## Criando um novo Banco de dados\n\n\nPara ilustrar o processo, vamos estabelecer um novo banco de dados que simula um ambiente bancário convencional.\n\nComeçaremos criando um novo banco de dados. Em seguida, empregaremos os comandos `CREATE TABLE` e `INSERT INTO`.\n\n```sql\nCREATE TABLE IF NOT EXISTS clients (\n    id SERIAL PRIMARY KEY NOT NULL,\n    limite INTEGER NOT NULL,\n    saldo INTEGER NOT NULL\n);\n\nCREATE TABLE IF NOT EXISTS transactions (\n    id SERIAL PRIMARY KEY NOT NULL,\n    tipo CHAR(1) NOT NULL,\n    descricao VARCHAR(10) NOT NULL,\n    valor INTEGER NOT NULL,\n    cliente_id INTEGER NOT NULL,\n    realizada_em TIMESTAMP NOT NULL DEFAULT NOW()\n);\n```\n\nCaso queira usar UUID (para cenários de produção)\n\n```\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\nCREATE TABLE IF NOT EXISTS clients (\n    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    limite INTEGER NOT NULL,\n    saldo INTEGER NOT NULL\n);\n```\n\n\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\nCREATE TABLE IF NOT EXISTS clients (\n    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    limite INTEGER NOT NULL,\n    saldo INTEGER NOT NULL,\n    CHECK (saldo > limite)\n);\n\n\n\n**CREATE TABLE:**\n\n* O comando `CREATE TABLE` é usado para criar uma nova tabela no banco de dados.\n* O `IF NOT EXISTS` é uma cláusula opcional que garante que a tabela só será criada se ainda não existir no banco de dados, evitando erros caso a tabela já exista.\n* Em seguida, é especificada o nome da tabela (`clients` e `transactions` neste caso), seguido por uma lista de colunas e suas definições.\n* Cada coluna é definida com um nome, um tipo de dado e opcionalmente outras restrições, como a definição de uma chave primária (`PRIMARY KEY`) e a obrigatoriedade de não ser nula (`NOT NULL`).\n\n```sql\nINSERT INTO clients (limite, saldo)\nVALUES\n    (10000, 0),\n    (80000, 0),\n    (1000000, 0),\n    (10000000, 0),\n    (500000, 0);\n```\n\n**INSERT INTO:**\n\n* O comando `INSERT INTO` é usado para adicionar novos registros a uma tabela existente.\n* Na cláusula `INSERT INTO`, é especificado o nome da tabela (`clients` neste caso) seguido da lista de colunas entre parênteses, se necessário.\n* Em seguida, a cláusula `VALUES` é usada para especificar os valores a serem inseridos nas colunas correspondentes.\n* Cada linha de valores corresponde a um novo registro a ser inserido na tabela, com os valores na mesma ordem que as colunas foram listadas.\n\nEm resumo, esses comandos são fundamentais para definir a estrutura e inserir dados em tabelas no banco de dados, criando assim a base para armazenar e manipular informações de forma organizada e eficiente.\n\n## Vamos agora simular uma transação bancaria\n\nPara realizar a compra de um Carro, de 80 mil reais, no cliente 1.\n\nVamos realizar esse processo em 2 etapas.\n\nA primeira será um comando de `INSERT INTO` e depois um comando de `UPDATE`\n\n```sql\nINSERT INTO transactions (tipo, descricao, valor, cliente_id)\nVALUES ('d', 'Compra de carro', 80000, 1)\n```\n\n```sql\nUPDATE clients\nSET saldo  saldo + CASE WHEN 'd'  'd' THEN -80000 ELSE 80000 END\nWHERE id  1; -- Substitua pelo ID do cliente desejado\n```\n\n* **Vamos olhar a situação do cliente 1 agora**\n\n```sql\nSELECT saldo, limite \nFROM clients\nWHERE id  1\n```\n\n## Vamos precisar corrigir isso\n\nO comando `DELETE` é uma instrução do SQL usada para remover registros de uma tabela com base em uma condição específica. Ele permite que você exclua dados de uma tabela de banco de dados de forma controlada e precisa. Aqui estão alguns pontos-chave sobre o comando `DELETE`:\n\n1. **Sintaxe Básica**: A sintaxe básica do comando `DELETE` é a seguinte:\n    \n    ```sql\n    DELETE FROM nome_da_tabela\n    WHERE condição;\n    ```\n    \n2. **Cláusula WHERE**: A cláusula `WHERE` é opcional, mas geralmente é usada para especificar quais registros devem ser excluídos. Se não for especificada, todos os registros da tabela serão excluídos.\n    \n3. **Remoção Condicional**: Você pode usar a cláusula `WHERE` para definir uma condição para determinar quais registros serão excluídos. Apenas os registros que atendem a essa condição serão removidos.\n    \n4. **Impacto da Exclusão**: O comando `DELETE` remove permanentemente os registros da tabela, o que significa que os dados excluídos não podem ser recuperados.\n    \n5. **Uso Cauteloso**: É importante usar o comando `DELETE` com cuidado, especialmente sem uma cláusula `WHERE` específica, pois ele pode resultar na exclusão de todos os registros da tabela.\n    \n6. **Transações**: Assim como outros comandos SQL de modificação de dados, como `INSERT` e `UPDATE`, o comando `DELETE` pode ser usado dentro de transações para garantir a consistência e a integridade dos dados.\n    \n\nNo exemplo que você forneceu:\n\n```sql\nDELETE FROM transactions\nWHERE id  1;\n```\n\nEste comando remove o registro da tabela `transactions` onde o `id` é igual a `1`. Isso resultará na exclusão permanente desse registro específico da tabela. Certifique-se sempre de usar o comando `DELETE` com cuidado e de verificar duas vezes a condição antes de executá-lo para evitar a exclusão acidental de dados importantes.\n\n```sql\nDELETE FROM transactions\nWHERE id  1;\n```\n\nVamos precisar voltar também com o saldo atual do cliente, que era de 0.\n\n```sql\nUPDATE clients\nSET saldo  0\nWHERE id  1;\n```\n\n## Como evitar isso? Stored Procedures\n\nStored Procedures são rotinas armazenadas no banco de dados que contêm uma série de instruções SQL e podem ser executadas por aplicativos ou usuários conectados ao banco de dados. Elas oferecem várias vantagens, como:\n\n1. **Reutilização de código:** As stored procedures permitem que blocos de código SQL sejam escritos uma vez e reutilizados em várias partes do aplicativo.\n    \n2. **Desempenho:** Por serem compiladas e armazenadas no banco de dados, as stored procedures podem ser executadas de forma mais eficiente do que várias consultas SQL enviadas separadamente pelo aplicativo.\n    \n3. **Segurança:** As stored procedures podem ajudar a proteger o banco de dados, pois os aplicativos só precisam de permissão para executar a stored procedure, não para acessar diretamente as tabelas.\n    \n4. **Abstração de dados:** Elas podem ser usadas para ocultar a complexidade do modelo de dados subjacente, fornecendo uma interface simplificada para os usuários ou aplicativos.\n    \n5. **Controle de transações:** As stored procedures podem incluir instruções de controle de transações para garantir a integridade dos dados durante operações complexas.\n\nVamos entender cada parte da stored procedure `realizar_transacao`:\n\n1. **Criação da Procedure:**\n    \n    ```sql\n    CREATE OR REPLACE PROCEDURE realizar_transacao(\n        IN p_tipo CHAR(1),\n        IN p_descricao VARCHAR(10),\n        IN p_valor INTEGER,\n        IN p_cliente_id INTEGER\n    )\n    ```\n    \n    * Esta declaração cria ou substitui uma stored procedure chamada `realizar_transacao`.\n    * A procedure tem quatro parâmetros de entrada: `p_tipo`, `p_descricao`, `p_valor` e `p_cliente_id`, cada um com seu tipo de dado especificado.\n2. **Definição da Linguagem:**\n    \n    Sobre a languages na documentação do postgresql tem 4 linguagens padrões  disponíveis: PL/pgSQL (Chapter 43), PL/Tcl (Chapter 44), PL/Perl (Chapter 45), and PL/Python (Chapter 46)\n\n    ```sql\n    LANGUAGE plpgsql\n    ```\n    \n    * Define a linguagem da stored procedure como PL/pgSQL, que é uma linguagem procedural para o PostgreSQL.\n3. **Corpo da Procedure:**\n    \n    ```sql\n    AS $$\n    DECLARE\n        saldo_atual INTEGER;\n        limite_cliente INTEGER;\n    BEGIN\n        -- Corpo da procedure...\n    END;\n    $$;\n    ```\n    \n    * O corpo da stored procedure é definido entre `AS $$` e `$$;`.\n    * Dentro do corpo, declaramos variáveis locais usando `DECLARE`.\n    * A execução da procedure ocorre entre `BEGIN` e `END;`.\n4. **Obtenção de Dados:**\n    \n    ```sql\n    -- Obtém o saldo atual e o limite do cliente\n    SELECT saldo, limite INTO saldo_atual, limite_cliente\n    FROM clients\n    WHERE id  p_cliente_id;\n    ```\n    \n    * Esta parte do código executa uma consulta para obter o saldo atual e o limite do cliente com o ID fornecido.\n5. **Verificação da Transação:**\n    \n    ```sql\n    -- Verifica se a transação é válida com base no saldo e no limite\n    IF p_tipo  'd' AND saldo_atual - p_valor < -limite_cliente THEN\n        RAISE EXCEPTION 'Saldo insuficiente para realizar a transação';\n    END IF;\n    ```\n    \n    * Aqui, é feita uma verificação para garantir que a transação seja válida com base no tipo de transação ('d' para débito) e se o saldo atual menos o valor da transação é menor que o limite de crédito do cliente. Se a condição for verdadeira, uma exceção é lançada.\n6. **Atualização do Saldo:**\n    \n    ```sql\n    -- Atualiza o saldo do cliente\n    UPDATE clients\n    SET saldo  saldo + CASE WHEN p_tipo  'd' THEN -p_valor ELSE p_valor END\n    WHERE id  p_cliente_id;\n    ```\n    \n    * Nesta parte, o saldo do cliente é atualizado com base no tipo de transação. Se for um débito ('d'), o valor é subtraído do saldo atual; caso contrário, é adicionado.\n7. **Inserção da Transação:**\n    \n    ```sql\n    -- Insere uma nova transação\n    INSERT INTO transactions (tipo, descricao, valor, cliente_id)\n    VALUES (p_tipo, p_descricao, p_valor, p_cliente_id);\n    ```\n    \n    * Por fim, uma nova transação é inserida na tabela `transactions` com os detalhes fornecidos.\n\nEssa stored procedure encapsula todo o processo de realização de uma transação bancária, desde a validação do saldo e limite do cliente até a atualização do saldo e a inserção da transação. Ela oferece uma maneira conveniente e segura de executar essas operações de forma consistente e controlada.\n\nPara chamar a stored procedure `realizar_transacao` com os parâmetros fornecidos, você pode executar o seguinte comando SQL no PostgreSQL:\n\n```sql\nCALL realizar_transacao('d', 'carro', 80000, 1);\n```\n\nIsso invocará a procedure `realizar_transacao` com os parâmetros fornecidos:\n\n* `p_tipo`: 'd'\n* `p_descricao`: 'carro'\n* `p_valor`: 80000\n* `p_cliente_id`: 1\n\nCertifique-se de executar esse comando em um ambiente onde a stored procedure `realizar_transacao` esteja definida e acessível.\n\n## Desafio\n\nCriar uma stored procedure \"ver_extrato\" para fornecer uma visão detalhada do extrato bancário de um cliente, incluindo seu saldo atual e as informações das últimas 10 transações realizadas. Esta operação recebe como entrada o ID do cliente e retorna uma mensagem com o saldo atual do cliente e uma lista das últimas 10 transações, contendo o ID da transação, o tipo de transação (depósito ou retirada), uma breve descrição, o valor da transação e a data em que foi realizada.\n\n**Explicação Detalhada:**\n\n1. **Entrada de Parâmetros:**\n    \n    * A stored procedure recebe o ID do cliente como parâmetro de entrada.\n2. **Obtenção do Saldo Atual:**\n    \n    * É realizada uma consulta na tabela \"clients\" para obter o saldo atual do cliente com base no ID fornecido.\n3. **Exibição do Saldo Atual:**\n    \n    * O saldo atual do cliente é exibido por meio de uma mensagem de aviso.\n4. **Obtenção das Últimas 10 Transações:**\n    \n    * É realizada uma consulta na tabela \"transactions\" para obter as últimas 10 transações do cliente, ordenadas pela data de realização em ordem decrescente.\n5. **Exibição das Transações:**\n    \n    * Utilizando um loop `FOR`, cada transação é iterada e suas informações são exibidas por meio de mensagens de aviso.\n    * Para cada transação, são exibidos o ID da transação, o tipo de transação (depósito ou retirada), uma breve descrição da transação, o valor da transação e a data em que foi realizada.\n    * O loop é interrompido após exibir as informações das últimas 10 transações.\n\n```sql\nCREATE OR REPLACE PROCEDURE ver_extrato(\n    IN p_cliente_id INTEGER\n)\nLANGUAGE plpgsql\nAS $$\nDECLARE\n    saldo_atual INTEGER;\n    transacao RECORD;\n    contador INTEGER : 0;\nBEGIN\n    -- Obtém o saldo atual do cliente\n    SELECT saldo INTO saldo_atual\n    FROM clients\n    WHERE id  p_cliente_id;\n\n    -- Retorna o saldo atual do cliente\n    RAISE NOTICE 'Saldo atual do cliente: %', saldo_atual;\n\n    -- Retorna as 10 últimas transações do cliente\n    RAISE NOTICE 'Últimas 10 transações do cliente:';\n    FOR transacao IN\n        SELECT *\n        FROM transactions\n        WHERE cliente_id  p_cliente_id\n        ORDER BY realizada_em DESC\n        LIMIT 10\n    LOOP\n        contador : contador + 1;\n        RAISE NOTICE 'ID: %, Tipo: %, Descrição: %, Valor: %, Data: %', transacao.id, transacao.tipo, transacao.descricao, transacao.valor, transacao.realizada_em;\n        EXIT WHEN contador > 10;\n    END LOOP;\nEND;\n$$;\n```\n\nBootcamp - SQL e Analytics/Aula-08/table.sql\n\nSELECT product_id,\n    sum((((quantity)::double precision * unit_price) * ((1)::double precision - discount))) AS sold_value,\n    rank() OVER (ORDER BY (sum((((quantity)::double precision * unit_price) * ((1)::double precision - discount)))) DESC) AS rank\n   FROM order_details det\n  GROUP BY product_id\n\nBootcamp - SQL e Analytics/Aula-09/README.md\n\n## Aula 09 - Triggers (Gatilhos) e Projeto Prático II\n\nPapars recomendados https://github.com/rxin/db-readings?tabreadme-ov-file\n\n## O que sâo Triggers?\n\n#### 1. O que são Triggers\n\n* **Definição**: Triggers são procedimentos armazenados, que são automaticamente executados ou disparados quando eventos específicos ocorrem em uma tabela ou visão.\n* **Funcionamento**: Eles são executados em resposta a eventos como INSERT, UPDATE ou DELETE.\n\n#### 2. Por que usamos Triggers em projetos\n\n* **Automatização de tarefas**: Para realizar ações automáticas que são necessárias após modificações na base de dados, como manutenção de logs ou atualização de tabelas relacionadas.\n* **Integridade de dados**: Garantir a consistência e a validação de dados ao aplicar regras de negócio diretamente no banco de dados.\n\n#### 3. Origem e finalidade da criação dos Triggers\n\n* **História**: Os triggers foram criados para oferecer uma maneira de responder automaticamente a eventos de modificação em bancos de dados, permitindo a execução de procedimentos de forma automática e transparente.\n* **Problemas resolvidos**: Antes dos triggers, muitas dessas tarefas precisavam ser controladas manualmente no código da aplicação, o que poderia levar a erros e inconsistências.\n\n1. **Tabela Funcionario**:\n    \n    * Armazena os dados dos funcionários, incluindo ID, nome, salário e data de contratação.\n2. **Tabela Funcionario_Auditoria**:\n    \n    * Armazena o histórico de alterações dos salários dos funcionários, incluindo o salário antigo, o novo salário e a data da modificação.\n\n```sql\n-- Criação da tabela Funcionario\nCREATE TABLE Funcionario (\n    id SERIAL PRIMARY KEY,\n    nome VARCHAR(100),\n    salario DECIMAL(10, 2),\n    dtcontratacao DATE\n);\n\n-- Criação da tabela Funcionario_Auditoria\nCREATE TABLE Funcionario_Auditoria (\n    id INT,\n    salario_antigo DECIMAL(10, 2),\n    novo_salario DECIMAL(10, 2),\n    data_de_modificacao_do_salario TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (id) REFERENCES Funcionario(id)\n);\n\n-- Inserção de dados na tabela Funcionario\nINSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('Maria', 5000.00, '2021-06-01');\nINSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('João', 4500.00, '2021-07-15');\nINSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('Ana', 4000.00, '2022-01-10');\nINSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('Pedro', 5500.00, '2022-03-20');\nINSERT INTO Funcionario (nome, salario, dtcontratacao) VALUES ('Lucas', 4700.00, '2022-05-25');\n```\n\n### Criação do Trigger\n\nO trigger `trg_salario_modificado` será disparado após uma atualização no salário na tabela `Funcionario`. Ele registrará os detalhes da modificação na tabela `Funcionario_Auditoria`.\n\n```sql\n-- Criação do Trigger para auditoria de alterações de salário\nCREATE OR REPLACE FUNCTION registrar_auditoria_salario() RETURNS TRIGGER AS $$\nBEGIN\n    INSERT INTO Funcionario_Auditoria (id, salario_antigo, novo_salario)\n    VALUES (OLD.id, OLD.salario, NEW.salario);\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER trg_salario_modificado\nAFTER UPDATE OF salario ON Funcionario\nFOR EACH ROW\nEXECUTE FUNCTION registrar_auditoria_salario();\n```\n\nEsse exemplo cria uma infraestrutura completa para monitorar as alterações de salário, garantindo que qualquer ajuste seja devidamente registrado, oferecendo uma trilha de auditoria clara e útil para análises futuras.\n\nPara verificar se o trigger está funcionando corretamente, podemos realizar um comando `UPDATE` no salário de um dos funcionários e, em seguida, consultar a tabela `Funcionario_Auditoria` para ver se a alteração foi registrada conforme esperado. Vamos fazer isso com o funcionário cujo nome é \"Ana\".\n\n### Comando de Atualização do Salário\n\n```sql\n-- Atualiza o salário da Ana\nUPDATE Funcionario SET salario  4300.00 WHERE nome  'Ana';\n```\n\n### Consulta à Tabela de Auditoria\n\nApós realizar a atualização, podemos verificar a tabela `Funcionario_Auditoria` para garantir que o registro da mudança de salário foi feito.\n\n```sql\n-- Consulta à tabela Funcionario_Auditoria para verificar as mudanças\nSELECT * FROM Funcionario_Auditoria WHERE id  (SELECT id FROM Funcionario WHERE nome  'Ana');\n```\n\nEste comando SQL irá retornar os registros da tabela de auditoria que correspondem ao funcionário \"Ana\". Você deve ver uma linha com o salário antigo (4000.00), o novo salário (4300.00) e a data/hora da modificação, indicando que o trigger operou conforme o esperado.\n\n## Exemplo com desafio de Estoque\n\nNeste exercício, você irá implementar um sistema simples de gestão de estoque para uma loja que vende camisetas como Basica, Dados e Verao. A loja precisa garantir que as vendas sejam registradas apenas se houver estoque suficiente para atender os pedidos. Você será responsável por criar um trigger no banco de dados que previna a inserção de vendas que excedam a quantidade disponível dos produtos.\n\n```sql\n-- Criação da tabela Produto\nCREATE TABLE Produto (\n    cod_prod INT PRIMARY KEY,\n    descricao VARCHAR(50) UNIQUE,\n    qtde_disponivel INT NOT NULL DEFAULT 0\n);\n\n-- Inserção de produtos\nINSERT INTO Produto VALUES (1, 'Basica', 10);\nINSERT INTO Produto VALUES (2, 'Dados', 5);\nINSERT INTO Produto VALUES (3, 'Verao', 15);\n\n-- Criação da tabela RegistroVendas\nCREATE TABLE RegistroVendas (\n    cod_venda SERIAL PRIMARY KEY,\n    cod_prod INT,\n    qtde_vendida INT,\n    FOREIGN KEY (cod_prod) REFERENCES Produto(cod_prod) ON DELETE CASCADE\n);\n```\n\n```sql\n-- Criação de um TRIGGER\nCREATE OR REPLACE FUNCTION verifica_estoque() RETURNS TRIGGER AS $$\nDECLARE\n    qted_atual INTEGER;\nBEGIN\n    SELECT qtde_disponivel INTO qted_atual\n    FROM Produto WHERE cod_prod  NEW.cod_prod;\n    IF qted_atual < NEW.qtde_vendida THEN\n        RAISE EXCEPTION 'Quantidade indisponivel em estoque'\n    ELSE\n        UPDATE Produto SET qtde_disponivel  qtde_disponivel - NEW.qtde_vendida\n        WHERE cod_prod  NEW.cod_prod;\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER trg_verifica_estoque \nBEFORE INSERT ON RegistroVendas\nFOR EACH ROW \nEXECUTE FUNCTION verifica_estoque();\n```\n    \n```sql\n-- Tentativa de venda de 5 unidades de Basico (deve ser bem-sucedida, pois há 10 unidades disponíveis)\nINSERT INTO RegistroVendas (cod_prod, qtde_vendida) VALUES (1, 5);\n\n-- Tentativa de venda de 6 unidades de Dados (deve ser bem-sucedida, pois há 5 unidades disponíveis e a quantidade vendida não excede o estoque)\nINSERT INTO RegistroVendas (cod_prod, qtde_vendida) VALUES (2, 5);\n\n-- Tentativa de venda de 16 unidades de Versao (deve falhar, pois só há 15 unidades disponíveis)\nINSERT INTO RegistroVendas (cod_prod, qtde_vendida) VALUES (3, 16);\n```\n\n\n",
        "Bootcamp - SQL e Analytics/Aula-10/README.md\n\n## Transação\n\n### O que é uma transação?\n\n- Uma coleção de \"queries\"\n- Uma unidade de trabalho\nMuitas vezes precisamos mais de uma \"querie\" para querer o que queremos\nex: para fazer uma transação financeira, precisamos selecionar uma conta e verificar se ela possui o dnheiro (SELECT), fazer a remoção do dinheiro da conta que irá transferir o dinheiro (UPDATE) e fazer o incremento do dinheiro na conta alvo (UPDATE). Tudo isso precisa estar dentro da mesma transação.\n- Toda transação inicia com um BEGIN\n- Toda transação finaliza com um COMMIT (em memória)\n- Toda transação, pode falhar, precisa de um ROLLBACK \n- Normalmente transações são usadas para MODIFICAR dados, mas é possível ter uma transação com somente leitura , exemplo: você quer gerar um relatório e quer que esses dados sejam confiáveis e ter uma SNAPSHOT daquela cenário\n\n## Atomicidade\n\n- Uma transação tem que ser \"indivisivel\"\n- Ou seja, todas as \"queries\" em uma transação precisam ter sucesso\n- Exemplo: se não existir a segunda pessoa, se você não tiver 100 reais, se cair a luz no meio dessa transação, etc. Ela volta para o estado anterior e nada acontece.\n\n```sql\n-- Criar tabela\nCREATE TABLE exemplo (\n    id SERIAL PRIMARY KEY,\n    nome VARCHAR(50)\n);\n\n-- Inserir dados\nINSERT INTO exemplo (nome) VALUES ('A'), ('B'), ('C');\n\nSELECT * FROM exemplo\n```\n\nRead comitted:\n```\n\nexemplo 1 t1\n```\nBEGIN;\nSELECT nome, count(nome) FROM exemplo\nGROUP by nome\n\nexemplo 1 t2\n```\nBEGIN;\nINSERT INTO exemplo (nome) VALUES ('A');\nCOMMIT;\n```\n\nexemplo 1 t1\n```sql\nSELECT nome FROM exemplo;\n```\n\n\nComo evitar isso?\n\nexemplo 1 t1\n```sql\nSET TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nBEGIN;\nSELECT nome, count(nome) FROM exemplo\nGROUP by nome;\nSELECT * FROM exemplo;\nCOMMIT;\n```\n\nexemplo 2 t2\n```\nBEGIN;\nINSERT INTO exemplo (nome) VALUES ('A');\nCOMMIT;\n```\n\n-- Configuração para Serializable\n```sql\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN;\nSELECT * FROM exemplo;\n\n-- Configuração para Serializable\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN;\nSELECT * FROM exemplo;\n\n-- Voltar pro T1\nINSERT INTO exemplo (nome) VALUES ('A');\nCOMMIT;\n\n-- Voltar pro T2\nINSERT INTO exemplo (nome) VALUES ('A');\nCOMMIT;\n\nBootcamp - SQL e Analytics/Aula-11/README.md\n\n# Aula 11 : Ordem de consulta\n\nPara maximizar a velocidade da sua consulta em qualquer mecanismo SQL, é essencial entender a ordem de execução do SQL. Embora seja possível trabalhar sem esse conhecimento, recomendo a leitura deste artigo para obter um entendimento rápido sobre isso.\n\nO mecanismo SQL não segue a mesma ordem que você define na sua consulta, portanto, é crucial lembrar disso. Por exemplo, embora comecemos com uma instrução SELECT, o mecanismo não começará com esse comando. Neste artigo, examinaremos uma consulta complexa passo a passo para entender como o mecanismo SQL opera nos bastidores.\n\nImportante: todos os exemplos são feitos no PostgreSQL, então as sintaxes podem variar de mecanismo para mecanismo. Ainda assim, esse conceito é aplicável a todos os outros tipos de mecanismos SQL.\n\nDefinir a Consulta\nPara este exemplo, gostaria de discutir uma consulta típica usada em fluxos de trabalho do mundo real. Suponha que temos um banco de dados para carros com uma tabela para diferentes modelos, e cada modelo tem suas próprias especificações de motor listadas em uma tabela separada. Para ilustrar isso, podemos criar tabelas para este cenário.\n\n```sql\nDROP TABLE IF EXISTS cars, engines;\nCREATE TABLE cars (\n manufacturer VARCHAR(64),\n model VARCHAR(64),\n country VARCHAR(64),\n engine_name VARCHAR(64),\n year INT\n);\nCREATE TABLE engines (\n name VARCHAR(64),\n horse_power INT\n);\n\nINSERT INTO cars\nVALUES \n ('BMW', 'M4', 'Germany', 'S58B30T0-353', 2021),\n ('BMW', 'M4', 'Germany', 'S58B30T0-375', 2021),\n ('Chevrolet', 'Corvette', 'USA', 'LT6', 2023),\n ('Chevrolet', 'Corvette', 'USA', 'LT2', 2023),\n ('Audi', 'R8', 'Germany', 'DOHC FSI V10-5.2-456', 2019),\n ('McLaren', 'GT', 'UK', 'M840TE', 2019),\n ('Mercedes', 'AMG C 63 S E', 'Germany', 'M139L', 2023);\n \nINSERT INTO engines\nVALUES \n ('S58B30T0-353', 473),\n ('S58B30T0-375', 510),\n ('LT6', 670),\n ('LT2', 495),\n ('DOHC FSI V10-5.2-456', 612),\n ('M840TE', 612),\n ('M139L', 469);\n```\n\nPara alcançar nosso objetivo de identificar os dois carros mais potentes da Alemanha, estaremos olhando para automóveis modernos que não sejam mais antigos do que oito anos. Para isso, usaremos instruções SQL conhecidas como SELECT, FROM, JOIN, WHERE, GROUP BY, HAVING, ORDER BY e LIMIT.\n\n```sql\nSELECT\n  cars.manufacturer,\n  cars.model,\n  cars.country,\n  cars.year,\n  MAX(engines.horse_power) as maximum_horse_power\nFROM cars\nJOIN engines ON cars.engine_name  engines.name\nWHERE cars.year > 2015 AND cars.country  'Germany'\nGROUP BY cars.manufacturer, cars.model, cars.country, cars.year\nHAVING MAX(engines.horse_power)> 200\nORDER BY maximum_horse_power DESC\nLIMIT 2\n```\n\nSaída da consulta — os dois carros alemães mais potentes do nosso banco de dados de amostra\n\nAgora que temos nossa consulta, vamos entender como o mecanismo a ordena ao executar. Aqui está a ordem:\n\n1. FROM\n2. JOIN (e ON)\n3. WHERE\n4. GROUP BY\n5. HAVING\n6. SELECT\n7. ORDER BY\n8. LIMIT\n\nFonte: https://blog.bytebytego.com/p/ep50-visualizing-a-sql-query\n\nÉ importante notar que, antes de executar a consulta, o mecanismo SQL cria um plano de execução para reduzir o consumo de recursos. Este plano oferece detalhes valiosos como custos estimados, algoritmos de junção, ordem das operações e mais. Este é um resultado abrangente, e pode ser acessado se necessário.\n\nPasso a Passo\nFROM e JOIN\n```sql\nFROM cars\nJOIN engines\n```\nAo iniciar uma consulta SQL, o mecanismo precisa saber quais tabelas usar. Isso é realizado começando com uma instrução FROM. Você pode adicionar mais tabelas usando a palavra-chave JOIN,\n\n desde que compartilhem uma coluna comum que será usada na consulta. É um processo direto que você deve ter em mente.\n\nON\n```sql\nON cars.engine_name  engines.name\n```\nA seguir na sequência vem o ON, onde definimos como juntar diferentes tabelas. Este processo também envolve o uso de índices pré-definidos, como B-tree e Bitmap, para acelerar os cálculos. É importante notar que existem vários tipos de índices que podem ajudar neste caso. Estas duas etapas requerem uma quantidade considerável de processamento, portanto, é crucial focar e começar a otimizar neste ponto.\n\nWHERE\n```sql\nWHERE cars.year > 2015 AND cars.country  'Germany'\n```\nAo analisar nossos dados, é importante ter em mente que usar a cláusula WHERE apenas com colunas indexadas pode melhorar o desempenho, especialmente ao lidar com grandes tabelas. Além disso, pode ser benéfico filtrar dados em subconsultas ou CTEs antes da declaração WHERE em alguns cenários para aumentar ainda mais o desempenho.\n\nNo entanto, é importante notar que muitos problemas relacionados ao desempenho de consultas estão além do escopo deste artigo. Recomendo aprofundar-se nesses problemas e experimentar várias técnicas para escrever consultas mais rápidas.\n\nGROUP BY e HAVING\n```sql\nGROUP BY cars.manufacturer, cars.model, cars.country, cars.year\nHAVING MAX(engines.horse_power) > 200\n```\nA seguir na sequência, precisamos seguir a ordem especificada da consulta adequadamente. Depois disso, temos que determinar todas as agregações necessárias que temos que realizar. Quando se trata da cláusula HAVING, é intrigante porque não podemos empregar um alias da linha SELECT. Isso ocorre porque o motor SQL ainda não está ciente desta definição.\n\nSELECT\n```sql\nSELECT\n  cars.manufacturer,\n  cars.model,\n  cars.country,\n  cars.year,\n  MAX(engines.horse_power) as maximum_horse_power\n```\nUma vez que todos os passos necessários foram completados, prosseguimos para executar a instrução SELECT. Neste ponto, simplesmente especificamos as colunas a serem incluídas na saída final. É importante ter em mente que muitas operações, como mesclagem e agregação, já foram concluídas nesta fase.\n\nORDER BY e LIMIT\n```sql\nORDER BY maximum_horse_power DESC\nLIMIT 2\n```\nUma vez que executamos os comandos finais, tomamos conhecimento dos aliases que mencionamos na instrução SELECT. Como resultado, podemos utilizar o alias maximum_horse_power em vez do nome da função, embora ainda possamos usar este último. É melhor evitar ordenar uma grande quantidade de dados de saída, pois isso pode consumir uma quantidade significativa de tempo.\n\n## Conclusão\n\nO plano de execução que você visualizou para sua consulta SQL no PostgreSQL detalha como o mecanismo de banco de dados planeja buscar e processar os dados necessários para produzir o resultado desejado. Vamos analisar cada etapa e entender a ordem em que ocorrem, explicando o que cada uma representa:\n\n1. **Seq Scan on cars as cars**\n   - **Descrição**: Uma varredura sequencial (Seq Scan) é realizada na tabela `cars`. \n   - **Filtros aplicados**: A consulta verifica cada linha para ver se o ano (`year`) é maior que 2015 e se o país (`country`) é 'Germany'. \n   - **Resultado**: As linhas que não atendem a esses critérios são descartadas, indicado por \"Rows Removed by Filter: 3\".\n\n2. **Hash**\n   - **Descrição**: Esta etapa prepara uma estrutura de dados de hash para a tabela `cars` com base nos resultados da varredura que passaram pelos filtros.\n   - **Detalhes**: A hash é construída para otimizar a junção subsequente, usando colunas que serão ligadas com a outra tabela (`engines`).\n\n3. **Seq Scan on engines as engines**\n   - **Descrição**: Assim como foi feito com `cars`, uma varredura sequencial é realizada na tabela `engines`.\n   - **Resultado**: Todas as 7 linhas da tabela `engines` são lidas.\n\n4. **Hash Inner Join**\n   - **Descrição**: Um Hash Join (junção por hash) é realizado entre as tabelas `cars` e `engines`.\n   - **Condição de junção**: A junção é feita onde o nome do motor (`engine_name` de `cars` e `name` de `engines`) são iguais.\n   - **Resultado**: O resultado são 4 linhas onde a condição de junção é verdadeira.\n\n5. **Sort (rows4 loops1)**\n   - **Descrição**: As linhas resultantes do Join são ordenadas. O atributo exato da ordenação não está especificado aqui, mas é provável que seja preparação para a agregação.\n\n6. **Aggregate**\n   - **Descrição**: Uma função de agregação é aplicada.\n   - **Filtro**: O filtro aplicado na agregação é que a potência máxima do motor (`max(engines.horse_power)`) deve ser maior que 200.\n   - **Resultado**: Após aplicar o filtro, 3 linhas permanecem.\n\n7. **Sort**\n   - **Descrição**: As linhas são novamente ordenadas, desta vez provavelmente pelo valor de potência máxima do motor em ordem descendente.\n\n8. **Limit**\n   - **Descrição**: Apenas as duas primeiras linhas do resultado ordenado são retidas, conforme especificado pela cláusula `LIMIT 2` na consulta.\n\nA ordem das operações mostra claramente como o PostgreSQL lida com a consulta, otimizando o processo ao usar técnicas como varreduras sequenciais, hash para junção e filtragem rigorosa antes de aplicar funções de agregação e ordenação, culminando na aplicação de um limite para o resultado final. Esta abordagem ajuda a minimizar o volume de dados manipulados nas etapas finais do processamento da consulta.\n\n## Tudo foi uma mentira\n\n```sql\nSELECT\n  cars.manufacturer,\n  cars.model,\n  cars.engine_name,\n  engines.horse_power\nFROM cars\nJOIN engines ON cars.engine_name  engines.name\nLIMIT 2;\n```\n\nOtimização do Join: O uso do Nested Loop Inner Join sugere que o otimizador percebeu que é mais eficiente processar o join linha a linha devido ao pequeno tamanho do resultado esperado da tabela engines.\n\nAplicação precoce do Limit: O fato de apenas 2 linhas serem processadas na varredura da tabela engines e resultarem em 2 linhas após o join indica que o LIMIT pode estar influenciando a execução da consulta mais cedo do que o plano sugere visualmente. Isto é, o PostgreSQL está provavelmente limitando o número de linhas processadas em cada etapa para cumprir eficientemente o LIMIT.\n\nEficiência do Plano: Este plano mostra um uso eficiente de recursos, processando o mínimo de dados necessário para alcançar o resultado desejado, que é fundamental em grandes bases de dados ou em sistemas com recursos limitados.\nPortanto, mesmo que o LIMIT apareça ao final no plano visual, sua influência é evidente em todas as etapas anteriores, demonstrando a capacidade do otimizador de consulta do PostgreSQL de integrar profundamente considerações de limitação no plano de execução global.\n\n",
        "Bootcamp - SQL e Analytics/Aula-12/README.md\n\n# Aula 12 : Database Indexing\n\n**Tópico 1: Índices em Bancos de Dados**\n\n- **Introdução aos Índices:** Índices em bancos de dados são estruturas utilizadas para melhorar a eficiência de consultas, permitindo acesso rápido aos dados. Por exemplo, considere uma tabela de alunos em um banco de dados escolar. Sem índices, uma consulta para encontrar o aluno com um determinado ID exigiria uma busca sequencial na tabela. Com um índice, o banco de dados pode ir diretamente para a linha correspondente ao ID especificado.\n- **Tipos de Índices:** Existem vários tipos de índices, incluindo índices de árvore B, índices hash e índices de bitmap. Cada tipo tem suas próprias características e utilizações adequadas.\n- **Funcionamento dos Índices:** Os índices são geralmente criados com base em uma ou mais colunas de uma tabela. Quando uma consulta é feita usando uma dessas colunas, o banco de dados pode usar o índice correspondente para localizar rapidamente as linhas relevantes na tabela.\n- **Vantagens e Desvantagens:** As vantagens dos índices incluem consultas mais rápidas e eficientes, enquanto as desvantagens incluem custo adicional de armazenamento e sobrecarga de atualização durante operações de inserção, atualização e exclusão.\n\n**Tópico 2: Estruturas de Dados B-Tree**\n- **Introdução às Estruturas de Dados B-Tree:** Uma B-Tree é uma árvore balanceada que é frequentemente usada em bancos de dados e sistemas de arquivos. Ela é projetada para permitir inserções, exclusões e pesquisas eficientes em grandes conjuntos de dados, mantendo a árvore balanceada e otimizando a profundidade da árvore.\n- **Estrutura da B-Tree:** Uma B-Tree consiste em nós, onde cada nó pode ter várias chaves e vários ponteiros para outros nós. Cada nó tem um número mínimo e máximo de chaves e ponteiros, mantendo a árvore balanceada.\n- **Operações em B-Tree:** As operações básicas em uma B-Tree incluem inserção, remoção e busca. Por exemplo, durante uma busca, a árvore é percorrida de acordo com a chave procurada, reduzindo eficientemente o espaço de busca a cada passo.\n- **Propriedades das B-Trees:** As B-Trees possuem várias propriedades, como balanceamento automático, garantindo que a profundidade da árvore seja mantida em um nível aceitável, mesmo com muitas inserções e remoções.\n- **Aplicações Práticas:** As B-Trees são amplamente utilizadas em bancos de dados para índices, como índices de chaves primárias e secundárias, devido à sua eficiência e capacidade de manipular grandes volumes de dados.\n\nVamos detalhar cada parte do código:\n\n1. **Criação da Tabela com UUID:**\n   - Este trecho de código cria uma tabela chamada `pessoas` com três colunas: `id`, `first_name` e `last_name`. A coluna `id` é definida como uma chave primária (`PRIMARY KEY`) e tem o tipo de dados `UUID`. O valor padrão da coluna `id` é gerado usando a extensão `uuid-ossp`, que é usada para gerar UUIDs aleatórios.\n   - Exemplo:\n     ```sql\n     CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\n     CREATE TABLE pessoas (\n         id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n         first_name VARCHAR(3),\n         last_name VARCHAR(3)\n     );\n     ```\n\n2. **Verificação de Índices Existente:**\n   - Este trecho de código verifica os índices existentes na tabela `pessoas` e exibe suas informações, como nome e definição.\n   - Exemplo:\n     ```sql\n     SELECT \n         tablename AS \"Tabela\",\n         indexname AS \"Índice\",\n         indexdef AS \"Definição do Índice\"\n     FROM \n         pg_indexes \n     WHERE \n         tablename  'pessoas'; -- Substitua 'pessoas' pelo nome da sua tabela\n     ```\n\n3. **Dropar a Tabela com UUID e Criar uma SERIAL:**\n   - Este trecho de código remove a tabela `pessoas` existente (se houver) e a recria com uma coluna `id` do tipo `SERIAL`, que é uma sequência autoincrementada. Essa abordagem é mais rápida para gerar valores de ID do que o uso de UUIDs.\n   - Exemplo:\n     ```sql\n     CREATE TABLE pessoas (\n         id SERIAL PRIMARY KEY,\n         first_name VARCHAR(3),\n         last_name VARCHAR(3)\n     );\n     ```\n\n4. **Inserção de 1 Milhão de Registros:**\n   - Este trecho de código insere 1 milhão de registros na tabela `pessoas`, gerando valores aleatórios para as colunas `first_name` e `last_name`.\n   - Exemplo:\n     ```sql\n     INSERT INTO pessoas (first_name, last_name)\n     SELECT \n         substring(md5(random()::text), 0, 3),\n         substring(md5(random()::text), 0, 3)\n     FROM \n         generate_series(1, 1000000);\n     ```\n\n5. **Explicação sobre a Velocidade de Geração entre SERIAL e UUID:**\n\nUma das razões pelas quais uma coluna do tipo `serial` é mais rápida para gerar do que uma coluna do tipo `UUID` é a forma como os valores são criados e armazenados.\n\n1. **Serial:**\n   - Uma coluna do tipo `serial` é uma sequência numérica que é automaticamente incrementada pelo PostgreSQL. Quando uma nova linha é inserida na tabela e não é especificado um valor para essa coluna, o PostgreSQL gera automaticamente o próximo número na sequência e o atribui à coluna. Esse processo é altamente eficiente, pois não envolve cálculos complicados ou geração de valores aleatórios.\n\n2. **UUID:**\n   - Por outro lado, uma coluna do tipo `UUID` geralmente armazena identificadores únicos universais (UUIDs), que são cadeias de caracteres alfanuméricos de 128 bits (ou 16 bytes) gerados usando um algoritmo específico. A geração de um UUID geralmente envolve cálculos mais complexos e aleatórios para garantir que os valores sejam únicos globalmente. Isso pode ser mais demorado em comparação com a simples incrementação de um número inteiro.\n\nEm resumo, a geração de valores para uma coluna do tipo `serial` é mais rápida porque envolve apenas a incrementação de um número, enquanto a geração de valores para uma coluna do tipo `UUID` pode ser mais demorada devido à complexidade do algoritmo de geração e à aleatoriedade necessária para garantir a unicidade global.\n\nVerificando o tempo e buscando somente index\n\nClaro, aqui está o texto aprimorado com mais contexto:\n\n---\n\n**Verificando o tempo e buscando somente pelo índice:**\n\n```sql\nSELECT id FROM pessoas WHERE id  100000;\nEXPLAIN ANALYZE SELECT id FROM pessoas WHERE id  100000;\n```\n\nAo executar essas consultas, estamos analisando o desempenho da busca direta por um registro específico na tabela `pessoas`. Como estamos consultando apenas o índice associado à coluna `id`, esperamos uma execução rápida e eficiente, já que o banco de dados pode usar diretamente o índice para localizar o registro desejado.\n\n**Buscando somente pelo índice, mas observando os detalhes da tabela:**\n\n```sql\nSELECT first_name FROM pessoas WHERE id  100000;\nEXPLAIN ANALYZE SELECT first_name FROM pessoas WHERE id  100000;\n```\n\nNesse caso, mesmo que estejamos consultando apenas o índice da coluna `id`, estamos selecionando uma coluna adicional, `first_name`, da tabela `pessoas`. Isso pode resultar em uma consulta mais lenta, pois o banco de dados pode precisar acessar as páginas de dados da tabela para recuperar os valores de `first_name` associados aos registros encontrados no índice.\n\n**Buscando e trazendo dados da tabela de maneira eficiente:**\n\n```sql\nSELECT first_name FROM pessoas WHERE first_name  'aa';\n```\n\nAgora estamos buscando registros na tabela `pessoas` com base no valor exato de `first_name`. Se houver um índice na coluna `first_name`, essa consulta deve ser executada de maneira rápida e eficiente, pois o banco de dados pode usar o índice para localizar diretamente os registros correspondentes.\n\n**Buscando e trazendo dados da tabela da pior maneira possível:**\n\n```sql\nSELECT first_name FROM pessoas WHERE first_name LIKE '%a%';\n```\n\nNesta consulta, estamos buscando por valores parciais de `first_name` usando a cláusula `LIKE`. Esta consulta pode ser significativamente mais lenta, especialmente em grandes conjuntos de dados, pois não aproveita eficientemente os índices. O `%` no padrão de correspondência significa que estamos buscando por qualquer valor que contenha o caractere 'a' em qualquer posição da coluna `first_name`, o que pode resultar em uma varredura completa da tabela.\n\n**Criando nosso índice:**\n\n```sql\nCREATE INDEX first_name_index ON pessoas(first_name);\n```\n\nAqui estamos criando um índice na coluna `first_name` da tabela `pessoas`, o que nos permitirá otimizar consultas que buscam por valores nessa coluna.\n\n**Comparação após a criação do índice:**\n\n```sql\nSELECT first_name FROM pessoas WHERE first_name  'aa';\n```\n\nAgora, após a criação do índice na coluna `first_name`, vamos comparar novamente a consulta que busca por valores exatos de `first_name`. Com o índice em vigor, esperamos uma melhoria significativa no desempenho dessa consulta.\n\n**Comparando agora com o operador LIKE:**\n\n```sql\nSELECT first_name FROM pessoas WHERE first_name LIKE '%aa%';\n``` \n\nNesta consulta, estamos buscando valores parciais de first_name que contenham a sequência 'aa' em qualquer posição. Embora tenhamos criado um índice na coluna first_name, o operador LIKE com o uso de % antes e depois do padrão de correspondência '%aa%' não pode fazer uso eficiente desse índice.\n\nVarredura Completa da Tabela: O operador LIKE com um % no início do padrão significa que o banco de dados precisa verificar cada valor na coluna first_name para encontrar aqueles que contêm a sequência 'aa' em qualquer posição. Isso pode exigir uma varredura completa da tabela, mesmo com um índice criado na coluna.\n\nUso Ineficiente do Índice: O índice criado na coluna first_name é mais útil para consultas que buscam por valores exatos ou prefixos específicos. No entanto, como o padrão de correspondência '%aa%' não possui um prefixo definido, o otimizador de consultas pode optar por não utilizar o índice, pois uma varredura completa\n\n## Entendendo um pouco mais do Explain\n\nAqui estão exemplos de consultas e suas respectivas saídas explicadas usando o comando `EXPLAIN`:\n\n1. **Consulta simples:**\n```sql\nEXPLAIN SELECT * FROM pessoas;\n```\nEssa consulta irá explicar como o PostgreSQL planeja executar a consulta `SELECT * FROM pessoas`, que simplesmente seleciona todas as colunas da tabela `pessoas`. O resultado pode incluir informações sobre como o PostgreSQL acessa os dados na tabela, como pode ser feito um scan sequencial (percorrer todas as linhas) ou se algum índice será utilizado.\n\n2. **Consulta com ordenação por `id`:**\n```sql\nEXPLAIN SELECT * FROM pessoas ORDER BY id;\n```\nEsta consulta adiciona uma cláusula `ORDER BY id`, que ordena os resultados da consulta com base na coluna `id`. O resultado do `EXPLAIN` mostrará como o PostgreSQL planeja executar a ordenação, se um índice na coluna `id` pode ser utilizado e se a ordenação será feita em memória ou em disco.\n\n3. **Consulta com ordenação por `last_name`:**\n```sql\nEXPLAIN SELECT * FROM pessoas ORDER BY last_name;\n```\nSimilar ao exemplo anterior, esta consulta adiciona uma cláusula `ORDER BY last_name`, que ordena os resultados da consulta com base na coluna `last_name`. O resultado do `EXPLAIN` mostrará como o PostgreSQL planeja executar a ordenação, se um índice na coluna `last_name` pode ser utilizado e como a ordenação será realizada.\n\nAo analisar a saída do comando `EXPLAIN`, você pode identificar oportunidades de otimização de consulta, como a criação de índices adicionais, ajustes na configuração do banco de dados ou alterações na estrutura da consulta para melhorar o desempenho.\n\nClaro, vou explicar as diferenças entre as operações de busca em índices e varredura de tabelas no contexto do PostgreSQL, e fornecer exemplos de consultas que resultam em cada tipo de operação.\n\n1. **Table Scan (Varredura de Tabela):**\n   - A varredura de tabela ocorre quando o PostgreSQL precisa examinar todas as linhas de uma tabela para atender a uma consulta. Isso pode acontecer quando não há índices adequados para a consulta ou quando o custo de usar um índice é maior do que o de percorrer a tabela inteira.\n   - Exemplo:\n     ```sql\n     SELECT * FROM pessoas;\n     ```\n\n2. **Index Scan (Varredura de Índice):**\n   - Uma varredura de índice ocorre quando o PostgreSQL utiliza um índice para acessar as linhas de uma tabela que satisfazem os critérios da consulta. O banco de dados pode usar um índice se este for mais eficiente do que uma varredura de tabela.\n   - Exemplo:\n     ```sql\n     SELECT * FROM pessoas WHERE id  100;\n     ```\n   - Se houver um índice na coluna `id`, o PostgreSQL pode realizar uma varredura de índice para localizar rapidamente as linhas com `id` igual a 100.\n\n3. **Bitmap Index Scan (Varredura de Bitmap de Índice):**\n   - Uma varredura de bitmap de índice é uma técnica utilizada pelo PostgreSQL para combinar múltiplos índices em uma única operação. Em vez de procurar diretamente nas linhas da tabela, o PostgreSQL primeiro gera \"bitmaps\" para cada índice individualmente, representando as linhas que satisfazem os critérios da consulta. Em seguida, ele combina esses bitmaps para encontrar as linhas que satisfazem todos os critérios.\n   - Exemplo:\n     ```sql\n     SELECT id, first_name FROM pessoas WHERE id  100 OR first_name  'aa';\n     ```\n   - Se houver índices separados nas colunas `id` e `first_name`, o PostgreSQL pode realizar uma varredura de bitmap de índice para encontrar as linhas que têm tanto `id` igual a 100 quanto `first_name` igual a 'aa'.\n\nClaro, vou explicar o `Index Only Scan` (Varredura Apenas no Índice) e fornecer um exemplo de consulta que resulta nesse tipo de operação.\n\n5. **Index Only Scan (Varredura Apenas no Índice):**\n\n- O `Index Only Scan` ocorre quando o PostgreSQL pode satisfazer uma consulta apenas usando os dados armazenados no índice, sem a necessidade de acessar a tabela subjacente. Isso é possível quando todas as colunas necessárias para a consulta estão presentes no índice, o que elimina a necessidade de acessar as páginas de dados da tabela.\n- Esse tipo de operação é particularmente eficiente, pois reduz a quantidade de E/S (entrada/saída) necessária para atender à consulta, já que apenas o índice precisa ser lido em vez da tabela inteira.\n- Para que um `Index Only Scan` ocorra, todas as colunas na cláusula `SELECT` devem ser cobertas pelo índice. Além disso, não deve haver nenhuma coluna não indexada referenciada na consulta.\n- Este tipo de operação é especialmente útil para consultas que precisam apenas de colunas indexadas e podem melhorar significativamente o desempenho em comparação com um `Index Scan` ou `Bitmap Index Scan` seguido de uma consulta à tabela.\n- Exemplo:\n  ```sql\n  SELECT first_name FROM pessoas WHERE first_name  'aa';\n  ```\n  Se houver um índice na coluna `first_name` e se todas as consultas de seleção envolverem apenas a coluna `first_name`, o PostgreSQL pode usar um `Index Only Scan` para atender a essas consultas, acessando apenas o índice e não a tabela subjacente.\n\nO `Index Only Scan` é uma operação de busca muito eficiente quando todas as colunas necessárias para a consulta estão cobertas pelo índice, resultando em uma redução significativa na quantidade de E/S necessária para atender à consulta.\n\nVamos examinar cada parte em detalhes:\n\n1. **Custo do Índice:**\n   - Este trecho de código calcula o tamanho em disco ocupado pelo índice `first_name_index`. O resultado será exibido em uma unidade de tamanho legível, como KB, MB ou GB. Isso pode ser útil para entender o impacto do índice no armazenamento do banco de dados.\n   - Exemplo:\n     ```sql\n     SELECT pg_size_pretty(pg_relation_size('first_name_index'));\n     ```\n   - Este comando retornará o tamanho ocupado pelo índice `first_name_index` em disco.\n\n2. **Tamanho Total da Coluna:**\n   - Este trecho de código calcula o tamanho total ocupado pela coluna `first_name` em todas as linhas da tabela `pessoas`. O resultado será exibido em uma unidade de tamanho legível, como KB, MB ou GB. Isso pode ser útil para entender quanto espaço a coluna está consumindo no banco de dados.\n   - Exemplo:\n     ```sql\n     SELECT pg_size_pretty(pg_column_size(first_name)::bigint) AS tamanho_total\n     FROM pessoas;\n     ```\n   - Este comando retornará o tamanho total ocupado pela coluna `first_name` em todas as linhas da tabela `pessoas`.\n\n3. **Tamanho Total de Todas as Colunas:**\n   - Este trecho de código calcula o tamanho total ocupado por todas as colunas em todas as linhas da tabela `pessoas`. Ele soma os tamanhos individuais de todas as colunas e retorna o resultado em uma unidade de tamanho legível, como KB, MB ou GB. Isso pode ser útil para entender quanto espaço total a tabela está consumindo no banco de dados.\n   - Exemplo:\n     ```sql\n     SELECT pg_size_pretty(SUM(pg_column_size(first_name)::bigint)) AS tamanho_total\n     FROM pessoas;\n     ```\n   - Este comando retornará o tamanho total ocupado por todas as linhas em todas as colunas da tabela `pessoas`.\n\n",
        "Bootcamp - SQL e Analytics/Aula-12/main.py\n\nclass Node:\n    def __init__(self, leafFalse):\n        self.keys  []\n        self.children  []\n        self.leaf  leaf\n\n\nclass BTree:\n    def __init__(self, t):\n        self.root  Node(True)\n        self.t  t\n\n    def search(self, key, nodeNone):\n        node  self.root if node  None else node\n    \n        i  0\n        while i < len(node.keys) and key > node.keys[i]:\n            i + 1\n        if i < len(node.keys) and key  node.keys[i]:\n            return (node, i)\n        elif node.leaf:\n            return None\n        else:\n            return self.search(key, node.children[i])\n\n    def split_child(self, x, i):\n        t  self.t\n\n        # y is a full child of x\n        y  x.children[i]\n        \n        # create a new node and add it to x's list of children\n        z  Node(y.leaf)\n        x.children.insert(i + 1, z)\n\n        # insert the median of the full child y into x\n        x.keys.insert(i, y.keys[t - 1])\n\n        # split apart y's keys into y & z\n        z.keys  y.keys[t: (2 * t) - 1]\n        y.keys  y.keys[0: t - 1]\n\n        # if y is not a leaf, we reassign y's children to y & z\n        if not y.leaf:\n            z.children  y.children[t: 2 * t]\n            y.children  y.children[0: t] # video incorrectly has t-1\n\n    def insert(self, k):\n        t  self.t\n        root  self.root\n\n        # if root is full, create a new node - tree's height grows by 1\n        if len(root.keys)  (2 * t) - 1:\n            new_root  Node()\n            self.root  new_root\n            new_root.children.insert(0, root)\n            self.split_child(new_root, 0)\n            self.insert_non_full(new_root, k)\n        else:\n            self.insert_non_full(root, k)\n\n    def insert_non_full(self, x, k):\n        t  self.t\n        i  len(x.keys) - 1\n\n        # find the correct spot in the leaf to insert the key\n        if x.leaf:\n            x.keys.append(None)\n            while i > 0 and k < x.keys[i]:\n                x.keys[i + 1]  x.keys[i]\n                i - 1\n            x.keys[i + 1]  k\n        # if not a leaf, find the correct subtree to insert the key\n        else:\n            while i > 0 and k < x.keys[i]:\n                i - 1\n            i + 1\n            # if child node is full, split it\n            if len(x.children[i].keys)  (2 * t) - 1:\n                self.split_child(x, i)\n                if k > x.keys[i]:\n                    i + 1\n            self.insert_non_full(x.children[i], k)\n\n    def delete(self, x, k):\n        t  self.t\n        i  0\n\n        while i < len(x.keys) and k > x.keys[i]:\n            i + 1\n        if x.leaf:\n            if i < len(x.keys) and x.keys[i]  k:\n                x.keys.pop(i)\n            return\n\n        if i < len(x.keys) and x.keys[i]  k:\n            return self.delete_internal_node(x, k, i)\n        elif len(x.children[i].keys) > t:\n            self.delete(x.children[i], k)\n        else:\n            if i ! 0 and i + 2 < len(x.children):\n                if len(x.children[i - 1].keys) > t:\n                    self.delete_sibling(x, i, i - 1)\n                elif len(x.children[i + 1].keys) > t:\n                    self.delete_sibling(x, i, i + 1)\n                else:\n                    self.delete_merge(x, i, i + 1)\n            elif i  0:\n                if len(x.children[i + 1].keys) > t:\n                    self.delete_sibling(x, i, i + 1)\n                else:\n                    self.delete_merge(x, i, i + 1)\n            elif i + 1  len(x.children):\n                if len(x.children[i - 1].keys) > t:\n                    self.delete_sibling(x, i, i - 1)\n                else:\n                    self.delete_merge(x, i, i - 1)\n            self.delete(x.children[i], k)\n\n    def delete_internal_node(self, x, k, i):\n        t  self.t\n        if x.leaf:\n            if x.keys[i]  k:\n                x.keys.pop(i)\n            return\n\n        if len(x.children[i].keys) > t:\n            x.keys[i]  self.delete_predecessor(x.children[i])\n            return\n        elif len(x.children[i + 1].keys) > t:\n            x.keys[i]  self.delete_successor(x.children[i + 1])\n            return\n        else:\n            self.delete_merge(x, i, i + 1)\n            self.delete_internal_node(x.children[i], k, self.t - 1)\n\n    def delete_predecessor(self, x):\n        if x.leaf:\n            return x.keys.pop()\n        n  len(x.keys) - 1\n        if len(x.children[n].keys) > self.t:\n            self.delete_sibling(x, n + 1, n)\n        else:\n            self.delete_merge(x, n, n + 1)\n        self.delete_predecessor(x.children[n])\n\n    def delete_successor(self, x):\n        if x.leaf:\n            return x.keys.pop(0)\n        if len(x.children[1].keys) > self.t:\n            self.delete_sibling(x, 0, 1)\n        else:\n            self.delete_merge(x, 0, 1)\n        self.delete_successor(x.children[0])\n\n    def delete_merge(self, x, i, j):\n        cnode  x.children[i]\n\n        if j > i:\n            rsnode  x.children[j]\n            cnode.keys.append(x.keys[i])\n            for k in range(len(rsnode.keys)):\n                cnode.keys.append(rsnode.keys[k])\n                if len(rsnode.children) > 0:\n                    cnode.children.append(rsnode.children[k])\n            if len(rsnode.children) > 0:\n                cnode.children.append(rsnode.children.pop())\n            new  cnode\n            x.keys.pop(i)\n            x.children.pop(j)\n        else:\n            lsnode  x.children[j]\n            lsnode.keys.append(x.keys[j])\n            for i in range(len(cnode.keys)):\n                lsnode.keys.append(cnode.keys[i])\n                if len(lsnode.children) > 0:\n                    lsnode.children.append(cnode.children[i])\n            if len(lsnode.children) > 0:\n                lsnode.children.append(cnode.children.pop())\n            new  lsnode\n            x.keys.pop(j)\n            x.children.pop(i)\n\n        if x  self.root and len(x.keys)  0:\n            self.root  new\n\n    def delete_sibling(self, x, i, j):\n        cnode  x.children[i]\n        if i < j:\n            rsnode  x.children[j]\n            cnode.keys.append(x.keys[i])\n            x.keys[i]  rsnode.keys[0]\n            if len(rsnode.children) > 0:\n                cnode.children.append(rsnode.children[0])\n                rsnode.children.pop(0)\n            rsnode.keys.pop(0)\n        else:\n            lsnode  x.children[j]\n            cnode.keys.insert(0, x.keys[i - 1])\n            x.keys[i - 1]  lsnode.keys.pop()\n            if len(lsnode.children) > 0:\n                cnode.children.insert(0, lsnode.children.pop())\n\n    def print_tree(self, x, level0):\n        print(f'Level {level}', end\": \")\n\n        for i in x.keys:\n            print(i, end\" \")\n\n        print()\n        level + 1\n\n        if len(x.children) > 0:\n            for i in x.children:\n                self.print_tree(i, level)\n\n\ndef delete_example():\n    first_leaf  Node(True)\n    first_leaf.keys  [1, 9]\n\n    second_leaf  Node(True)\n    second_leaf.keys  [17, 19, 21]\n\n    third_leaf  Node(True)\n    third_leaf.keys  [23, 25, 27]\n\n    fourth_leaf  Node(True)\n    fourth_leaf.keys  [31, 32, 39]\n\n    fifth_leaf  Node(True)\n    fifth_leaf.keys  [41, 47, 50]\n\n    sixth_leaf  Node(True)\n    sixth_leaf.keys  [56, 60]\n\n    seventh_leaf  Node(True)\n    seventh_leaf.keys  [72, 90]\n\n    root_left_child  Node()\n    root_left_child.keys  [15, 22, 30]\n    root_left_child.children.append(first_leaf)\n    root_left_child.children.append(second_leaf)\n    root_left_child.children.append(third_leaf)\n    root_left_child.children.append(fourth_leaf)\n\n    root_right_child  Node()\n    root_right_child.keys  [55, 63]\n    root_right_child.children.append(fifth_leaf)\n    root_right_child.children.append(sixth_leaf)\n    root_right_child.children.append(seventh_leaf)\n\n    root  Node()\n    root.keys  [40]\n    root.children.append(root_left_child)\n    root.children.append(root_right_child)\n\n    B  BTree(3)\n    B.root  root\n    print('\\n--- Original B-Tree ---\\n')\n    B.print_tree(B.root)\n\n    print('\\n--- Case 1: DELETED 21 ---\\n')\n    B.delete(B.root, 21)\n    B.print_tree(B.root)\n\n    print('\\n--- Case 2a: DELETED 30 ---\\n')\n    B.delete(B.root, 30)\n    B.print_tree(B.root)\n\n    print('\\n--- Case 2b: DELETED 27 ---\\n')\n    B.delete(B.root, 27)\n    B.print_tree(B.root)\n\n    print('\\n--- Case 2c: DELETED 22 ---\\n')\n    B.delete(B.root, 22)\n    B.print_tree(B.root)\n\n    print('\\n--- Case 3b: DELETED 17 ---\\n')\n    B.delete(B.root, 17)\n    B.print_tree(B.root)\n\n    print('\\n--- Case 3a: DELETED 9 ---\\n')\n    B.delete(B.root, 9)\n    B.print_tree(B.root)\n\n\ndef insert_and_search_example():\n    B  BTree(3)\n\n    for i in range(10):\n        B.insert(i)\n\n    B.print_tree(B.root)\n    print()\n\n    keys_to_search_for  [2, 9, 11, 4]\n    for key in keys_to_search_for:\n        if B.search(key) is not None:\n            print(f'{key} is in the tree')\n        else:\n            print(f'{key} is NOT in the tree')\n\n\ndef main():\n    print('\\n--- INSERT & SEARCH ---\\n')\n    insert_and_search_example()\n\n    delete_example()\n\n\nmain()\n\nBTree.insert\n\nBTree.delete\n\nlista  [1,2,3,4]\n\nlista.insert\n\nlista.append\n\nBootcamp - SQL e Analytics/Aula-13/README.md\n\n# Aula 13 : Database Partition\n\nVamos detalhar cada parte do código:\n\n1. **Criação da Tabela:**\n   - Exemplo:\n     ```sql\n     CREATE TABLE pessoas (\n         id SERIAL PRIMARY KEY,\n         first_name VARCHAR(3),\n         last_name VARCHAR(3),\n         estado VARCHAR(3)\n     );\n     ```\n\n2. **Inserção de 1 Milhão de Registros:**\n\n   ```sql\n   CREATE OR REPLACE FUNCTION random_estado()\n   RETURNS VARCHAR(3) AS $$\n   BEGIN\n      RETURN CASE floor(random() * 5)\n            WHEN 0 THEN 'SP'\n            WHEN 1 THEN 'RJ'\n            WHEN 2 THEN 'MG'\n            WHEN 3 THEN 'ES'\n            ELSE 'DF'\n            END;\n   END;\n   $$ LANGUAGE plpgsql;\n\n   -- Inserir dados na tabela pessoas com estados aleatórios\n   INSERT INTO pessoas (first_name, last_name, estado)\n   SELECT \n      substring(md5(random()::text), 0, 3),\n      substring(md5(random()::text), 0, 3),\n      random_estado()\n   FROM \n      generate_series(1, 10000000);\n     ```\n\n3. **Criando um INDEX no first_name**\n\n```sql\nCREATE INDEX first_name_index ON pessoas(first_name)\n```\n\n4. **Fazendo  uma busca usando um INDEX**\n\n```sql\nSELECT COUNT(*) FROM pessoas WHERE first_name  'aa'\n```\n\nTotal query runtime: 585 msec.\n\n5. **Fazendo  uma busca sem usar INDEX**\n\n```sql\nSELECT COUNT(*) FROM pessoas WHERE last_name  'aa'\n```\n\nTotal query runtime: 2 secs 552 msec.\n\n6. **Vamos criar uma tabela com particionamento**\n\n\n```sql\n     CREATE TABLE pessoas (\n         id SERIAL PRIMARY KEY,\n         first_name VARCHAR(3),\n         last_name VARCHAR(3),\n         estado VARCHAR(3)\n     ) PARTITION BY RANGE (id);\n```\n\nOpção mais simples\n\n```sql\nCREATE TABLE pessoas_part1 PARTITION OF pessoas FOR VALUES FROM (MINVALUE) TO (2000001);\nCREATE TABLE pessoas_part2 PARTITION OF pessoas FOR VALUES FROM (2000001) TO (4000001);\nCREATE TABLE pessoas_part3 PARTITION OF pessoas FOR VALUES FROM (4000001) TO (6000001);\nCREATE TABLE pessoas_part4 PARTITION OF pessoas FOR VALUES FROM (6000001) TO (8000001);\nCREATE TABLE pessoas_part5 PARTITION OF pessoas FOR VALUES FROM (8000001) TO (MAXVALUE);\n```\n\nOpção indireta\n\n```sql\n-- Criar as tabelas particionadas\nCREATE TABLE pessoas_part1 (\n    LIKE pessoas INCLUDING ALL,\n    CHECK (id > 1 AND id < 2000000)\n);\n\nCREATE TABLE pessoas_part2 (\n    LIKE pessoas INCLUDING ALL,\n    CHECK (id > 2000000 AND id < 4000000)\n);\n\nCREATE TABLE pessoas_part3 (\n    LIKE pessoas INCLUDING ALL,\n    CHECK (id > 4000000 AND id < 6000000)\n);\n\nCREATE TABLE pessoas_part4 (\n    LIKE pessoas INCLUDING ALL,\n    CHECK (id > 6000000 AND id < 8000000)\n);\n\nCREATE TABLE pessoas_part5 (\n    LIKE pessoas INCLUDING ALL,\n    CHECK (id > 8000000)  -- A última partição não precisa de limite superior\n);\n```\n\n```sql\nALTER TABLE pessoas ATTACH PARTITION pessoas_part1 FOR VALUES FROM (MINVALUE) TO (2000001);\nALTER TABLE pessoas ATTACH PARTITION pessoas_part2 FOR VALUES FROM (2000001) TO (4000001);\nALTER TABLE pessoas ATTACH PARTITION pessoas_part3 FOR VALUES FROM (4000001) TO (6000001);\nALTER TABLE pessoas ATTACH PARTITION pessoas_part4 FOR VALUES FROM (6000001) TO (8000001);\nALTER TABLE pessoas ATTACH PARTITION pessoas_part5 FOR VALUES FROM (8000001) TO (MAXVALUE);\n```\n\n```sql\n   INSERT INTO pessoas (first_name, last_name, estado)\n   SELECT \n      substring(md5(random()::text), 0, 3),\n      substring(md5(random()::text), 0, 3),\n      random_estado()\n   FROM \n      generate_series(1, 10000000);\n```\n\n## Funciona\n\n```sql\nselect * from pessoas\n```\n\n\n## Criando com base em lista\n\n```sql\nCREATE TABLE pessoas (\n    id SERIAL,\n    first_name VARCHAR(3),\n    last_name VARCHAR(3),\n    estado VARCHAR(3),\n    PRIMARY KEY (id, estado)\n) PARTITION BY LIST (estado);\n\n-- Criar as partições\nCREATE TABLE pessoas_sp PARTITION OF pessoas FOR VALUES IN ('SP');\nCREATE TABLE pessoas_rj PARTITION OF pessoas FOR VALUES IN ('RJ');\nCREATE TABLE pessoas_mg PARTITION OF pessoas FOR VALUES IN ('MG');\nCREATE TABLE pessoas_es PARTITION OF pessoas FOR VALUES IN ('ES');\nCREATE TABLE pessoas_df PARTITION OF pessoas FOR VALUES IN ('DF');\n```\n\n"
    ],
    "03-deploy-de-apps-dados-com-docker": [
        "03-deploy-de-apps-dados-com-docker/README copy.md\n\n# Deploy de Aplicações de Dados Utilizando Docker\n\n## Objetivo do Workshop\n\nEste workshop de 4 horas visa fornecer aos analistas de dados e engenheiros de dados uma introdução prática ao uso do Docker para deploy de aplicações de dados. Os participantes aprenderão a empacotar e implantar eficientemente aplicações de dados, incluindo uma ETL em Python, um banco de dados PostgreSQL e um dashboard interativo usando Streamlit, tudo dentro de containers Docker.\n\n![Solução](./pics/arquitetura.png)\n\n## Cinco Motivos para Aprender Docker em Nosso Workshop\n\n1. **Ensino do Zero**: Independentemente do seu nível de experiência prévia, vamos começar com os conceitos básicos de Docker, assegurando que todos os participantes tenham uma compreensão sólida dos fundamentos para construir sobre eles.\n\n2. **Facilidade para Subir o Deploy**: Demonstraremos como Docker simplifica o processo de deploy de aplicações, permitindo que você foque na construção e no aprimoramento de suas aplicações, em vez de gastar tempo com configurações complexas de ambiente.\n\n3. **Solução Versátil**: Docker é uma ferramenta poderosa que resolve uma variedade de problemas de desenvolvimento e operações, facilitando a colaboração entre equipes e melhorando a eficiência no ciclo de vida de desenvolvimento de software.\n\n4. **Vantagens de Utilizar Docker na Cloud**: Explore como Docker se integra perfeitamente com serviços de cloud, potencializando a escalabilidade, a portabilidade e a eficiência dos recursos em ambientes de cloud computing.\n\n5. **Solução Moderna**: Aprenda sobre as práticas atuais de desenvolvimento e operações que estão moldando o futuro da tecnologia. Docker é uma habilidade essencial em muitas áreas de TI, incluindo engenharia de dados, e dominá-la abrirá novas oportunidades profissionais.\n\n## Agenda\n\n### Parte 1: Introdução ao Docker (9:00 - 10:30)\n\n- **9:00 - 9:15**: Boas-vindas e Introdução\n- **9:15 - 9:45**: Docker e o Ecossistema de Cloud\n- **9:45 - 10:00**: Introdução ao Heroku\n- **10:00 - 10:30**: Prática: Deploy de uma Aplicação Simples no Docker\n\n### Intervalo (10:30 - 11:00)\n\n### Parte 2: Aplicações de Dados Avançadas com Docker (11:00 - 13:00)\n\n- **11:00 - 11:30**: Deploy de um Banco de Dados PostgreSQL com Docker\n- **11:30 - 12:15**: Construção de uma Solução de Dashboard com Streamlit e DuckDB\n- **12:15 - 12:55**: Projeto Integrado: Dashboard com Dados do PostgreSQL\n- **12:55 - 13:00**: Conclusão e Encerramento\n\n## Pré-requisitos\n\n- Assistir ao vídeo de Python + Vscode + Git\n- Assistir ao vídeo de Poetry\n\n## Estrutura do Projeto\n\nEste repositório contém os diretórios e arquivos necessários para acompanhar as atividades práticas do workshop:\n\n- `dashboard/`: Contém os arquivos para o deploy do dashboard Streamlit.\n- `etl/`: Contém os scripts de ETL para processamento de dados.\n- `postgres/`: Contém os arquivos necessários para configurar o banco de dados PostgreSQL em um container Docker.\n\n## Como Usar\n\nCada diretório no repositório contém um `README.md` com instruções específicas sobre como construir e executar as aplicações correspondentes usando Docker.\n\n03-deploy-de-apps-dados-com-docker/pyproject.toml\n\n[tool.poetry]\nname  \"deploy-de-apps-dados-no-docker\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"Luciano Filho <lvgalvaofilho@gmail.com>\"]\nreadme  \"README.md\"\npackages  [{include  \"deploy_de_apps_dados_no_docker\"}]\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\nstreamlit  \"^1.31.1\"\nduckdb  \"^0.10.0\"\npython-dotenv  \"^1.0.1\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\n03-deploy-de-apps-dados-com-docker/.python-version\n\n3.12\n\n\n03-deploy-de-apps-dados-com-docker/src/collector/requirements.txt\n\nstreamlit\nduckdb\nsqlalchemy\n\n03-deploy-de-apps-dados-com-docker/src/dashboard/Dockerfile\n\n# dashboard/Dockerfile\nFROM python:3.8-slim\nWORKDIR /app\nCOPY . /app\nRUN pip install -r requirements.txt\nCMD [\"streamlit\", \"run\", \"app.py\"]\n\n03-deploy-de-apps-dados-com-docker/src/dashboard/app.py\n\n# dashboard/app.py\nimport streamlit as st\nimport duckdb\nfrom dotenv import load_dotenv\nimport os\n\n# Carrega as variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Obtém as variáveis de ambiente\nDB_HOST  os.getenv('DB_HOST')\nDB_PORT  os.getenv('DB_PORT')\nDB_NAME  os.getenv('DB_NAME')\nDB_USER  os.getenv('DB_USER')\nDB_PASSWORD  os.getenv('DB_PASSWORD')\n\nst.title('Dashboard de Dados')\n\n# Cria a conexão com DuckDB em memória\ncon  duckdb.connect(database':memory:', read_onlyFalse)\n\n# Instala e carrega a extensão Postgres no DuckDB\ncon.execute(\"INSTALL 'postgres';\")\ncon.execute(\"LOAD 'postgres';\")\n\n# Conexão com o banco de dados PostgreSQL utilizando as variáveis de ambiente\npg_connection_string  f\"host{DB_HOST} port{DB_PORT} dbname{DB_NAME} user{DB_USER} password{DB_PASSWORD}\"\n\n# Anexa o banco de dados PostgreSQL ao DuckDB\ncon.execute(f\"ATTACH '{pg_connection_string}' AS pg_db (TYPE 'postgres');\")\n\n# Agora você pode realizar consultas SQL como se as tabelas do PostgreSQL fossem tabelas do DuckDB\nresult  con.execute(\"SELECT * FROM pg_db.public.my_table;\").fetchall()\n\n# Exibe o resultado no Streamlit\nst.write(result)\n\n\n03-deploy-de-apps-dados-com-docker/src/dashboard/requirements.txt\n\nstreamlit\nduckdb\nsqlalchemy\n\n"
    ],
    "Workshop - Git e Github": [
        "Workshop - Git e Github/Aula_01/README.md - Parte (1/2)\nWorkshop - Git e Github/Aula_01/README.md\n\n# Introdução ao Git - Aula 01\n\nBem-vindo ao workshop de introdução ao Git! Neste workshop, nosso objetivo é fornecer uma visão clara e prática sobre o uso do Git, ajudando você a aplicar essas habilidades em seus projetos de dados. O Git é uma ferramenta fundamental para o versionamento de código e colaboração em equipes de desenvolvimento, especialmente quando múltiplas pessoas estão trabalhando em um mesmo projeto.\n\n## 📜 Problema no Desenvolvimento\n\n### Desafios com o Desenvolvimento Colaborativo\n\nEm projetos de software, especialmente na área de dados, é comum termos várias pessoas contribuindo ao mesmo tempo. Isso pode gerar conflitos de código, perda de trabalho e dificuldades em gerenciar diferentes versões de um mesmo arquivo. Antes do Git, essas situações frequentemente resultavam em erros e retrabalho, pois não existia um controle eficiente sobre as mudanças feitas no código.\n\n**Perguntas para Reflexão:**\n\n- **Como vocês costumam compartilhar código em equipe?** Será que enviar arquivos por e-mail ou usar drives compartilhados é a forma mais eficiente de colaborar?\n- **Como garantir que todos os membros da equipe estão trabalhando na versão mais atual do código?** O que acontece quando diferentes pessoas fazem mudanças no mesmo arquivo simultaneamente?\n- **Qual seria o impacto de perder uma semana de trabalho por causa de um conflito de código não resolvido?** Como podemos prevenir a perda de progresso no desenvolvimento de software?\n- **Como vocês organizam as diferentes versões de um projeto?** Existe alguma estratégia que vocês usam para controlar quais mudanças foram feitas e por quem?\n- **O que acontece quando queremos testar uma nova funcionalidade sem impactar o código que já está funcionando?** Como isolar essas mudanças para garantir que não introduzimos novos bugs?\n\n## 📂 Vamos Criar um Projeto Simples\n\n### 1. Criação de uma Pasta\n\nPrimeiro, vamos criar uma pasta para o nosso projeto:\n\n```bash\nmkdir projeto-git\ncd projeto-git\n```\n\n### 2. Criação de um Arquivo Python\n\nDentro da pasta, vamos criar um arquivo Python simples:\n\n```bash\ntouch main.py\n```\n\nAbra o arquivo `main.py` e adicione o seguinte código:\n\n```python\nprint(\"Hello, World!\")\n```\n\n### Exemplo de Modificações Sem o Git\n\nVamos seguir o exemplo onde modificamos um arquivo Python várias vezes e criamos novos arquivos, mas sem utilizar o Git para rastrear essas mudanças. Isso demonstra o problema de não ter controle de versão e como é fácil perder o histórico do que foi feito.\n\n#### Passo 1: Criando e Modificando o Arquivo Inicial\n\nPrimeiro, criamos o arquivo `main.py` com o seguinte conteúdo:\n\n```python\nprint(\"Hello, World!\")\n```\n\nEssa é a primeira versão do arquivo. Agora, imagine que você precisa fazer algumas mudanças.\n\n#### Passo 2: Primeira Modificação\n\nVocê modifica o arquivo `main.py` para adicionar uma nova linha de código:\n\n```python\nprint(\"Hello, World!\")\nprint(\"Primeira Modificação\")\n```\n\nEssa mudança é feita diretamente no arquivo, substituindo a versão anterior. Como não estamos usando Git, a versão original do arquivo é perdida, e não temos mais acesso a ela.\n\n#### Passo 3: Segunda Modificação\n\nDepois de algum tempo, você decide modificar o arquivo novamente:\n\n```python\nprint(\"Hello, World!\")\nprint(\"Primeira Modificação\")\nprint(\"Segunda Modificação\")\n```\n\nMais uma vez, o arquivo original e a primeira modificação são substituídos por essa nova versão. Sem o Git, não há registro das mudanças anteriores.\n\n#### Passo 4: Terceira Modificação e Criação de um Novo Arquivo\n\nAgora, você faz uma terceira modificação no `main.py` e também cria um novo arquivo chamado `auxiliary.py`:\n\n**main.py:**\n\n```python\nprint(\"Hello, World!\")\nprint(\"Primeira Modificação\")\nprint(\"Segunda Modificação\")\nprint(\"Terceira Modificação\")\n```\n\n**auxiliary.py:**\n\n```python\ndef helper():\n    print(\"Função Auxiliar\")\n```\n\nEssas novas mudanças também são feitas diretamente no arquivo, substituindo tudo o que havia antes. Como não estamos usando Git, o histórico das três versões anteriores do `main.py` é completamente perdido.\n\n### O Problema Sem Git\n\nSem o Git, não temos como recuperar o estado anterior do arquivo `main.py` em nenhum desses momentos. Se algo der errado, não há como voltar para uma versão anterior. Além disso, se tivermos conflitos ou dúvidas sobre o que foi mudado ao longo do tempo, não temos um histórico para consultar.\n\n### Como o Git Resolveria Esse Problema\n\nSe estivéssemos usando Git, cada uma dessas modificações poderia ter sido feita em um novo commit. Isso significaria que, a qualquer momento, poderíamos voltar a uma versão anterior do arquivo ou ver exatamente o que mudou entre os commits. Também poderíamos criar branches para testar novas funcionalidades sem afetar o código principal, garantindo que sempre tivéssemos uma versão estável do projeto.\n\n## 🐧 História do Git\n\n### História do Git: A Palestra Famosa de Linus Torvalds\n\nGit foi criado em 2005 por Linus Torvalds, o criador do Linux, em resposta à necessidade de um sistema de controle de versão robusto e eficiente para o desenvolvimento do kernel Linux. Na época, o time de desenvolvimento enfrentava desafios significativos para gerenciar contribuições de milhares de desenvolvedores ao redor do mundo. O Git foi concebido para ser rápido, distribuído e capaz de lidar com a complexidade de projetos desse porte.\n\n### A Famosa Palestra de Linus Torvalds\n\nEm uma palestra bem conhecida, Linus Torvalds falou sobre a criação do Git e como ele o desenvolveu em apenas alguns dias. Com seu humor característico, ele mencionou que decidiu criar o Git durante um final de semana, após ficar frustrado com as limitações das ferramentas de controle de versão existentes na época.\n\nLinus explicou que, ao criar o Git, ele focou em três principais características: velocidade, simplicidade no design e suporte para desenvolvimento distribuído. Ele queria uma ferramenta que fosse fácil de usar para desenvolvedores de todo o mundo, permitindo que cada um tivesse uma cópia completa do repositório, sem a necessidade de um servidor central, e que fosse rápida o suficiente para lidar com as necessidades do kernel Linux.\n\nDurante a palestra, Linus destacou a importância da liberdade e do controle que o Git proporciona aos desenvolvedores, algo que faltava nas ferramentas anteriores. Ele também brincou sobre o fato de que, embora tenha começado o projeto em um final de semana, o Git rapidamente se tornou uma das ferramentas mais importantes e amplamente adotadas na história do desenvolvimento de software.\n\nEssa história ilustra não só a genialidade de Linus Torvalds, mas também a urgência e a necessidade que existiam por uma solução como o Git. Desde então, o Git se tornou a ferramenta padrão para controle de versão em quase todos os projetos de software no mundo.\n\nPara entender mais sobre Git:\n- [Sobre Git com Akita e Palestra de Linus Torvalds sobre Git](https://www.youtube.com/watch?v6Czd1Yetaac)\n\n## 🛠️ O Que é o Git?\n\n### Um Programa Como Qualquer Outro\n\nGit é um programa que você instala em seu computador, semelhante a como o PowerBI é utilizado para criar dashboards. No caso do Git, ele é usado para versionar arquivos de texto, como código-fonte, de maneira eficiente. Isso permite que você:\n\n- **Versione Arquivos:** Mantenha um histórico detalhado de todas as mudanças feitas em seus arquivos.\n- **Colabore Facilmente:** Trabalhe com muitos contribuidores de forma organizada.\n- **Distribua Código:** Compartilhe seu código com outras pessoas, garantindo que todos estejam sincronizados com a versão mais recente.\n\n## 💻 Como Instalar o Git\n\n### Windows\n\n1. Baixe o instalador do Git [aqui](https://git-scm.com/download/win).\n2. Execute o instalador e siga as instruções.\n\n### Linux\n\n1. Abra o terminal.\n2. Execute o comando:\n   \n   ```bash\n   sudo apt-get install git\n   ```\n\n### Mac\n\n1. Abra o terminal.\n2. Execute o comando:\n   \n   ```bash\n   brew install git\n   ```\n\n## 🎯 Configuração do Git\n\nAntes de começar a usar o Git, precisamos configurar o nome e o e-mail do usuário:\n\n```bash\ngit config --global user.name \"Seu Nome\"\ngit config --global user.email \"seu.email@exemplo.com\"\n```\n\nOs arquivos de configuração do Git que armazenam as configurações feitas com os comandos `git config` são armazenados em diferentes locais, dependendo do nível de configuração:\n\n1. **Configurações Globais (`--global`)**:\n   - As configurações globais são salvas no arquivo `.gitconfig` localizado no diretório home do usuário.\n   - **Localização**:\n     - **Linux/Mac**: `~/.gitconfig`\n     - **Windows**: `C:\\Users\\SeuNomeDeUsuario\\.gitconfig`\n   - Você pode abrir esse arquivo em um editor de texto para visualizar ou editar as configurações.\n\n2. **Configurações de Sistema (`--system`)**:\n   - As configurações de sistema são aplicadas a todos os usuários da máquina e são armazenadas no arquivo de configuração global do sistema.\n   - **Localização**:\n     - **Linux**: `/etc/gitconfig`\n     - **Windows**: Pode estar em um caminho como `C:\\Program Files\\Git\\etc\\gitconfig`\n   - Essas configurações requerem permissões de administrador para serem alteradas.\n\n3. **Configurações Locais (por repositório)**:\n   - As configurações locais são específicas para um único repositório Git e são salvas no arquivo `config` dentro da pasta `.git` do repositório.\n   - **Localização**:\n     - No diretório do repositório Git: `.git/config`\n\nVocê pode visualizar as configurações atuais usando os seguintes comandos:\n\n- **Para ver todas as configurações globais**:\n  ```bash\n  git config --global --list\n  ```\n\nEsses comandos vão listar as configurações e seus valores, permitindo que você veja detalhes como o nome de usuário e o e-mail configurados para o Git.\n\n### Exemplo Completo Usando Git: Passo a Passo\n\nVamos seguir um exemplo onde fazemos modificações em um arquivo Python e gerenciamos essas alterações usando Git. Este processo incluirá a criação de commits para cada modificação, além de explorar conceitos importantes como o `HEAD`, branches, e o comando `git checkout`.\n\n### 1. Criação do Repositório e Primeira Modificação\n\n#### Inicializando o Repositório\n\nPrimeiro, vamos inicializar um novo repositório Git no diretório do projeto:\n\n```bash\ngit init\n```\n\nEste comando cria um repositório Git vazio, onde começaremos a rastrear nossas alterações.\n\n#### Criando e Adicionando o Arquivo ao Controle de Versão\n\nVamos criar um arquivo Python chamado `main.py`:\n\n```bash\ntouch main.py\n```\n\nAbra o arquivo `main.py` e adicione o\n\n seguinte código:\n\n```python\nprint(\"Hello, World!\")\n```\n\nAgora, vamos verificar o estado do repositório para ver como o Git está reconhecendo o arquivo:\n\n```bash\ngit status\n```\n\nVocê verá que `main.py` está listado como um arquivo não rastreado (untracked). Vamos adicionar esse arquivo ao Git para que ele comece a ser rastreado:\n\n```bash\ngit add main.py\n```\n\nAgora, faremos o primeiro commit para salvar o estado inicial do projeto:\n\n```bash\ngit commit -m \"Adiciona o arquivo main.py com um simples print\"\n```\n\n### 2. Primeira Modificação e Novo Commit\n\nAgora, vamos modificar o arquivo `main.py`:\n\n```python\nprint(\"Hello, World!\")\nprint(\"Primeira Modificação\")\n```\n\nDepois de fazer a modificação, vamos verificar novamente o estado dos arquivos:\n\n```bash\ngit status\n```\n\nAgora observamos que temos duas opções\n\n```mermaid\ngraph TD;\n\nA[Modified main.py in Working Directory] --> B{Choose an Option};\n\nB --> |\"git add main.py\"| C[Staging Area];\nC --> D[\"git commit -m 'Update main.py'\"];\nD --> E[New Commit Saved in Git Repository];\n\nB --> |\"git restore main.py\"| F[Working Directory Restored to Last Commit];\nF --> G[\"Restored from Git Repository\"];\n\nsubgraph Git Repository\n    E\n    G\nend\n```\n\n### Explicação do Fluxo:\n\n- **Opção 1: `git add`**:\n    - **`git add main.py`**: As mudanças no `main.py` são movidas para a Staging Area.\n    - **`git commit -m 'Update main.py'`**: Um novo commit é criado, e as mudanças são salvas na caixa do Git Repository.\n\n- **Opção 2: `git restore`**:\n    - **`git restore main.py`**: O arquivo `main.py` no Working Directory é restaurado a partir da última versão salva no Git Repository, descartando as mudanças feitas localmente.\n\nEsse diagrama ilustra claramente como as mudanças fluem entre o Working Directory, a Staging Area, e o Git Repository, dependendo da ação escolhida (`git add` ou `git restore`).\n\nO Git mostrará que o arquivo `main.py` foi modificado. Vamos adicionar essa modificação à área de staging e fazer um novo commit:\n\n### 2. Primeira Modificação e Novo Commit\n\n```bash\ngit add main.py\n```\n\n### Git status\n\nAo realizar o Git status observamos que temos 2 opções novamente\n\n```mermaid\ngraph TD;\n\nA[Modified main.py in Staging Area] --> B{Choose an Option};\n\nB --> |\"git commit -m 'Update main.py'\"| C[New Commit Saved in Git Repository];\nB --> |\"git restore --staged main.py\"| D[Unstaged, Returned to Working Directory];\n\nsubgraph Git Repository\n    C\nend\n\nD --> E[main.py in Working Directory];\n```\n\n### Explicação do Fluxo:\n\n- **Opção 1: `git commit`**:\n    - **`git commit -m 'Update main.py'`**: Cria um novo commit no Git Repository, salvando as mudanças que estavam na Staging Area.\n\n- **Opção 2: `git restore --staged`**:\n    - **`git restore --staged main.py`**: Remove o arquivo `main.py` da Staging Area, retornando-o ao Working Directory sem as mudanças serem come",
        "Workshop - Git e Github/Aula_01/README.md - Parte (2/2)\ntidas. Ele volta ao estado antes de ser adicionado à Staging Area.\n\nVamos seguir com o commit\n\n### 2. Primeiro Save no Commit\n\n```bash\ngit commit -m \"Adiciona a primeira modificação ao arquivo main.py\"\n```\n\n### Fluxo\n\n```mermaid\ngraph TD;\n\nsubgraph Working Directory\n    A[main.py Modified]\nend\n\nsubgraph Staging Area\n    B[main.py Staged]\nend\n\nsubgraph Git Repository\n    C[main.py Committed]\nend\n\nA --> |\"git add\"| B;\nB --> |\"git commit\"| C;\nC --> |\"git restore\"| A;\nB --> |\"git restore --staged\"| A;\n```\n\n### 3. Segunda Modificação e Novo Commit\n\nVamos modificar o arquivo novamente:\n\n```python\nprint(\"Hello, World!\")\nprint(\"Primeira Modificação\")\nprint(\"Segunda Modificação\")\n```\n\nNovamente, adicionamos e fazemos um commit dessas mudanças:\n\n```bash\ngit add main.py\ngit commit -m \"Adiciona a segunda modificação ao arquivo main.py\"\n```\n\n### 4. Terceira Modificação e Novo Commit\n\nFinalmente, vamos adicionar uma terceira modificação:\n\n```python\nprint(\"Hello, World!\")\nprint(\"Primeira Modificação\")\nprint(\"Segunda Modificação\")\nprint(\"Terceira Modificação\")\n```\n\nE, novamente, fazemos o commit:\n\n```bash\ngit add main.py\ngit commit -m \"Adiciona a terceira modificação ao arquivo main.py\"\n```\n\n### 5. Verificando o Histórico de Commits\n\nAgora, podemos usar o `git log` para visualizar o histórico de commits e ver todas as modificações que fizemos até agora:\n\n```bash\ngit log\n```\n\nO `git log` exibirá uma lista de todos os commits, mostrando as mensagens e os identificadores únicos (hashes) dos commits.\n\n### 6. Entendendo o `HEAD`\n\nO `HEAD` é um apontador especial que indica o commit atual em que você está trabalhando. Normalmente, o `HEAD` aponta para a branch `main`, que é a linha principal de desenvolvimento do projeto.\n\n**Ilustração com Mermaid:**\n\n```mermaid\ngraph TD;\n    A[Commit Inicial] --> B[Primeira Modificação];\n    B --> C[Segunda Modificação];\n    C --> D[Terceira Modificação];\n    E(HEAD -> main) --> D;\n```\n\nAqui, o `HEAD` está apontando para o commit mais recente na branch `main`. Isso significa que todas as operações, como novos commits, partirão desse ponto.\n\n### 7. Trabalhando com Branches\n\nBranches são como linhas do tempo paralelas no seu repositório. Elas permitem que você trabalhe em diferentes funcionalidades ou correções de bugs sem afetar a `main`.\n\n**Criando uma Nova Branch:**\n\nVamos criar uma nova branch chamada `nova-feature` para trabalhar em uma nova funcionalidade:\n\n```bash\ngit branch nova-feature\n```\n\nAgora, podemos mudar para essa branch e começar a trabalhar nela:\n\n```bash\ngit checkout nova-feature\n```\n\nIsso muda o `HEAD` para a nova branch `nova-feature`, o que significa que qualquer commit feito agora será registrado nessa branch.\n\n**Ilustração com Mermaid:**\n\n```mermaid\ngraph TD;\n    A[Commit Inicial] --> B[Primeira Modificação];\n    B --> C[Segunda Modificação];\n    C --> D[Terceira Modificação];\n    E(HEAD -> nova-feature) --> D;\n    F(nova-feature) --> D;\n    G(main) --> D;\n```\n\nAqui, a `nova-feature` diverge da `main` a partir do mesmo ponto, permitindo que você desenvolva funcionalidades de forma isolada.\n\n### 8. Usando `git checkout` para Navegar Entre Commits e Branches\n\nO comando `git checkout` permite que você navegue entre diferentes branches e commits. Se você quiser voltar para a branch `main`, pode usar:\n\n```bash\ngit checkout main\n```\n\nSe quiser explorar o estado do projeto em um commit anterior, use:\n\n```bash\ngit checkout <hash_do_commit>\n```\n\nIsso coloca você em um estado de \"detached HEAD\", onde você pode ver o estado do projeto naquele momento específico.\n\n### 9. Usando `git reset` para Voltar ao Commit Anterior\n\nSe você deseja desfazer as últimas mudanças e voltar ao estado de um commit anterior, pode usar o `git reset`. Aqui estão as opções:\n\n- **`git reset --soft <idCommit>`**: Volta para o commit anterior e mantém todas as alterações na área de staging.\n\n- **`git reset --mixed <idCommit>`**: Volta para o commit anterior, remove as alterações da área de staging, mas as mantém no diretório de trabalho.\n\n- **`git reset --hard <idCommit>`**: Volta para o commit anterior e descarta completamente todas as alterações feitas após esse commit.\n\n**Ilustração com Mermaid:**\n\n```mermaid\ngraph TD;\n    A[Commit Inicial] --> B[Primeira Modificação];\n    B --> C[Segunda Modificação];\n    C --> D[Terceira Modificação];\n    E(HEAD -> nova-feature) --> D;\n    F(nova-feature) --> D;\n    G(main) --> D;\n    H[Reset --hard] --> B;\n```\n\nAqui, o comando `git reset --hard` move o `HEAD` de volta para a \"Primeira Modificação\", descartando todas as alterações feitas após esse ponto.\n\n### 10. Criando uma Branch a Partir de um Commit Anterior\n\nSe você quer preservar o estado atual do projeto, mas precisa voltar a um commit anterior para experimentar algo novo, pode criar uma nova branch a partir desse commit:\n\n```bash\ngit checkout -b experiment <hash_do_commit>\n```\n\nIsso cria uma nova branch chamada `experiment`, começando a partir do commit que você especificou.\n\n**Ilustração com Mermaid:**\n\n```mermaid\ngraph TD;\n    A[Commit Inicial] --> B[Primeira Modificação];\n    B --> C[Segunda Modificação];\n    C --> D[Terceira Modificação];\n    E(HEAD -> experiment) --> B;\n    F(main) --> D;\n```\n\n### Caso real\n\nVamos expandir o exemplo para incluir três branches diferentes, cada uma com uma modificação específica em um arquivo, e um branch `main` que representa o código em produção. Vou descrever a situação e depois mostrar o diagrama em Mermaid.\n\n### Situação:\n\n1. **Branch `main`**: Contém o código de produção, sem as novas funcionalidades que estamos desenvolvendo.\n2. **Branch `feature-1`**: Modifica o `file1.py` para adicionar uma nova funcionalidade.\n3. **Branch `feature-2`**: Modifica o `file2.py` para adicionar outra funcionalidade.\n4. **Branch `feature-3`**: Modifica o `file3.py` para adicionar uma terceira funcionalidade.\n\n### Passos:\n\n1. **Criar e mudar para a branch `feature-1`**:\n    ```bash\n    git branch feature-1\n    git checkout feature-1\n    # Modificar file1.py e fazer commit\n    git commit -am \"Adiciona nova funcionalidade em file1.py\"\n    ```\n\n2. **Criar e mudar para a branch `feature-2`**:\n    ```bash\n    git branch feature-2\n    git checkout feature-2\n    # Modificar file2.py e fazer commit\n    git commit -am \"Adiciona nova funcionalidade em file2.py\"\n    ```\n\n3. **Criar e mudar para a branch `feature-3`**:\n    ```bash\n    git branch feature-3\n    git checkout feature-3\n    # Modificar file3.py e fazer commit\n    git commit -am \"Adiciona nova funcionalidade em file3.py\"\n    ```\n\n### Diagrama Mermaid:\n\nAqui está o diagrama que ilustra essas operações:\n\n```mermaid\ngraph TD;\n    subgraph main [Branch: main Production]\n        A[Initial Commit] --> B[Main Codebase];\n    end\n\n    subgraph feature-1 [Branch: feature-1]\n        B --> C[Commit: Modifica file1.py];\n    end\n\n    subgraph feature-2 [Branch: feature-2]\n        B --> D[Commit: Modifica file2.py];\n    end\n\n    subgraph feature-3 [Branch: feature-3]\n        B --> E[Commit: Modifica file3.py];\n    end\n\n    A --- B;\n    C --> G[feature-1];\n    D --> H[feature-2];\n    E --> I[feature-3];\n\n    style B fill:#f9f,stroke:#333,stroke-width:4px;\n    style C fill:#bbf,stroke:#333,stroke-width:2px;\n    style D fill:#bfb,stroke:#333,stroke-width:2px;\n    style E fill:#fbf,stroke:#333,stroke-width:2px;\n```\n\n### Explicação do Diagrama:\n\n- **Branch `main`**: Representa o código em produção, onde não foram aplicadas as novas funcionalidades.\n- **Branch `feature-1`**: Diverge do `main` após o commit inicial e inclui uma modificação em `file1.py`.\n- **Branch `feature-2`**: Diverge do `main` após o commit inicial e inclui uma modificação em `file2.py`.\n- **Branch `feature-3`**: Diverge do `main` após o commit inicial e inclui uma modificação em `file3.py`.\n\nCada branch permite que você trabalhe em funcionalidades diferentes de forma isolada. Os commits em cada branch representam o trabalho feito nessas funcionalidades. Quando as funcionalidades estiverem prontas e testadas, você poderá mesclar (`merge`) essas branches de volta ao `main` para que as novas funcionalidades sejam incorporadas ao código de produção.\n\nVamos continuar o exemplo, adicionando os comandos para mesclar as branches de funcionalidades (`feature-1`, `feature-2`, `feature-3`) de volta ao `main` quando as funcionalidades estiverem prontas.\n\n### Situação Revisada:\n\n1. **Branch `main`**: Contém o código de produção.\n2. **Branch `feature-1`**: Modifica o `file1.py` para adicionar uma nova funcionalidade.\n3. **Branch `feature-2`**: Modifica o `file2.py` para adicionar outra funcionalidade.\n4. **Branch `feature-3`**: Modifica o `file3.py` para adicionar uma terceira funcionalidade.\n\n### Passos Revisados:\n\n1. **Criar e mudar para a branch `feature-1`**:\n    ```bash\n    git branch feature-1\n    git checkout feature-1\n    # Modificar file1.py e fazer commit\n    git commit -am \"Adiciona nova funcionalidade em file1.py\"\n    ```\n\n2. **Criar e mudar para a branch `feature-2`**:\n    ```bash\n    git branch feature-2\n    git checkout feature-2\n    # Modificar file2.py e fazer commit\n    git commit -am \"Adiciona nova funcionalidade em file2.py\"\n    ```\n\n3. **Criar e mudar para a branch `feature-3`**:\n    ```bash\n    git branch feature-3\n    git checkout feature-3\n    # Modificar file3.py e fazer commit\n    git commit -am \"Adiciona nova funcionalidade em file3.py\"\n    ```\n\n### Mesclando as Branches de Funcionalidade no `main`:\n\nDepois que cada funcionalidade estiver pronta, você pode mesclar essas branches de volta ao `main`. Aqui estão os passos:\n\n1. **Mesclar `feature-1` no `main`**:\n    ```bash\n    git checkout main\n    git merge feature-1\n    ```\n\n2. **Mesclar `feature-2` no `main`**:\n    ```bash\n    git checkout main\n    git merge feature-2\n    ```\n\n3. **Mesclar `feature-3` no `main`**:\n    ```bash\n    git checkout main\n    git merge feature-3\n    ```\n\n### Diagrama Mermaid Atualizado:\n\nAqui está o diagrama que ilustra essas operações, incluindo as etapas de merge:\n\n```mermaid\ngraph TD;\n    subgraph main [Branch: main Production]\n        A[Initial Commit] --> B[Main Codebase];\n        F[Merge feature-1] --> G[Merge feature-2];\n        G --> H[Merge feature-3];\n    end\n\n    subgraph feature-1 [Branch: feature-1]\n        B --> C[Commit: Modifica file1.py];\n    end\n\n    subgraph feature-2 [Branch: feature-2]\n        B --> D[Commit: Modifica file2.py];\n    end\n\n    subgraph feature-3 [Branch: feature-3]\n        B --> E[Commit: Modifica file3.py];\n    end\n\n    C --> F;\n    D --> G;\n    E --> H;\n```\n\n### Explicação do Diagrama Atualizado:\n\n- **Branch `main`**: Representa o código de produção. Inicialmente, contém apenas o commit inicial e o código base.\n- **Branches de Funcionalidade**:\n  - **`feature-1`**: Contém a modificação em `file1.py`.\n  - **`feature-2`**: Contém a modificação em `file2.py`.\n  - **`feature-3`**: Contém a modificação em `file3.py`.\n- **Mesclagens (`Merges`)**:\n  - Cada branch de funcionalidade é mesclada de volta ao `main`, integrando as novas funcionalidades no código de produção.\n\n### Comandos de Merge:\n\n- **Mesclar `feature-1` no `main`**:\n  ```bash\n  git checkout main\n  git merge feature-1\n  ```\n\n- **Mesclar `feature-2` no `main`**:\n  ```bash\n  git checkout main\n  git merge feature-2\n  ```\n\n- **Mesclar `feature-3` no `main`**:\n  ```bash\n  git checkout main\n  git merge feature-3\n  ```\n\n### Conclusão:\n\nEsse fluxo permite que cada funcionalidade seja desenvolvida em isolamento, testada individualmente e, quando pronta, integrada ao código de produção sem afetar o `main` até que tudo esteja pronto. Isso torna o processo de desenvolvimento mais seguro e organizado, minimizando conflitos e problemas na integração das funcionalidades.\n\n### O que vamos ver amanhã:\n\nAmanhã, vamos explorar em detalhes o conceito de **Remote Repository** no Git. Até agora, vimos como o **Working Directory**, a **Staging Area**, e o **Local Git Repository** trabalham juntos na sua máquina local para gerenciar as mudanças no seu projeto.\n\nAgora, vamos entender como o **Remote Repository** se encaixa nesse fluxo. O **Remote Repository** é uma versão do seu repositório que fica armazenada em um servidor remoto, como o GitHub, GitLab ou Bitbucket. Ele permite que você:\n\n- **Compartilhe Código com Outros Desenvolvedores**: Enviar (push) seus commits para um repositório remoto permite que outros desenvolvedores acessem e colaborem no seu projeto.\n- **Mantenha um Backup Externo**: Ter uma cópia do seu repositório em um servidor remoto fornece uma camada extra de segurança para o seu trabalho.\n- **Colabore de Forma Eficiente**: Usar um repositório remoto facilita a colaboração entre times, onde cada membro pode clonar, puxar (pull) e enviar mudanças para o repositório compartilhado.\n\nNo diagrama que revisamos, o **Remote Repository** é representado como o destino para onde você envia as mudanças feitas no **Local Git Repository**. Amanhã, veremos como configurar e trabalhar com repositórios remotos, incluindo comandos essenciais como `git push`, `git pull`, e `git clone`, para que você possa colaborar efetivamente em projetos de dados com outras pessoas.\n\n```mermaid\ngraph TD;\n\nA[Working Directory] --> |\"git add\"| B[Staging Area];\nB --> |\"git commit\"| C[Commit];\nC --> |\"Stored in\"| D[Local Git Repository];\n\nsubgraph Local Machine\n    A\n    B\n    C\n    D\nend\n\nD --> |\"git push\"| E[Remote Repository];\n```\n\n",
        "Workshop - Git e Github/Aula_02/README.md - Parte (1/2)\nWorkshop - Git e Github/Aula_02/README.md\n\n# Introdução ao GitHub e Repositórios Remotos - Aula 02\n\nBem-vindo à nossa aula sobre GitHub e repositórios remotos! Hoje, vamos explorar como o uso de um repositório remoto, como o GitHub, pode resolver vários desafios que surgem ao trabalhar com um repositório Git local. Veremos como configurar sua conta no GitHub, migrar seu projeto local para lá, e utilizar as principais funcionalidades dessa plataforma.\n\n## 1. Problemas ao Ter Apenas um Git Local\n\n### Desafios de Trabalhar Somente com Git Local\n\nQuando trabalhamos apenas com um repositório Git local, enfrentamos algumas limitações:\n\n- **Colaboração Limitada**: Compartilhar código com outros desenvolvedores exige o uso de métodos manuais, como enviar arquivos por e-mail, o que é ineficiente e propenso a erros.\n- **Falta de Backup**: Sem um repositório remoto, todo o código fica armazenado em sua máquina local. Se o disco rígido falhar, você pode perder todo o trabalho.\n- **Histórico de Projetos Restrito**: Manter um histórico de versões só em sua máquina impede que outros colaboradores acessem facilmente o progresso do projeto.\n\n**Diagrama Ilustrativo**:\n\n```mermaid\ngraph TD;\n\nA1[Working Directory - Colaborador 1] --> |\"git add\"| B1[Staging Area - Colaborador 1];\nB1 --> |\"git commit\"| C1[Commit - Colaborador 1];\nC1 --> |\"Stored in\"| D1[Local Git Repository - Colaborador 1];\n\nA2[Working Directory - Colaborador 2] --> |\"git add\"| B2[Staging Area - Colaborador 2];\nB2 --> |\"git commit\"| C2[Commit - Colaborador 2];\nC2 --> |\"Stored in\"| D2[Local Git Repository - Colaborador 2];\n\nA3[Working Directory - Colaborador 3] --> |\"git add\"| B3[Staging Area - Colaborador 3];\nB3 --> |\"git commit\"| C3[Commit - Colaborador 3];\nC3 --> |\"Stored in\"| D3[Local Git Repository - Colaborador 3];\n\nsubgraph Local Machine - Colaborador 1\n    A1\n    B1\n    C1\n    D1\nend\n\nsubgraph Local Machine - Colaborador 2\n    A2\n    B2\n    C2\n    D2\nend\n\nsubgraph Local Machine - Colaborador 3\n    A3\n    B3\n    C3\n    D3\nend\n\nD1 --> |\"git push\"| E[Remote Repository];\nD2 --> |\"git push\"| E[Remote Repository];\nD3 --> |\"git push\"| E[Remote Repository];\n\n```\n\n## 2. Quais Opções Temos?\n\n### Opções de Repositórios Remotos\n\nExistem várias plataformas que fornecem serviços de repositórios remotos, cada uma com suas características:\n\n- **GitHub**: Popular e amplamente utilizado, com forte integração com ferramentas de CI/CD e uma grande comunidade de desenvolvedores.\n- **GitLab**: Focado em DevOps, com recursos avançados de CI/CD e privacidade aprimorada.\n- **Bitbucket**: Integrado com a Atlassian (Jira, Confluence), popular em ambientes corporativos.\n\n**Diagrama Ilustrativo**:\n\n```mermaid\ngraph LR;\n    A[Local Git Repository] --> B[GitHub];\n    A --> C[GitLab];\n    A --> D[Bitbucket];\n```\n\n## 3. GitHub: Uma Visão Geral\n\n### O que é GitHub?\n\nO GitHub é uma plataforma de hospedagem de código que oferece controle de versão distribuído e funcionalidades de colaboração para desenvolvedores de software. Ele facilita o gerenciamento de repositórios Git e fornece ferramentas para revisão de código, gerenciamento de projetos, integração contínua, e mais.\n\n**Principais Recursos**:\n- **Pull Requests (PRs)**: Facilita a revisão de código e a colaboração.\n- **Issues**: Gerenciamento de tarefas e bugs.\n- **Actions**: Automação de fluxos de trabalho com CI/CD.\n\n**Diagrama Ilustrativo**:\n\n```mermaid\ngraph TD;\n    A[GitHub] --> B[Pull Requests];\n    A --> C[Issues];\n    A --> D[Actions];\n```\n\n## 4. Criando Nossa Conta e Configurando o GitHub\n\n### Configurando GitHub e SSH\n\n1. **Criando uma Conta no GitHub**:\n   - Visite [GitHub.com](https://github.com) e crie uma conta gratuita.\n\n### Configurando Autenticação SSH para GitHub\n\nA autenticação SSH é uma forma segura de conectar seu repositório local ao GitHub, sem precisar inserir seu nome de usuário e senha toda vez que fizer um push ou pull. Aqui está o passo a passo para criar uma chave SSH, adicionar essa chave à sua conta do GitHub, e usá-la para autenticação.\n\n### Passo a Passo para Criar uma Chave SSH\n\n1. **Gerar uma Nova Chave SSH**:\n\n   No terminal, execute o seguinte comando:\n   ```bash\n   ssh-keygen -t rsa\n   ```\n\n   - **O que esse comando faz**:\n     - **`ssh-keygen`**: Este comando é usado para gerar um novo par de chaves SSH.\n     - **`-t rsa`**: Especifica o tipo de chave a ser gerado, que neste caso é RSA (um dos tipos mais comuns e seguros).\n\n   - **O que ele cria**:\n     - Este comando gera dois arquivos:\n       - **`id_rsa`**: A chave privada (não compartilhe este arquivo com ninguém).\n       - **`id_rsa.pub`**: A chave pública (essa é a chave que você vai adicionar ao GitHub).\n\n2. **Salvar a Chave SSH**:\n\n   Após executar o comando, você verá um prompt pedindo onde salvar o arquivo:\n\n   ```\n   Enter file in which to save the key (/Users/username/.ssh/id_rsa):\n   ```\n\n   - **O que fazer**: Apenas pressione **Enter** para aceitar o caminho padrão (`~/.ssh/id_rsa`). Isso salva a chave na pasta `.ssh` no diretório home do seu usuário.\n\n3. **Definir uma Senha para a Chave (Opcional)**:\n\n   Em seguida, você será solicitado a definir uma senha para proteger a chave SSH:\n\n   ```\n   Enter passphrase (empty for no passphrase):\n   ```\n\n   - **O que fazer**: Pressione **Enter** novamente para não definir uma senha. Isso significa que você não precisará digitar uma senha cada vez que usar a chave SSH.\n\n   ```\n   Enter same passphrase again:\n   ```\n\n   - **O que fazer**: Pressione **Enter** novamente.\n\n   **Nota**: Definir uma senha adiciona uma camada extra de segurança, mas pode ser inconveniente se você precisar digitar a senha frequentemente.\n\n4. **Verificando a Chave SSH Gerada**:\n\n   Para ver o conteúdo da chave pública (aquela que você vai compartilhar com o GitHub), execute o seguinte comando:\n\n   ```bash\n   cat ~/.ssh/id_rsa.pub\n   ```\n\n   - **O que isso faz**: Mostra o conteúdo do arquivo `id_rsa.pub` no terminal. Este conteúdo é a chave pública que você precisa copiar e adicionar à sua conta do GitHub.\n   - **Resultado esperado**: Algo parecido com isso será exibido:\n     ```\n     ssh-rsa AAAAB3... rest of the key ... your.email@example.com\n     ```\n\n5. **Adicionar a Chave SSH ao GitHub**:\n\n   Agora, com a chave pública copiada, vá até sua conta do GitHub e siga os passos:\n\n   - Vá para *Settings* (Configurações).\n   - Na seção *SSH and GPG keys*, clique em *New SSH key*.\n   - Cole a chave pública que você copiou do terminal no campo *Key*.\n   - Dê um nome à chave para identificá-la facilmente (por exemplo, \"Meu PC\").\n   - Clique em *Add SSH key*.\n\n6. **Verificar a Conexão SSH com o GitHub**:\n\n   Para garantir que tudo está configurado corretamente, você pode testar a conexão SSH com o GitHub:\n\n   ```bash\n   ssh -T git@github.com\n   ```\n\n   - **O que esperar**: Se tudo estiver configurado corretamente, você verá uma mensagem como:\n     ```\n     Hi username! You've successfully authenticated, but GitHub does not provide shell access.\n     ```\n\n### Conclusão\n\nCom esses passos, você configurou uma autenticação SSH para o GitHub. Isso significa que, da próxima vez que fizer um `git push` ou `git pull`, o Git utilizará a chave SSH para autenticação, evitando a necessidade de inserir seu nome de usuário e senha manualmente. Esse método é mais seguro e conveniente, especialmente para desenvolvedores que frequentemente interagem com repositórios Git.\n\n## 5. Migrando Nosso Projeto para o GitHub\n\n### Usando `git remote` e `git push`\n\n1. **Criando um Novo Repositório no GitHub**:\n   - No GitHub, clique em \"New Repository\" e crie um repositório vazio.\n\n2. **Adicionando o Repositório Remoto**:\n   - No terminal, vincule seu repositório local ao GitHub:\n     ```bash\n     git remote add origin https://github.com/usuario/repo.git\n     ```\n\nO comando `git remote add origin https://github.com/usuario/repo.git` é usado para vincular seu repositório Git local a um repositório remoto no GitHub (ou em outra plataforma de hospedagem de código). Esse comando essencialmente cria uma referência para o repositório remoto, permitindo que você envie (push) ou traga (pull) alterações entre o repositório local e o remoto.\n\nAqui está uma explicação mais detalhada:\n\n- **`git remote add`**: Esse é o comando que adiciona um novo repositório remoto ao seu repositório local. Você pode ter múltiplos repositórios remotos associados a um único repositório local, cada um com um nome diferente (por exemplo, `origin`, `upstream`, etc.).\n\n- **`origin`**: Este é o nome padrão dado ao repositório remoto principal. O termo \"origin\" é apenas um nome que você escolhe para se referir ao repositório remoto. Você pode renomeá-lo, se desejar, mas `origin` é o nome padrão e mais comum.\n\n- **`https://github.com/usuario/repo.git`**: Esse é o URL do repositório remoto no GitHub. Ele indica a localização exata do repositório no qual você deseja enviar seu código. Esse URL pode estar no formato HTTP(S) (como neste caso) ou SSH, dependendo de como você configurou a autenticação.\n\nDepois de executar esse comando, o repositório local agora conhece a localização do repositório remoto e o nome `origin` está associado a esse URL. Isso permite que você use comandos como `git push origin main` para enviar suas alterações para o repositório remoto ou `git pull origin main` para trazer as mudanças mais recentes do remoto para o seu repositório local.\n\n3. **Enviando o Código para o GitHub**:\n   - Envie o código para o GitHub:\n     ```bash\n     git push -u origin main\n     ```\n\nAo executar o comando `git push -u origin main`, o Git tentará enviar os commits do seu repositório local para o repositório remoto no GitHub. **Neste momento**, se você estiver usando HTTPS para acessar o GitHub (o que é indicado pelo URL `https://github.com/usuario/repo.git`), o Git solicitará que você insira seu nome de usuário e senha do GitHub para autenticar a operação.\n\n### O que esperar:\n\n- **Nome de Usuário e Senha**: Quando o Git tenta fazer o push para o repositório remoto pela primeira vez, ele precisa autenticar sua identidade com o GitHub. Você verá uma solicitação no terminal pedindo para inserir seu nome de usuário e senha.\n  \n- **Autenticação com Tokens**: Desde agosto de 2021, o GitHub não aceita mais senhas para autenticação ao usar HTTPS. Em vez disso, você precisará usar um \"Personal Access Token\" (token de acesso pessoal) no lugar da senha. Este token pode ser gerado na sua conta do GitHub, na seção de configurações de desenvolvedor. Quando solicitado pela senha, insira o token.\n\n### Explicando em mais detalhes:\n\n- **Nome de Usuário e Token**: \n  - **Nome de Usuário**: O seu nome de usuário do GitHub.\n  - **Token**: Um token de acesso pessoal que você deve gerar no GitHub e usar no lugar da senha.\n\n- **Por que isso acontece?**\n  - **Autenticação HTTPS**: Quando você usa HTTPS para acessar o repositório, o Git precisa garantir que você tem as permissões necessárias para enviar (push) alterações. Isso é feito pedindo sua autenticação.\n  \n- **Após a primeira vez**: Se você configurar o cache de credenciais ou usar SSH, o Git pode armazenar essas informações para que você não precise digitá-las novamente para cada push.\n\nEssa autenticação é essencial para garantir que apenas usuários autorizados possam enviar alterações para o repositório remoto.\n\nO parâmetro `-u` no comando `git push -u origin main` é utilizado para definir a branch local (`main` neste caso) como a branch \"upstream\" padrão para a branch remota associada. Isso significa que, depois de utilizar esse comando uma vez, você pode simplesmente executar `git push` ou `git pull` sem precisar especificar explicitamente o repositório (`origin`) e a branch (`main`) novamente.\n\n### Explicando em mais detalhes:\n\n- **`-u` ou `--set-upstream`**: Este parâmetro faz com que o Git associe a branch local (no exemplo, `main`) com a branch remota correspondente no repositório remoto (`origin`). Após essa associação, o Git \"sabe\" de onde puxar (`pull`) e para onde empurrar (`push`) as alterações por padrão.\n\n- **Por que isso é útil?** \n  - Facilita comandos futuros: Uma vez que você fez o `git push -u origin main`, você não precisa mais escrever `git push origin main` em futuros pushes; basta usar `git push`.\n  - Automatiza o comportamento: Com essa associação configurada, comandos como `git pull` saberão automaticamente de onde trazer as mudanças, simplificando o fluxo de trabalho.\n\n### Exemplo de Uso:\n\nSuponha que você esteja trabalhando em um novo repositório local e ainda não tenha feito nenhum push para o repositório remoto. Você usa o seguinte comando:\n\n```bash\ngit push -u origin main\n```\n\nIsso faz com que:\n1. A branch `main` no seu repositório local seja enviada para o repositório `origin`.\n2. O Git configure a branch `main` local para \"rastrear\" a branch `main` no `origin`. Com isso, no futuro, você pode usar apenas `git push` ou `git pull` para enviar ou buscar atualizações, sem precisar especificar `origin` ou `main` novamente.\n\nEssa conveniência é especialmente útil em projetos onde você frequentemente faz push e pull da mesma branch remota.\n\n**Diagrama Ilustrativo**:\n\n```mermaid\ngraph TD;\n    A[Local Git Repository] --> |\"git remote add\"| B[GitHub Remote];\n    B --> |\"git push\"| C[Remote Repository];\n```\n\n## 6. Principais Features do GitHub\n\n### Explorando PRs, Issues, e Actions\n\n- **Pull Requests**: Facilita a revisão de código, permite discussões e aprovação antes da integração ao código principal.\n- **Issues**: Ferramenta para rastrear bugs, melhori",
        "Workshop - Git e Github/Aula_02/README.md - Parte (2/2)\nas e tarefas.\n- **Actions**: Automatiza testes, deploys e outras tarefas de CI/CD.\n\n**Diagrama Ilustrativo**:\n\n```mermaid\ngraph TD;\n    A[Developer 1] --> |\"Create PR\"| B[GitHub Repository];\n    B --> |\"Review PR\"| C[Developer 2];\n    C --> |\"Merge\"| D[Main Branch];\n```\n\nVamos refatorar o exemplo anterior utilizando Pull Requests (PRs) para melhorar o fluxo de trabalho e a colaboração entre os desenvolvedores.\n\n## 7. Estratégias de Pull Requests (PRs)\n\n### Melhorando a Colaboração com PRs\n\n1. **Branch Naming**: Utilize nomes de branches que reflitam a tarefa ou bug a ser resolvido.\n2. **PR Review Process**: Estabeleça um processo claro para a revisão de PRs, incluindo revisores designados.\n3. **Squash Commits**: Combine múltiplos commits em um único antes de fazer o merge, para manter o histórico de commits limpo.\n\n**Diagrama Ilustrativo**:\n\n```mermaid\ngraph TD;\n    A[Feature Branch] --> |\"Squash Commits\"| B[Single Commit];\n    B --> |\"Create PR\"| C[Main Branch];\n```\n\n### Situação\n\n1. **Branch `main`**: Contém o código de produção, sem as novas funcionalidades que estamos desenvolvendo.\n2. **Branch `feature-1`**: Modifica o `file1.py` para adicionar uma nova funcionalidade.\n3. **Branch `feature-2`**: Modifica o `file2.py` para adicionar outra funcionalidade.\n4. **Branch `feature-3`**: Modifica o `file3.py` para adicionar uma terceira funcionalidade.\n\n### Passos para Implementação com PRs\n\n1. **Criar e Mudar para a Branch `feature-1`**:\n    ```bash\n    git checkout -b feature-1\n    # Modificar file1.py e fazer commit\n    git commit -am \"Adiciona nova funcionalidade em file1.py\"\n    # Enviar a branch para o repositório remoto\n    git push -u origin feature-1\n    ```\n\n2. **Criar um Pull Request para `feature-1`**:\n    - No GitHub, crie um Pull Request da branch `feature-1` para a `main`.\n    - Aguarde a revisão e aprovação do PR.\n\n3. **Criar e Mudar para a Branch `feature-2`**:\n    ```bash\n    git checkout -b feature-2\n    # Modificar file2.py e fazer commit\n    git commit -am \"Adiciona nova funcionalidade em file2.py\"\n    # Enviar a branch para o repositório remoto\n    git push -u origin feature-2\n    ```\n\n4. **Criar um Pull Request para `feature-2`**:\n    - No GitHub, crie um Pull Request da branch `feature-2` para a `main`.\n    - Aguarde a revisão e aprovação do PR.\n\n5. **Criar e Mudar para a Branch `feature-3`**:\n    ```bash\n    git checkout -b feature-3\n    # Modificar file3.py e fazer commit\n    git commit -am \"Adiciona nova funcionalidade em file3.py\"\n    # Enviar a branch para o repositório remoto\n    git push -u origin feature-3\n    ```\n\n6. **Criar um Pull Request para `feature-3`**:\n    - No GitHub, crie um Pull Request da branch `feature-3` para a `main`.\n    - Aguarde a revisão e aprovação do PR.\n\n### Diagrama Mermaid Atualizado com PRs:\n\nAqui está o diagrama que ilustra o processo de criação de branches e PRs:\n\n```mermaid\ngraph TD;\n    subgraph main [Branch: main Production]\n        A[Initial Commit] --> B[Main Codebase];\n    end\n\n    subgraph feature-1 [Branch: feature-1]\n        B --> C[Commit: Modifica file1.py];\n        C --> D[Pull Request];\n        D --> E[Merge to Main];\n    end\n\n    subgraph feature-2 [Branch: feature-2]\n        B --> F[Commit: Modifica file2.py];\n        F --> G[Pull Request];\n        G --> H[Merge to Main];\n    end\n\n    subgraph feature-3 [Branch: feature-3]\n        B --> I[Commit: Modifica file3.py];\n        I --> J[Pull Request];\n        J --> K[Merge to Main];\n    end\n\n    A --- B;\n```\n\n### Explicação do Diagrama Atualizado:\n\n- **Branch `main`**: Representa o código de produção.\n- **Branches de Funcionalidade**:\n  - **`feature-1`**: Modifica `file1.py` e é enviada como um PR para a `main`.\n  - **`feature-2`**: Modifica `file2.py` e segue o mesmo processo.\n  - **`feature-3`**: Modifica `file3.py` e também segue o processo de PR.\n- **Pull Requests e Merges**:\n  - Cada branch é enviada como um PR. Após revisão e aprovação, ela é mesclada (`merged`) na `main`, garantindo que o código de produção seja atualizado de forma controlada e revisada.\n\n### Conclusão:\n\nEsse fluxo de trabalho com Pull Requests (PRs) promove uma colaboração mais organizada e segura, garantindo que todas as modificações sejam revisadas antes de serem integradas ao código de produção. Cada PR permite a discussão, revisão e validação das mudanças, assegurando a qualidade do código e a minimização de bugs em produção.\n\nAqui está um ciclo de desenvolvimento típico usando Git, desde a criação de mudanças no diretório de trabalho até a integração dessas mudanças na branch principal (main) após um Pull Request (PR).\n\n### Fluxo de Desenvolvimento\n\n1. **Diretório de Trabalho (Work Directory)**: Onde você faz as modificações nos arquivos.\n2. **Staging Area**: Onde você adiciona as mudanças que deseja incluir no próximo commit.\n3. **.git (Repositório Local)**: Onde os commits são armazenados localmente.\n4. **Push**: Envia os commits do repositório local para o repositório remoto (GitHub).\n5. **GitHub Branch**: A branch específica no GitHub onde as mudanças são enviadas.\n6. **Pull Request (PR) para Main**: As mudanças na branch específica são revisadas e, se aprovadas, mescladas na branch principal (`main`).\n7. **Pull Main**: A branch `main` atualizada é puxada (pull) de volta para o repositório local, sincronizando as mudanças aprovadas.\n\n### Verificação e Alterações\n\nO fluxo descrito está correto, mas vamos detalhar cada etapa no diagrama Mermaid para garantir que todas as etapas estão cobertas.\n\n### Diagrama Mermaid\n\n```mermaid\ngraph TD;\n    A[Work Directory] --> |\"git add\"| B[Staging Area];\n    B --> |\"git commit\"| C[.git Local Repository];\n    C --> |\"git push\"| D[GitHub Branch feature-branch];\n    D --> |\"Create Pull Request\"| E[Pull Request to Main];\n    E --> |\"Review and Merge PR\"| F[GitHub Branch main];\n    F --> |\"git pull\"| G[.git Local Repository];\n    G --> |\"Update Work Directory\"| A;\n```\n\n### Explicação do Diagrama\n\n1. **Work Directory**: Você começa fazendo alterações no código, que estão no diretório de trabalho do seu projeto.\n2. **Staging Area**: Com o comando `git add`, você move as alterações para a Staging Area, preparando-as para o commit.\n3. **.git (Local Repository)**: Usando `git commit`, as mudanças na Staging Area são registradas no repositório local, criando um snapshot do código naquele momento.\n4. **Push para GitHub Branch**: O comando `git push` envia os commits do repositório local para uma branch específica no GitHub, como `feature-branch`.\n5. **GitHub Branch (feature-branch)**: Essa branch no GitHub é onde o código modificado reside enquanto aguarda revisão.\n6. **Pull Request para Main**: Um PR é criado a partir da `feature-branch` para a `main`. Outros desenvolvedores revisam as mudanças, discutem e sugerem melhorias.\n7. **Review e Merge do PR**: Após a revisão, o PR é mesclado na branch principal (`main`) no GitHub.\n8. **Pull Main**: Finalmente, você sincroniza o repositório local com o repositório remoto atualizado usando `git pull`, trazendo as mudanças aprovadas na branch `main` de volta para o seu ambiente local.\n\n### Conclusão\n\nEsse fluxo reflete um ciclo de desenvolvimento completo e organizado, promovendo boas práticas de controle de versão, colaboração e integração contínua. Cada etapa garante que as mudanças sejam revisadas antes de serem integradas ao código principal, mantendo a integridade do projeto.\n\n## 8. Fazendo `git clone`\n\n### Como Clonar um Repositório\n\n1. **Clonando um Repositório Existente**:\n   - No terminal, clone o repositório:\n     ```bash\n     git clone https://github.com/usuario/repo.git\n     ```\n\n2. **Trabalhando com o Repositório Clonado**:\n   - Navegue até o diretório clonado e comece a trabalhar:\n     ```bash\n     cd repo\n     ```\n\n**Diagrama Ilustrativo**:\n\n```mermaid\ngraph TD;\n    A[GitHub Repository] --> |\"git clone\"| B[Local Machine];\n    B --> |\"Local Copy\"| C[Working Directory];\n```\n\n### 9. Trabalhando com o `.gitignore`\n\n### O que é o `.gitignore`?\n\nO arquivo `.gitignore` é um arquivo especial que você pode incluir em seu repositório Git para especificar quais arquivos ou diretórios devem ser ignorados pelo Git. Isso significa que qualquer coisa listada no `.gitignore` não será rastreada, versionada ou enviada ao repositório remoto, mesmo se estiver no diretório de trabalho.\n\n### Por que Usar um `.gitignore`?\n\n- **Arquivos Temporários**: Muitas vezes, projetos geram arquivos temporários ou de build que não precisam ser versionados. Exemplos incluem arquivos `.log`, diretórios `node_modules/`, arquivos de compilação, entre outros.\n- **Dados Sensíveis**: Evitar a inclusão de arquivos que contenham informações sensíveis, como senhas, chaves de API, ou configurações locais específicas que não deveriam ser compartilhadas.\n- **Configurações de Ambiente**: Arquivos de configuração que variam de acordo com o ambiente (por exemplo, `.env`) e não devem ser incluídos no repositório para evitar conflitos entre diferentes ambientes de desenvolvimento.\n\n### Como Criar e Usar um `.gitignore`?\n\n1. **Criando um Arquivo `.gitignore`**:\n   - No diretório raiz do seu projeto, crie um arquivo chamado `.gitignore`.\n     ```bash\n     touch .gitignore\n     ```\n\n2. **Adicionando Padrões ao `.gitignore`**:\n   - Especifique os arquivos e diretórios que você deseja ignorar.\n     ```plaintext\n     # Ignorar todos os arquivos .log\n     *.log\n\n     # Ignorar o diretório node_modules/\n     node_modules/\n\n     # Ignorar arquivos de configuração de ambiente\n     .env\n     ```\n\n3. **Aplicando o `.gitignore`**:\n   - Após adicionar arquivos ao `.gitignore`, eles não serão mais rastreados pelo Git. Se algum arquivo já estiver sendo rastreado, você precisará removê-lo do índice do Git (sem removê-lo do seu diretório de trabalho):\n     ```bash\n     git rm --cached nome_do_arquivo\n     ```\n\n### Exemplo de `.gitignore` para um Projeto Python\n\nAqui está um exemplo de um arquivo `.gitignore` típico para um projeto Python:\n\n```plaintext\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# Diretórios de ambiente virtual\nvenv/\nenv/\n.venv/\n\n# Arquivos de configuração local\n.env\n\n# Arquivos de log\n*.log\n\n# Arquivos de configuração do IDE\n.vscode/\n.idea/\n```\n\n**Diagrama Ilustrativo**:\n\n```mermaid\ngraph TD;\n    A[Projeto Python] --> B[.gitignore];\n    B --> C[Ignora venv/, __pycache__/, *.log];\n    B --> D[Ignora .env, .vscode/];\n    B --> E[Rastreia apenas arquivos relevantes];\n```\n\n### Dicas ao Usar `.gitignore`:\n\n- **Coloque o `.gitignore` no início do projeto**: É uma boa prática configurar o `.gitignore` logo no início do desenvolvimento para evitar que arquivos indesejados sejam adicionados ao repositório.\n- **Não ignore demais**: Certifique-se de que apenas arquivos realmente desnecessários sejam ignorados. Ignorar demais pode levar à perda de arquivos importantes.\n- **Modelos de `.gitignore`**: Existem modelos prontos de `.gitignore` para diferentes linguagens e frameworks. O GitHub, por exemplo, oferece uma ampla coleção de templates de `.gitignore` [aqui](https://github.com/github/gitignore).\n\n### Conclusão\n\nO uso do `.gitignore` é essencial para manter um repositório Git limpo e organizado, rastreando apenas os arquivos que realmente importam para o desenvolvimento do projeto. Ele ajuda a evitar conflitos e garante que dados sensíveis ou desnecessários não sejam acidentalmente compartilhados ou versionados.\n\n### 10. Importância do `README.md`\n\n### O que é o `README.md`?\n\nO `README.md` é um arquivo de texto, geralmente escrito em formato Markdown, que é incluído na raiz de um repositório Git. Ele serve como a \"porta de entrada\" do seu projeto, fornecendo informações essenciais e contextuais para qualquer pessoa que acessar o repositório. Um bom `README.md` é fundamental para tornar o projeto acessível, compreensível e útil para outros desenvolvedores, colaboradores ou até mesmo para você no futuro.\n\n### Por que o `README.md` é Importante?\n\n- **Primeira Impressão**: O `README.md` é geralmente a primeira coisa que as pessoas veem quando visitam seu repositório. Um arquivo bem escrito e claro pode atrair e engajar colaboradores, usuários, ou mesmo empregadores potenciais.\n- **Documentação Essencial**: Ele fornece uma visão geral do projeto, incluindo seu propósito, como configurá-lo, utilizá-lo, e contribuir com ele. Isso ajuda a diminuir a curva de aprendizado para novos usuários ou desenvolvedores.\n- **Facilita a Colaboração**: Um `README.md` bem documentado esclarece como os colaboradores podem contribuir, incluindo diretrizes sobre pull requests, issues, e outros aspectos do desenvolvimento colaborativo.\n\n### O Que Incluir em um `README.md`?\n\n1. **Título do Projeto**:\n   - Nome claro e descritivo do projeto.\n\n2. **Descrição**:\n   - Uma breve descrição do que o projeto faz e qual problema ele resolve.\n\n3. **Instalação**:\n   - Instruções claras sobre como instalar ou configurar o projeto. Isso pode incluir requisitos de sistema, dependências e passos de instalação.\n\n4. **Uso**:\n   - Exemplos de como usar o projeto. Isso pode incluir exemplos de código, comandos ou capturas de tela.\n\n5. **Contribuição**:\n   - Diretrizes para contribuir com o projeto. Inclua informações sobre como enviar pull requests, reportar bugs ou sugerir melhorias.\n\n6. **Licença**:\n   - Especificar a licença sob a qual o projeto é distribuído, como MIT, GPL, Apache, etc.\n\n7. **Referências e Créditos**:\n   - Agradecimentos e links para recursos, bibliotecas ou pessoas que contribuíram para o projeto."
    ],
    "02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados": [
        "02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/README.md\n\n# workshop_02_aovivo\n\nVisite minha documentacao\n\n[![image](/pic/print.png)](https://lvgalvao.github.io/workshop_02_aovivo/)\n\n\n1. Clone o repositório:\n\n```bash\ngit clone https://github.com/lvgalvao/workshop_02_aovivo.git\ncd workshop_02_aovivo\n```\n\n2. Configure a versão correta do Python com `pyenv`:\n\n```bash\npyenv install 3.11.5\npyenv local 3.11.5\n```\n\n3. Configurar poetry para Python version 3.11.5 e ative o ambiente virtual:\n\n```bash\npoetry env use 3.11.5\npoetry shell\n```\n\n4. Instale as dependencias do projeto:\n\n```bash\npoetry install\n```\n\n5. Execute os testes para garantir que tudo está funcionando como esperado:\n\n```bash\npoetry run task test\n```\n\n6. Execute o comando para ver a documentação do projeto:\n\n```bash\npoetry run task test\n```\n\n7. Execute o comando de execucão da pipeline para realizar a ETL:\n\n```bash\npoetry run python app/etl.py\n```\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/mkdocs.yml\n\nsite_name: My Docs\n\ntheme:\n  name: material\n\nplugins:\n  - search\n  - mermaid2\n  - mkdocstrings\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/pyproject.toml\n\n[tool.poetry]\nname  \"workshop-02-aovivo\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"lvgalvaofilho <lvgalvaofilho@gmail.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\nmkdocs  \"^1.5.3\"\nmkdocs-mermaid2-plugin  \"^1.1.1\"\nmkdocs-material  \"^9.5.11\"\nmkdocstrings  {extras  [\"python\"], version  \"^0.24.0\"}\ntaskipy  \"^1.12.2\"\nisort  \"^5.13.2\"\nblack  \"^24.2.0\"\npytest  \"^8.0.1\"\npandas  \"^2.2.1\"\ntqdm  \"^4.66.2\"\nduckdb  \"^0.10.0\"\npydantic  \"^2.6.2\"\npandera  {extras  [\"io\"], version  \"^0.18.0\"}\nsqlalchemy  \"^2.0.27\"\npython-dotenv  \"^1.0.1\"\npsycopg2  \"^2.9.9\"\npsycopg2-binary  \"^2.9.9\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n[tool.taskipy.tasks]\nformat  \"\"\"\nisort .\nblack .\n\"\"\"\nkill  \"kill -9 $(lsof -t -i :8000)\"\ntest  \"pytest -v\"\nrun  \"\"\"\npython3 app/main.py\n\"\"\"\ndoc  \"mkdocs serve\"\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/schema_crm.py\n\nfrom pandera import Check, Column, DataFrameSchema, Index, MultiIndex\n\nschema  DataFrameSchema(\n    columns{\n        \"id_produto\": Column(\n            dtype\"int64\",\n            checks[\n                Check.greater_than_or_equal_to(min_value1.0),\n                Check.less_than_or_equal_to(max_value10.0),\n            ],\n            nullableFalse,\n            uniqueFalse,\n            coerceFalse,\n            requiredTrue,\n            regexFalse,\n            descriptionNone,\n            titleNone,\n        ),\n        \"nome\": Column(\n            dtype\"object\",\n            checksNone,\n            nullableFalse,\n            uniqueFalse,\n            coerceFalse,\n            requiredTrue,\n            regexFalse,\n            descriptionNone,\n            titleNone,\n        ),\n        \"quantidade\": Column(\n            dtype\"int64\",\n            checks[\n                Check.greater_than_or_equal_to(min_value20.0),\n                Check.less_than_or_equal_to(max_value200.0),\n            ],\n            nullableFalse,\n            uniqueFalse,\n            coerceFalse,\n            requiredTrue,\n            regexFalse,\n            descriptionNone,\n            titleNone,\n        ),\n        \"preco\": Column(\n            dtype\"float64\",\n            checks[\n                Check.greater_than_or_equal_to(min_value5.0),\n                Check.less_than_or_equal_to(max_value120.0),\n            ],\n            nullableFalse,\n            uniqueFalse,\n            coerceFalse,\n            requiredTrue,\n            regexFalse,\n            descriptionNone,\n            titleNone,\n        ),\n        \"categoria\": Column(\n            dtype\"object\",\n            checksNone,\n            nullableFalse,\n            uniqueFalse,\n            coerceFalse,\n            requiredTrue,\n            regexFalse,\n            descriptionNone,\n            titleNone,\n        ),\n    },\n    checksNone,\n    indexIndex(\n        dtype\"int64\",\n        checks[\n            Check.greater_than_or_equal_to(min_value0.0),\n            Check.less_than_or_equal_to(max_value9.0),\n        ],\n        nullableFalse,\n        coerceFalse,\n        nameNone,\n        descriptionNone,\n        titleNone,\n    ),\n    dtypeNone,\n    coerceTrue,\n    strictFalse,\n    nameNone,\n    orderedFalse,\n    uniqueNone,\n    report_duplicates\"all\",\n    unique_column_namesFalse,\n    add_missing_columnsFalse,\n    titleNone,\n    descriptionNone,\n)\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/.python-version\n\n3.11.5\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/etl.py\n\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nimport pandera as pa\nfrom dotenv import load_dotenv\nfrom sqlalchemy import create_engine\n\nfrom schema import ProdutoSchema, ProductSchemaKPI\n\ndef load_settings():\n    \"\"\"Carrega as configurações a partir de variáveis de ambiente.\"\"\"\n    dotenv_path  Path.cwd() / '.env'\n    load_dotenv(dotenv_pathdotenv_path)\n\n    settings  {\n        \"db_host\": os.getenv(\"POSTGRES_HOST\"),\n        \"db_user\": os.getenv(\"POSTGRES_USER\"),\n        \"db_pass\": os.getenv(\"POSTGRES_PASSWORD\"),\n        \"db_name\": os.getenv(\"POSTGRES_DB\"),\n        \"db_port\": os.getenv(\"POSTGRES_PORT\"),\n    }\n    return settings\n\n@pa.check_output(ProdutoSchema, lazyTrue)\ndef extrair_do_sql(query: str) -> pd.DataFrame:\n    \"\"\"\n    Extrai dados do banco de dados SQL usando a consulta fornecida.\n\n    Args:\n        query: A consulta SQL para extrair dados.\n\n    Returns:\n        Um DataFrame do Pandas contendo os dados extraídos.\n    \"\"\"\n    settings  load_settings()\n\n    # Criar a string de conexão com base nas configurações\n    connection_string  f\"postgresql://{settings['db_user']}:{settings['db_pass']}@{settings['db_host']}:{settings['db_port']}/{settings['db_name']}\"\n\n    # Criar engine de conexão\n    engine  create_engine(connection_string)\n\n    with engine.connect() as conn, conn.begin():\n            df_crm  pd.read_sql(query, conn)\n\n    return df_crm\n\n@pa.check_input(ProdutoSchema, lazyTrue)\n@pa.check_output(ProductSchemaKPI, lazyTrue)\ndef transformar(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transforma os dados do DataFrame aplicando cálculos e normalizações.\n\n    Args:\n        df: DataFrame do Pandas contendo os dados originais.\n\n    Returns:\n        DataFrame do Pandas após a aplicação das transformações.\n    \"\"\"\n    # Calcular valor_total_estoque\n    df['valor_total_estoque']  df['quantidade'] * df['preco']\n    \n    # Normalizar categoria para maiúsculas\n    df['categoria_normalizada']  df['categoria'].str.lower()\n    \n    # Determinar disponibilidade (True se quantidade > 0)\n    df['disponibilidade']  df['quantidade'] > 0\n    \n    return df\n\nimport duckdb\nimport pandas as pd\n\n@pa.check_input(ProductSchemaKPI, lazyTrue)\ndef load_to_duckdb(df: pd.DataFrame, table_name: str, db_file: str  'my_duckdb.db'):\n    \"\"\"\n    Carrega o DataFrame no DuckDB, criando ou substituindo a tabela especificada.\n\n    Args:\n        df: DataFrame do Pandas para ser carregado no DuckDB.\n        table_name: Nome da tabela no DuckDB onde os dados serão inseridos.\n        db_file: Caminho para o arquivo DuckDB. Se não existir, será criado.\n    \"\"\"\n    # Conectar ao DuckDB. Se o arquivo não existir, ele será criado.\n    con  duckdb.connect(databasedb_file, read_onlyFalse)\n    \n    # Registrar o DataFrame como uma tabela temporária\n    con.register('df_temp', df)\n    \n    # Utilizar SQL para inserir os dados da tabela temporária em uma tabela permanente\n    # Se a tabela já existir, substitui.\n    con.execute(f\"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df_temp\")\n    \n    # Fechar a conexão\n    con.close()\n\n\nif __name__  \"__main__\":\n    \n    query  \"SELECT * FROM produtos_bronze_email\"\n    df_crm  extrair_do_sql(queryquery)\n    df_crm_kpi  transformar(df_crm)\n\n    with open(\"inferred_schema.json\", \"r\") as file:\n         file.write(df_crm_kpi.to_json())\n\n    load_to_duckdb(dfdf_crm_kpi, table_name\"tabela_kpi\")\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/etl_infer_schema.py\n\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nimport pandera as pa\nfrom dotenv import load_dotenv\nfrom sqlalchemy import create_engine\n\n\ndef load_settings():\n    \"\"\"Carrega as configurações a partir de variáveis de ambiente.\"\"\"\n    dotenv_path  Path.cwd() / '.env'\n    load_dotenv(dotenv_pathdotenv_path)\n\n    settings  {\n        \"db_host\": os.getenv(\"POSTGRES_HOST\"),\n        \"db_user\": os.getenv(\"POSTGRES_USER\"),\n        \"db_pass\": os.getenv(\"POSTGRES_PASSWORD\"),\n        \"db_name\": os.getenv(\"POSTGRES_DB\"),\n        \"db_port\": os.getenv(\"POSTGRES_PORT\"),\n    }\n    return settings\n\ndef extrair_do_sql(query: str) -> pd.DataFrame:\n\n    settings  load_settings()\n\n    # Criar a string de conexão com base nas configurações\n    connection_string  f\"postgresql://{settings['db_user']}:{settings['db_pass']}@{settings['db_host']}:{settings['db_port']}/{settings['db_name']}\"\n\n    # Criar engine de conexão\n    engine  create_engine(connection_string)\n\n    with engine.connect() as conn, conn.begin():\n            df_crm  pd.read_sql(query, conn)\n\n    return df_crm\n\nif __name__  \"__main__\":\n    \n    query  \"SELECT * FROM produtos_bronze\"\n    df_crm  extrair_do_sql(queryquery)\n    schema_crm  pa.infer_schema(df_crm)\n\n    with open(\"schema_crm.py\", \"w\", encoding\"utf-8\") as arquivo:\n         arquivo.write(schema_crm.to_script())\n\n    print(schema_crm)\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/ler_duckdb.py\n\nimport duckdb\n\ndef read_from_duckdb_and_print(table_name: str, db_file: str  'my_duckdb.db'):\n    \"\"\"\n    Lê dados de uma tabela DuckDB e imprime os resultados.\n\n    Parâmetros:\n    - table_name: Nome da tabela de onde os dados serão lidos.\n    - db_file: Caminho para o arquivo DuckDB.\n    \"\"\"\n    # Conectar ao DuckDB\n    con  duckdb.connect(databasedb_file)\n\n    # Executar consulta SQL\n    query  f\"SELECT * FROM {table_name}\"\n    result  con.execute(query).fetchall()\n\n    # Fechar a conexão\n    con.close()\n\n    # Imprimir os resultados\n    for row in result:\n        print(row)\n\nif __name__  \"__main__\":\n    # Nome da tabela para consulta\n    table_name  \"tabela_kpi\"\n    \n    # Ler dados da tabela e imprimir os resultados\n    read_from_duckdb_and_print(table_name)\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/schema.py\n\nimport pandera as pa\nfrom pandera.typing import DataFrame, Series\n\nemail_regex  r\"[^@]+@[^@]+\\.[^@]+\"\n\nclass ProdutoSchema(pa.SchemaModel):\n    \"\"\"\n    Define o esquema para a validação de dados de produtos com Pandera.\n    \n    Este esquema inclui campos básicos para produtos, incluindo um campo de e-mail\n    validado por uma expressão regular.\n\n    Attributes:\n        id_produto (Series[int]): Identificador do produto, deve estar entre 1 e 20.\n        nome (Series[str]): Nome do produto.\n        quantidade (Series[int]): Quantidade disponível do produto, deve estar entre 20 e 200.\n        preco (Series[float]): Preço do produto, deve estar entre 5.0 e 120.0.\n        categoria (Series[str]): Categoria do produto.\n        email (Series[str]): E-mail associado ao produto, deve seguir o formato padrão de e-mails.\n    \"\"\"\n    id_produto: Series[int]\n    nome: Series[str]\n    quantidade: Series[int]  pa.Field(ge20, le200)\n    preco: Series[float]  pa.Field(ge05.0, le120.0)\n    categoria: Series[str]\n    email: Series[str]  pa.Field(regexemail_regex)\n\n    class Config:\n        coerce  True\n        strict  True\n\nclass ProductSchemaKPI(ProdutoSchema):\n\n    valor_total_estoque: Series[float]  pa.Field(ge0)  # O valor total em estoque deve ser > 0\n    categoria_normalizada: Series[str]  # Assume-se que a categoria será uma string, não precisa de check específico além de ser uma string\n    disponibilidade: Series[bool]  # Disponibilidade é um booleano, então não precisa de check específico\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/app/schema_email.py\n\nimport pandera as pa\nfrom pandera.typing import Series\n\nemail_regex  r\"[^@]+@[^@]+\\.[^@]+\"\n\nclass ProdutoSchemaEmail(pa.SchemaModel):\n    id_produto: Series[int]  pa.Field(ge1, le10)\n    nome: Series[str]\n    quantidade: Series[int]  pa.Field(ge20, le200)\n    preco: Series[float]  pa.Field(ge5.0, le120.0)\n    categoria: Series[str]\n    email: Series[str]  pa.Field(regexemail_regex)\n\n    class Config:\n        coerce  True\n        strict  True\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/docs/index.md\n\n# Workshop 02 - Data Quality\n\nPara desenvolver o desafio de negocio, vamos montar a seguinte ETL\n\n## Fluxo\n\n```mermaid\ngraph TD;\n    A[Configura Variáveis] --> B[Ler o Banco SQL];\n    B --> V[Validação do Schema de Entrada];\n    V -->|Falha| X[Alerta de Erro];\n    V -->|Sucesso| C[Transformar os KPIs];\n    C --> Y[Validação do Schema de Saída];\n    Y -->|Falha| Z[Alerta de Erro];\n    Y -->|Sucesso| D[Salvar no DuckDB];\n```\n\n# Contrato de dados\n\n::: app.schema.ProdutoSchema\n\n# Transformacoes\n\n## Configura Variáveis\n\n::: app.etl.load_settings\n\n## Ler o Banco SQL\n::: app.etl.extrair_do_sql\n\n## Transformar os KPIs\n\n::: app.etl.transformar\n\n## Salvar no DuckDB\nS\n::: app.etl.load_to_duckdb\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/exemplo/exemplo00.py\n\nfrom pydantic import BaseModel, PositiveFloat, PositiveInt\n\ndados  {\n    \"id_produto\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    \"nome\": [\"Produto A\", \"Produto B\", \"Produto C\", \"Produto D\", \"Produto E\", \n             \"Produto F\", \"Produto G\", \"Produto H\", \"Produto I\", \"Produto J\"],\n    \"quantidade\": [100, 150, 200, 50, 120, 80, 60, 30, 90, 20],\n    \"preco\": [10.0, 20.0, 15.0, 5.0, 22.0, 45.0, 120.0, 85.0, 55.0, 100.0],\n    \"categoria\": [\"eletronicos\", \"mobilia\", \"informatica\", \"decoracao\", \"eletronicos\", \n                  \"mobilia\", \"informatica\", \"decoracao\", \"eletronicos\", \"mobilia\"]\n}\n\nclass SchemaDados(BaseModel):\n    id_produto: int\n    nome: str\n    quantidade: PositiveInt\n    preco: PositiveFloat\n    categoria: str\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/exemplo/exemplo01.py\n\nfrom datetime import datetime\nfrom typing import Tuple\n\nfrom pydantic import BaseModel, PositiveInt, validate_call\n\n\nclass NumeroPositivo(BaseModel):\n    numero: PositiveInt\n\n@validate_call()\ndef calculadora(x: NumeroPositivo, y: NumeroPositivo) -> NumeroPositivo:\n    return x + y\n\nprint(calculadora(4,-5))\nprint(calculadora(6,7))\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/exemplo/exemplo02.py\n\nimport pandas as pd\nimport pandera as pa\nfrom pandera import Check, Column, DataFrameSchema\n\ndf  pd.DataFrame({\n    \"column1\": [5, 10, 20],\n    \"column2\": [\"a\", \"b\", \"c\"],\n    \"column3\": pd.to_datetime([\"2010\", \"2011\", \"2012\"]),\n})\nschema  pa.infer_schema(df)\n\nwith open(\"inferred_schema.py\", \"w\") as file:\n         file.write(schema.to_script())\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/create_table_produtos_bronze.sql\n\nCREATE TABLE produtos_bronze (\n    id_produto SERIAL PRIMARY KEY,\n    nome VARCHAR(255) NOT NULL,\n    quantidade INT NOT NULL,\n    preco FLOAT NOT NULL,\n    categoria VARCHAR(255) NOT NULL\n);\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/create_table_produtos_bronze_email.sql\n\nCREATE TABLE produtos_bronze_email (\n    id_produto SERIAL PRIMARY KEY,\n    nome VARCHAR(255) NOT NULL,\n    quantidade INT NOT NULL,\n    preco FLOAT NOT NULL,\n    categoria VARCHAR(255) NOT NULL,\n    email VARCHAR(255) NOT NULL\n);\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/insert_into_tabela_bronze.sql\n\nINSERT INTO produtos_bronze (nome, quantidade, preco, categoria) VALUES\n('Produto A', 100, 10.0, 'eletronicos'),\n('Produto B', 150, 20.0, 'mobilia'),\n('Produto C', 200, 15.0, 'informatica'),\n('Produto D', 50, 5.0, 'decoracao'),\n('Produto E', 120, 22.0, 'eletronicos'),\n('Produto F', 80, 45.0, 'mobilia'),\n('Produto G', 60, 120.0, 'informatica'),\n('Produto H', 30, 85.0, 'decoracao'),\n('Produto I', 90, 55.0, 'eletronicos'),\n('Produto J', 20, 100.0, 'mobilia');\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/insert_into_tabela_bronze_email.sql\n\nINSERT INTO produtos_bronze (nome, quantidade, preco, categoria, email) VALUES\n('Produto A', 100, 10.0, 'eletronicos','produtoA@example.com' ),\n('Produto B', 150, 20.0, 'mobilia', 'produtoA@example.com'),\n('Produto C', 200, 15.0, 'informatica', 'produtoA@example.com'),\n('Produto D', 50, 5.0, 'decoracao', 'produtoA@example.com'),\n('Produto E', 120, 22.0, 'eletronicos', 'produtoA@example.com'),\n('Produto F', 80, 45.0, 'mobilia', 'produtoA@example.com'),\n('Produto G', 60, 120.0, 'informatica', 'produtoA@example.com'),\n('Produto H', 30, 85.0, 'decoracao', 'produtoA@example.com'),\n('Produto I', 90, 55.0, 'eletronicos', 'produtoA@example.com'),\n('Produto J', 20, 100.0, 'mobilia', 'produtoA@example.com');\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/sql/insert_wrong_values_into_tabela_bronze.sql\n\nINSERT INTO produtos_bronze (nome, quantidade, preco, categoria) VALUES\n('Produto A', 100, 10.0, 'eletronicos'),\n('Produto B', -150, 20.0, 'mobilia'),\n('Produto C', 200, 15.0, 'informatica'),\n('Produto D', 50, 5.0, 'decoracao'),\n('Produto E', 120, 22.0, 'eletronicos'),\n('Produto F', 80, 45.0, 'mobilia'),\n('Produto G', 60, 120.0, 'informatica'),\n('Produto H', 30, 85.0, 'decoracao'),\n('Produto I', 90, 55.0, 'eletronicos'),\n('Produto J', 20, 100.0, 'mobilia');\n\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/tests/test_func_etl.py\n\nimport pandas as pd\n\nfrom app.etl import transformar\n\ndef test_calculo_valor_total_estoque():\n    # Preparação\n    df  pd.DataFrame({\n        'quantidade': [10, 5],\n        'preco': [20.0, 100.0],\n        'categoria': ['brinquedos', 'eletrônicos']\n    })\n    expected  pd.Series([200.0, 500.0], name'valor_total_estoque')\n\n    # Ação\n    result  transformar(df)\n\n    # Verificação\n    pd.testing.assert_series_equal(result['valor_total_estoque'], expected)\n\ndef test_normalizacao_categoria():\n    # Preparação\n    df  pd.DataFrame({\n        'quantidade': [1, 2],\n        'preco': [10.0, 20.0],\n        'categoria': ['brinquedos', 'eletrônicos']\n    })\n    expected  pd.Series(['BRINQUEDOS', 'ELETRÔNICOS'], name'categoria_normalizada')\n\n    # Ação\n    result  transformar(df)\n\n    # Verificação\n    pd.testing.assert_series_equal(result['categoria_normalizada'], expected)\n\ndef test_determinacao_disponibilidade():\n    # Preparação\n    df  pd.DataFrame({\n        'quantidade': [0, 2],\n        'preco': [10.0, 20.0],\n        'categoria': ['brinquedos', 'eletrônicos']\n    })\n    expected  pd.Series([False, True], name'disponibilidade')\n\n    # Ação\n    result  transformar(df)\n\n    # Verificação\n    pd.testing.assert_series_equal(result['disponibilidade'], expected)\n\n# Para rodar os testes, execute `pytest nome_do_arquivo.py` no terminal.\n\n02-pydantic-data-quality-e-tdd-aplicado-em-projeto-de-dados/.github/workflows/CI.yml\n\nname: ci\n\non: push\njobs:\n    build-and-test:\n        runs-on: ubuntu-latest\n        steps:\n            - name: Baixar o repositório\n              uses: actions/checkout@v4\n\n            - name: Instalar o Python\n              uses: actions/setup-python@v5\n              with:\n                python-version: 3.11.5\n\n            - name: Instalar o Poetry via pip\n              run: pip install poetry\n\n            - name: Instalar dependências com o Poetry\n              run: poetry install\n\n            - name: Rodar minha rotina de testes com o Poetry\n              run: poetry run pytest tests -v\n\n"
    ],
    "Bootcamp - Python para dados": [
        "Bootcamp - Python para dados/README.md\n\n## Calendário Bootcamp - Python do zero\n\nUm intensivo único para você iniciar com Python e ir até tópicos avançados (API por exemplo) para você resolver problemas reais\n\n| Aula  | Workshop                                                                 | Horário |\n|-------|--------------------------------------------------------------------------|---------|\n| [Aula 01](./aula01) | Python, Git e VScode: Python do Zero                                    | 12am    |\n| [Aula 02](./aula02) | TypeError, Type Check, Type Conversion, try-except e if                | 12am    |\n| [Aula 03](./aula03) | Controle de Fluxo: DEBUG, IF, FOR, While, Listas e Dicionários         | 12am    |\n| [Aula 04](./aula04) | Tipos complexos e Type Hint (Dicionários vs DataFrames Vs Tabelas Vs Excel) | 12am    |\n| [Aula 05](./aula05) | Projeto 01: Leitura e Escrita de Arquivos, lendo 1 bilhão de linhas    | 12am    |\n| [Aula 06](./aula06) | Exercício de revisão                                                    | 12am    |\n| [Aula 07](./aula07) | Funções em Python e Estrutura de Dados - Parte 1                       | 12am    |\n| [Aula 08](./aula08) | Funções em Python e Estrutura de Dados - Parte 2                       | 12am    |\n| [Aula 09](./aula09) | Funções em Python e Estrutura de Dados - Parte 3                       | 12am    |\n| Aula 10 | Aula de revisão                                                         | 12am    |\n| [Aula 11](./aula11-15) | Introdução a POO                                                        | 19pm    |\n| [Aula 12](./aula11-15) | Introdução às Classes em Python - Parte 01                             | 19pm    |\n| [Aula 13](./aula11-15) | Introdução às Classes em Python - Parte 02                             | 19pm    |\n| [Aula 14](./aula11-15) | Introdução às Classes em Python - Parte 03                             | 19pm    |\n| [Aula 15](./aula11-15) | Introdução às Classes em Python - Parte 04                             | 19pm    |\n| [Aula 16](./aula16) | Aula de revisão de programação orientada a objetos + SQLModel          | 12am    |\n| [Aula 17](./aula17) | SQLAlchemy - Conjunto de ferramentas para manipular SQL em Python      | 12am    |\n| [Aula 18](./aula18) | O que é uma API? Request, Pydantic e fazendo nosso CRUD                | 12am    |\n| [Aula 19](./aula19) | O que é uma API? Criando nossa primeira API                            | 12am    |\n| [Aula 20](./aula20) | Nosso Projeto de CRUD Backend + Frontend + Banco de Dados              | 12am    |\n\n\nBootcamp - Python para dados/aula01/README.md\n\n## Bootcamp Aula 01\n\nBem-vindo ao material da Aula 01, focado no setup inicial das ferramentas essenciais para a programação em Python: Python, Git & Github, e VSCode.\n\nSe você está revisando ou não conseguiu acompanhar o primeiro vídeo no Youtube, este é o lugar certo para começar!\n\nAssista ao vídeo no Youtube sobre o Setup de Python do Zero aqui: [Python para dados](https://youtu.be/-M4pMd2yQOM?siHVY3EDLt6aApJRG5)\n\nAlém disso, vamos criar nossa primeira aplicação em Python, e estou muito empolgado para guiá-lo nessa jornada!\n\n## Desafio do dia: Cálculo de Bônus com Entrada do Usuário\n\nEscreva um programa em Python que solicita ao usuário para digitar seu nome, o valor do seu salário mensal e o valor do bônus que recebeu. O programa deve, então, imprimir uma mensagem saudando o usuário pelo nome e informando o valor do salário em comparação com o bônus recebido.\n\n![imagem](pic.png)\n\n![imagem](pic2.png)\n\n## Agenda (1 hora)\n\n1. **Introdução** (5 minutos)\n2. **Instalação do Python** (5 minutos)\n3. **Instalação do VSCode** (5 minutos)\n4. **Configuração do Git e Github** (5 minutos)\n5. **Cálculo do nosso primeiro KPI** (20 minutos)\n6. **Dúvidas e desafio** (20 minutos)\n\n### 1. Introdução \n\nNesta aula, focaremos em preparar seu ambiente de desenvolvimento. Antes de começarmos a programar em Python, precisamos configurar as ferramentas necessárias para tornar nosso trabalho mais eficiente e organizado.\n\n- Nossa plataforma\n- Grupos de ajuda e de whatsapp\n- Por que estudar Python?\n- Python na engenharia de dados\n\n### 2. Instalação do Python\n\nComeçaremos instalando o Python, a linguagem que utilizaremos neste curso.\n\n* Baixe o instalador do Python no [site oficial](https://www.python.org/).\n* Siga as instruções de instalação de acordo com seu sistema operacional.\n* Após a instalação, abra o terminal e verifique se o Python foi instalado corretamente digitando `python --version`.\n\nAo digitar `python` no terminal, iniciaremos o interpretador Python.\n\nVamos praticar com alguns exemplos simples:\n\n1. **Imprimir uma mensagem de boas-vindas:**\n    \n    ```python\n    print(\"Bom dia turma do Bootcamp!\")\n    ```\n    \n2. **Realizar uma operação matemática simples (soma):**\n    \n    ```python\n    print(3 + 5)\n    ```\n\n3) **Atribuir um valor a uma variável e imprimi-la:**\n\n    ```python\n    variavel  \"Bom dia turma!\"\n    print(variavel)\n    ```\n\n## 3. Instalação do VSCode\n\nAgora, vamos instalar o VSCode, um ambiente de desenvolvimento leve e altamente personalizável.\n\n* Baixe o instalador do VSCode no site oficial.\n* Siga as instruções de instalação de acordo com seu sistema operacional.\n* Após a instalação, abra o VSCode e familiarize-se com a interface.\n\n## 4. Instalação do Git\n\nPor fim, vamos configurar o Git para versionamento de código.\n\n* Baixe o instalador do Git no site oficial.\n* Siga as instruções de instalação de acordo com seu sistema operacional.\n* Após a instalação, abra o terminal e configure seu nome de usuário e email no Git utilizando os comandos:\n    \n    ```arduino\n    git config --global user.name \"Seu Nome\"\n    git config --global user.email \"seuemail@example.com\"\n    ```\n    \n## 5. Exercícios de `print()`, `input()`, variáveis e estrutura de dados\n\nParabéns! Agora que configuramos todas as ferramentas necessárias, vamos concluir nossa aula com um simples \"Hello World\" em Python.\n\n* Abra o VSCode.\n* Crie um novo arquivo Python.\n* Crie um repositório no Github\n* Crie nosso primeiro arquivo `main.py`\n\n### Comandos\n\n### 1) `print()`\n\nPara usar o comando print basta digitar `print(\"Alguma coisa\")`\n\nRepare que ao redor do nosso texto, coloquei `\"o que eu quero escrever\"`\n\nAssim, eu consigo avisar ao Python que o que eu quero imprimir é um texto, uma `string`.\n\nCaso eu retire o parentese, irá dar errado.\n\n```\nprint(\"Alguma coisa)\nprint(Alguma coisa\")\nprint(Alguma coisa)\n```\n\n**Como lidar com erros?**\n\nO primeiro passo quando você tem algum erro é buscar no Google uma solução para ele\n\nTambém é possíve somar operações\n\n```python\nprint(3 + 5)\n```\n\n```python\nprint(\"Olá\" + \" \" + \"Turma\")\n```\n\n#### 2) `input()`\n\nO comando input() em Python é uma função incorporada usada para capturar dados de entrada do usuário. Quando esse comando é executado, o programa pausa sua execução e espera que o usuário digite algo no console (ou terminal) e pressione Enter. Os dados inseridos pelo usuário são então retornados pela função input() como uma string (texto). Isso permite que programas interativos recebam informações do usuário para diversos fins, como parâmetros de execução, dados para processamento, escolhas em menus interativos, entre outros.\n\n**Exemplo:**\n\n```python\ninput(\"Digite seu nome: \")\n```\n\n**Concatenando texto**\n\n```python\nprint(\"Olá, \" + input(\"Digite seu nome: \") + \"!\")\n```\n\n**Exercício 01**\n\nCrie programa que o usuário digita o seu nome e retorna o número de caracteres\n\n```bash\nDigite o seu nome: Luciano\n7\n```\n\n**Exercício 02**\n\nCriar um programa onde o usuário digite dois valores e apareça a soma\n\n```python\nDigite um valor: 7\nDigite outro valo: 10\n17\n```\n\n#### Considerações Importantes\n\n* **Tipo de Dados**: Por padrão, tudo o que é capturado pelo `input()` é tratado como uma `string`. Se você precisar trabalhar com outro tipo de dado (como inteiros ou floats), será necessário converter a entrada do usuário para o tipo desejado usando funções como `int()` ou `float()`.\n    \n    ```python\n    idade  int(input(\"Digite sua idade: \"))\n    ```\n    \n* **Segurança**: Ao usar `input()` para receber dados do usuário, é importante considerar a validação desses dados, especialmente se eles forem usados em operações críticas ou transmitidos a outras partes do sistema.\n    \n* **Usabilidade**: O `prompt` deve ser claro e informativo para guiar o usuário sobre o que precisa ser inserido, melhorando a usabilidade e a experiência do usuário.\n    \nO comando `input()` é uma ferramenta fundamental para criar scripts e programas interativos em Python, permitindo a coleta de dados de entrada de uma maneira fácil e acessível.\n\n#### 3) Declaração e Atribuição de Variáveis\n\nVariáveis em Python são fundamentais para o desenvolvimento de programas, pois atuam como \"recipientes\" para armazenar dados que podem ser modificados ao longo da execução de um script. Ao contrário de algumas outras linguagens de programação, Python é dinamicamente tipado, o que significa que você não precisa declarar explicitamente o tipo de uma variável antes de usá-la. O tipo de uma variável é determinado automaticamente pelo Python no momento da atribuição de um valor.\n\n**Declaração e Atribuição de Variáveis**\n\nA atribuição de um valor a uma variável em Python é feita com o operador ``. Por exemplo:\n\n```python\nnumero  10\nmensagem  \"Olá, mundo!\"\n```\n\nNo exemplo acima, `numero` é uma variável que armazena um inteiro (`10`), e `mensagem` é uma variável que armazena uma string (`\"Olá, mundo!\"`).\n\n**Tipos de Dados**\n\nPython suporta vários tipos de dados, incluindo, mas não se limitando a:\n\n* Inteiros (`int`)\n* Números de ponto flutuante (`float`)\n* Strings (`str`)\n* Listas (`list`)\n* Tuplas (`tuple`)\n* Dicionários (`dict`)\n* Booleanos (`bool`)\n\nA linguagem determina o tipo de dados de uma variável no momento da atribuição, o que permite grande flexibilidade, mas também exige atenção para evitar erros de tipo.\n\n**Nomes de Variáveis**\n\nPython tem algumas regras e convenções para nomes de variáveis:\n\n* Os nomes podem conter letras, números e sublinhados (`_`), mas não podem começar com um número.\n* Os nomes de variáveis são _case-sensitive_, o que significa que `variavel`, `Variavel`, e `VARIaVEL` são consideradas três variáveis diferentes.\n* Existem algumas palavras reservadas que não podem ser usadas como nomes de variáveis, como `if`, `for`, `class`, entre outras.\n* É recomendado seguir a convenção _snake_case_ para nomes de variáveis que consistem em mais de uma palavra, como `nome_usuario` ou `total_pedidos`.\n\n**Dinamismo e Reatribuição**\n\nUma característica importante das variáveis em Python é a possibilidade de reatribuí-las a diferentes tipos de dados:\n\n```python\nx  100        # x é um inteiro\nx  \"Python\"   # Agora x é uma string\n```\n\nIsso demonstra a tipagem dinâmica do Python, mas também destaca a importância de gerenciar tipos de dados com cuidado para evitar confusão ou erros em programas mais complexos.\n\n**Escopo de Variáveis**\n\nO escopo de uma variável determina onde ela é acessível dentro do código. Variáveis definidas em um bloco principal são globalmente acessíveis, enquanto variáveis definidas dentro de funções são locais a essas funções, a menos que sejam explicitamente declaradas como `global`.\n\nEntender variáveis e tipos de dados é essencial para programação em Python, pois permite manipular dados de maneira eficaz e criar programas dinâmicos e flexíveis. A capacidade de Python de inferir tipos de dados torna a linguagem acessível para iniciantes, ao mesmo tempo em que oferece poderosas funcionalidades para programadores experientes.\n\n**Exercício 03: Refatore o exercício 02 atribuindo variáveis**\n\n```python\nprint(len(input(\"Digite o seu nome: \")))\n```\n\n```\nQual é o seu nome? Luciano\n7\n```\n\n## Questão: Cálculo de Bônus com Entrada do Usuário\n\nEscreva um programa em Python que solicita ao usuário para digitar seu nome, o valor do seu salário mensal e o valor do bônus que recebeu. O programa deve, então, imprimir uma mensagem saudando o usuário pelo nome e informando o valor do salário em comparação com o bônus recebido.\n\n![imagem](pic.png)\n\n![imagem](pic2.png)\n\n#### Instruções:\n\n1. O programa deve começar solicitando ao usuário que insira seu nome.\n2. Em seguida, o programa deve pedir ao usuário para inserir o valor do seu salário. Considere que este valor pode ser um número decimal.\n3. Depois, o programa deve solicitar a porcentagem do bônus recebido pelo usuário, que também pode ser um número decimal.\n4. O cálculo do KPI do bônus de 2024 é de `1000 + salario * bônus`\n5. Finalmente, o programa deve imprimir uma mensagem no seguinte formato: \"Olá [nome], o seu valor bônus foi de 5000\".\n\n#### Exemplo de Saída:\n\nSe o usuário digitar \"Luciano\" como nome, \"5000\" como salário e \"1.5\" como bônus, o programa deve imprimir:\n\n```bash\nOlá Luciano, o seu bônus foi de 8500\n```\n\n6. Salve esse script python como `kpi.py`\n7. Faça uma documentação simples de como executar esse programa, utilize o `README`\n8. Salve no Git e no Github\n\nIsso ajudará a consolidar seu conhecimento sobre o Git e a familiarizar-se com o processo de versionamento de código.\n\n## Conclusão\n\nNesta aula, aprendemos a configurar nosso ambiente de desenvolvimento para começar a programar em Python. Com o Python, o VSCode, e o Git instalados e configurados, estamos prontos para mergulhar mais fundo no mundo da programação! Nos vemos na próxima aula!\n\nBootcamp - Python para dados/aula01/kpi.py\n\n# 1) Solicita ao usuário que digite seu nome\n\n# 2) Solicita ao usuário que digite o valor do seu salário\n# Converte a entrada para um número de ponto flutuante\n\n# 3) Solicita ao usuário que digite o valor do bônus recebido\n# Converte a entrada para um número de ponto flutuante\n\n# 4) Calcule o valor do bônus final\n\n# 5) Imprima cálculo do KPI para o usuário\n\n# 6) Imprime a mensagem personalizada incluindo o nome do usuário, salário e bônus\n\n# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?\n\nBootcamp - Python para dados/aula01/aovivo/desafio.py\n\nCONSTANTE_BONUS  1000\n\n# 1) Solicita ao usuário que digite seu nome\nnome_usuario  input(\"Digite o seu nome: \")\n\n# 2) Solicita ao usuário que digite o valor do seu salário\n# Converte a entrada para um número de ponto flutuante\nsalario_usuario  float(input(\"Digite o seu salario: \"))\n\n# 3) Solicita ao usuário que digite o valor do bônus recebido\n# Converte a entrada para um número de ponto flutuante\nbonus_usuario  float(input(\"Digite o seu bonus: \"))\n\n# 4) Calcule o valor do bônus final\n\nvalor_do_bonus  CONSTANTE_BONUS + salario_usuario * bonus_usuario\n\n# 5) Imprime a mensagem personalizada incluindo o nome do usuário e o valor do bonus\nprint(f\"O usuario {nome_usuario} possui o bonus de {valor_do_bonus}\")\n\n# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?\n\nBootcamp - Python para dados/aula01/aovivo/exemplo_01.py\n\n# Crie programa que o usuário digita o seu nome e retorna o número de caracteres\n\nprint(len(input(\"Digite o seu nome: \")))\n\n",
        "Bootcamp - Python para dados/aula02/README.md\n\n# Aula 02: TypeError, Type Check, Type Conversion, try-except e if\n\nBem-vindo à segunda aula do bootcamp! \n\n![imagem_01](./pics/1.png)\n\nHoje, vamos explorar mais a fundo um dos conceitos mais fundamentais da programação: variáveis. As variáveis são essenciais para armazenar e manipular dados em qualquer linguagem de programação, e em Python não é diferente. Nesta aula, vamos entender o que são variáveis, como declará-las, os tipos de dados simples suportados por Python e algumas boas práticas para nomeá-las.\n\nAlém disso, vamos mostrar como lidar e trabalhar com erros usando TypeError, Type Check, Type Conversion, try-except e if\n\n## 1. Tipos primitivos\n\nVariáveis são espaços de memória designados para armazenar dados que podem ser modificados durante a execução de um programa. Em Python, a declaração de variáveis é dinâmica, o que significa que o tipo de dado é inferido durante a atribuição.\n\n**Exemplo em Python:**\n\nPython suporta vários tipos de dados simples, tais como:\n\n- **Inteiros (`int`)**: Representam números inteiros.\n- **Ponto Flutuante (`float`)**: Representam números reais.\n- **Strings (`str`)**: Representam sequências de caracteres.\n- **Booleanos (`bool`)**: Representam valores verdadeiros (`True`) ou falsos (`False`).\n\n![imagem_02](./pics/2.png)\n\n#### 1. Inteiros (`int`)\n\n* **Métodos e operações:**\n    1. `+` (adição)\n    2. `-` (subtração)\n    3. `*` (multiplicação)\n    4. `//` (divisão inteira)\n    5. `%` (módulo - resto da divisão)\n\n#### 2. Números de Ponto Flutuante (`float`)\n\n* **Métodos e operações:**\n    1. `+` (adição)\n    2. `-` (subtração)\n    3. `*` (multiplicação)\n    4. `/` (divisão)\n    5. `**` (potenciação)\n\n#### 3. Strings (`str`)\n\n* **Métodos e operações:**\n    1. `.upper()` (converte para maiúsculas)\n    2. `.lower()` (converte para minúsculas)\n    3. `.strip()` (remove espaços em branco no início e no final)\n    4. `.split(sep)` (divide a string em uma lista, utilizando `sep` como delimitador)\n    5. `+` (concatenação de strings)\n\n#### 4. Booleanos (`bool`)\n\n* **Operações lógicas:**\n    1. `and` (E lógico)\n    2. `or` (OU lógico)\n    3. `not` (NÃO lógico)\n    4. `` (igualdade)\n    5. `!` (diferença)\n\n### Exercícios\n\n#### Inteiros (`int`)\n\n1. Escreva um programa que soma dois números inteiros inseridos pelo usuário.\n2. Crie um programa que receba um número do usuário e calcule o resto da divisão desse número por 5.\n3. Desenvolva um programa que multiplique dois números fornecidos pelo usuário e mostre o resultado.\n4. Faça um programa que peça dois números inteiros e imprima a divisão inteira do primeiro pelo segundo.\n5. Escreva um programa que calcule o quadrado de um número fornecido pelo usuário.\n\n#### Números de Ponto Flutuante (`float`)\n\n6. Escreva um programa que receba dois números flutuantes e realize sua adição.\n7. Crie um programa que calcule a média de dois números flutuantes fornecidos pelo usuário.\n8. Desenvolva um programa que calcule a potência de um número (base e expoente fornecidos pelo usuário).\n9. Faça um programa que converta a temperatura de Celsius para Fahrenheit.\n10. Escreva um programa que calcule a área de um círculo, recebendo o raio como entrada.\n\n#### Strings (`str`)\n\n11. Escreva um programa que receba uma string do usuário e a converta para maiúsculas.\n12. Crie um programa que receba o nome completo do usuário e imprima o nome com todas as letras minúsculas.\n13. Desenvolva um programa que peça ao usuário para inserir uma frase e, em seguida, imprima esta frase sem espaços em branco no início e no final.\n14. Faça um programa que peça ao usuário para digitar uma data no formato \"dd/mm/aaaa\" e, em seguida, imprima o dia, o mês e o ano separadamente.\n15. Escreva um programa que concatene duas strings fornecidas pelo usuário.\n\n#### Booleanos (`bool`)\n\n16. Escreva um programa que avalie duas expressões booleanas inseridas pelo usuário e retorne o resultado da operação AND entre elas.\n17. Crie um programa que receba dois valores booleanos do usuário e retorne o resultado da operação OR.\n18. Desenvolva um programa que peça ao usuário para inserir um valor booleano e, em seguida, inverta esse valor.\n19. Faça um programa que compare se dois números fornecidos pelo usuário são iguais.\n20. Escreva um programa que verifique se dois números fornecidos pelo usuário são diferentes.\n\n### Exercícios Resolução\n\n### Exercício 1: Soma de Dois Números Inteiros\n\n```python\n# num1  int(input(\"Digite o primeiro número inteiro: \"))\n# num2  int(input(\"Digite o segundo número inteiro: \"))\nnum1  8  # Exemplo de entrada\nnum2  12  # Exemplo de entrada\nresultado_soma  num1 + num2\nprint(\"A soma é:\", resultado_soma)\n```\n\n### Exercício 2: Resto da Divisão por 5\n\n```python\n# num  int(input(\"Digite um número: \"))\nnum  18  # Exemplo de entrada\nresultado_resto  num % 5\nprint(\"O resto da divisão por 5 é:\", resultado_resto)\n```\n\n### Exercício 3: Multiplicação de Dois Números\n\n```python\n# num1  int(input(\"Digite o primeiro número: \"))\n# num2  int(input(\"Digite o segundo número: \"))\nnum1  5  # Exemplo de entrada\nnum2  7  # Exemplo de entrada\nresultado_multiplicacao  num1 * num2\nprint(\"O resultado da multiplicação é:\", resultado_multiplicacao)\n```\n\n### Exercício 4: Divisão Inteira do Primeiro pelo Segundo Número\n\n```python\n# num1  int(input(\"Digite o primeiro número inteiro: \"))\n# num2  int(input(\"Digite o segundo número inteiro: \"))\nnum1  20  # Exemplo de entrada\nnum2  3  # Exemplo de entrada\nresultado_divisao_inteira  num1 // num2\nprint(\"O resultado da divisão inteira é:\", resultado_divisao_inteira)\n```\n\n### Exercício 5: Quadrado de um Número\n\n```python\n# num  int(input(\"Digite um número: \"))\nnum  6  # Exemplo de entrada\nresultado_quadrado  num ** 2\nprint(\"O quadrado do número é:\", resultado_quadrado)\n```\n\n### Exercício 6: Adição de Dois Números Flutuantes\n\n```python\n# num1  float(input(\"Digite o primeiro número flutuante: \"))\n# num2  float(input(\"Digite o segundo número flutuante: \"))\nnum1  2.5  # Exemplo de entrada\nnum2  4.5  # Exemplo de entrada\nresultado_soma  num1 + num2\nprint(\"A soma é:\", resultado_soma)\n```\n\n### Exercício 7: Média de Dois Números Flutuantes\n\n```python\n# num1  float(input(\"Digite o primeiro número flutuante: \"))\n# num2  float(input(\"Digite o segundo número flutuante: \"))\nnum1  3.5  # Exemplo de entrada\nnum2  7.5  # Exemplo de entrada\nmedia  (num1 + num2) / 2\nprint(\"A média é:\", media)\n```\n\n### Exercício 8: Potência de um Número\n\n```python\n# base  float(input(\"Digite a base: \"))\n# expoente  float(input(\"Digite o expoente: \"))\nbase  2.0  # Exemplo de entrada\nexpoente  3.0  # Exemplo de entrada\npotencia  base ** expoente\nprint(\"O resultado da potência é:\", potencia)\n```\n\n### Exercício 9: Conversão de Celsius para Fahrenheit\n\n```python\n# celsius  float(input(\"Digite a temperatura em Celsius: \"))\ncelsius  30.0  # Exemplo de entrada\nfahrenheit  (celsius * 9/5) + 32\nprint(f\"{celsius}°C é igual a {fahrenheit}°F\")\n```\n\n### Exercício 10: Área de um Círculo\n\n```python\n# raio  float(input(\"Digite o raio do círculo: \"))\nraio  5.0  # Exemplo de entrada\narea  3.14159 * raio ** 2\nprint(\"A área do círculo é:\", area)\n```\n\n### Exercício 11: Converter String para Maiúsculas\n\n```python\n# texto  input(\"Digite um texto: \")\ntexto  \"Olá, mundo!\"  # Exemplo de entrada\ntexto_maiusculas  texto.upper()\nprint(\"Texto em maiúsculas:\", texto_maiusculas)\n```\n\n### Exercício 12: Imprimir Nome Completo em Minúsculas\n\n```python\n# nome_completo  input(\"Digite seu nome completo: \")\nnome_completo  \"Fulano de Tal\"  # Exemplo de entrada\nnome_minusculas  nome_completo.lower()\nprint(\"Nome em minúsculas:\", nome_minusculas)\n```\n\n### Exercício 13: Remover Espaços em Branco de uma Frase\n\n```python\n# frase  input(\"Digite uma frase: \")\nfrase  \"  Olá, mundo!  \"  # Exemplo de entrada\nfrase_sem_espacos  frase.strip()\nprint(\"Frase sem espaços no início e no final:\", frase_sem_espacos)\n```\n\n### Exercício 14: Separar Dia, Mês e Ano de uma Data\n\n```python\n# data  input(\"Digite uma data no formato dd/mm/aaaa: \")\ndata  \"01/01/2024\"  # Exemplo de entrada\ndia, mes, ano  data.split(\"/\")\nprint(\"Dia:\", dia)\nprint(\"Mês:\", mes)\nprint(\"Ano:\", ano)\n```\n\n### Exercício 15: Concatenar Duas Strings\n\n```python\n# parte1  input(\"Digite a primeira parte do texto: \")\n# parte2  input(\"Digite a segunda parte do texto: \")\nparte1  \"Olá,\"  # Exemplo de entrada\nparte2  \" mundo!\"  # Exemplo de entrada\ntexto_concatenado  parte1 + parte2\nprint(\"Texto concatenado:\", texto_concatenado)\n```\n\n#### Exercício 16. Operador `and` (E lógico)\n\n```python\n# Exemplo de entrada\nvalor1  True\nvalor2  False\nresultado_and  valor1 and valor2\nprint(\"Resultado do AND lógico:\", resultado_and)\n```\n\n#### Exercício 17. Operador `or` (OU lógico)\n\n```python\n# Exemplo de entrada\nresultado_or  valor1 or valor2\nprint(\"Resultado do OR lógico:\", resultado_or)\n```\n\n#### Exercício  18. Operador `not` (NÃO lógico)\n\n```python\n# Exemplo de entrada\nresultado_not  not valor1\nprint(\"Resultado do NOT lógico:\", resultado_not)\n```\n\n#### Exercício 19. Operador `` (Igualdade)\n\n```python\n# Exemplo de entrada\nnum1  5\nnum2  5\nresultado_igualdade  (num1  num2)\nprint(\"Resultado da igualdade:\", resultado_igualdade)\n```\n\n#### Exercício 20. Operador `!` (Diferença)\n\n```python\n# Exemplo de entrada\nresultado_diferenca  (num1 ! num2)\nprint(\"Resultado da diferença:\", resultado_diferenca)\n```\n\n# TypeError, Type Check e Type Conversion em Python\n\nPython é uma linguagem de programação dinâmica, mas fortemente tipada, o que significa que não é necessário declarar tipos de variáveis explicitamente, mas o tipo de uma variável é determinado pelo valor que ela armazena. Isso introduz a necessidade de compreender como Python lida com diferentes tipos de dados, especialmente quando se trata de operações que envolvem múltiplos tipos. Vamos explorar três conceitos importantes: `TypeError`, verificação de tipo (`type check`), e conversão de tipo (`type conversion`).\n\n## TypeError\n\nUm `TypeError` ocorre em Python quando uma operação ou função é aplicada a um objeto de tipo inadequado. Python não sabe como aplicar a operação porque os tipos de dados não são compatíveis.\n\n### Exemplo de TypeError\n\nUm exemplo clássico é tentar utilizar a função `len()` com um inteiro, o que resulta em `TypeError`, pois `len()` espera um objeto iterável, como uma string, lista, ou tupla, não um inteiro.\n\n```python\n# Exemplo que causa TypeError\ntry:\n    resultado  len(5)\nexcept TypeError as e:\n    print(e)  # Imprime a mensagem de erro\n```\n\nO código acima tenta obter o comprimento de um inteiro, o que não faz sentido, resultando na mensagem de erro: \"object of type 'int' has no len()\".\n\n## Type Check\n\nVerificação de tipo (`type check`) é o processo de verificar o tipo de uma variável. Isso pode ser útil para garantir que operações ou funções sejam aplicadas apenas a tipos de dados compatíveis, evitando erros em tempo de execução.\n\n### Exemplo de Type Check\n\nPara verificar o tipo de uma variável em Python, você pode usar a função `type()` ou `isinstance()`.\n\n```python\nnumero  10\nif isinstance(numero, int):\n    print(\"A variável é um inteiro.\")\nelse:\n    print(\"A variável não é um inteiro.\")\n```\n\nEste código verifica se `numero` é uma instância de `int` e imprime uma mensagem apropriada.\n\n## Type Conversion\n\nConversão de tipo (`type conversion`), também conhecida como casting, é o processo de converter o valor de uma variável de um tipo para outro. Python oferece várias funções integradas para realizar conversões explícitas de tipo, como `int()`, `float()`, `str()`, etc.\n\n### Exemplo de Type Conversion\n\nSe você quiser somar um inteiro e um número flutuante, pode ser necessário converter o inteiro para flutuante ou vice-versa para garantir que a operação de soma seja realizada corretamente.\n\n```python\nnumero_inteiro  5\nnumero_flutuante  2.5\n# Converte o inteiro para flutuante e realiza a soma\nsoma  float(numero_inteiro) + numero_flutuante\nprint(soma)  # Resultado: 7.5\n```\n\n### try-except\n\nA estrutura `try-except` é usada para tratamento de exceções em Python. Uma exceção é um erro que ocorre durante a execução do programa e que, se não tratado, interrompe o fluxo normal do programa e termina sua execução. O tratamento de exceções permite que o programa lide com erros de maneira elegante, permitindo que continue a execução ou falhe de forma controlada.\n\n* **try:** Este bloco é o primeiro na estrutura de tratamento de exceções. Python tenta executar o código dentro deste bloco.\n* **except:** Se uma exceção ocorrer no bloco `try`, a execução imediatamente salta para o bloco `except`. Você pode especificar tipos de exceção específicos para capturar e tratar apenas essas exceções. Se nenhum tipo de exceção for especificado, ele captura todas as exceções.\n\n#### Exemplo de try-except\n\n```python\ntry:\n    # Código que pode gerar uma exceção\n    resultado  10 / 0\nexcept ZeroDivisionError:\n    # Código que executa se a exceção ZeroDivisionError for levantada\n    print(\"Divisão por zero não é permitida.\")\n```\n\n### if\n\nO `if` é uma estrutura de controle de fluxo que permite ao programa executar diferentes ações com base em diferentes condições. Se a condição avaliada pelo `if` for verdadeira (`True`), o bloco de código indentado sob ele será executado. Se a condição for falsa (`False`), o bloco de código será ignorado.\n\n* **if:** Avalia uma condição. Se a condição for verdadeira, executa o bloco de código associado.\n* **elif:** Abreviação de \"else if\". Permite verificar múltiplas condições em sequência.\n* **else:** Executa um bloco de código se todas as condições anteriores no `if` e `elif` forem falsas.\n\n#### Exemplo de if\n\n```python\nidade  20\nif idade < 18:\n    print(\"Menor de idade\")\nelif idade  18:\n    print(\"Exatamente 18 anos\")\nelse:\n    print(\"Maior de idade\")\n```\n\nAmbas as estruturas, `try-except` e `if`, são fundamentais para a criação de programas em Python que são capazes de lidar com situações inesperadas (como erros de execução) e tomar decisões com base em condições, permitindo assim que você construa programas mais robustos, flexíveis e seguros.\n\n## Exercícios\n\nAqui estão cinco exercícios que envolvem `TypeError`, verificação de tipo (`type check`), o uso de `try-except` para tratamento de exceções e a utilização da estrutura condicional `if`. Esses exercícios aumentam progressivamente em dificuldade e abordam situações práticas onde você pode aplicar esses conceitos.\n\n### Exercício 21: Conversor de Temperatura\n\nEscreva um programa que converta a temperatura de Celsius para Fahrenheit. O programa deve solicitar ao usuário a temperatura em Celsius e, utilizando `try-except`, garantir que a entrada seja numérica, tratando qualquer `ValueError`. Imprima o resultado em Fahrenheit ou uma mensagem de erro se a entrada não for válida.\n\n### Exercício 22: Verificador de Palíndromo\n\nCrie um programa que verifica se uma palavra ou frase é um palíndromo (lê-se igualmente de trás para frente, desconsiderando espaços e pontuações). Utilize `try-except` para garantir que a entrada seja uma string. Dica: Utilize a função `isinstance()` para verificar o tipo da entrada.\n\n### Exercício 23: Calculadora Simples\n\nDesenvolva uma calculadora simples que aceite duas entradas numéricas e um operador (+, -, *, /) do usuário. Use `try-except` para lidar com divisões por zero e entradas não numéricas. Utilize `if-elif-else` para realizar a operação matemática baseada no operador fornecido. Imprima o resultado ou uma mensagem de erro apropriada.\n\n### Exercício 24: Classificador de Números\n\nEscreva um programa que solicite ao usuário para digitar um número. Utilize `try-except` para assegurar que a entrada seja numérica e utilize `if-elif-else` para classificar o número como \"positivo\", \"negativo\" ou \"zero\". Adicionalmente, identifique se o número é \"par\" ou \"ímpar\".\n\n### Exercício 25: Conversão de Tipo com Validação\n\nCrie um script que solicite ao usuário uma lista de números separados por vírgula. O programa deve converter a string de entrada em uma lista de números inteiros. Utilize `try-except` para tratar a conversão de cada número e validar que cada elemento da lista convertida é um inteiro. Se a conversão falhar ou um elemento não for um inteiro, imprima uma mensagem de erro. Se a conversão for bem-sucedida para todos os elementos, imprima a lista de inteiros.\n\n## Exercícios Resolvidos\n\n### Exercício 21: Conversor de Temperatura\n\n```python\ntry:\n    celsius  float(input(\"Digite a temperatura em Celsius: \"))\n    fahrenheit  (celsius * 9/5) + 32\n    print(f\"{celsius}°C é igual a {fahrenheit}°F.\")\nexcept ValueError:\n    print(\"Por favor, digite um número válido para a temperatura.\")\n```\n\n### Exercício 22: Verificador de Palíndromo\n\n```python\nentrada  input(\"Digite uma palavra ou frase: \")\nif isinstance(entrada, str):\n    formatado  entrada.replace(\" \", \"\").lower()\n    if formatado  formatado[::-1]:\n        print(\"É um palíndromo.\")\n    else:\n        print(\"Não é um palíndromo.\")\nelse:\n    print(\"Entrada inválida. Por favor, digite uma palavra ou frase.\")\n```\n\n### Exercício 23: Calculadora Simples\n\n```python\ntry:\n    num1  float(input(\"Digite o primeiro número: \"))\n    num2  float(input(\"Digite o segundo número: \"))\n    operador  input(\"Digite o operador (+, -, *, /): \")\n    if operador  '+':\n        resultado  num1 + num2\n    elif operador  '-':\n        resultado  num1 - num2\n    elif operador  '*':\n        resultado  num1 * num2\n    elif operador  '/' and num2 ! 0:\n        resultado  num1 / num2\n    else:\n        print(\"Operador inválido ou divisão por zero.\")\n    print(\"Resultado:\", resultado)\nexcept ValueError:\n    print(\"Erro: Entrada inválida. Certifique-se de inserir números.\")\n```\n\n### Exercício 24: Classificador de Números\n\n```python\ntry:\n    numero  int(input(\"Digite um número: \"))\n    if numero > 0:\n        print(\"Positivo\")\n    elif numero < 0:\n        print(\"Negativo\")\n    else:\n        print(\"Zero\")\n    if numero % 2  0:\n        print(\"Par\")\n    else:\n        print(\"Ímpar\")\nexcept ValueError:\n    print(\"Por favor, digite um número inteiro válido.\")\n```\n\n### Exercício 25: Conversão de Tipo com Validação\n\n```python\nentrada_lista  input(\"Digite uma lista de números separados por vírgula: \")\nnumeros_str  entrada_lista.split(\",\")\nnumeros_int  []\ntry:\n    for num in numeros_str:\n        numeros_int.append(int(num.strip()))\n    print(\"Lista de inteiros:\", numeros_int)\nexcept ValueError:\n    print(\"Erro: certifique-se de que todos os elementos são números inteiros válidos.\")\n```\n\n![imagem_03](./pics/3.png)\n\n### Desafio - Refatorar o projeto da aula anterior evitando Bugs!\n\nPara resolver os bugs identificados — tratamento de entradas inválidas que não podem ser convertidas para um número de ponto flutuante e prevenção de valores negativos para salário e bônus, você pode modificar o código diretamente. Isso envolve adicionar verificações adicionais após a tentativa de conversão para garantir que os valores sejam positivos.\n\n![imagem_05](./pics/5.png)\n\n```python\n# Solicita ao usuário que digite seu nome\ntry:\n    nome  input(\"Digite seu nome: \")\n\n    # Verifica se o nome está vazio\n    if len(nome)  0:\n        raise ValueError(\"O nome não pode estar vazio.\")\n    # Verifica se há números no nome\n    elif any(char.isdigit() for char in nome):\n        raise ValueError(\"O nome não deve conter números.\")\n    else:\n        print(\"Nome válido:\", nome)\nexcept ValueError as e:\n    print(e)\n\n# Solicita ao usuário que digite o valor do seu salário e converte para float\n\ntry:\n    salario  float(input(\"Digite o valor do seu salário: \"))\n    if salario < 0:\n        print(\"Por favor, digite um valor positivo para o salário.\")\nexcept ValueError:\n    print(\"Entrada inválida para o salário. Por favor, digite um número.\")\n\n# Solicita ao usuário que digite o valor do bônus recebido e converte para float\ntry:\n    bonus_recebido  float(input(\"Digite o valor do bônus recebido: \"))\n    if bonus_recebido < 0:\n        print(\"Por favor, digite um valor positivo para o bônus.\")\nexcept ValueError:\n    print(\"Entrada inválida para o bônus. Por favor, digite um número.\")\n\n# Assumindo uma lógica de cálculo para o bônus final e KPI\nbonus_final  bonus_recebido * 1.2  # Exemplo, ajuste conforme necessário\nkpi  (salario + bonus_final) / 1000  # Exemplo simples de KPI\n\n# Imprime as informações para o usuário\nprint(f\"Seu KPI é: {kpi:.2f}\")\nprint(f\"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_final:.2f}.\")\n```\n\n![imagem_04](./pics/4.png)\n\n",
        "Bootcamp - Python para dados/aula02/desafio.py\n\n### Desafio - Refatorar o projeto da aula anterior evitando Bugs!\n\n# 1) Solicita ao usuário que digite seu nome\n\n# 2) Solicita ao usuário que digite o valor do seu salário\n# Converte a entrada para um número de ponto flutuante\n\n# 3) Solicita ao usuário que digite o valor do bônus recebido\n# Converte a entrada para um número de ponto flutuante\n\n# 4) Calcule o valor do bônus final\n\n# 5) Imprime a mensagem personalizada incluindo o nome do usuário, salário e bônus\n\n# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?\n\nBootcamp - Python para dados/aula02/desafio_resolvido.py\n\n# Solicita ao usuário que digite seu nome\ntry:\n    nome  input(\"Digite seu nome: \")\n\n    # Verifica se o nome está vazio\n    if len(nome)  0:\n        raise ValueError(\"O nome não pode estar vazio.\")\n        exit()\n    # Verifica se há números no nome\n    elif any(char.isdigit() for char in nome):\n        raise ValueError(\"O nome não deve conter números.\")\n        exit()\n    else:\n        print(\"Nome válido:\", nome)\nexcept ValueError as e:\n    print(e)\n    exit()\n\n# Solicita ao usuário que digite o valor do seu salário e converte para float\n\ntry:\n    salario  float(input(\"Digite o valor do seu salário: \"))\n    if salario < 0:\n        print(\"Por favor, digite um valor positivo para o salário.\")\nexcept ValueError:\n    print(\"Entrada inválida para o salário. Por favor, digite um número.\")\n    exit()\n\n# Solicita ao usuário que digite o valor do bônus recebido e converte para float\ntry:\n    bonus  float(input(\"Digite o valor do bônus recebido: \"))\n    if bonus < 0:\n        print(\"Por favor, digite um valor positivo para o bônus.\")\nexcept ValueError:\n    print(\"Entrada inválida para o bônus. Por favor, digite um número.\")\n    exit()\n\nbonus_recebido  1000 + salario * bonus  # Exemplo simples de KPI\n\n# Imprime as informações para o usuário\nprint(f\"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.\")\n\nBootcamp - Python para dados/aula02/exercicios.py\n\n# #### Inteiros (`int`)\n\n# 1. Escreva um programa que soma dois números inteiros inseridos pelo usuário.\n# 2. Crie um programa que receba um número do usuário e calcule o resto da divisão desse número por 5.\n# 3. Desenvolva um programa que multiplique dois números fornecidos pelo usuário e mostre o resultado.\n# 4. Faça um programa que peça dois números inteiros e imprima a divisão inteira do primeiro pelo segundo.\n# 5. Escreva um programa que calcule o quadrado de um número fornecido pelo usuário.\n\n# #### Números de Ponto Flutuante (`float`)\n\n# 6. Escreva um programa que receba dois números flutuantes e realize sua adição.\n# 7. Crie um programa que calcule a média de dois números flutuantes fornecidos pelo usuário.\n# 8. Desenvolva um programa que calcule a potência de um número (base e expoente fornecidos pelo usuário).\n# 9. Faça um programa que converta a temperatura de Celsius para Fahrenheit.\n# 10. Escreva um programa que calcule a área de um círculo, recebendo o raio como entrada.\n\n# #### Strings (`str`)\n\n# 11. Escreva um programa que receba uma string do usuário e a converta para maiúsculas.\n# 12. Crie um programa que receba o nome completo do usuário e imprima o nome com todas as letras minúsculas.\n# 13. Desenvolva um programa que peça ao usuário para inserir uma frase e, em seguida, imprima esta frase sem espaços em branco no início e no final.\n# 14. Faça um programa que peça ao usuário para digitar uma data no formato \"dd/mm/aaaa\" e, em seguida, imprima o dia, o mês e o ano separadamente.\n# 15. Escreva um programa que concatene duas strings fornecidas pelo usuário.\n\n# #### Booleanos (`bool`)\n\n# 16. Escreva um programa que avalie duas expressões booleanas inseridas pelo usuário e retorne o resultado da operação AND entre elas.\n# 17. Crie um programa que receba dois valores booleanos do usuário e retorne o resultado da operação OR.\n# 18. Desenvolva um programa que peça ao usuário para inserir um valor booleano e, em seguida, inverta esse valor.\n# 19. Faça um programa que compare se dois números fornecidos pelo usuário são iguais.\n# 20. Escreva um programa que verifique se dois números fornecidos pelo usuário são diferentes.\n\n# #### try-except e if\n\n# 21: Conversor de Temperatura\n# 22: Verificador de Palíndromo\n# 23: Calculadora Simples\n# 24: Classificador de Números\n# 25: Conversão de Tipo com Validação\n\n\n\nBootcamp - Python para dados/aula02/aovivo/desafio.py\n\nCONSTANTE_BONUS  1000\n\n# 1) Solicita ao usuário que digite seu nome\n#nome_usuario  input(\"Digite o seu nome: \")\n\n# nome_usuario  33 isso e um erro?\n\nnome_usuario  input(\"Digite o seu nome: \")\n\nif nome_usuario.isdigit():\n    print(\"Voce digitou seu nome errado\")\n    exit()\nelif len(nome_usuario)  0:\n    print(\"Voce nao digitou nada\")\n    exit()\nelif nome_usuario.isspace():\n    print(\"Voce digitou so espaco\")\n    exit()\n\n# 2) Solicita ao usuário que digite o valor do seu salário\n# Converte a entrada para um número de ponto flutuante\nsalario_usuario  float(input(\"Digite o seu salario: \"))\n\n# 3) Solicita ao usuário que digite o valor do bônus recebido\n# Converte a entrada para um número de ponto flutuante\nbonus_usuario  float(input(\"Digite o seu bonus: \"))\n\n# 4) Calcule o valor do bônus final\n\nvalor_do_bonus  CONSTANTE_BONUS + salario_usuario * bonus_usuario\n\n# 5) Imprime a mensagem personalizada incluindo o nome do usuário e o valor do bonus\nprint(f\"O usuario {nome_usuario} possui o bonus de {valor_do_bonus}\")\n\n# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?\n\nBootcamp - Python para dados/aula02/aovivo/exercicios.py\n\nimport math\n\n# #### Inteiros (`int`)\n\n# 1. Escreva um programa que soma dois números inteiros inseridos pelo usuário.\n# 2. Crie um programa que receba um número do usuário e calcule o resto da divisão desse número por 5.\n# 3. Desenvolva um programa que multiplique dois números fornecidos pelo usuário e mostre o resultado.\n# 4. Faça um programa que peça dois números inteiros e imprima a divisão inteira do primeiro pelo segundo.\n\nnumero_01  int(input(\"Inserir um numero inteiro: \"))\nnumero_02  int(input(\"Inserir outro numero inteiro: \"))\nresultado  numero_01 // numero_02\nprint(resultado)\n\n# 5. Escreva um programa que calcule o quadrado de um número fornecido pelo usuário.\n\n# #### Números de Ponto Flutuante (`float`)\n\n# 6. Escreva um programa que receba dois números flutuantes e realize sua adição.\n# 7. Crie um programa que calcule a média de dois números flutuantes fornecidos pelo usuário.\n# 8. Desenvolva um programa que calcule a potência de um número (base e expoente fornecidos pelo usuário).\n# 9. Faça um programa que converta a temperatura de Celsius para Fahrenheit.\n# 10. Escreva um programa que calcule a área de um círculo, recebendo o raio como entrada.\n\n#raio_do_circulo  float(input(\"Digite o raio: \"))\n#area_do_circulo  math.pi * raio_do_circulo ** 2\n# area_do_circulo_formatada  \"{:.2f}\".format(area_do_circulo)\n#print(f\"{area_do_circulo:.2f}\")\n\n# #### Strings (`str`)\n\n# 11. Escreva um programa que receba uma string do usuário e a converta para maiúsculas.\n# 12. Crie um programa que receba o nome completo do usuário e imprima o nome com todas as letras minúsculas.\n# 13. Desenvolva um programa que peça ao usuário para inserir uma frase e, em seguida, imprima esta frase sem espaços em branco no início e no final.\n# 14. Faça um programa que peça ao usuário para digitar uma data no formato \"dd/mm/aaaa\" e, em seguida, imprima o dia, o mês e o ano separadamente.\n# 15. Escreva um programa que concatene duas strings fornecidas pelo usuário.\n\n# data_do_usuario  input(\"Insira uma data no formato dd/mm/aaaa: \")\n# lista_de_dia_mes_ano  data_do_usuario.split(\"/\")\n# print(f\"O elemento 1 e o: {lista_de_dia_mes_ano[0]}\")\n# print(f\"O elemento 2 e o: {lista_de_dia_mes_ano[1]}\")\n# print(f\"O elemento 3 e o: {lista_de_dia_mes_ano[2]}\")\n\n# #### Booleanos (`bool`)\n\n# 16. Escreva um programa que avalie duas expressões booleanas inseridas pelo usuário e retorne o resultado da operação AND entre elas.\n# 17. Crie um programa que receba dois valores booleanos do usuário e retorne o resultado da operação OR.\n# 18. Desenvolva um programa que peça ao usuário para inserir um valor booleano e, em seguida, inverta esse valor.\n# 19. Faça um programa que compare se dois números fornecidos pelo usuário são iguais.\n# 20. Escreva um programa que verifique se dois números fornecidos pelo usuário são diferentes.\n\n# #### try-except e if\n\n# 21: Conversor de Temperatura\n# 22: Verificador de Palíndromo\n# 23: Calculadora Simples\n# 24: Classificador de Números\n# 25: Conversão de Tipo com Validação\n\nBootcamp - Python para dados/aula02/aovivo/main.py\n\n# Exemplo que causa TypeError\n\n# try:\n#     resultado  len(3)\n#     print(resultado)\n# except TypeError as e:\n#     print(e)\n# else:\n#     print(\"tudo ocorreu bem\")\n# finally:\n#     print(\"o importante e participar\")  \n\n# numero  int(input(\"Insira um numero :\"))\n# if isinstance(numero, ):\n#     print(\"A variável é um inteiro.\")\n# else:\n#     print(\"A variável não é um inteiro.\")\n\n\n\n",
        "Bootcamp - Python para dados/aula03/README.md\n\n# Aula 03: DEBUG, IF, FOR, While, Listas e Dicionários em Python\n\n![imagem_01](./pics/1.png)\n\nBem-vindo à terceira aula do bootcamp!\n\nHoje, vamos explorar estruturas de controle de fluxo como if, for, e while. \n\nUsamos estrutura de Controle de Fluxo para tomar decisões!\n\nDatabricks tem workflow\n![imagem_06](./pics/6.webp)\n\nAirflow principal ferramenta de workflow\n![imagem_07](./pics/7.png)\n\n### Estruturas de Controle de Fluxo\n\nExploraremos como utilizar `if` para tomar decisões baseadas em condições, `for` para iterar sobre sequências de dados, e `while` para executar blocos de código enquanto uma condição for verdadeira.\n\nPara saber mais:\n[Doc](https://docs.python.org/pt-br/3/tutorial/controlflow.html)\n\n![imagem_02](./pics/2.png)\n\n## Estruturas de Controle de Fluxo\n\nO if é uma estrutura condicional fundamental em Python que avalia se uma condição é verdadeira (True) e, se for, executa um bloco de código. Se a condição inicial não for verdadeira, você pode usar elif (else if) para verificar condições adicionais, e else para executar um bloco de código quando nenhuma das condições anteriores for verdadeira.\n\nProvavelmente o mais conhecido comando de controle de fluxo é o if. Por exemplo:\n\n```python\nx  int(input(\"Please enter an integer: \"))\n\nif x < 0:\n    x  0\n    print('Negative changed to zero')\nelif x  0:\n    print('Zero')\nelif x  1:\n    print('Single')\nelse:\n    print('More')\n```\n\n### Exercício 1: Verificação de Qualidade de Dados\n\nVocê está analisando um conjunto de dados de vendas e precisa garantir que todos os registros tenham valores positivos para `quantidade` e `preço`. Escreva um programa que verifique esses campos e imprima \"Dados válidos\" se ambos forem positivos ou \"Dados inválidos\" caso contrário.\n\n```python\nquantidade  10  # Exemplo de valor, substitua com input do usuário se necessário\npreço  20  # Exemplo de valor, substitua com input do usuário se necessário\n\nif quantidade > 0 and preço > 0:\n    print(\"Dados válidos\")\nelse:\n    print(\"Dados inválidos\")\n```\n\n### Exercício 2: Classificação de Dados de Sensor\n\nImagine que você está trabalhando com dados de sensores IoT. Os dados incluem medições de temperatura. Você precisa classificar cada leitura como 'Baixa', 'Normal' ou 'Alta'. Considerando que:\n\n* Temperatura < 18°C é 'Baixa'\n* Temperatura > 18°C e < 26°C é 'Normal'\n* Temperatura > 26°C é 'Alta'\n\n```python\ntemperatura  22  # Exemplo de valor, substitua com input do usuário se necessário\n\nif temperatura < 18:\n    print(\"Baixa\")\nelif 18 < temperatura < 26:\n    print(\"Normal\")\nelse:\n    print(\"Alta\")\n```\n\n### Exercício 3: Filtragem de Logs por Severidade\n\nVocê está analisando logs de uma aplicação e precisa filtrar mensagens com severidade 'ERROR'. Dado um registro de log em formato de dicionário como `log  {'timestamp': '2021-06-23 10:00:00', 'level': 'ERROR', 'message': 'Falha na conexão'}`, escreva um programa que imprima a mensagem se a severidade for 'ERROR'.\n\n```python\nlog  {'timestamp': '2021-06-23 10:00:00', 'level': 'ERROR', 'message': 'Falha na conexão'}\n\nif log['level']  'ERROR':\n    print(log['message'])\n```\n\n### Exercício 4: Validação de Dados de Entrada\n\nAntes de processar os dados de usuários em um sistema de recomendação, você precisa garantir que cada usuário tenha idade entre 18 e 65 anos e tenha fornecido um email válido. Escreva um programa que valide essas condições e imprima \"Dados de usuário válidos\" ou o erro específico encontrado.\n\n```python\nidade  25  # Exemplo de valor, substitua com input do usuário se necessário\nemail  \"usuario@exemplo.com\"  # Exemplo de valor, substitua com input do usuário se necessário\n\nif not 18 < idade < 65:\n    print(\"Idade fora do intervalo permitido\")\nelif \"@\" not in email or \".\" not in email:\n    print(\"Email inválido\")\nelse:\n    print(\"Dados de usuário válidos\")\n```\n\n### Exercício 5: Detecção de Anomalias em Dados de Transações\n\nVocê está trabalhando em um sistema de detecção de fraude e precisa identificar transações suspeitas. Uma transação é considerada suspeita se o valor for superior a R$ 10.000 ou se ocorrer fora do horário comercial (antes das 9h ou depois das 18h). Dada uma transação como `transacao  {'valor': 12000, 'hora': 20}`, verifique se ela é suspeita.\n\n```python\ntransacao  {'valor': 12000, 'hora': 20}\n\nif transacao['valor'] > 10000 or transacao['hora'] < 9 or transacao['hora'] > 18:\n    print(\"Transação suspeita\")\nelse:\n    print(\"Transação normal\")\n```\n\n### FOR\n\nO loop `for` é utilizado para iterar sobre os itens de qualquer sequência, como listas, strings, ou objetos de dicionário, e executar um bloco de código para cada item. É especialmente útil quando você precisa executar uma operação para cada elemento de uma coleção.\n\nO comando for em Python é um pouco diferente do que costuma ser em C ou Pascal. Ao invés de sempre iterar sobre uma progressão aritmética de números (como no Pascal), ou permitir ao usuário definir o passo de iteração e a condição de parada (como C), o comando for do Python itera sobre os itens de qualquer sequência (seja uma lista ou uma string), na ordem que aparecem na sequência. Por exemplo:\n\n```python\n# Measure some strings:\nwords  ['cat', 'window', 'defenestrate']\nfor w in words:\n    print(w, len(w))\n```\n\n```python\n# Measure some strings:\nnome  ['Luciano']\nfor letra in nome:\n    print(letra)\n```\n\nSe você precisa iterar sobre sequências numéricas, a função embutida `range()` é a resposta. Ela gera progressões aritméticas:\n\n\n```python\nfor i in range(5):\n    print(i)\n```\n\nO ponto de parada fornecido nunca é incluído na lista; range(10) gera uma lista com 10 valores, exatamente os índices válidos para uma sequência de comprimento 10. É possível iniciar o intervalo com outro número, ou alterar a razão da progressão (inclusive com passo negativo):\n\n\n```python\nlist(range(5, 10))\n[5, 6, 7, 8, 9]\n\nlist(range(0, 10, 3))\n[0, 3, 6, 9]\n\nlist(range(-10, -100, -30))\n[-10, -40, -70]\n```\n\nPara iterar sobre os índices de uma sequência, combine range() e len() da seguinte forma:\n\n```python\na  ['Mary', 'had', 'a', 'little', 'lamb']\nfor i in range(len(a)):\n    print(i, a[i])\n```\n\n[Material sobre Dicionários](https://www.youtube.com/watch?vZWj8o692qGY)\n\n#### 6. Contagem de Palavras em Textos\n\n**Objetivo:** Dado um texto, contar quantas vezes cada palavra única aparece nele.\n\n```python\ntexto  \"a raposa marrom salta sobre o cachorro preguiçoso\"\npalavras  texto.split()\ncontagem_palavras  {}\n\nfor palavra in palavras:\n    if palavra in contagem_palavras:\n        contagem_palavras[palavra] + 1\n    else:\n        contagem_palavras[palavra]  1\n\nprint(contagem_palavras)\n```\n\n#### 7. Normalização de Dados\n\n**Objetivo:** Normalizar uma lista de números para que fiquem na escala de 0 a 1.\n\n```python\nnumeros  [10, 20, 30, 40, 50]\nminimo  min(numeros)\nmaximo  max(numeros)\nnormalizados  [(x - minimo) / (maximo - minimo) for x in numeros]\n\nprint(normalizados)\n```\n\n#### 8. Filtragem de Dados Faltantes\n\n**Objetivo:** Dada uma lista de dicionários representando dados de usuários, filtrar aqueles que têm um campo específico faltando.\n\n```python\nusuarios  [\n    {\"nome\": \"Alice\", \"email\": \"alice@example.com\"},\n    {\"nome\": \"Bob\", \"email\": \"\"},\n    {\"nome\": \"Carol\", \"email\": \"carol@example.com\"}\n]\n\nusuarios_validos  [usuario for usuario in usuarios if usuario[\"email\"]]\n\nprint(usuarios_validos)\n```\n\n#### 9. Extração de Subconjuntos de Dados\n\n**Objetivo:** Dada uma lista de números, extrair apenas aqueles que são pares.\n\n```python\nnumeros  range(1, 11)\npares  [x for x in numeros if x % 2  0]\n\nprint(pares)\n```\n\n#### 10. Agregação de Dados por Categoria\n\n**Objetivo:** Dado um conjunto de registros de vendas, calcular o total de vendas por categoria.\n\n```python\nvendas  [\n    {\"categoria\": \"eletrônicos\", \"valor\": 1200},\n    {\"categoria\": \"livros\", \"valor\": 200},\n    {\"categoria\": \"eletrônicos\", \"valor\": 800}\n]\n\ntotal_por_categoria  {}\nfor venda in vendas:\n    categoria  venda[\"categoria\"]\n    valor  venda[\"valor\"]\n    if categoria in total_por_categoria:\n        total_por_categoria[categoria] + valor\n    else:\n        total_por_categoria[categoria]  valor\n\nprint(total_por_categoria)\n```\n\n### Exercícios com WHILE\n\nO loop while é uma estrutura de controle de fluxo fundamental em Python, permitindo executar um bloco de código repetidamente enquanto uma condição especificada é avaliada como verdadeira (True). Na engenharia de dados, o uso do while pode ser extremamente útil para diversas tarefas, como monitoramento contínuo de fontes de dados, execução de processos de ETL (Extract, Transform, Load) até que não haja mais dados para processar, ou mesmo para implementar tentativas de reconexão automáticas a serviços ou bancos de dados quando a primeira tentativa falha.\n\n#### Exemplo de Uso do while em Engenharia de Dados\nUm cenário comum em engenharia de dados é a necessidade de executar uma tarefa de maneira periódica, como verificar novos dados em um diretório, fazer polling de uma API para novas respostas ou monitorar mudanças em um banco de dados. Nestes casos, um loop while pode ser utilizado para manter o script rodando continuamente ou até que uma condição específica seja atingida (por exemplo, um sinal para desligar ou uma condição de erro).\n\n#### Exemplo Prático: while True com Pausa\n\nUm exemplo direto do uso de while True em Python é criar um loop infinito que executa uma ação a cada intervalo definido, como imprimir uma mensagem a cada 10 segundos. Isso pode ser útil para monitorar processos ou dados em tempo real com uma verificação periódica.\n\n```python\nimport time\n\nwhile True:\n    print(\"Verificando novos dados...\")\n    # Aqui você pode adicionar o código para verificar novos dados,\n    # por exemplo, checar a existência de novos arquivos em um diretório,\n    # fazer uma consulta a um banco de dados ou API, etc.\n    \n    time.sleep(10)  # Pausa o loop por 10 segundos\n```\nNeste exemplo, o while True cria um loop infinito, que é uma maneira poderosa de manter um script rodando continuamente. O print simula a ação de verificar novos dados, e o time.sleep(10) pausa a execução do loop por 10 segundos antes da próxima iteração. Essa abordagem é simples, mas eficaz para muitos cenários de monitoramento e polling em engenharia de dados, permitindo que o script execute uma verificação ou tarefa de maneira periódica.\n\nContudo, é importante usar loops infinitos com cautela para evitar criar condições em que o script possa consumir recursos desnecessários ou tornar-se difícil de encerrar de forma controlada. Em ambientes de produção, outras abordagens como agendamento de tarefas (por exemplo, usando cron jobs em sistemas Unix) ou o uso de sistemas de enfileiramento de mensagens e triggers de banco de dados podem ser mais adequados para algumas dessas tarefas.\n\n#### 11. Leitura de Dados até Flag\n\n**Objetivo:** Ler dados de entrada até que uma palavra-chave específica (\"sair\") seja fornecida.\n\n```python\ndados  []\nentrada  \"\"\nwhile entrada.lower() ! \"sair\":\n    entrada  input(\"Digite um valor (ou 'sair' para terminar): \")\n    if entrada.lower() ! \"sair\":\n```\n\n#### 12. Validação de Entrada\n\n**Objetivo:** Solicitar ao usuário um número dentro de um intervalo específico até que a entrada seja válida.\n\n```python\nnumero  int(input(\"Digite um número entre 1 e 10: \"))\nwhile numero < 1 or numero > 10:\n    print(\"Número fora do intervalo!\")\n    numero  int(input(\"Por favor, digite um número entre 1 e 10: \"))\n\nprint(\"Número válido!\")\n```\n\n#### 13. Consumo de API Simulado\n\n**Objetivo:** Simular o consumo de uma API paginada, onde cada \"página\" de dados é processada em loop até que não haja mais páginas.\n\n```python\npagina_atual  1\npaginas_totais  5  # Simulação, na prática, isso viria da API\n\nwhile pagina_atual < paginas_totais:\n    print(f\"Processando página {pagina_atual} de {paginas_totais}\")\n    # Aqui iria o código para processar os dados da página\n    pagina_atual + 1\n\nprint(\"Todas as páginas foram processadas.\")\n```\n\n#### 14. Tentativas de Conexão\n\n**Objetivo:** Simular tentativas de reconexão a um serviço com um limite máximo de tentativas.\n\n```python\ntentativas_maximas  5\ntentativa  1\n\nwhile tentativa < tentativas_maximas:\n    print(f\"Tentativa {tentativa} de {tentativas_maximas}\")\n    # Simulação de uma tentativa de conexão\n    # Aqui iria o código para tentar conectar\n    if True:  # Suponha que a conexão foi bem-sucedida\n        print(\"Conexão bem-sucedida!\")\n        break\n    tentativa + 1\nelse:\n    print(\"Falha ao conectar após várias tentativas.\")\n```\n\n#### 15. Processamento de Dados com Condição de Parada\n\n**Objetivo:** Processar itens de uma lista até encontrar um valor específico que indica a parada.\n\n```python\nitens  [1, 2, 3, \"parar\", 4, 5]\n\ni  0\nwhile i < len(itens):\n    if itens[i]  \"parar\":\n        print(\"Parada encontrada, encerrando o processamento.\")\n        break\n    # Processa o item\n    print(f\"Processando item: {itens[i]}\")\n    i + 1\n```\n## Estruturas de Controle de Fluxo\n\n![imagem_03](./pics/3.png)\n\nIntegre na solução anterior um fluxo de While que repita o fluxo até que o usuário insira as informações corretas\n    \n##### Solução\n```python\n# Inicializa as variáveis para o controle do loop\nnome_valido  False\nsalario_valido  False\nbonus_valido  False\n\n# Loop para verificar o nome\nwhile not nome_valido:\n    try:\n        nome  input(\"Digite seu nome: \")\n        if len(nome)  0:\n            raise ValueError(\"O nome não pode estar vazio.\")\n        elif any(char.isdigit() for char in nome):\n            raise ValueError(\"O nome não deve conter números.\")\n        else:\n            print(\"Nome válido:\", nome)\n            nome_valido  True\n    except ValueError as e:\n        print(e)\n\n# Loop para verificar o salário\nwhile not salario_valido:\n    try:\n        salario  float(input(\"Digite o valor do seu salário: \"))\n        if salario < 0:\n            print(\"Por favor, digite um valor positivo para o salário.\")\n        else:\n            salario_valido  True\n    except ValueError:\n        print(\"Entrada inválida para o salário. Por favor, digite um número.\")\n\n# Loop para verificar o bônus\nwhile not bonus_valido:\n    try:\n        bonus  float(input(\"Digite o valor do bônus recebido: \"))\n        if bonus < 0:\n            print(\"Por favor, digite um valor positivo para o bônus.\")\n        else:\n            bonus_valido  True\n    except ValueError:\n        print(\"Entrada inválida para o bônus. Por favor, digite um número.\")\n\nbonus_recebido  1000 + salario * bonus  # Exemplo simples de cálculo de bônus\n\n# Imprime as informações para o usuário\nprint(f\"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.\")\n```\n\n![imagem_0](./pics/4.png)\n\n\nBootcamp - Python para dados/aula03/desafio.py\n\n# Integre na solução anterior um fluxo de While \n# que repita o fluxo até que o usuário insira as \n# informações corretas\n\n# Solicita ao usuário que digite seu nome\nnome_valido  False\nsalario_valido  False\nbonus_valido  False\n\nwhile not nome_valido:\n    try:\n        nome  input(\"Digite seu nome: \")\n\n        # Verifica se o nome está vazio\n        if len(nome)  0:\n            raise ValueError(\"O nome não pode estar vazio.\")\n        # Verifica se há números no nome\n        elif any(char.isdigit() for char in nome):\n            raise ValueError(\"O nome não deve conter números.\")\n        else:\n            print(\"Nome válido:\", nome)\n            nome_valido  True\n    except ValueError as e:\n        print(e)\n\n# Solicita ao usuário que digite o valor do seu salário e converte para float\n\ntry:\n    salario  float(input(\"Digite o valor do seu salário: \"))\n    if salario < 0:\n        print(\"Por favor, digite um valor positivo para o salário.\")\nexcept ValueError:\n    print(\"Entrada inválida para o salário. Por favor, digite um número.\")\n    exit()\n\n# Solicita ao usuário que digite o valor do bônus recebido e converte para float\ntry:\n    bonus  float(input(\"Digite o valor do bônus recebido: \"))\n    if bonus < 0:\n        print(\"Por favor, digite um valor positivo para o bônus.\")\nexcept ValueError:\n    print(\"Entrada inválida para o bônus. Por favor, digite um número.\")\n    exit()\n\nbonus_recebido  1000 + salario * bonus  # Exemplo simples de KPI\n\n# Imprime as informações para o usuário\nprint(f\"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.\")\n\nBootcamp - Python para dados/aula03/exercicios.py\n\n### Exercício 1: Verificação de Qualidade de Dados\n# Você está analisando um conjunto de dados de vendas e precisa garantir \n# que todos os registros tenham valores positivos para `quantidade` e `preço`. \n# Escreva um programa que verifique esses campos e imprima \"Dados válidos\" se ambos \n# forem positivos ou \"Dados inválidos\" caso contrário.\n\n### Exercício 2: Classificação de Dados de Sensor\n# Imagine que você está trabalhando com dados de sensores IoT. \n# Os dados incluem medições de temperatura. Você precisa classificar cada leitura \n# como 'Baixa', 'Normal' ou 'Alta'. Considerando que:\n\n### Exercício 3: Filtragem de Logs por Severidade\n# Você está analisando logs de uma aplicação e precisa filtrar mensagens \n# com severidade 'ERROR'. Dado um registro de log em formato de dicionário \n# como `log  {'timestamp': '2021-06-23 10:00:00', 'level': 'ERROR', 'message': 'Falha na conexão'}`, \n# escreva um programa que imprima a mensagem se a severidade for 'ERROR'.\n\n### Exercício 4: Validação de Dados de Entrada\n# Antes de processar os dados de usuários em um sistema de recomendação, \n# você precisa garantir que cada usuário tenha idade entre 18 e 65 anos e tenha \n# fornecido um email válido. Escreva um programa que valide essas condições \n# e imprima \"Dados de usuário válidos\" ou o erro específico encontrado.\n\n### Exercício 5: Detecção de Anomalias em Dados de Transações\n# Você está trabalhando em um sistema de detecção de fraude e precisa identificar \n# transações suspeitas. Uma transação é considerada suspeita se o valor for superior \n# a R$ 10.000 ou se ocorrer fora do horário comercial (antes das 9h ou depois das 18h). \n# Dada uma transação como `transacao  {'valor': 12000, 'hora': 20}`, verifique se ela é suspeita.\n\n### Exercício 6. Contagem de Palavras em Textos\n# Objetivo:** Dado um texto, contar quantas vezes cada palavra única aparece nele.\n\n### Exercício 7. Normalização de Dados\n# Objetivo:** Normalizar uma lista de números para que fiquem na escala de 0 a 1.\n\n### Exercício 8. Filtragem de Dados Faltantes\n# Objetivo:** Dada uma lista de dicionários representando dados de usuários, filtrar aqueles que têm um campo específico faltando\n\n### Exercício 9. Extração de Subconjuntos de Dados\n# Objetivo:** Dada uma lista de números, extrair apenas aqueles que são pares.\n\n### Exercício 10. Agregação de Dados por Categoria\n# Objetivo:** Dado um conjunto de registros de vendas, calcular o total de vendas por categoria.\n\n### Exercícios com WHILE\n\n### Exercício 11. Leitura de Dados até Flag\n# Ler dados de entrada até que uma palavra-chave específica (\"sair\") seja fornecida.\n\n### Exercício 12. Validação de Entrada\n# Solicitar ao usuário um número dentro de um intervalo específico até que a entrada seja válida.\n\n### Exercício 13. Consumo de API Simulado\n# Simular o consumo de uma API paginada, onde cada \"página\" de dados é processada em loop até que não haja mais páginas.\n\n### Exercício 14. Tentativas de Conexão\n# Simular tentativas de reconexão a um serviço com um limite máximo de tentativas.\n\n### Exercício 15. Processamento de Dados com Condição de Parada\n# Processar itens de uma lista até encontrar um valor específico que indica a parada.\n\nBootcamp - Python para dados/aula03/aovivo/README.md\n\n# aula04_bootcamp\n\n\nBootcamp - Python para dados/aula03/aovivo/arquivos.py\n\nimport csv\n\ncaminho_do_arquivo: str  \"exemplo.csv\"\n\narquivo_csv: list  []\n\nwith open(filecaminho_do_arquivo, mode\"r\", encoding'utf-8') as arquivo:\n    leitor_csv: csv.DictReader  csv.DictReader(arquivo)\n\n    for linha in leitor_csv:\n        arquivo_csv.append(linha)\n\nprint(arquivo_csv)\n\nBootcamp - Python para dados/aula03/aovivo/aula_algoritmo.py\n\nlista_de_numeros: list  [40,50,60,70,0,-408593,1,50]\nlista_de_numeros_02: list  [40,60,70,0,-408593,1,50]\nlista_de_numeros_03: list  [40,60,70,0,1,50]\n\n# [40,50,60,70,0,-408593,1,50]\n# [50,60,,700,-408593,1,50]\n\nnome  \"luciano\"\n\ndef ordernar_lista(numeros: list) -> list:\n    \n    nova_lista_de_numeros  []\n    \n    try:\n        nova_lista_de_numeros  numeros.copy()\n        \n        for i in range(len(nova_lista_de_numeros)):\n            for j in range(i+1, len(nova_lista_de_numeros)):\n                if nova_lista_de_numeros[i] > nova_lista_de_numeros[j]:\n                    nova_lista_de_numeros[i], nova_lista_de_numeros[j]  nova_lista_de_numeros[j], nova_lista_de_numeros[i]\n    \n    except:\n        print(\"Voce colocou uma str e ao inves de uma lista\")\n    \n    return nova_lista_de_numeros\n\nordernar_lista(nome)\n\nBootcamp - Python para dados/aula03/aovivo/data_science.py\n\nfrom aula_algoritmo import ordernar_lista_de_numeros\n\nlista  [3,5,10,-1,-3804]\n\nnova_lista  ordernar_lista_de_numeros(numeros  lista)\nprint(nova_lista)\n\n",
        "Bootcamp - Python para dados/aula03/aovivo/desafio.py\n\n# Integre na solução anterior um fluxo de While \n# que repita o fluxo até que o usuário insira as \n# informações corretas\n\n# Solicita ao usuário que digite seu nome\n\nnome_valido  False\nsalario_valido  False\nbonus_valido  False\n\nwhile nome_valido is not True:\n    try:\n        nome  \"Luciano\"\n\n        # Verifica se o nome está vazio\n        if len(nome)  0:\n            raise ValueError(\"O nome não pode estar vazio.\")\n        # Verifica se há números no nome\n        elif nome.isdigit():\n            raise ValueError(\"O nome não deve conter números.\") \n        else:\n            nome_valido  True\n            print(\"Nome válido:\", nome)\n    except ValueError as e:\n        print(e)\n# Solicita ao usuário \n# que digite o valor do seu salário e converte para float\n\nwhile salario_valido is not True:\n    try:\n        salario  2000\n        if salario < 0:\n            print(\"Por favor, digite um valor positivo para o salário.\")\n        else:\n            salario_valido  True\n    except ValueError:\n        print(\"Entrada inválida para o salário. Por favor, digite um número.\")\n\n# Solicita ao usuário que digite o valor do bônus recebido e converte para float\n        \nwhile bonus_valido is not True:        \n    try:\n        bonus  3.0\n        if bonus < 0:\n            print(\"Por favor, digite um valor positivo para o bônus.\")\n        else:\n            bonus_valido  True\n    except ValueError:\n        print(\"Entrada inválida para o bônus. Por favor, digite um número.\")\n\nbonus_recebido  1000 + salario * bonus  # Exemplo simples de KPI\n\n# Imprime as informações para o usuário\nprint(f\"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.\")\n\nBootcamp - Python para dados/aula03/aovivo/exemplo_lista.py\n\nfrom typing import Dict, Optional, Any\n\nimport json\n\nlista: Any  [\"Sapato\", 39, 10.38, True]\n\nproduto_01: Dict[str, Any]  {\n    \"nome\":\"Sapato\",\n    \"quantidade\":39,\n    \"preco\": 10.38,\n    \"diponibilidade\": True\n}\n\nproduto_02: dict  {\n    \"nome\":\"Televisao\",\n    \"quantidade\":10,\n    \"preco\": 70.38,\n    \"diponibilidade\": \"false\"\n}\n\ncarrinho: list  []\n\ncarrinho.append(produto_01)\ncarrinho.append(produto_02)\n\ncarrinho_json  json.dumps(carrinho)\nprint(carrinho_json)\n\n\nBootcamp - Python para dados/aula03/aovivo/exercicios.py\n\n# Crie um dicionário para armazenar informações de um livro, \n# incluindo título, autor e ano de publicação. Imprima cada informação.\n\nfrom typing import Dict, Any\n\nlivro_01: Dict[str, Any]  {\n    \"Titulo\": \"Game of Thrones\",\n    \"Autor\": \"Estagiario\",\n    \"Ano\": 2005\n}\n\nlivro_02: Dict[str, Any]  {\n    \"Titulo\": \"Game of Thrones 2\",\n    \"Autor\": \"Estagiario\",\n    \"Ano\": 2007\n}\n\nlista_de_livros  []\n\nlista_de_livros.append(livro_01)\nlista_de_livros.append(livro_02)\n\n# print(lista_de_livros)\n\nlista_de_livros_usando_dict:dict  {\n    \"livro_01\": {\"Titulo\": \"Game of Thrones\",\n    \"Autor\": \"Estagiario\",\n    \"Ano\": 2005},\n\n    \"livro_02\": {    \"Titulo\": \"Game of Thrones 2\",\n    \"Autor\": \"Estagiario\",\n    \"Ano\": 2007}\n}\n\nBootcamp - Python para dados/aula03/aovivo/gustavo.py\n\nnome_valido: bool  False\nsalario_valido: bool  False\nbonus_valido: bool  False\n\nBootcamp - Python para dados/aula03/aovivo/main.py\n\nnome_valido: bool  False\nsalario_valido: bool  False\nbonus_valido: bool  False\n\nwhile not nome_valido:\n    try:\n        nome: str  input(\"Digite seu nome: \")\n\n        # Verifica se o nome está vazio\n        if len(nome)  0:\n            raise ValueError(\"O nome não pode estar vazio.\")\n        # Verifica se há números no nome\n        elif any(char.isdigit() for char in nome):\n            raise ValueError(\"O nome não deve conter números.\")\n        else:\n            print(\"Nome válido:\", nome)\n            nome_valido  True\n    except ValueError as e:\n        print(e)\n\n# Solicita ao usuário que digite o valor do seu salário e converte para float\n\ntry:\n    salario: float  float(input(\"Digite o valor do seu salário: \"))\n    if salario < 0:\n        print(\"Por favor, digite um valor positivo para o salário.\")\nexcept ValueError:\n    print(\"Entrada inválida para o salário. Por favor, digite um número.\")\n    exit()\n\n# Solicita ao usuário que digite o valor do bônus recebido e converte para float\ntry:\n    bonus: float  float(input(\"Digite o valor do bônus recebido: \"))\n    if bonus < 0:\n        print(\"Por favor, digite um valor positivo para o bônus.\")\nexcept ValueError:\n    print(\"Entrada inválida para o bônus. Por favor, digite um número.\")\n    exit()\n\nbonus_recebido: float  1000 + salario * bonus  # Exemplo simples de KPI\n\n\n# Imprime as informações para o usuário\nprint(f\"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.\")\n\n",
        "Bootcamp - Python para dados/aula04/README.md\n\n# Aula 04 | Type hint, Tipos complexos (Dicionários vs DataFrames Vs Tabelas Vs Excel) e Funções\n\nBem-vindos à quarta aula de Python e SQL focada em Engenharia de Dados. Nesta aula você vai aprender sobre: Type Hint, Listas e Dicionários e Funções. Esses elementos são essenciais para a manipulação de dados, ajudando na organização, interpretação e análise eficiente das informações. \n\n![imagem_01](./pic/1.jpg)\n\nVamos começar com uma introdução a cada um desses temas para nos prepararmos para o nosso primeiro prsojeto, como ler 1 bilhão de linhas!\n\n![imagem_05](./pic/5.jpeg)\n\nE o nosso workshop de sabado, dia 24 as 9am, como validar 1 bilhão de linhass.\n\n![imagem_06](./pic/6.jpeg)\n\n![imagem_02](./pic/2.jpg)\n\n## 1. Type Hint\n\nO uso de Type Hint ajuda a tornar o código mais legível e seguro, especificando o tipo de dados esperados por funções e variáveis. Na engenharia de dados, isso é especialmente útil para garantir que as funções de manipulação e análise de dados recebam os dados no formato correto, reduzindo erros em tempo de execução.\n\nPara demonstrar como utilizar Type Hints com tipos primitivos em Python, vamos criar quatro variáveis representando os tipos mais comuns: int para números inteiros, float para números de ponto flutuante, str para strings (cadeias de caracteres) e bool para valores booleanos. Type Hints são usados para indicar o tipo de uma variável no momento da sua declaração, melhorando a legibilidade do código e facilitando a detecção de erros.\n\nSem Type Hint\n```python\nidade  30\naltura  1.75\nnome  \"Alice\"\nis_estudante  True\n```\n\nCom Type Hint\n```python\nidade: int  30\naltura: float  1.75\nnome: str  \"Alice\"\nestudante: bool  True\n```\n\nO uso de Type Hint ajuda a tornar o código mais legível e seguro, especificando o tipo de dados esperados por funções e variáveis. Na engenharia de dados, isso é especialmente útil para garantir que as funções de manipulação e análise de dados recebam os dados no formato correto, reduzindo erros em tempo de execução.\n\nNa Python, a tipagem de funções é facilitada pelo uso de \"Type Hints\" (Dicas de Tipo), uma característica introduzida no Python 3.5 através do PEP 484. Os Type Hints permitem aos desenvolvedores especificar os tipos de dados esperados para os parâmetros de uma função e o tipo de dado que a função deve retornar. Embora essas dicas de tipo não sejam estritamente aplicadas em tempo de execução, elas são extremamente úteis para ferramentas de análise estática de código, melhorando a legibilidade do código e ajudando na detecção precoce de erros.\n\n### Tipagem Fraca vs. Forte\n\n* **Tipagem Forte**: Em linguagens com tipagem forte, uma vez que uma variável é atribuída a um tipo, não pode ser automaticamente tratada como outro tipo sem uma conversão explícita. Python é um exemplo de linguagem com tipagem forte. Isso significa que operações que misturam tipos incompatíveis (como adicionar um número a uma string) resultarão em erro.\n    \n* **Tipagem Fraca**: Linguagens com tipagem fraca permitem maior flexibilidade nas operações entre diferentes tipos, fazendo conversões de tipo implícitas. JavaScript é um exemplo clássico, onde você pode adicionar números a strings sem erros, resultando em uma concatenação de texto.\n    \n### Tipagem Estática vs. Dinâmica\n\n* **Tipagem Estática**: Linguagens de tipagem estática, como Java e C++, exigem que o tipo de cada variável seja declarado explicitamente no momento da compilação. Isso ajuda a detectar erros de tipo antes da execução do programa, aumentando a segurança do tipo e potencialmente melhorando o desempenho.\n    \n* **Tipagem Dinâmica**: Python é um exemplo de linguagem com tipagem dinâmica, onde os tipos são inferidos em tempo de execução e não precisam ser declarados explicitamente. Isso oferece flexibilidade e rapidez no desenvolvimento, mas pode aumentar o risco de erros de tipo que só serão detectados em tempo de execução.\n\nExercício será tipar o desafio da aula 03\n\n```python\nnome_valido  False\nsalario_valido  False\nbonus_valido  False\n\nwhile not nome_valido:\n    try:\n        nome  input(\"Digite seu nome: \")\n\n        # Verifica se o nome está vazio\n        if len(nome)  0:\n            raise ValueError(\"O nome não pode estar vazio.\")\n        # Verifica se há números no nome\n        elif any(char.isdigit() for char in nome):\n            raise ValueError(\"O nome não deve conter números.\")\n        else:\n            print(\"Nome válido:\", nome)\n            nome_valido  True\n    except ValueError as e:\n        print(e)\n\n# Solicita ao usuário que digite o valor do seu salário e converte para float\n\ntry:\n    salario  float(input(\"Digite o valor do seu salário: \"))\n    if salario < 0:\n        print(\"Por favor, digite um valor positivo para o salário.\")\nexcept ValueError:\n    print(\"Entrada inválida para o salário. Por favor, digite um número.\")\n    exit()\n\n# Solicita ao usuário que digite o valor do bônus recebido e converte para float\ntry:\n    bonus  float(input(\"Digite o valor do bônus recebido: \"))\n    if bonus < 0:\n        print(\"Por favor, digite um valor positivo para o bônus.\")\nexcept ValueError:\n    print(\"Entrada inválida para o bônus. Por favor, digite um número.\")\n    exit()\n\nbonus_recebido  1000 + salario * bonus  # Exemplo simples de KPI\n\n# Imprime as informações para o usuário\nprint(f\"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.\")\n```\n\n## 2. Listas e Dicionários\n\n### Importância na Engenharia de Dados\n\nListas e dicionários são estruturas de dados versáteis que permitem armazenar e manipular coleções de dados de forma eficiente. Na engenharia de dados, essas estruturas são fundamentais para organizar dados coletados de diversas fontes, facilitando operações como filtragem, busca, agregação e transformação de dados.\n\n### Exercícios de Listas e Dicionários\n\n1. Crie uma lista com os números de 1 a 10 e use um loop para imprimir cada número elevado ao quadrado.\n2. Dada a lista `[\"Python\", \"Java\", \"C++\", \"JavaScript\"]`, remova o item \"C++\" e adicione \"Ruby\".\n3. Crie um dicionário para armazenar informações de um livro, incluindo título, autor e ano de publicação. Imprima cada informação.\n4. Escreva um programa que conta o número de ocorrências de cada caractere em uma string usando um dicionário.\n5. Dada a lista `[\"maçã\", \"banana\", \"cereja\"]` e o dicionário `{\"maçã\": 0.45, \"banana\": 0.30, \"cereja\": 0.65}`, calcule o preço total da lista de compras.\n\n### Exercícios de Listas e Dicionários resolvidos\n\n## Resoluções de Listas e Dicionários\n\n### 1. Lista de números ao quadrado\n\n```python\nnumeros  list(range(1, 11))\nfor numero in numeros:\n    print(quadrados**2)\n```\n\n### 2. Modificar lista de linguagens\n\n```python\nlinguagens  [\"Python\", \"Java\", \"C++\", \"JavaScript\"]\nlinguagens.remove(\"C++\")\nlinguagens.append(\"Ruby\")\nprint(linguagens)\n```\n\n### 3. Informações de um livro\n\n```python\nlivro  {\"titulo\": \"1984\", \"autor\": \"George Orwell\", \"ano\": 1949}\nfor chave, valor in livro.items():\n    print(f\"{chave}: {valor}\")\n```\n\n### 4. Contar ocorrências de caracteres\n\n```python\ndef contar_caracteres(s):\n    contagem  {}\n    for caractere in s:\n        contagem[caractere]  contagem.get(caractere, 0) + 1\n    return contagem\n\nprint(contar_caracteres(\"engenharia de dados\"))\n```\n\n### 5. Preço total da lista de compras\n\n```python\nlista_compras  [\"maçã\", \"banana\", \"cereja\"]\nprecos  {\"maçã\": 0.45, \"banana\": 0.30, \"cereja\": 0.65}\ntotal  sum(precos[item] for item in lista_compras)\nprint(f\"Preço total: {total}\")\n```\n\n## Exercícios intermediários e mais avançados\n\n### 6. Eliminação de Duplicatas\n\n**Objetivo:** Dada uma lista de emails, remover todos os duplicados.\n\n```python\nemails  [\"user@example.com\", \"admin@example.com\", \"user@example.com\", \"manager@example.com\"]\nemails_unicos  list(set(emails))\n\nprint(emails_unicos)\n```\n\n#### 7. Filtragem de Dados\n\n**Objetivo:** Dada uma lista de idades, filtrar apenas aquelas que são maiores ou iguais a 18.\n\n```python\nidades  [22, 15, 30, 17, 18]\nidades_validas  [idade for idade in idades if idade > 18]\n\nprint(idades_validas)\n```\n\n#### 8. Ordenação Personalizada\n\n**Objetivo:** Dada uma lista de dicionários representando pessoas, ordená-las pelo nome.\n\n```python\npessoas  [\n    {\"nome\": \"Alice\", \"idade\": 30},\n    {\"nome\": \"Bob\", \"idade\": 25},\n    {\"nome\": \"Carol\", \"idade\": 20}\n]\npessoas.sort(keylambda pessoa: pessoa[\"nome\"])\n\nprint(pessoas)\n```\n\n#### 9. Agregação de Dados\n\n**Objetivo:** Dado um conjunto de números, calcular a média.\n\n```python\nnumeros  [10, 20, 30, 40, 50]\nmedia  sum(numeros) / len(numeros)\n\nprint(\"Média:\", media)\n```\n\n#### 10. Divisão de Dados em Grupos\n\n**Objetivo:** Dada uma lista de valores, dividir em duas listas: uma para valores pares e outra para ímpares.\n\n```python\nvalores  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\npares  [valor for valor in valores if valor % 2  0]\nimpares  [valor for valor in valores if valor % 2 ! 0]\n\nprint(\"Pares:\", pares)\nprint(\"Ímpares:\", impares)\n```\n\n### Exercícios com Dicionários\n\n#### 11. Atualização de Dados\n\n**Objetivo:** Dada uma lista de dicionários representando produtos, atualizar o preço de um produto específico.\n\n```python\nprodutos  [\n    {\"id\": 1, \"nome\": \"Teclado\", \"preço\": 100},\n    {\"id\": 2, \"nome\": \"Mouse\", \"preço\": 80},\n    {\"id\": 3, \"nome\": \"Monitor\", \"preço\": 300}\n]\n\n# Atualizar o preço do produto com id 2 para 90\nfor produto in produtos:\n    if produto[\"id\"]  2:\n        produto[\"preço\"]  90\n\nprint(produtos)\n```\n\n#### 12. Fusão de Dicionários\n\n**Objetivo:** Dados dois dicionários, fundi-los em um único dicionário.\n\n```python\ndicionario1  {\"a\": 1, \"b\": 2}\ndicionario2  {\"c\": 3, \"d\": 4}\n\ndicionario_fundido  {**dicionario1, **dicionario2}\n\nprint(dicionario_fundido)\n```\n\n#### 13. Filtragem de Dados em Dicionário\n\n**Objetivo:** Dado um dicionário de estoque de produtos, filtrar aqueles com quantidade maior que 0.\n\n```python\nestoque  {\"Teclado\": 10, \"Mouse\": 0, \"Monitor\": 3, \"CPU\": 0}\n\nestoque_positivo  {produto: quantidade for produto, quantidade in estoque.items() if quantidade > 0}\n\nprint(estoque_positivo)\n```\n\n#### 14. Extração de Chaves e Valores\n\n**Objetivo:** Dado um dicionário, criar listas separadas para suas chaves e valores.\n\n```python\ndicionario  {\"a\": 1, \"b\": 2, \"c\": 3}\nchaves  list(dicionario.keys())\nvalores  list(dicionario.values())\n\nprint(\"Chaves:\", chaves)\nprint(\"Valores:\", valores)\n```\n\n#### 15. Contagem de Frequência de Itens\n\n**Objetivo:** Dada uma string, contar a frequência de cada caractere usando um dicionário.\n\n```python\ntexto  \"engenharia de dados\"\nfrequencia  {}\n\nfor caractere in texto:\n    if caractere in frequencia:\n        frequencia[caractere] + 1\n    else:\n        frequencia[caractere]  1\n\nprint(frequencia)\n```\n\n## 3.Lendo arquivos\n\nPara ler um arquivo CSV em Python utilizando o módulo nativo, você pode usar a combinação do comando with open... para abrir o arquivo e o método .reader() do módulo csv para ler o arquivo linha por linha. O uso de with assegura que o arquivo será fechado corretamente após sua leitura, mesmo que ocorram erros durante o processo. Abaixo está um exemplo básico de como realizar essa operação:\n\n```python\nimport csv\n\n# Caminho para o arquivo CSV\ncaminho_arquivo  'exemplo.csv'\n\n# Inicializa uma lista vazia para armazenar os dados\ndados  []\n\n# Usa o gerenciador de contexto `with` para abrir o arquivo\nwith open(caminho_arquivo, mode'r', encoding'utf-8') as arquivo:\n    # Cria um objeto leitor de CSV\n    leitor_csv  csv.DictReader(arquivo)\n    \n    # Itera sobre as linhas do arquivo CSV\n    for linha in leitor_csv:\n        # Adiciona cada linha (um dicionário) à lista de dados\n        dados.append(linha)\n\n# Exibe os dados lidos do arquivo CSV\nfor registro in dados:\n    print(registro)\n```\n\n## 4. Funções\n\n### Importância na Engenharia de Dados\n\nFunções permitem modularizar e reutilizar código, essencial para processar e analisar grandes conjuntos de dados. Na engenharia de dados, funções são usadas para encapsular lógicas de transformação, limpeza, agregação e análise de dados, tornando o código mais organizado e mantendo a qualidade do código.\n\nAs funções em programação são abstrações poderosas que permitem encapsular blocos de código para realizar tarefas específicas. Elas servem não apenas para organizar o código e torná-lo mais legível, mas também para abstrair complexidades, permitindo que os programadores pensem em problemas em um nível mais alto. Uma função bem projetada pode ser vista como um \"mini-programa\" dentro de um programa maior, com sua própria lógica e dados de entrada e saída.\n\nUm exemplo clássico dessa abstração é a ordenação de uma lista. Vamos primeiro desenvolver uma função simples em Python que ordena uma lista usando o algoritmo de ordenação por seleção, um método simples mas eficaz para listas pequenas e médias. Em seguida, mostraremos como essa tarefa pode ser realizada de forma mais direta usando o método `sort()` built-in do Python, que é uma abstração fornecida pela linguagem para realizar a mesma tarefa.\n\n### Função de Ordenação Personalizada\n\n```python\n# Implementação do algoritmo de ordenação por seleção\nlista  [64, 34, 25, 12, 22, 11, 90]\n\nfor i in range(len(lista)):\n    for j in range(i+1, len(lista)):\n        if lista[i] > lista[j]:\n            lista[i], lista[j]  lista[j], lista[i]\n\n# Ordenando a lista\nprint(\"Lista ordenada com função personalizada:\", lista)\n```\n\n### Usando o Método Built-in `sort()`\n\nO Python fornece uma abstração poderosa através do método `sort()`, que pode ordenar listas in-place de maneira eficiente e com uma sintaxe simples.\n\n```python\n# Lista de exemplo\nlista_exemplo  [64, 34, 25, 12, 22, 11, 90]\n\n# Ordenando a lista com sort()\nlista_exemplo.sort()\n\nprint(\"Lista ordenada com método built-in:\", lista_exemplo)\n```\n\nA comparação entre a função de ordenação personalizada e o método `sort()` ilustra perfeitamente como as abstrações em programação, como funções e métodos built-in, podem simplificar significativamente o desenvolvimento de software. Enquanto a implementação manual de um algoritmo de ordenação é uma ótima maneira de entender os princípios da computação e algoritmos, na prática, utilizar abstrações fornecidas pela linguagem pode economizar tempo e evitar erros, permitindo que os desenvolvedores se concentrem na lógica de negócios e nos aspectos de alto nível de seus programas.\n\n#### Exemplo: Transformação de Dados com Funções\n\nSuponhamos a necessidade de limpar e transformar nomes de usuários em um conjunto de dados. Uma função dedicada pode ser implementada para essa tarefa.\n\n```python\ndef normalizar_nome(nome: str) -> str:\n    return nome.strip().lower()\n\nnomes  [\" Alice \", \"BOB\", \"Carlos\"]\nnomes_normalizados  [normalizar_nome(nome) for nome em nomes]\nprint(nomes_normalizados)\n```\n\nCada um desses temas desempenha um papel crucial na engenharia de dados, permitindo a manipulação eficiente de dados, garantindo a qualidade do código e facilitando a análise de dados complexos. Esses exemplos ilustram como listas, dicionários, type hints e funções podem ser aplicados para resolver problemas comuns encontrados nesse campo.\n\n### Exercícios de Funções\n\n16. Escreva uma função que receba uma lista de números e retorne a soma de todos os números.\n17. Crie uma função que receba um número como argumento e retorne `True` se o número for primo e `False` caso contrário.\n18. Desenvolva uma função que receba uma string como argumento e retorne essa string revertida.\n19. Implemente uma função que receba dois argumentos: uma lista de números e um número. A função deve retornar todas as combinações de pares na lista que somem ao número dado.\n20. Escreva uma função que receba um dicionário e retorne uma lista de chaves ordenadas\n\nO padrão de nomeação de funções em Python segue convenções que são amplamente aceitas pela comunidade Python, conforme recomendado no PEP 8, o guia de estilo para a codificação em Python. Adotar esses padrões não só melhora a legibilidade do código, mas também facilita a compreensão e a manutenção por outros desenvolvedores, incluindo aqueles novos ao projeto.\n\n### Padrões de Nomes de Funções\n\n* **Nomes Claros e Descritivos**: O nome de uma função deve ser descritivo o suficiente para indicar sua finalidade ou o que ela faz. Por exemplo, `calcular_area_circulo` é mais descritivo do que simplesmente `area`.\n    \n* **Letras Minúsculas com Sublinhados**: Funções em Python devem ser nomeadas usando letras minúsculas, com palavras separadas por sublinhados para melhorar a legibilidade. Este estilo é algumas vezes referido como snake_case. Por exemplo, `carregar_dados_usuario` é um bom exemplo.\n    \n* **Evitar Nomes Genéricos**: Nomes como `processo`, `executar`, ou `fazer_algo` são muito genéricos e não fornecem informações suficientes sobre o que a função faz. Prefira nomes que ofereçam um nível adequado de detalhe.\n    \n* **Evitar Abreviações Obscuras**: Embora abreviações possam encurtar o nome de uma função, elas podem tornar o código menos acessível para outros desenvolvedores. Por exemplo, `calc_media_notas` é preferível a `cmn`.\n    \n* **Prefixos com Verbo**: Muitas vezes, funções realizam ações, então é útil iniciar o nome da função com um verbo que descreve essa ação, como `obter_`, `calcular_`, `processar_`, `validar_` ou `limpar_`.\n\nNa Python, a tipagem de funções é facilitada pelo uso de \"Type Hints\" (Dicas de Tipo), uma característica introduzida no Python 3.5 através do PEP 484. Os Type Hints permitem aos desenvolvedores especificar os tipos de dados esperados para os parâmetros de uma função e o tipo de dado que a função deve retornar. Embora essas dicas de tipo não sejam estritamente aplicadas em tempo de execução, elas são extremamente úteis para ferramentas de análise estática de código, melhorando a legibilidade do código e ajudando na detecção precoce de erros.\n\n### Tipagem dos Parâmetros\n\nVocê pode especificar o tipo de cada parâmetro ao definir uma função. Isso indica claramente o tipo de argumento que a função espera.\n\n```python\ndef saudacao(nome: str, idade: int) -> str:\n    return f\"Olá, {nome}, você tem {idade} anos.\"\n```\n\n### Parâmetros com Valores Default\n\nPython permite definir valores default para os parâmetros, o que significa que a função pode ser chamada sem fornecer todos os argumentos, desde que os omitidos tenham um valor padrão definido. A tipagem funciona da mesma forma, com o tipo sendo especificado antes do sinal de igual.\n\n```python\ndef saudacao(nome: str, idade: int  30) -> str:\n    return f\"Olá, {nome}, você tem {idade} anos.\"\n```\n\n\n![imagem_03](./pic/3.jpg)\n\nRefatorar nosso código usando Dicionário, Type Hint e Funcões.\n\n\n![imagem_04](./pic/4.jpg)\n\nDuvidas\n\nBootcamp - Python para dados/aula04/exercicios.py\n\nimport csv\n\n# Caminho para o arquivo CSV\ncaminho_arquivo  'exemplo.csv'\n\n# Inicializa uma lista vazia para armazenar os dados\ndados  []\n\n# Usa o gerenciador de contexto `with` para abrir o arquivo\nwith open(caminho_arquivo, mode'r', encoding'utf-8') as arquivo:\n    # Cria um objeto leitor de CSV\n    leitor_csv  csv.DictReader(arquivo)\n    \n    # Itera sobre as linhas do arquivo CSV\n    for linha in leitor_csv:\n        # Adiciona cada linha (um dicionário) à lista de dados\n        dados.append(linha)\n\n# Exibe os dados lidos do arquivo CSV\nfor registro in dados:\n    print(registro)\n\nBootcamp - Python para dados/aula04/ordem.py\n\nlista  [64, 34, 25, 12, 22, 11, 90]\n\ndef ordernar_lista(lista: list) -> list:\n    lista_ordenada  lista.copy()\n\n    for i in range(len(lista)):\n        for j in range(i+1, len(lista)):\n            if lista[i] > lista[j]:\n                lista[i], lista[j]  lista[j], lista[i]\n\n    return lilista_ordenadasta\n\nprint(lista)\n\n",
        "Bootcamp - Python para dados/aula05/README.md\n\n# Projeto 01: Um Bilhão de Linhas: Desafio de Processamento de Dados com Python\n\n![imagem_01](./pic/1.jpg)\n\n## Introdução\n\nO objetivo deste projeto é demonstrar como processar eficientemente um arquivo de dados massivo contendo 1 bilhão de linhas (~14GB), especificamente para calcular estatísticas (Incluindo agregação e ordenação que são operações pesadas) utilizando Python. \n\nEste desafio foi inspirado no [The One Billion Row Challenge](https://github.com/gunnarmorling/1brc), originalmente proposto para Java.\n\nO arquivo de dados consiste em medições de temperatura de várias estações meteorológicas. Cada registro segue o formato `<string: nome da estação>;<double: medição>`, com a temperatura sendo apresentada com precisão de uma casa decimal.\n\nAqui estão dez linhas de exemplo do arquivo:\n\n```\nHamburg;12.0\nBulawayo;8.9\nPalembang;38.8\nSt. Johns;15.2\nCracow;12.6\nBridgetown;26.9\nIstanbul;6.2\nRoseau;34.4\nConakry;31.2\nIstanbul;23.0\n```\n\nO desafio é desenvolver um programa Python capaz de ler esse arquivo e calcular a temperatura mínima, média (arredondada para uma casa decimal) e máxima para cada estação, exibindo os resultados em uma tabela ordenada por nome da estação.\n\n| station      | min_temperature | mean_temperature | max_temperature |\n|--------------|-----------------|------------------|-----------------|\n| Abha         | -31.1           | 18.0             | 66.5            |\n| Abidjan      | -25.9           | 26.0             | 74.6            |\n| Abéché       | -19.8           | 29.4             | 79.9            |\n| Accra        | -24.8           | 26.4             | 76.3            |\n| Addis Ababa  | -31.8           | 16.0             | 63.9            |\n| Adelaide     | -31.8           | 17.3             | 71.5            |\n| Aden         | -19.6           | 29.1             | 78.3            |\n| Ahvaz        | -24.0           | 25.4             | 72.6            |\n| Albuquerque  | -35.0           | 14.0             | 61.9            |\n| Alexandra    | -40.1           | 11.0             | 67.9            |\n| ...          | ...             | ...              | ...             |\n| Yangon       | -23.6           | 27.5             | 77.3            |\n| Yaoundé      | -26.2           | 23.8             | 73.4            |\n| Yellowknife  | -53.4           | -4.3             | 46.7            |\n| Yerevan      | -38.6           | 12.4             | 62.8            |\n| Yinchuan     | -45.2           | 9.0              | 56.9            |\n| Zagreb       | -39.2           | 10.7             | 58.1            |\n| Zanzibar City| -26.5           | 26.0             | 75.2            |\n| Zürich       | -42.0           | 9.3              | 63.6            |\n| Ürümqi       | -42.1           | 7.4              | 56.7            |\n| İzmir        | -34.4           | 17.9             | 67.9            |\n\n## Desafio\n\n![imagem_03](./pic/3.jpg)\n\n- Clonar o repositorio do projeto [link](https://github.com/lvgalvao/One-Billion-Row-Challenge-Python)\n- Rodar o código utilizando Pyenv, Poetry e VSCode\n- Instalar as dependencias\n- Conseguir rodar todos os testes também\n- Após isso, aplicar algum dos conteudos que vimos durante a aula (estrutura de dados, type hint, estrutura condicional, try catch, funções etc) e propor uma melhoria no projeto através de uma PR\n\nBootcamp - Python para dados/aula06/aovivo/README.md\n\n# aula06_bootcamp\n\n\nBootcamp - Python para dados/aula06/aovivo/main.py\n\nis_nome_aluno: int  0\n\nwhile is_nome_aluno is not True:\n    nome_aluno  input(\"Digirw uma classe\")\n    if isinstance(nome_aluno, str):\n        nome_aluno_maiusculo  nome_aluno.upper()\n        print(nome_aluno_maiusculo)\n        is_nome_aluno  True\n    else:\n        print(\"voce digitou uma classe errada, precisa ser str\")\n        is_nome_aluno  is_nome_aluno + 1\n\n\nBootcamp - Python para dados/aula06/aovivo/main_02.py\n\n# Integre na solução anterior um fluxo de While\n# que repita o fluxo até que o usuário insira as\n# informações corretas\n\n# Solicita ao usuário que digite seu nome\nnome_valido  False\nsalario_valido  False\nbonus_valido  False\n\nwhile not nome_valido:\n    try:\n        nome  input(\"Digite seu nome: \")\n\n        # Verifica se o nome está vazio\n        if len(nome)  0:\n            raise ValueError(\"O nome não pode estar vazio.\")\n        # Verifica se há números no nome\n        elif any(char.isdigit() for char in nome):\n            raise ValueError(\"O nome não deve conter números.\")\n        else:\n            print(\"Nome válido:\", nome)\n            nome_valido  True\n    except ValueError as e:\n        print(e)\n\n# Solicita ao usuário que digite o valor do seu salário e converte para float\n\ntry:\n    salario  float(input(\"Digite o valor do seu salário: \"))\n    if salario < 0:\n        print(\"Por favor, digite um valor positivo para o salário.\")\nexcept ValueError:\n    print(\"Entrada inválida para o salário. Por favor, digite um número.\")\n    exit()\n\n# Solicita ao usuário que digite o valor do bônus recebido e converte para float\ntry:\n    bonus  float(input(\"Digite o valor do bônus recebido: \"))\n    if bonus < 0:\n        print(\"Por favor, digite um valor positivo para o bônus.\")\nexcept ValueError:\n    print(\"Entrada inválida para o bônus. Por favor, digite um número.\")\n    exit()\n\nbonus_recebido  1000 + salario * bonus  # Exemplo simples de KPI\n\n# Imprime as informações para o usuário\nprint(\n    f\"{nome}, seu salário é R${salario:.2f} e seu bônus final é R${bonus_recebido:.2f}.\"\n)\n\n\nBootcamp - Python para dados/aula06/aovivo/pyproject.toml\n\n[tool.poetry]\nname  \"aula06-bootcamp\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"lvgalvaofilho <lvgalvaofilho@gmail.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\nblack  \"^24.2.0\"\nflake8  \"^7.0.0\"\nisort  \"^5.13.2\"\ntaskipy  \"^1.12.2\"\npre-commit  \"^3.6.2\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n[tool.isort]\nprofile  \"black\"\n\n[tool.taskipy.tasks]\nformat  \"\"\"\nisort main.py\nblack main.py\nflake8 main.py\n\"\"\"\n\n\nBootcamp - Python para dados/aula06/aovivo/.flake8\n\n[flake8]\nmax-line-length  89\nextend-ignore  E203,E701,W291\n\n\nBootcamp - Python para dados/aula06/aovivo/.pre-commit-config.yaml\n\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: trailing-whitespace\n        args: [--markdown-linebreak-extmd]\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-toml\n      - id: detect-private-key\n      - id: check-added-large-files\n  - repo: https://github.com/psf/black-pre-commit-mirror\n    rev: 24.1.1\n    hooks:\n      - id: black\n        language_version: python3.11\n  - repo: https://github.com/pycqa/isort\n    rev: 5.13.2\n    hooks:\n      - id: isort\n        name: isort (python)\n  - repo: https://github.com/pycqa/flake8\n    rev: 7.0.0\n    hooks:\n      - id: flake8\n\n\nBootcamp - Python para dados/aula06/aovivo/.python-version\n\n3.11.5\n\n\nBootcamp - Python para dados/aula06/aovivo/helper/aws.py\n\ndef ler_arquivo_s3():\n    pass\n\n\ndef salvar_arquivo_s3():\n    pass\n\n\nBootcamp - Python para dados/aula06/aovivo/helper/azure.py\n\ndef ler_arquivo_s3():\n    pass\n\n\ndef salvar_arquivo_s3():\n    pass\n\n\nBootcamp - Python para dados/aula07/README.md\n\n# Aula 07: Funções em Python e Estrutura de Dados - Parte 1\n\n![imagem_01](./pic/1.jpg)\n\nAs funções em Python são uma das estruturas fundamentais da linguagem, permitindo a reutilização de código, a organização e a modularidade dos programas. Este guia aborda desde a motivação até a aplicação prática de funções, incluindo keywords, nomes, e como utilizá-las efetivamente.\n\n## Conteúdo\n\n![imagem_02](./pic/2.jpg)\n\n### Motivação\n\nA principal motivação para usar funções em Python é a **reutilização de código**. Funções permitem que você escreva um bloco de código uma vez e o execute múltiplas vezes, possivelmente com diferentes argumentos, para produzir diferentes resultados. Isso ajuda a tornar o código mais **legível**, **modular**, e **fácil de debugar**.\n\n### Definindo Funções\n\nPara criar uma função em Python, você usa a keyword `def`, seguida de um nome de função, parênteses `()` contendo zero ou mais \"parâmetros\", e dois pontos `:`. O bloco de código indentado que segue é o corpo da função.\n\n```python\ndef minha_funcao():\n    return \"Hello, World!\"\n```\n\n### Nomes de Funções\n\nOs nomes das funções seguem as mesmas regras de nomes de variáveis em Python: podem conter letras, números (não como primeiro caractere) e underscores (`_`), mas não espaços ou caracteres especiais. Nomes de funções devem ser descritivos e, por convenção, utilizam `snake_case`.\n\n### Parâmetros e Argumentos\n\n* **Parâmetros** são as variáveis listadas nos parênteses na definição da função. Eles são como placeholders para os dados que a função irá processar.\n    \n* **Argumentos** são os valores reais passados para a função quando ela é chamada.\n    \n\n```python\ndef soma(a, b):\n    return a + b\n```\n\n### Palavras-chave importantes\n\n* `def` inicia a definição de uma função.\n* `return` é usado para retornar um valor da função. Se omitido, a função retorna `None` por padrão.\n* `pass` pode ser usado como um placeholder para uma função vazia, significando \"nada\".\n\n### Chamando Funções\n\nPara chamar uma função, use o nome da função seguido por parênteses contendo os argumentos necessários.\n\n```python\nresultado  soma(5, 3)\nprint(resultado)  # Saída: 8\n```\n\n### Valores Padrão e Argumentos Nomeados\n\nFunções podem ter parâmetros com valores padrão, permitindo que sejam chamadas com menos argumentos.\n\n```python\ndef cumprimentar(nome, mensagem\"Olá\"):\n    print(f\"{mensagem}, {nome}!\")\n```\n\nVocê também pode chamar funções com argumentos nomeados para maior clareza.\n\n```python\ncumprimentar(mensagem\"Bem-vindo\", nome\"João\")\n```\n\n## Exercícios\n\nVamos revisar funções adicionando type hints e Pydantic\n\n1. **Calcular Média de Valores em uma Lista**\n\n```python\nfrom typing import List\n\ndef calcular_media(valores: List[float]) -> float:\n    return sum(valores) / len(valores)\n```\n\n2. **Filtrar Dados Acima de um Limite**\n\n```python\ndef filtrar_acima_de(valores: List[float], limite: float) -> List[float]:\n    resultado  []\n    for valor in valores:\n        if valor > limite:\n            resultado.append(valor)\n    return resultados\n```\n\n3. **Contar Valores Únicos em uma Lista**\n\n```python\ndef contar_valores_unicos(lista: List[int]) -> int:\n    return len(set(lista))\n```\n\n4. **Converter Celsius para Fahrenheit em uma Lista**\n\n```python\ndef celsius_para_fahrenheit(temperaturas_celsius: List[float]) -> List[float]:\n    return [(9/5) * temp + 32 for temp in temperaturas_celsius]\n```\n\n5. **Calcular Desvio Padrão de uma Lista**\n\n```python\ndef calcular_desvio_padrao(valores: List[float]) -> float:\n    media  sum(valores) / len(valores)\n    variancia  sum((x - media) ** 2 for x in valores) / len(valores)\n    return variancia ** 0.5\n```\n\n6. **Encontrar Valores Ausentes em uma Sequência**\n\n```python\ndef encontrar_valores_ausentes(sequencia: List[int]) -> List[int]:\n    completo  set(range(min(sequencia), max(sequencia) + 1))\n    return list(completo - set(sequencia))\n```\n\n![imagem_03](./pic/3.jpg)\n\nDesafio: Análise de Vendas de Produtos\nObjetivo: Dado um arquivo CSV contendo dados de vendas de produtos, o desafio consiste em ler os dados, processá-los em um dicionário para análise e, por fim, calcular e reportar as vendas totais por categoria de produto.\n\n**Fluxo**:\n\n```mermaid\ngraph TD;\n    A[Início] --> B{Ler CSV};\n    B --> C[Processar Dados];\n    C --> D[Calcular Vendas];\n    D --> E[Exibir Resultados];\n    E --> F[Fim];\n```\n\n**Tarefas**:\n\n1. Ler o arquivo CSV e carregar os dados.\n2. Processar os dados em um dicionário, onde a chave é a categoria, e o valor é uma lista de dicionários, cada um contendo informações do produto (`Produto`, `Quantidade`, `Venda`).\n3. Calcular o total de vendas (`Quantidade` * `Venda`) por categoria.\n\n### Funções\n\n1. **Ler CSV**:\n    \n    * Função: `ler_csv`\n    * Entrada: Nome do arquivo CSV\n    * Saída: Lista de dicionários com dados lidos\n2. **Processar Dados**:\n    \n    * Função: `processar_dados`\n    * Entrada: Lista de dicionários\n    * Saída: Dicionário processado conforme descrito\n3. **Calcular Vendas por Categoria**:\n    \n    * Função: `calcular_vendas_categoria`\n    * Entrada: Dicionário processado\n    * Saída: Dicionário com total de vendas por categoria\n\n![imagem_04](./pic/4.jpg)\n\nBootcamp - Python para dados/aula07/desafio.py\n\nimport csv\n\n# Função para ler o arquivo CSV\ndef ler_csv(nome_arquivo):\n    with open(nome_arquivo, mode'r', encoding'utf-8') as arquivo:\n        leitor  csv.DictReader(arquivo)\n        return list(leitor)\n\n# Função para processar os dados em um dicionário\ndef processar_dados(dados):\n    categorias  {}\n    for item in dados:\n        categoria  item['Categoria']\n        if categoria not in categorias:\n            categorias[categoria]  []\n        categorias[categoria].append(item)\n    return categorias\n\n# Função para calcular o total de vendas por categoria\ndef calcular_vendas_categoria(dados):\n    vendas_por_categoria  {}\n    for categoria, itens in dados.items():\n        total_vendas  sum(int(item['Quantidade']) * int(item['Venda']) for item in itens)\n        vendas_por_categoria[categoria]  total_vendas\n    return vendas_por_categoria\n\n# Função principal para integrar as funções anteriores\ndef main():\n    nome_arquivo  'vendas.csv'\n    dados_brutos  ler_csv(nome_arquivo)\n    dados_processados  processar_dados(dados_brutos)\n    vendas_categoria  calcular_vendas_categoria(dados_processados)\n    for categoria, total in vendas_categoria.items():\n        print(f'{categoria}: ${total}')\n\nif __name__  '__main__':\n    main()\n\n\nBootcamp - Python para dados/aula07/desafio_Lucas_Andrade.py\n\nimport csv \nfrom typing import List, Dict\nimport pandas as pd\n\ndef ler_csv(path: str) -> List:\n    with open(path, mode  'r', encoding 'utf-8') as file: \n        reader  csv.DictReader(file)\n        return list(reader)\n    \ndef processar_dados(lista_dict: List[dict]) -> Dict: \n    produtos  []\n    quantidades  []\n    vendas  []\n    for i in range(len(lista_dict)):\n        produtos.append(lista_dict[i]['Produto'])\n        quantidades.append(lista_dict[i]['Quantidade'])\n        vendas.append(lista_dict[i]['Venda'])\n    \n    return {'Produto': produtos, 'Quantidade': quantidades, 'Venda': vendas}\n\ndef somar_valores(dicionario: Dict) -> List[float]:\n    totais: List [] \n\n    for i in range(len(dicionario['Produto'])):\n        quantidade: int  int(dicionario['Quantidade'][i])\n        venda: int  int(dicionario['Venda'][i])\n        total  quantidade * venda \n        totais.append(total)\n    \n    dicionario['Total']  totais\n\n    return dicionario\n\ndef ler_DataFrame(dicionario_valores_categoria: Dict) -> pd.DataFrame:\n    df  pd.DataFrame(dicionario_valores_categoria) \n    print(df)\n\ndicionario_csv: Dict  ler_csv('./vendas.csv')\ndicionario_processado: Dict  processar_dados(dicionario_csv)\ndicionario_valores_categoria: List  somar_valores(dicionario_processado)\nprint(dicionario_valores_categoria)\nler_DataFrame(dicionario_valores_categoria)\n\nBootcamp - Python para dados/aula07/desafio_com_pydantic.py\n\nfrom pydantic import BaseModel, field_validator, ValidationError\nfrom typing import List, Dict, Optional\nimport csv\n\nclass ItemVenda(BaseModel):\n    Produto: str\n    Categoria: str\n    Quantidade: int\n    Venda: int\n\n    # Validador para garantir valores positivos\n    @field_validator('Quantidade', 'Venda')\n    def valores_positivos(cls, v):\n        assert v > 0, 'deve ser positivo'\n        return v\n\nclass CategoriaDados(BaseModel):\n    Categoria: str\n    Itens: List[ItemVenda]\n    TotalVendas: Optional[int]  0\n\n    def calcular_total_vendas(self):\n        self.TotalVendas  sum(item.Quantidade * item.Venda for item in self.Itens)\n\ndef ler_csv(nome_arquivo: str) -> List[ItemVenda]:\n    dados_validados  []\n    with open(nome_arquivo, mode'r', encoding'utf-8') as arquivo:\n        leitor  csv.DictReader(arquivo)\n        for linha in leitor:\n            try:\n                item  ItemVenda(**linha)\n                dados_validados.append(item)\n            except ValidationError as e:\n                print(f\"Erro de validação: {e.json()}\")\n    return dados_validados\n\ndef processar_dados(dados: List[ItemVenda]) -> Dict[str, CategoriaDados]:\n    categorias  {}\n    for item in dados:\n        if item.Categoria not in categorias:\n            categorias[item.Categoria]  CategoriaDados(Categoriaitem.Categoria, Itens[])\n        categorias[item.Categoria].Itens.append(item)\n    return categorias\n\ndef calcular_vendas_categoria(dados: Dict[str, CategoriaDados]) -> Dict[str, int]:\n    vendas_por_categoria  {}\n    for categoria, dados_categoria in dados.items():\n        dados_categoria.calcular_total_vendas()\n        vendas_por_categoria[categoria]  dados_categoria.TotalVendas\n    return vendas_por_categoria\n\ndef main():\n    nome_arquivo  'vendas.csv'\n    dados_brutos  ler_csv(nome_arquivo)\n    dados_processados  processar_dados(dados_brutos)\n    vendas_categoria  calcular_vendas_categoria(dados_processados)\n    for categoria, total in vendas_categoria.items():\n        print(f'{categoria}: ${total}')\n\nif __name__  '__main__':\n    main()\n\nBootcamp - Python para dados/aula07/desafio_com_type_hint.py\n\nimport csv\nfrom typing import List, Dict\n\n# Função para ler o arquivo CSV\ndef ler_csv(nome_arquivo: str) -> List[Dict[str, str]]:\n    with open(nome_arquivo, mode'r', encoding'utf-8') as arquivo:\n        leitor  csv.DictReader(arquivo)\n        return list(leitor)\n\n# Função para processar os dados em um dicionário\ndef processar_dados(dados: List[Dict[str, str]]) -> Dict[str, List[Dict[str, str]]]:\n    categorias  {}\n    for item in dados:\n        categoria  item['Categoria']\n        if categoria not in categorias:\n            categorias[categoria]  []\n        categorias[categoria].append(item)\n    return categorias\n\n# Função para calcular o total de vendas por categoria\ndef calcular_vendas_categoria(dados: Dict[str, List[Dict[str, str]]]) -> Dict[str, int]:\n    vendas_por_categoria  {}\n    for categoria, itens in dados.items():\n        total_vendas  sum(int(item['Quantidade']) * int(item['Venda']) for item in itens)\n        vendas_por_categoria[categoria]  total_vendas\n    return vendas_por_categoria\n\n# Função principal para integrar as funções anteriores\ndef main():\n    nome_arquivo  'vendas.csv'\n    dados_brutos  ler_csv(nome_arquivo)\n    dados_processados  processar_dados(dados_brutos)\n    vendas_categoria  calcular_vendas_categoria(dados_processados)\n    for categoria, total in vendas_categoria.items():\n        print(f'{categoria}: ${total}')\n\nif __name__  '__main__':\n    main()\n\n\nBootcamp - Python para dados/aula07/aovivo/README.md\n\n# aula07_bootcamp\n\n\nBootcamp - Python para dados/aula07/aovivo/cliente.py\n\nfrom desafio import pipeline\n\nprint(pipeline(\"vendas.csv\"))\n\nload_csv(\"vendas.csv\")\n\nBootcamp - Python para dados/aula07/aovivo/desafio.py\n\nimport csv\n\nfrom pydantic import validate_call\n\n@validate_call\ndef ler_csv_para_transformar_em_um_dict(path: str) -> list[dict]:\n    dados  []\n    with open(filepath, mode\"r\", encoding\"utf-8\") as file:\n        leitor_csv  csv.DictReader(file)\n        for linha in leitor_csv:\n            # Adiciona cada linha (um dicionário) à lista de dados\n            dados.append(linha)\n    return dados\n\n@validate_call\ndef filtra_produtos_entregues(lista  list[dict]) -> list[dict]:\n    lista_de_produtos_entregues  []\n    for produto in lista:\n        if produto.get(\"entregue\")  \"True\":\n            lista_de_produtos_entregues.append(produto)\n    return lista_de_produtos_entregues\n\n@validate_call\ndef somar_valores(lista  list[dict]) -> int:\n    valor_total  0\n    for produto in lista:\n        valor_total + int(produto.get(\"price\"))\n    return valor_total\n\n@validate_call\ndef pipeline(path: str):\n    lista_de_produtos  ler_csv_para_transformar_em_um_dict(path)\n    produtos_entregues  filtra_produtos_entregues(lista_de_produtos)\n    return somar_valores(produtos_entregues)\n\n\n\n\n\n\nBootcamp - Python para dados/aula07/aovivo/etl.py\n\nfrom pydantic import validate_call\n\n@validate_call\ndef filtrar_acima_de(lista_de_salarios: list[float], salario_max: float) -> list:\n    lista_de_salarios_acima: list  []\n    for salario in lista_de_salarios:\n        if salario > salario_max:\n            lista_de_salarios_acima.append(salario)\n    return lista_de_salarios_acima\n\nBootcamp - Python para dados/aula07/aovivo/main.py\n\nfrom etl import filtrar_acima_de\n\nlista  [3000, 4000, 150000]\nmax  100000 \n\nprint(filtrar_acima_de(lista_de_salarioslista, salario_maxmax))\n\nBootcamp - Python para dados/aula07/aovivo/pyproject.toml\n\n[tool.poetry]\nname  \"aula07-bootcamp\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"lvgalvaofilho <lvgalvaofilho@gmail.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\npydantic  \"^2.6.2\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\n",
        "Bootcamp - Python para dados/aula08/README.md\n\n# Aula 08: Funções em Python - ETL com Pandas, JSON e Parquet\n\n![imagem_01](./pic/1.jpg)\n\nPara realizar uma ETL (Extract, Transform, Load) simples utilizando Python e a biblioteca Pandas, vamos seguir os seguintes passos:\n\nExtract: Ler os dados de um arquivo JSON.\n\nTransform: Concatenar os dados extraídos em um único DataFrame e aplicar uma transformação. A transformação específica dependerá dos dados, mas vamos assumir uma operação simples como um exemplo.\n\nLoad: Salvar o DataFrame resultante em um arquivo CSV ou PARQUET. \n\n**Usando LOG**\n\n## Conteúdo\n\n![imagem_02](./pic/2.jpg)\n\nO Loguru é uma biblioteca de logging para Python que visa trazer uma experiência de uso mais simples e poderosa do que o módulo de logging padrão do Python. Com uma API simples, Loguru oferece várias funcionalidades úteis, como rotação de arquivos, serialização JSON, envio de mensagens para múltiplos destinos, e muito mais, tudo isso sem a necessidade de configuração inicial complicada.\n\n### O que é Logging?\n\nLogging é o processo de gravar mensagens que documentam os eventos que ocorrem durante a execução de um software. Essas mensagens podem indicar progresso da execução, falhas, erros, ou outras informações úteis. O logging é crucial para desenvolvimento e manutenção de software, pois permite aos desenvolvedores e administradores de sistema entender o que o aplicativo está fazendo, diagnosticar problemas e monitorar o desempenho em produção.\n\n### Como Utilizar o Loguru\n\nPara começar a usar o Loguru, você primeiro precisa instalá-lo. Isso pode ser feito facilmente via pip:\n\n```bash\npoetry add loguru\n```\n\nAgora, vamos aos exemplos de como utilizar o Loguru em seu código Python.\n\n#### Exemplo 1: Logging Básico\n\nEste exemplo mostra como fazer logging básico com Loguru, incluindo mensagens de diferentes níveis de severidade.\n\n```python\nfrom loguru import logger\n\n# Mensagens de log de diferentes níveis\nlogger.debug(\"Isso é uma mensagem de debug\")\nlogger.info(\"Isso é uma mensagem informativa\")\nlogger.warning(\"Isso é um aviso\")\nlogger.error(\"Isso é um erro\")\nlogger.critical(\"Isso é crítico\")\n\n# A saída será exibida no console\n```\n\nNeste exemplo, utilizamos o `logger` importado do Loguru para registrar mensagens de diferentes níveis de severidade. O Loguru se encarrega de formatar e exibir essas mensagens no console, por padrão.\n\n#### Exemplo 2: Configuração de Arquivo de Log\n\nAqui, configuramos o Loguru para salvar mensagens de log em um arquivo, incluindo a rotação do arquivo baseada no tamanho.\n\n```python\nfrom loguru import logger\n\n# Configurando o arquivo de log com rotação de 5MB\nlogger.add(\"meu_app.log\", rotation\"5 MB\")\n\nlogger.info(\"Essa mensagem será salva no arquivo\")\n```\n\nNo exemplo acima, `logger.add()` é usado para adicionar um \"sink\" (destino) que, neste caso, é um arquivo. A opção `rotation` determina que um novo arquivo será criado sempre que o atual atingir 5MB.\n\n#### Exemplo 3: Capturando Exceções com Log\n\nLoguru também facilita o logging de exceções, capturando automaticamente informações de traceback.\n\n```python\nfrom loguru import logger\n\ndef minha_funcao():\n    raise ValueError(\"Um erro aconteceu!\")\n\ntry:\n    minha_funcao()\nexcept Exception:\n    logger.exception(\"Uma exceção foi capturada\")\n```\n\nUsando `logger.exception()`, Loguru automaticamente captura e loga o traceback da exceção, o que é extremamente útil para diagnóstico de erros.\n\nVamos criar um decorador utilizando o Loguru para adicionar automaticamente logs a qualquer função Python. Isso nos permite registrar automaticamente quando uma função é chamada e quando ela termina, junto com qualquer informação relevante, como argumentos da função e o resultado retornado (ou exceção lançada).\n\nAgora, vamos ao código do decorador:\n\n```python\nfrom loguru import logger\n\ndef log_decorator(func):\n    def wrapper(*args, **kwargs):\n        logger.info(f\"Chamando '{func.__name__}' com {args} e {kwargs}\")\n        try:\n            result  func(*args, **kwargs)\n            logger.info(f\"'{func.__name__}' retornou {result}\")\n            return result\n        except Exception as e:\n            logger.exception(f\"'{func.__name__}' lançou uma exceção: {e}\")\n            raise\n    return wrapper\n```\n\nNeste decorador, `log_decorator`, usamos `logger.info` para registrar quando a função decorada é chamada e o que ela retorna. Se uma exceção for lançada, usamos `logger.exception` para registrar a exceção, incluindo o traceback.\n\n### Como Utilizar o Decorador\n\nAgora, veja como aplicar o `log_decorator` a uma função:\n\n```python\n@log_decorator\ndef soma(a, b):\n    return a + b\n\n@log_decorator\ndef falha():\n    raise ValueError(\"Um erro intencional\")\n\n# Testando as funções decoradas\nsoma(5, 3)  # Isso irá logar a chamada e o retorno\ntry:\n    falha()  # Isso irá logar a chamada e a exceção\nexcept ValueError:\n    pass  # Ignora a exceção para fins de demonstração\n```\n\nAo decorar as funções `soma` e `falha` com `@log_decorator`, automaticamente logamos a entrada e saída (ou exceção) dessas funções sem alterar o corpo delas. Isso é especialmente útil para debugar, monitorar a performance de aplicações ou simplesmente manter um registro de quais funções estão sendo chamadas e com quais argumentos.\n\n### Benefícios do Uso de Decoradores com Loguru\n\nO uso de decoradores em conjunto com o Loguru fornece uma abordagem elegante e poderosa para adicionar logs a aplicações Python. Sem a necessidade de modificar o corpo da função, podemos facilmente adicionar funcionalidades de logging, o que torna o código mais limpo, mantém a separação de preocupações e facilita a manutenção e o debugging.\n\nAlém disso, ao centralizar a lógica de logging no decorador, promovemos a reutilização de código e garantimos uma forma consistente de logar informações através de diferentes partes de uma aplicação.\n\n### Conclusão\n\nO Loguru oferece uma abordagem moderna e conveniente para logging em Python, simplificando muitos aspectos que requerem configuração manual detalhada com o módulo de logging padrão do Python. Seja para desenvolvimento, depuração ou produção, adicionar logging ao seu aplicativo com Loguru pode melhorar significativamente a visibilidade e a capacidade de diagnóstico do seu código.\n\n### Desafio\n\n![imagem_03](./pic/3.jpg)\n\n![imagem_03](./pic/pic_05.png)\n\n\n\nBootcamp - Python para dados/aula08/etl.py\n\nimport pandas as pd\nimport glob\nimport os\nimport pandera as pa\nfrom schema import VendasSchema\nfrom pathlib import Path\n\n@pa.check_output(VendasSchema)\ndef extrair_dados(pasta: str) -> pd.DataFrame:\n    arquivos_json  glob.glob(os.path.join(pasta, '*.json'))\n    df_list  [pd.read_json(arquivo) for arquivo in arquivos_json]\n    df_total  pd.concat(df_list, ignore_indexTrue)\n    print(df_total)\n    return df_total\n\ndef transformar_dados(df: pd.DataFrame):\n    df['Receita']  df['Quantidade'] * df['Venda']\n    print(df)\n    return df\n\ndef carregar_dados(df: pd.DataFrame, formatos: list):\n\n    for formato in formatos:\n        if formato  'csv':\n            df.to_csv(\"dados.csv\", indexFalse)\n        elif formato  'parquet':\n            df.to_parquet( \"dados.parquet\", indexFalse)\n\ndef pipeline(pasta_entrada: str, formato_saida: str):\n    dados  extrair_dados(pasta_entrada)\n    dados_transformados  transformar_dados(dados)\n    carregar_dados(dados_transformados, formato_saida)\n\nBootcamp - Python para dados/aula08/pipeline.py\n\nfrom etl import pipeline\nfrom pathlib import Path\n\nif __name__  \"__main__\":\n    # Define as pastas de entrada e saída usando pathlib\n    pasta_raiz  Path(__file__).parent\n    pasta_entrada  pasta_raiz / 'data'\n\n    formato_saida  [\"csv\"]  # Ou 'parquet', conforme a decisão de saída\n\n    pipeline(pasta_entrada, formato_saida)\n\n\nBootcamp - Python para dados/aula08/schema.py\n\nimport pandera as pa\nfrom pandera.typing import Series\n\nclass VendasSchema(pa.SchemaModel):\n    Produto: Series[str]\n    Categoria: Series[str]\n    Quantidade: Series[int]  pa.Field(ge0)  # ge0 significa \"maior ou igual a 0\"\n    Venda: Series[int]  pa.Field(ge0)\n    Data: Series[str]\n    \n    class Config:\n        coerce  True\n        strict  True\n\n\nBootcamp - Python para dados/aula08/aovivo/README.md\n\n# aula08_bootcamp\n\n\nBootcamp - Python para dados/aula08/aovivo/etl.py\n\nimport pandas as pd\nimport os\nimport glob\n# uma funcao de extract que le e consolida os json\n\ndef extrair_dados_e_consolidar(pasta: str) -> pd.DataFrame:\n    arquivos_json  glob.glob(os.path.join(pasta, '*.json'))\n    df_list  [pd.read_json(arquivo) for arquivo in arquivos_json]\n    df_total  pd.concat(df_list, ignore_indexTrue)\n    return df_total\n\n# uma funcao que transforma\n\ndef calcular_kpi_de_total_de_vendas(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"Total\"]  df[\"Quantidade\"] * df[\"Venda\"]\n    return df\n\ndef carregar_dados(df: pd.DataFrame, format_saida: list):\n    \"\"\"\n    parametro que vai ser ou \"csv\" ou \"parquet\" ou \"os dois\"\n    \"\"\"\n    for formato in format_saida:\n        if formato  'csv':\n            df.to_csv(\"dados.csv\", indexFalse)\n        if formato  'parquet':\n            df.to_parquet(\"dados.parquet\", indexFalse)\n\nextrair_dados_e_consolidar()\n@extrair_dados_e_consolidar\ndef pipeline_calcular_kpi_de_vendas_consolidado(pasta: str, formato_de_saida: list):\n    data_frame  extrair_dados_e_consolidar(pasta)\n    data_frame_calculado  calcular_kpi_de_total_de_vendas(data_frame)\n    carregar_dados(data_frame_calculado, formato_de_saida)\n\n# uma funcao que da load em csv ou parquet\n\nBootcamp - Python para dados/aula08/aovivo/pipeline.py\n\nfrom etl import pipeline_calcular_kpi_de_vendas_consolidado\n\npasta_argumento: str  'data'\nformato_de_saida: list  [\"csv\", \"parquet\"]\n\npipeline_calcular_kpi_de_vendas_consolidado(pasta_argumento, formato_de_saida)\n\n\nBootcamp - Python para dados/aula08/aovivo/pyproject.toml\n\n[tool.poetry]\nname  \"aula08-bootcamp\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"lvgalvaofilho <lvgalvaofilho@gmail.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\npandera  \"^0.18.0\"\npandas  \"^2.2.1\"\nfastparquet  \"^2024.2.0\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\nBootcamp - Python para dados/aula08/aovivo/.python-version\n\n3.11.5\n\n\nBootcamp - Python para dados/aula09/README.md\n\n# Aula 09: Funções em Python - Decoradores\n\nNa engenharia de dados, a eficiência, reusabilidade e confiabilidade do código são cruciais. Por isso, trabalhamos com decoradores.\n\n![imagem_01](./pic/1.jpg)\n\n**Usando LOG**\n\nQuando queremos entender mais sobre nossa aplicação, temos duas alternativas.\n\n- Utilizar o Print\n- Utilizar o Debugger\n\nHoje quero apresentar uma terceira opção, que é o logging.\n\n[![imagem_05](./pic/5.png)](https://www.linkedin.com/posts/lucianovasconcelosf_voc%C3%A9-%C3%A9-da-turma-do-print-ou-do-log-debugar-activity-7166127525763518464-z8AN?utm_sourceshare&utm_mediummember_desktop)\n\n## Conteúdo\n\n![imagem_02](./pic/2.jpg)\n\nO Loguru é uma biblioteca de logging para Python que visa trazer uma experiência de uso mais simples e poderosa do que o módulo de logging padrão do Python. Com uma API simples, Loguru oferece várias funcionalidades úteis, como rotação de arquivos, serialização JSON, envio de mensagens para múltiplos destinos, e muito mais, tudo isso sem a necessidade de configuração inicial complicada.\n\n### O que é Logging?\n\nLogging é o processo de gravar mensagens que documentam os eventos que ocorrem durante a execução de um software. Essas mensagens podem indicar progresso da execução, falhas, erros, ou outras informações úteis. O logging é crucial para desenvolvimento e manutenção de software, pois permite aos desenvolvedores e administradores de sistema entender o que o aplicativo está fazendo, diagnosticar problemas e monitorar o desempenho em produção.\n\n### Como Utilizar o Loguru\n\nPara começar a usar o Loguru, você primeiro precisa instalá-lo. Isso pode ser feito facilmente via pip:\n\n```bash\npoetry add loguru\n```\n\nAgora, vamos aos exemplos de como utilizar o Loguru em seu código Python.\n\n#### Exemplo 1: Logging Básico\n\nEste exemplo mostra como fazer logging básico com Loguru\n\n```python\nfrom loguru import logger\n\nlogger.info(\"Isso é uma mensagem informativa\")\n\n# A saída será exibida no console\n```\n\n#### Exemplo 2: Configuração de Arquivo de Log\n\nAqui, configuramos o Loguru para salvar mensagens de log em um arquivo, incluindo a rotação do arquivo baseada no tamanho.\n\n```python\nfrom loguru import logger\n\n# Configurando o arquivo de log com rotação de 5MB\nlogger.add(\"meu_app.log\", rotation\"5 MB\")\n\nlogger.info(\"Essa mensagem será salva no arquivo\")\n```\n\nNo exemplo acima, `logger.add()` é usado para adicionar um \"sink\" (destino) que, neste caso, é um arquivo. A opção `rotation` determina que um novo arquivo será criado sempre que o atual atingir 5MB.\n\n#### Exemplo 3: Capturando e salvando\n\nAqui está um exemplo de como configurar o `loguru` para salvar os logs tanto em um arquivo quanto exibi-los na saída padrão (`stderr`):\n\n```python\nfrom loguru import logger\nfrom sys import stderr\n\n# Configuração do logger para exibir logs no stderr e salvar em arquivo, com filtragem e formatação específicas\nlogger.add(\n    sinkstderr,\n    format\"{time} <r>{level}</r> <g>{message}</g> {file}\",\n    level\"INFO\"\n)\n\nlogger.add(\n    \"meu_arquivo_de_logs.log\",  # Arquivo onde os logs serão salvos\n    format\"{time} {level} {message} {file}\",\n    level\"INFO\"\n)\n\n# Exemplo de uso do logger\nlogger.info(\"Este é um log de informação.\")\nlogger.error(\"Este é um log de erro.\")\n```\n\nNeste código, dois \"sinks\" são adicionados ao `logger`:\n\n1. `stderr`, para exibir os logs, com uma formatação específica que inclui o tempo, nível de log, mensagem e arquivo de origem.\n\n2. `\"meu_arquivo_de_logs.log\"`, para salvar os logs em um arquivo com uma formatação que também inclui tempo, nível, mensagem e arquivo de origem.\n\nOs níveis de log em Python (e em muitos sistemas de logging em outras linguagens de programação) são usados para indicar a gravidade ou importância das mensagens registradas pelo aplicativo. Eles ajudam a diferenciar entre tipos de informações que estão sendo logadas, permitindo uma filtragem e análise mais eficazes dos dados de log. Aqui estão os níveis de log mais comuns, listados em ordem crescente de gravidade:\n\n### DEBUG\n\n* **Descrição**: O nível DEBUG é usado para informações detalhadas, tipicamente de interesse apenas quando se está diagnosticando problemas.\n* **Uso**: Desenvolvedores usam este nível para obter informações detalhadas sobre o fluxo da aplicação, variáveis de estado, e para entender como o código está operando durante o desenvolvimento e a depuração.\n\n### INFO\n\n* **Descrição**: O nível INFO é usado para confirmar que as coisas estão funcionando conforme o esperado.\n* **Uso**: Este nível é geralmente o padrão em produção para registrar eventos normais do sistema, como processos de inicialização, operações concluídas com sucesso, ou outras transações de rotina.\n\n### WARNING\n\n* **Descrição**: O nível WARNING indica que algo inesperado aconteceu, ou indica algum problema no futuro próximo (e.g., 'disco quase cheio'). O software está funcionando como esperado.\n* **Uso**: Utiliza-se este nível para alertar sobre situações que podem necessitar de atenção mas não impedem o funcionamento do sistema. Por exemplo, usar uma função obsoleta ou problemas de performance que não requerem uma ação imediata.\n\n### ERROR\n\n* **Descrição**: O nível ERROR indica que devido a um problema mais grave, a execução de alguma função ou operação falhou.\n* **Uso**: Este nível é usado para registrar eventos de erro que afetam a operação de uma parte do sistema ou funcionalidade, mas não necessariamente o sistema como um todo. Erros que são capturados e gerenciados ainda podem ser logados neste nível.\n\n### CRITICAL\n\n* **Descrição**: O nível CRITICAL indica um erro grave que impede a continuação da execução do programa.\n* **Uso**: É usado para erros que necessitam de atenção imediata, como um falha crítica no sistema que pode resultar em parada total do serviço ou aplicação. Este nível deve ser reservado para os problemas mais sérios.\n\n### Como Utilizar\n\nA seleção do nível de log adequado para diferentes mensagens permite que os desenvolvedores e administradores de sistema configurem os logs para capturar apenas as informações de que precisam. Por exemplo, em um ambiente de desenvolvimento, você pode querer ver todos os logs, desde DEBUG até CRITICAL, para entender completamente o comportamento da aplicação. Em contraste, em um ambiente de produção, você pode configurar para registrar apenas WARNING, ERROR, e CRITICAL, para reduzir o volume de dados gerados e se concentrar em problemas que necessitam de atenção.\n\n\n#### Exemplo 4: Capturando Exceções com Log\n\nLoguru também facilita o logging de exceções, capturando automaticamente informações de traceback.\n\n```python\nfrom loguru import logger\n\ndef minha_funcao():\n    raise ValueError(\"Um erro aconteceu!\")\n\ntry:\n    minha_funcao()\nexcept Exception:\n    logger.exception(\"Uma exceção foi capturada\")\n```\n\nUsando `logger.exception()`, Loguru automaticamente captura e loga o traceback da exceção, o que é extremamente útil para diagnóstico de erros.\n\nVamos criar um decorador utilizando o Loguru para adicionar automaticamente logs a qualquer função Python. Isso nos permite registrar automaticamente quando uma função é chamada e quando ela termina, junto com qualquer informação relevante, como argumentos da função e o resultado retornado (ou exceção lançada).\n\n#### Exemplo 5: Capturando Exceções com Log\n\nAgora, vamos ao código do decorador:\n\n```python\nfrom loguru import logger\nfrom sys import stderr\nfrom functools import wraps\n\nlogger.remove()\n\nlogger.add(\n                sinkstderr,\n                format\"{time} <r>{level}</r> <g>{message}</g> {file}\",\n                level\"INFO\"\n            )\n\nlogger.add(\n                \"meu_arquivo_de_logs.log\",\n                format\"{time} {level} {message} {file}\",\n                level\"INFO\"\n            )\n\ndef log_decorator(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        logger.info(f\"Chamando função '{func.__name__}' com args {args} e kwargs {kwargs}\")\n        try:\n            result  func(*args, **kwargs)\n            logger.info(f\"Função '{func.__name__}' retornou {result}\")\n            return result\n        except Exception as e:\n            logger.exception(f\"Exceção capturada em '{func.__name__}': {e}\")\n            raise  # Re-lança a exceção para não alterar o comportamento da função decorada\n    return wrapper\n```\n\nNeste decorador, `log_decorator`, usamos `logger.info` para registrar quando a função decorada é chamada e o que ela retorna. Se uma exceção for lançada, usamos `logger.exception` para registrar a exceção, incluindo o traceback.\n\n### Como Utilizar o Decorador\n\nAgora, veja como aplicar o `log_decorator` a uma função:\n\n```python\n@log_decorator\ndef soma(a, b):\n    return a + b\n\n@log_decorator\ndef falha():\n    raise ValueError(\"Um erro intencional\")\n\n# Testando as funções decoradas\nsoma(5, 3)  # Isso irá logar a chamada e o retorno\ntry:\n    falha()  # Isso irá logar a chamada e a exceção\nexcept ValueError:\n    pass  # Ignora a exceção para fins de demonstração\n```\n\nAo decorar as funções `soma` e `falha` com `@log_decorator`, automaticamente logamos a entrada e saída (ou exceção) dessas funções sem alterar o corpo delas. Isso é especialmente útil para debugar, monitorar a performance de aplicações ou simplesmente manter um registro de quais funções estão sendo chamadas e com quais argumentos.\n\n### Benefícios do Uso de Decoradores com Loguru\n\nO uso de decoradores em conjunto com o Loguru fornece uma abordagem elegante e poderosa para adicionar logs a aplicações Python. Sem a necessidade de modificar o corpo da função, podemos facilmente adicionar funcionalidades de logging, o que torna o código mais limpo, mantém a separação de preocupações e facilita a manutenção e o debugging.\n\nAlém disso, ao centralizar a lógica de logging no decorador, promovemos a reutilização de código e garantimos uma forma consistente de logar informações através de diferentes partes de uma aplicação.\n\n### Conclusão\n\nO Loguru oferece uma abordagem moderna e conveniente para logging em Python, simplificando muitos aspectos que requerem configuração manual detalhada com o módulo de logging padrão do Python. Seja para desenvolvimento, depuração ou produção, adicionar logging ao seu aplicativo com Loguru pode melhorar significativamente a visibilidade e a capacidade de diagnóstico do seu código.\n\n### Desafio\n\n![imagem_03](./pic/3.jpg)\n\nAplicar decorador de Log, Timer e Qualidade em nossa ETL\n\n![imagem_03](./pic/pic_05.png)\n\n\n\nBootcamp - Python para dados/aula09/etl.py\n\nimport pandas as pd\nimport os\nimport glob\n# uma funcao de extract que le e consolida os json\n\ndef extrair_dados_e_consolidar(pasta: str) -> pd.DataFrame:\n    arquivos_json  glob.glob(os.path.join(pasta, '*.json'))\n    df_list  [pd.read_json(arquivo) for arquivo in arquivos_json]\n    df_total  pd.concat(df_list, ignore_indexTrue)\n    return df_total\n\n# uma funcao que transforma\n\ndef calcular_kpi_de_total_de_vendas(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"Total\"]  df[\"Quantidade\"] * df[\"Venda\"]\n    return df\n\ndef carregar_dados(df: pd.DataFrame, format_saida: list):\n    \"\"\"\n    parametro que vai ser ou \"csv\" ou \"parquet\" ou \"os dois\"\n    \"\"\"\n    for formato in format_saida:\n        if formato  'csv':\n            df.to_csv(\"dados.csv\", indexFalse)\n        if formato  'parquet':\n            df.to_parquet(\"dados.parquet\", indexFalse)\n\ndef pipeline_calcular_kpi_de_vendas_consolidado(pasta: str, formato_de_saida: list):\n    data_frame  extrair_dados_e_consolidar(pasta)\n    data_frame_calculado  calcular_kpi_de_total_de_vendas(data_frame)\n    carregar_dados(data_frame_calculado, formato_de_saida)\n\n# uma funcao que da load em csv ou parquet\n    \nprint(carregar_dados.__doc__)\n\nBootcamp - Python para dados/aula09/exemplo_00.py\n\nfrom log import log_decorator\n\nfrom timer import time_measure_decorator\n\nfrom hello import hello\n\n\n@hello\ndef soma_1(a, b):\n    return a + b\n\nsoma_1(1,2)\n\nBootcamp - Python para dados/aula09/hello.py\n\nfrom functools import wraps\n\ndef hello(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result  func(*args, **kwargs)\n        print(\"Isso e um decorador\")\n        return result\n    return wrapper\n\n\n",
        "Bootcamp - Python para dados/aula09/log.py\n\nfrom loguru import logger\nfrom sys import stderr\nfrom functools import wraps\n\n# Removendo os handlers existentes para evitar duplicação\nlogger.remove()\n\n# Configuração do logger para stderr\nlogger.add(\n                sinkstderr,\n                format\"{time} <r>{level}</r> <g>{message}</g> {file}\",\n                level\"INFO\"\n            )\n\n# Configuração do logger para arquivo de log\nlogger.add(\n                \"meu_arquivo_de_logs.log\",\n                format\"{time} {level} {message} {file}\",\n                level\"INFO\"\n            )\n\ndef log_decorator(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        logger.info(f\"Chamando função '{func.__name__}' com args {args} e kwargs {kwargs}\")\n        try:\n            result  func(*args, **kwargs)\n            logger.info(f\"Função '{func.__name__}' retornou {result}\")\n            return result\n        except Exception as e:\n            logger.exception(f\"Exceção capturada em '{func.__name__}': {e}\")\n            raise  # Re-lança a exceção para não alterar o comportamento da função decorada\n    return wrapper\n\n\nBootcamp - Python para dados/aula09/main.py\n\nfrom loguru import logger\n\ndef minha_funcao():\n    raise ValueError(\"Um erro aconteceu!\")\n\ntry:\n    minha_funcao()\nexcept Exception:\n    logger.exception(\"Uma exceção foi capturada\")\n\nBootcamp - Python para dados/aula09/pipeline.py\n\nfrom etl import pipeline_calcular_kpi_de_vendas_consolidado\n\npasta_argumento: str  'data'\nformato_de_saida: list  [\"csv\", \"parquet\"]\n\npipeline_calcular_kpi_de_vendas_consolidado(pasta_argumento, formato_de_saida)\n\n\nBootcamp - Python para dados/aula09/singleton_decorator.py\n\ndef singleton(cls):\n    instances  {}\n    def get_instance(*args, **kwargs):\n        if cls not in instances:\n            instances[cls]  cls(*args, **kwargs)\n        return instances[cls]\n    return get_instance\n\n@singleton\nclass DatabaseConnection:\n    def __init__(self):\n        print(\"Inicializando uma nova instância da conexão com o banco de dados.\")\n\n# Testando o padrão Singleton\nif __name__  \"__main__\":\n    db1  DatabaseConnection()\n    db2  DatabaseConnection()\n    \n    print(f\"db1 é db2? {'Sim' if db1 is db2 else 'Não'}\")\n\n\nBootcamp - Python para dados/aula09/tenacity_decorator.py\n\nfrom tenacity import retry, stop_after_attempt, wait_fixed\n\n@retry(stopstop_after_attempt(3), waitwait_fixed(2))\ndef get_user_input():\n    user_input  input(\"Digite 'ok' para continuar: \")\n    if user_input.lower() ! 'ok':\n        print(\"Input incorreto. Por favor, tente novamente.\")\n        raise ValueError(\"Input incorreto\")\n    else:\n        print(\"Input correto. Continuando...\")\n\n# Chamar a função\ntry:\n    get_user_input()\nexcept Exception as e:\n    print(f\"Finalmente falhou após várias tentativas: {e}\")\n\n\nBootcamp - Python para dados/aula09/timer.py\n\nimport time\nfrom loguru import logger\nfrom functools import wraps\n\n\n# Decorador de medida de tempo\ndef time_measure_decorator(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time  time.time()\n        result  func(*args, **kwargs)\n        end_time  time.time()\n        logger.info(f\"Função '{func.__name__}' executada em {end_time - start_time:.4f} segundos\")\n        return result\n    return wrapper\n\nBootcamp - Python para dados/aula11-15/README.md\n\n# aula02_bootcamp\n\ninstalar pyenv\n\ninstalar poetry\n\ninstalar ....\n\nBootcamp - Python para dados/aula11-15/OOP.md\n\n# Orientação a Objetos (OOP) versus Programação Funcional\n\n## Orientação a Objetos (OOP):\nA Orientação a Objetos (OOP) é um paradigma de programação que se baseia no conceito de \"objetos\", que podem conter dados na forma de campos (também conhecidos como atributos ou propriedades) e códigos na forma de procedimentos (métodos ou funções). Os objetos são instâncias de classes, que definem as estruturas e comportamentos dos objetos. Os principais conceitos da OOP incluem encapsulamento, herança e polimorfismo.\n\n### Características da OOP:\n1. **Encapsulamento:** O encapsulamento permite ocultar detalhes de implementação dentro de um objeto, expondo apenas a interface pública.\n2. **Herança:** A herança permite que uma classe herde características e comportamentos de outra classe, promovendo a reutilização de código e a organização hierárquica de classes.\n3. **Polimorfismo:** O polimorfismo permite que objetos de diferentes classes sejam tratados de maneira uniforme, fornecendo interfaces comuns para comportamentos diferentes.\n\n## Programação Funcional:\nA Programação Funcional é outro paradigma de programação que se concentra na avaliação de funções matemáticas e na aplicação de funções para transformar dados. Na programação funcional, as funções são tratadas como cidadãos de primeira classe, o que significa que elas podem ser atribuídas a variáveis, passadas como argumentos para outras funções e retornadas como resultados de outras funções.\n\n### Características da Programação Funcional:\n1. **Imutabilidade:** As estruturas de dados são imutáveis, o que significa que não podem ser modificadas após serem criadas. Em vez disso, as funções de transformação retornam novas estruturas de dados.\n2. **Funções Puras:** As funções na programação funcional são consideradas \"puras\" se retornarem o mesmo resultado para os mesmos argumentos e não tiverem efeitos colaterais observáveis.\n3. **Recursão:** A recursão é comumente usada na programação funcional em vez de loops iterativos.\n\n## Diferenças entre OOP e Programação Funcional:\n1. **Abordagem de Solução de Problemas:** Na OOP, os problemas são resolvidos pensando-se em objetos e suas interações, enquanto na programação funcional, os problemas são resolvidos pensando-se em funções e suas composições.\n2. **Estado e Mutabilidade:** Na OOP, os objetos podem manter estados mutáveis, enquanto na programação funcional, as estruturas de dados geralmente são imutáveis.\n3. **Ênfase na Mutabilidade:** Na OOP, a mutabilidade é frequentemente aceita e até mesmo incentivada, enquanto na programação funcional, a ênfase é na imutabilidade e na evitação de efeitos colaterais.\n\n## Conclusão:\nTanto a Orientação a Objetos quanto a Programação Funcional são paradigmas de programação poderosos, cada um com suas próprias vantagens e casos de uso. A escolha entre eles depende do problema em questão, das preferências pessoais e das necessidades do projeto. Em muitos casos, é possível combinar elementos de ambos os paradigmas para criar soluções mais flexíveis e eficientes.\n\n\nBootcamp - Python para dados/aula11-15/desafio.py\n\nCONSTANTE_BONUS  1000\n\n# 1) Solicita ao usuário que digite seu nome\n#nome_usuario  input(\"Digite o seu nome: \")\n\n# nome_usuario  33 isso e um erro?\n\nnome_usuario  input(\"Digite o seu nome: \")\n\nif nome_usuario.isdigit():\n    print(\"Voce digitou seu nome errado\")\n    exit()\nelif len(nome_usuario)  0:\n    print(\"Voce nao digitou nada\")\n    exit()\nelif nome_usuario.isspace():\n    print(\"Voce digitou so espaco\")\n    exit()\n\n# 2) Solicita ao usuário que digite o valor do seu salário\n# Converte a entrada para um número de ponto flutuante\nsalario_usuario  float(input(\"Digite o seu salario: \"))\n\n# 3) Solicita ao usuário que digite o valor do bônus recebido\n# Converte a entrada para um número de ponto flutuante\nbonus_usuario  float(input(\"Digite o seu bonus: \"))\n\n# 4) Calcule o valor do bônus final\n\nvalor_do_bonus  CONSTANTE_BONUS + salario_usuario * bonus_usuario\n\n# 5) Imprime a mensagem personalizada incluindo o nome do usuário e o valor do bonus\nprint(f\"O usuario {nome_usuario} possui o bonus de {valor_do_bonus}\")\n\n# Bônus: Quantos bugs e riscos você consegue identificar nesse programa?\n\nBootcamp - Python para dados/aula11-15/exercicios.py\n\nimport math\n\n# #### Inteiros (`int`)\n\n# 1. Escreva um programa que soma dois números inteiros inseridos pelo usuário.\n# 2. Crie um programa que receba um número do usuário e calcule o resto da divisão desse número por 5.\n# 3. Desenvolva um programa que multiplique dois números fornecidos pelo usuário e mostre o resultado.\n# 4. Faça um programa que peça dois números inteiros e imprima a divisão inteira do primeiro pelo segundo.\n\nnumero_01  int(input(\"Inserir um numero inteiro: \"))\nnumero_02  int(input(\"Inserir outro numero inteiro: \"))\nresultado  numero_01 // numero_02\nprint(resultado)\n\n# 5. Escreva um programa que calcule o quadrado de um número fornecido pelo usuário.\n\n# #### Números de Ponto Flutuante (`float`)\n\n# 6. Escreva um programa que receba dois números flutuantes e realize sua adição.\n# 7. Crie um programa que calcule a média de dois números flutuantes fornecidos pelo usuário.\n# 8. Desenvolva um programa que calcule a potência de um número (base e expoente fornecidos pelo usuário).\n# 9. Faça um programa que converta a temperatura de Celsius para Fahrenheit.\n# 10. Escreva um programa que calcule a área de um círculo, recebendo o raio como entrada.\n\n#raio_do_circulo  float(input(\"Digite o raio: \"))\n#area_do_circulo  math.pi * raio_do_circulo ** 2\n# area_do_circulo_formatada  \"{:.2f}\".format(area_do_circulo)\n#print(f\"{area_do_circulo:.2f}\")\n\n# #### Strings (`str`)\n\n# 11. Escreva um programa que receba uma string do usuário e a converta para maiúsculas.\n# 12. Crie um programa que receba o nome completo do usuário e imprima o nome com todas as letras minúsculas.\n# 13. Desenvolva um programa que peça ao usuário para inserir uma frase e, em seguida, imprima esta frase sem espaços em branco no início e no final.\n# 14. Faça um programa que peça ao usuário para digitar uma data no formato \"dd/mm/aaaa\" e, em seguida, imprima o dia, o mês e o ano separadamente.\n# 15. Escreva um programa que concatene duas strings fornecidas pelo usuário.\n\n# data_do_usuario  input(\"Insira uma data no formato dd/mm/aaaa: \")\n# lista_de_dia_mes_ano  data_do_usuario.split(\"/\")\n# print(f\"O elemento 1 e o: {lista_de_dia_mes_ano[0]}\")\n# print(f\"O elemento 2 e o: {lista_de_dia_mes_ano[1]}\")\n# print(f\"O elemento 3 e o: {lista_de_dia_mes_ano[2]}\")\n\n# #### Booleanos (`bool`)\n\n# 16. Escreva um programa que avalie duas expressões booleanas inseridas pelo usuário e retorne o resultado da operação AND entre elas.\n# 17. Crie um programa que receba dois valores booleanos do usuário e retorne o resultado da operação OR.\n# 18. Desenvolva um programa que peça ao usuário para inserir um valor booleano e, em seguida, inverta esse valor.\n# 19. Faça um programa que compare se dois números fornecidos pelo usuário são iguais.\n# 20. Escreva um programa que verifique se dois números fornecidos pelo usuário são diferentes.\n\n# #### try-except e if\n\n# 21: Conversor de Temperatura\n# 22: Verificador de Palíndromo\n# 23: Calculadora Simples\n# 24: Classificador de Números\n# 25: Conversão de Tipo com Validação\n\nBootcamp - Python para dados/aula11-15/main.py\n\n# Exemplo que causa TypeError\n\n# try:\n#     resultado  len(3)\n#     print(resultado)\n# except TypeError as e:\n#     print(e)\n# else:\n#     print(\"tudo ocorreu bem\")\n# finally:\n#     print(\"o importante e participar\")  \n\n# numero  int(input(\"Insira um numero :\"))\n# if isinstance(numero, ):\n#     print(\"A variável é um inteiro.\")\n# else:\n#     print(\"A variável não é um inteiro.\")\n\n\n\nBootcamp - Python para dados/aula11-15/01-basico/class.py\n\nimport pandas as pd\n\nclass ProcessadorCSV:\n    def __init__(self, arquivo_csv):\n        self.arquivo_csv  arquivo_csv\n        self.df  None\n    \n    def carregar_csv(self):\n        # Carregar o arquivo CSV em um DataFrame\n        self.df  pd.read_csv(self.arquivo_csv)\n    \n    def remover_celulas_vazias(self):\n        # Verificar e remover células vazias\n        self.df  self.df.dropna()\n    \n    def filtrar_por_estado(self, estado):\n        # Filtrar as linhas pela coluna estado\n        self.df  self.df[self.df['estado']  estado]\n    \n    def processar(self, estado):\n        # Carregar CSV, remover células vazias e filtrar por estado\n        self.carregar_csv()\n        self.remover_celulas_vazias()\n        self.filtrar_por_estado(estado)\n        \n        return self.df\n\n# Exemplo de uso\narquivo_csv  './exemplo.csv'  # substitua 'exemplo.csv' pelo caminho do seu arquivo CSV\nestado_filtrado  'SP'  # estado que você quer filtrar\n\nprocessador  ProcessadorCSV(arquivo_csv)\ndf_filtrado  processador.processar(estado_filtrado)\n\nprint(df_filtrado)\n\n\nBootcamp - Python para dados/aula11-15/01-basico/functional.py\n\nimport pandas as pd\n\ndef carregar_csv_e_filtrar(arquivo_csv, estado):\n    # Carregar o arquivo CSV em um DataFrame\n    df  pd.read_csv(arquivo_csv)\n    \n    # Verificar e remover células vazias\n    df  df.dropna()\n    \n    # Filtrar as linhas pela coluna estado\n    df_filtrado  df[df['estado']  estado]\n    \n    return df_filtrado\n\n# Exemplo de uso\narquivo_csv  './exemplo.csv'  # substitua 'dados.csv' pelo caminho do seu arquivo CSV\nestado_filtrado  'SP'  # estado que você quer filtrar\ndf_filtrado  carregar_csv_e_filtrar(arquivo_csv, estado_filtrado)\n\nprint(df_filtrado)\n\n\nBootcamp - Python para dados/aula11-15/02-encapsulamento/conexao.py\n\nfrom .sqlite import BancoDeDadosSQLite\nfrom .postgre import BancoDeDadosPost\nfrom .encaps import BancoDeDados\n\n########## SQLITE #############\n\nnome_arquivo  \"exemplo.db\"\nbanco_sql  BancoDeDadosSQLite(nome_arquivo)\nbanco_sql.conectar()\n\n# Inserindo dados na tabela\ninsert_query  \"\"\"\nINSERT INTO usuarios (nome, email) VALUES\n('João', 'joao@example.com'),\n('Maria', 'maria@example.com');\n\"\"\"\nbanco_sql.executar_query(insert_query)\n\nbanco_sql.desconectar()\n\n########## POSTGRE #############\n\nhost  'localhost'\nporta  '5432'\nbanco  'nome_do_banco'\nusuario  'usuario'\nsenha  'senha'\n    \nbanco_post  BancoDeDadosPost(host, porta, banco, usuario, senha)\nbanco_post.conectar()\n\ninsert_query  \"\"\"\nINSERT INTO usuarios (nome, email) VALUES\n('João', 'joao@example.com'),\n('Maria', 'maria@example.com');\n\"\"\"\nbanco_post.executar_query(insert_query)\n\nbanco_post.desconectar()\n\n\n########## ENCAPSULADO #############\nbanco  BancoDeDados(\"tipo_banco\")\nbanco.conectar()\n\ninsert_query  \"\"\"\nINSERT INTO usuarios (nome, email) VALUES\n('João', 'joao@example.com'),\n('Maria', 'maria@example.com');\n\"\"\"\nbanco.executar_query(insert_query)\n\nbanco.desconectar()\n\nBootcamp - Python para dados/aula11-15/02-encapsulamento/encaps.py\n\nimport os\nimport sqlite3\nimport psycopg2\n\nclass BancoDeDados:\n    def __init__(self, tipo_banco):\n        self.tipo_banco  tipo_banco\n        self.conexao  None\n\n    def conectar(self):\n        if self.tipo_banco  'sqlite':\n            try:\n                nome_arquivo  os.getenv('NOME_ARQUIVO_SQLITE')\n                self.conexao  sqlite3.connect(nome_arquivo)\n                print(\"Conexão SQLite estabelecida com sucesso!\")\n            except sqlite3.Error as e:\n                print(\"Erro ao conectar ao banco de dados SQLite:\", e)\n        elif self.tipo_banco  'postgres':\n            try:\n                host  os.getenv('HOST_PG')\n                porta  os.getenv('PORTA_PG')\n                banco  os.getenv('BANCO_PG')\n                usuario  os.getenv('USUARIO_PG')\n                senha  os.getenv('SENHA_PG')\n                self.conexao  psycopg2.connect(\n                    hosthost,\n                    portporta,\n                    databasebanco,\n                    userusuario,\n                    passwordsenha\n                )\n                print(\"Conexão PostgreSQL estabelecida com sucesso!\")\n            except psycopg2.Error as e:\n                print(\"Erro ao conectar ao banco de dados PostgreSQL:\", e)\n        else:\n            print(\"Tipo de banco de dados não suportado.\")\n\n    def desconectar(self):\n        if self.conexao:\n            self.conexao.close()\n            if self.tipo_banco  'sqlite':\n                print(\"Conexão SQLite fechada.\")\n            elif self.tipo_banco  'postgres':\n                print(\"Conexão PostgreSQL fechada.\")\n\n    def executar_query(self, query):\n        try:\n            cursor  self.conexao.cursor()\n            cursor.execute(query)\n            self.conexao.commit()\n            print(\"Query executada com sucesso!\")\n        except (sqlite3.Error, psycopg2.Error) as e:\n            print(\"Erro ao executar a query:\", e)\n\n\n# Exemplo de uso\nif __name__  \"__main__\":\n    tipo_banco  os.getenv('TIPO_BANCO')  # 'sqlite' ou 'postgres'\n    \n    banco  BancoDeDados(tipo_banco)\n    banco.conectar()\n\n    # Exemplo de criação de tabela\n    create_table_query  \"\"\"\n    CREATE TABLE IF NOT EXISTS usuarios (\n        id SERIAL PRIMARY KEY,\n        nome TEXT NOT NULL,\n        email TEXT NOT NULL\n    );\n    \"\"\"\n    banco.executar_query(create_table_query)\n\n    # Exemplo de inserção de dados\n    insert_query  \"\"\"\n    INSERT INTO usuarios (nome, email) VALUES\n    ('João', 'joao@example.com'),\n    ('Maria', 'maria@example.com');\n    \"\"\"\n    banco.executar_query(insert_query)\n\n    banco.desconectar()\n\n\nBootcamp - Python para dados/aula11-15/02-encapsulamento/postgre.py\n\nimport psycopg2\n\nclass BancoDeDadosPost:\n    def __init__(self, host, porta, banco, usuario, senha):\n        self.host  host\n        self.porta  porta\n        self.banco  banco\n        self.usuario  usuario\n        self.senha  senha\n        self.conexao  None\n\n    def conectar(self):\n        try:\n            self.conexao  psycopg2.connect(\n                hostself.host,\n                portself.porta,\n                databaseself.banco,\n                userself.usuario,\n                passwordself.senha\n            )\n            print(\"Conexão estabelecida com sucesso!\")\n        except psycopg2.Error as e:\n            print(\"Erro ao conectar ao banco de dados:\", e)\n\n    def desconectar(self):\n        if self.conexao:\n            self.conexao.close()\n            print(\"Conexão fechada.\")\n\n    def executar_query(self, query):\n        try:\n            cursor  self.conexao.cursor()\n            cursor.execute(query)\n            self.conexao.commit()\n            print(\"Query executada com sucesso!\")\n        except psycopg2.Error as e:\n            print(\"Erro ao executar a query:\", e)\n\n\n# Exemplo de uso\nif __name__  \"__main__\":\n    host  'localhost'\n    porta  '5432'\n    banco  'nome_do_banco'\n    usuario  'usuario'\n    senha  'senha'\n    \n    banco  BancoDeDadosPost(host, porta, banco, usuario, senha)\n    banco.conectar()\n\n    # Exemplo de criação de tabela\n    create_table_query  \"\"\"\n    CREATE TABLE IF NOT EXISTS usuarios (\n        id SERIAL PRIMARY KEY,\n        nome TEXT NOT NULL,\n        email TEXT NOT NULL\n    );\n    \"\"\"\n    banco.executar_query(create_table_query)\n\n    # Exemplo de inserção de dados\n    insert_query  \"\"\"\n    INSERT INTO usuarios (nome, email) VALUES\n    ('João', 'joao@example.com'),\n    ('Maria', 'maria@example.com');\n    \"\"\"\n    banco.executar_query(insert_query)\n\n    banco.desconectar()\n\n\nBootcamp - Python para dados/aula11-15/02-encapsulamento/sqlite.py\n\nimport sqlite3\n\nclass BancoDeDadosSQLite:\n    def __init__(self, nome_arquivo):\n        self.nome_arquivo  nome_arquivo\n        self.conexao  None\n\n    def conectar(self):\n        try:\n            self.conexao  sqlite3.connect(self.nome_arquivo)\n            print(\"Conexão estabelecida com sucesso!\")\n        except sqlite3.Error as e:\n            print(\"Erro ao conectar ao banco de dados:\", e)\n\n    def desconectar(self):\n        if self.conexao:\n            self.conexao.close()\n            print(\"Conexão fechada.\")\n\n    def executar_query(self, query):\n        try:\n            cursor  self.conexao.cursor()\n            cursor.execute(query)\n            self.conexao.commit()\n            print(\"Query executada com sucesso!\")\n        except sqlite3.Error as e:\n            print(\"Erro ao executar a query:\", e)\n\n\n# Exemplo de uso\nif __name__  \"__main__\":\n    nome_arquivo  \"exemplo.db\"\n    banco  BancoDeDadosSQLite(nome_arquivo)\n    banco.conectar()\n\n    # Criando uma tabela\n    create_table_query  \"\"\"\n    CREATE TABLE IF NOT EXISTS usuarios (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        nome TEXT NOT NULL,\n        email TEXT NOT NULL\n    );\n    \"\"\"\n    banco.executar_query(create_table_query)\n\n    # Inserindo dados na tabela\n    insert_query  \"\"\"\n    INSERT INTO usuarios (nome, email) VALUES\n    ('João', 'joao@example.com'),\n    ('Maria', 'maria@example.com');\n    \"\"\"\n    banco.executar_query(insert_query)\n\n    banco.desconectar()\n\n\nBootcamp - Python para dados/aula11-15/03-heranca/etl.py\n\nimport pandas as pd\n\nclass ETLProcess:\n    def __init__(self, fonte_dados):\n        self.fonte_dados  fonte_dados\n\n    def extrair_dados(self):\n        raise NotImplementedError(\"Método extrair_dados deve ser implementado nas classes filhas.\")\n\n    def transformar_dados(self, dados):\n        raise NotImplementedError(\"Método transformar_dados deve ser implementado nas classes filhas.\")\n\n    def carregar_dados(self, dados_transformados):\n        raise NotImplementedError(\"Método carregar_dados deve ser implementado nas classes filhas.\")\n\n    def executar_etl(self):\n        dados_extraidos  self.extrair_dados()\n        dados_transformados  self.transformar_dados(dados_extraidos)\n        self.carregar_dados(dados_transformados)\n\n\nclass ETLCSV(ETLProcess):\n    def extrair_dados(self):\n        return pd.read_csv(self.fonte_dados)\n\n    def transformar_dados(self, dados):\n        # Exemplo simples de transformação: converter todas as letras em maiúsculas\n        return dados.applymap(lambda x: x.upper() if isinstance(x, str) else x)\n\n    def carregar_dados(self, dados_transformados):\n        # Aqui você pode implementar a lógica para carregar os dados transformados para onde desejar\n        print(\"Dados transformados:\")\n        print(dados_transformados)\n\n\n# Exemplo de uso\nif __name__  \"__main__\":\n    fonte_csv  'dados.csv'  # Substitua 'dados.csv' pelo caminho do seu arquivo CSV\n    etl_csv  ETLCSV(fonte_csv)\n    etl_csv.executar_etl()\n\n\nBootcamp - Python para dados/aula11-15/04-polimorfismo/overload.py\n\n## Em Python, a sobrecarga de método não é diretamente suportada como em algumas outras linguagens de programação, \n## mas você pode simular sobrecarga usando parâmetros padrão ou argumentos variáveis.\n\n\nclass Calculadora:\n    def soma(self, *args):\n        total  0\n        for num in args:\n            total + num\n        return total\n\n# Exemplo de uso\ncalculadora  Calculadora()\nprint(calculadora.soma(1, 2))        # Saída: 3\nprint(calculadora.soma(1, 2, 3, 4))  # Saída: 10\nprint(calculadora.soma(5, 10, 15))   # Saída: 30\n\n\n",
        "Bootcamp - Python para dados/aula11-15/04-polimorfismo/override.py\n\nimport pandas as pd\n\nclass ETLProcess:\n    def __init__(self, fonte_dados):\n        self.fonte_dados  fonte_dados\n\n    def extrair_dados(self):\n        raise NotImplementedError(\"Método extrair_dados deve ser implementado nas classes filhas.\")\n\n    def transformar_dados(self, dados):\n        raise NotImplementedError(\"Método transformar_dados deve ser implementado nas classes filhas.\")\n\n    def carregar_dados(self, dados_transformados):\n        raise NotImplementedError(\"Método carregar_dados deve ser implementado nas classes filhas.\")\n\n    def executar_etl(self):\n        dados_extraidos  self.extrair_dados()\n        dados_transformados  self.transformar_dados(dados_extraidos)\n        self.carregar_dados(dados_transformados)\n\n\nclass ETLCSV(ETLProcess):\n    def extrair_dados(self):\n        return pd.read_csv(self.fonte_dados)\n\n    def transformar_dados(self, dados):\n        # Exemplo simples de transformação: converter todas as letras em maiúsculas\n        return dados.applymap(lambda x: x.upper() if isinstance(x, str) else x)\n\n    def carregar_dados(self, dados_transformados):\n        # Aqui você pode implementar a lógica para carregar os dados transformados de um arquivo CSV\n        print(\"Dados transformados (CSV):\")\n        print(dados_transformados)\n\n\nclass ETLExcel(ETLProcess):\n    def extrair_dados(self):\n        return pd.read_excel(self.fonte_dados)\n\n    def transformar_dados(self, dados):\n        # Exemplo simples de transformação: converter todas as letras em minúsculas\n        return dados.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n\n    def carregar_dados(self, dados_transformados):\n        # Aqui você pode implementar a lógica para carregar os dados transformados de um arquivo Excel\n        print(\"Dados transformados (Excel):\")\n        print(dados_transformados)\n\n\n# Exemplo de uso\nif __name__  \"__main__\":\n    fonte_csv  'dados.csv'  # Substitua 'dados.csv' pelo caminho do seu arquivo CSV\n    etl_csv  ETLCSV(fonte_csv)\n    etl_csv.executar_etl()\n\n    fonte_excel  'dados.xlsx'  # Substitua 'dados.xlsx' pelo caminho do seu arquivo Excel\n    etl_excel  ETLExcel(fonte_excel)\n    etl_excel.executar_etl()\n\n\nBootcamp - Python para dados/aula11-15/05-GettereSetter/basico.py\n\nclass Pessoa:\n    def __init__(self, nome, idade):\n        self._nome  nome\n        self._idade  idade\n\n    # Getter para o atributo 'nome'\n    def get_nome(self):\n        return self._nome\n\n    # Setter para o atributo 'nome'\n    def set_nome(self, novo_nome):\n        self._nome  novo_nome\n\n    # Getter para o atributo 'idade'\n    def get_idade(self):\n        return self._idade\n\n    # Setter para o atributo 'idade'\n    def set_idade(self, nova_idade):\n        if nova_idade > 0:\n            self._idade  nova_idade\n        else:\n            print(\"A idade deve ser um número positivo.\")\n\n# Exemplo de uso\npessoa  Pessoa(\"João\", 30)\n\n# Usando o método getter para acessar o atributo 'nome'\nprint(\"Nome:\", pessoa.get_nome())  # Saída: Nome: João\n\n# Usando o método setter para alterar o atributo 'nome'\npessoa.set_nome(\"Maria\")\nprint(\"Novo nome:\", pessoa.get_nome())  # Saída: Novo nome: Maria\n\n# Usando o método getter para acessar o atributo 'idade'\nprint(\"Idade:\", pessoa.get_idade())  # Saída: Idade: 30\n\n# Usando o método setter para alterar o atributo 'idade'\npessoa.set_idade(25)\nprint(\"Nova idade:\", pessoa.get_idade())  # Saída: Nova idade: 25\n\n# Tentando definir uma idade negativa\npessoa.set_idade(-5)  # Saída: A idade deve ser um número positivo.\n\n\nBootcamp - Python para dados/aula11-15/05-GettereSetter/decorator.py\n\nclass Pessoa:\n    def __init__(self, nome, idade):\n        self._nome  nome\n        self._idade  idade\n\n    @property\n    def nome(self):\n        return self._nome\n\n    @nome.setter\n    def nome(self, novo_nome):\n        self._nome  novo_nome\n\n    @property\n    def idade(self):\n        return self._idade\n\n    @idade.setter\n    def idade(self, nova_idade):\n        if nova_idade > 0:\n            self._idade  nova_idade\n        else:\n            print(\"A idade deve ser um número positivo.\")\n\n# Exemplo de uso\npessoa  Pessoa(\"João\", 30)\n\n# Usando o método getter para acessar o atributo 'nome'\nprint(\"Nome:\", pessoa.nome)  # Saída: Nome: João\n\n# Usando o método setter para alterar o atributo 'nome'\npessoa.nome  \"Maria\"\nprint(\"Novo nome:\", pessoa.nome)  # Saída: Novo nome: Maria\n\n# Usando o método get\n\n\nBootcamp - Python para dados/aula11-15/Aula02/csv-test.py\n\nimport pandas as pd\n\n\ndf  pd.read_csv('./exemplo.csv')\n\ndf_filtrado  df[df['estado']  'SP']\n\ndf_filtrado  df[df['preço']  '10,50']\n\n\nprint(df_filtrado)\n\n\n\ndf2  pd.read_csv('./examplo2.csv')\n\ndf_filtrado2  df2[df2['estado']  'DF']\n\ndf_filtrado2  df2[df2['preço']  '10,50']\n\nprint(df_filtrado)\n\nBootcamp - Python para dados/aula11-15/Aula02/src/usar.py\n\nfrom interface.classes.csv_class import CsvProcessor\n# import pandas as pd\n\narquivo_csv  './exemplo.csv'\nfiltro  'estado'\nlimite  'SP'\n\narquivo_CSV  CsvProcessor(arquivo_csv)\narquivo_CSV.carregar_csv()  # Load the CSV\nprint(arquivo_CSV.filtrar_por(['estado', 'preço'], ['SP', '10,50']))\n# print(arquivo_CSV.df)\nprint(\"#########################\")\n# arquivo_csv2  './examplo2.csv'\n# filtro2  'estado'  # Changed to filtro2\n# limite2  'DF'\n\n# arquivo_CSV2  CsvProcessor(arquivo_csv2)\n# arquivo_CSV2.carregar_csv()  # Load the CSV\n# print(arquivo_CSV2.filtrar_por(filtro2, limite2))  # Changed to filtro2\n# print(arquivo_CSV2.sub_filtro('preço', '10,50'))\n\n\nBootcamp - Python para dados/aula11-15/Aula02/src/interface/classes/csv_class.py\n\nimport pandas as pd\n\nclass CsvProcessor:\n    def __init__(self, file_path: str):\n        self.file_path  file_path\n        self.df  None\n        self.df_filtrado  None\n\n    def carregar_csv(self):\n        self.df  pd.read_csv(self.file_path)\n        return self.df  # Return the DataFrame after loading\n\n    ## receber um str str[]\n    def filtrar_por(self, colunas, atributos):\n        if len(colunas) ! len(atributos):\n            raise ValueError(\"Não tem o mesmo número de colunas e atributos\")\n        \n        if len(colunas)  0:\n            return self.df\n        \n        coluna_atual  colunas[0]\n        atributo_atual  atributos[0]\n\n        df_filtrado  self.df[self.df[coluna_atual]  atributo_atual]\n\n        if len(colunas)  1:\n            return df_filtrado\n        else:\n            return self.filtrar_por(colunas[1:], atributos[1:])\n\n    \n\n\nBootcamp - Python para dados/aula11-15/Aula03/README.md\n\nMinha empresa recebe arquivos nos formatos .csv e .txt em duas pastas distintas,\ne preciso consolidá-los em um único dataframe. \n\nQual seria a melhor abordagem para realizar essa tarefa?\n\n\n\ndata/csv_files\ndata/txt_files\n\n\nFormato dos arquivos \nid,name\n\n\ns3 -> trigger -> ec2 ou lambda ou ecs \n\n\n\nBootcamp - Python para dados/aula11-15/Aula03/data/txt_files/test.txt\n\nid,name\n1,fabio\n\nBootcamp - Python para dados/aula11-15/Aula03/src/__main__.py\n\nimport schedule\nimport time\nfrom lib.classes.CsvSource import CsvSource\nfrom lib.classes.JsonSource import JsonSource\nfrom lib.classes.TxtSource import TxtSource\n\n# Função para verificar novos arquivos\ndef check_for_new_files():\n    csv_source.check_for_new_files()  # Chama o método check_for_new_files da instância\n    txt_source.check_for_new_files()\n    json_source.check_for_new_files()\n\n# Agendando a execução da função check_for_new_files() a cada segundo\nschedule.every(10).seconds.do(check_for_new_files)\n\ncsv_source  CsvSource()\ntxt_source  TxtSource()\njson_source  JsonSource()\n\n# Executa o loop principal\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)  # Aguarda 1 segundo para que o loop não consuma muito processamento\n\n\nBootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/AbstractDataSource.py\n\nfrom abc import ABC, abstractmethod\n\nclass AbstractDataSource(ABC):\n    def __init__(self):\n        pass\n     \n    @abstractmethod\n    def start(self):\n        raise NotImplementedError(\"Método não implementado\")\n\n    @abstractmethod\n    def get_data(self):\n        raise NotImplementedError(\"Método não implementado\")\n\n    @abstractmethod\n    def transform_data_to_df(self):\n        raise NotImplementedError(\"Método não implementado\")\n\n    @abstractmethod\n    def save_data(self):\n        raise NotImplementedError(\"Método não implementado\")\n\n    def hello_world(self):\n        print('Hello World')\n\nBootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/CsvSource.py\n\nimport os\nimport pandas as pd\nfrom lib.classes.FilesSources import FilesSources\n\n\nclass CsvSource(FilesSources):\n    def create_path(self):\n        current_directory  os.getcwd()\n        self.folder_path  os.path.join(current_directory, 'data', 'csv_files')\n        if not os.path.exists(self.folder_path):\n            os.makedirs(self.folder_path)\n\n    def check_for_new_files(self):\n        current_files  os.listdir(self.folder_path)\n        new_files  [file for file in current_files if file not in self.previous_files and file.endswith('.csv')]\n\n        if new_files:\n            print(\"New files detected:\", new_files)\n            # Update the list of previous files\n            self.previous_files  current_files\n        else:\n            print(\"No new CSV files detected.\")\n            self.get_data()\n\n    def get_data(self):\n        # Implement getting data from CSV files in the specified folder\n        data_frames  []\n        for file_path in self.previous_files:\n            try:\n                path  f'{self.folder_path}/{file_path}'\n                data  pd.read_csv(path)\n                data_frames.append(data)\n            except Exception as e:\n                print(\"An error occurred while reading the CSV file:\", e)\n        if data_frames:\n            self.combined_data  pd.concat(data_frames, ignore_indexTrue)\n            print(self.combined_data)\n            return self.combined_data\n        else:\n            return None\n    \n    def transform_data_to_df(self):\n        return super().transform_data_to_df()\n        \n\nBootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/FilesSources.py\n\nimport os\nfrom lib.classes.AbstractDataSource import AbstractDataSource\n\nclass FilesSources(AbstractDataSource):\n    def __init__(self):\n        self.previous_files  []\n        self.start()\n\n    def create_path(self):\n        current_directory  os.getcwd()\n        self.folder_path  os.path.join(current_directory, 'data', 'extension_files')\n        if not os.path.exists(self.folder_path):\n            os.makedirs(self.folder_path)\n\n    def check_for_new_files(self):\n        current_files  os.listdir(self.folder_path)\n        new_files  [file for file in current_files if file not in self.previous_files]\n\n        if new_files:\n            print(\"New files detected:\", new_files)\n            # Update the list of previous files\n            self.previous_files  current_files\n        else:\n            print(\"No new files detected.\")\n\n    def get_data(self):\n        pass\n    \n    def transform_data_to_df(self):\n        pass\n\n    def save_data(self):\n        pass\n\n    def show_files(self):\n        print(self.previous_files)\n\n    def start(self):\n        self.create_path()\n\nBootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/JsonSource.py\n\nimport json\nimport os\nfrom lib.classes.FilesSources import FilesSources\n\nclass JsonSource(FilesSources):\n    def create_path(self):\n        current_directory  os.getcwd()\n        self.folder_path  os.path.join(current_directory, 'json_files')\n        if not os.path.exists(self.folder_path):\n            os.makedirs(self.folder_path)\n\n    def check_for_new_files(self):\n        current_files  os.listdir(self.folder_path)\n        new_files  [file for file in current_files if file not in self.previous_files and file.endswith('.json')]\n\n        if new_files:\n            print(\"New files detected:\", new_files)\n            # Update the list of previous files\n            self.previous_files  current_files\n        else:\n            print(\"No new JSON files detected.\")\n            self.get_data()\n\n    def read_json_file(self, file_path):\n        try:\n            with open(file_path, 'r') as f:\n                data  json.load(f)\n            return data\n        except Exception as e:\n            print(\"Erro ao acessar o JSON\")\n            return None\n    \n    def get_data(self):\n\n        data  []\n        for file_path in self.previous_files:\n            if file_path.endswith('.json'):\n                path  os.path.join(self.folder_path, file_path)\n                json_data  self.read_json_file(path)\n                if json_data is not None:\n                    data.append(json_data)\n        print(data)\n        return data\n\nBootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/TxtSource.py\n\nimport os\nimport pandas as pd\nfrom lib.classes.FilesSources import FilesSources\n\n\nclass TxtSource(FilesSources):\n    def create_path(self):\n        current_directory  os.getcwd()\n        self.folder_path  os.path.join(current_directory, 'data', 'txt_files')\n        if not os.path.exists(self.folder_path):\n            os.makedirs(self.folder_path)\n\n    def check_for_new_files(self):\n        current_files  os.listdir(self.folder_path)\n        new_files  [file for file in current_files if file not in self.previous_files and file.endswith('.txt')]\n\n        if new_files:\n            print(\"New TXT files detected:\", new_files)\n            # Update the list of previous files\n            self.previous_files  current_files\n        else:\n            print(\"No new TXT files detected.\")\n            self.get_data()\n\n    def get_data(self):\n        # Implement getting data from TXT files in the specified folder\n        data_frames  []\n        for file_path in self.previous_files:\n            try:\n                path  os.path.join(self.folder_path, file_path)\n                data  pd.read_csv(path, sep'\\t')  # Assume que os arquivos .txt estão tabulados\n                data_frames.append(data)\n            except Exception as e:\n                print(\"An error occurred while reading the TXT file:\", e)\n        if data_frames:\n            self.combined_data  pd.concat(data_frames, ignore_indexTrue)\n            print(self.combined_data)\n            return self.combined_data\n        else:\n            return None\n\n\nBootcamp - Python para dados/aula11-15/Aula03/src/lib/classes/aws/s3.py\n\nimport boto3\n\nclass S3DataCollector:\n    def __init__(self, access_key, secret_key, bucket_name):\n        self.access_key  access_key\n        self.secret_key  secret_key\n        self.bucket_name  bucket_name\n        self.s3_client  boto3.client('s3', aws_access_key_idself.access_key, aws_secret_access_keyself.secret_key)\n\n    def list_objects(self, prefix''):\n        response  self.s3_client.list_objects_v2(Bucketself.bucket_name, Prefixprefix)\n        if 'Contents' in response:\n            return [obj['Key'] for obj in response['Contents']]\n        else:\n            return []\n\n    def download_file(self, key, local_path):\n        return self.s3_client.download_file(self.bucket_name, key, local_path)\n        \n\nBootcamp - Python para dados/aula11-15/Aula04/Collector/main.py\n\nfrom datasource.api import APICollector\nfrom contracts.schema import CompraSchema\nfrom tools.aws.client import S3Client\n\nimport time\nimport schedule\n\nschema  CompraSchema\naws  S3Client()\n\n\ndef apiCollector(schema, aws, repeat):\n    reponse  APICollector(schema, aws).start(repeat)\n    print('Executei')\n    return\n\nschedule.every(1).minutes.do(apiCollector,schema, aws, 50)\n\n\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)\n\n\nBootcamp - Python para dados/aula11-15/Aula04/Collector/contracts/schema.py\n\nfrom typing import Union, Dict\n\n\nGenericSchema  Dict[str, Union[str, float, int]]\n\n\nCompraSchema: GenericSchema  {\n    \"ean\" : int,\n    \"price\" : float,\n    \"store\" : int,\n    \"dateTime\" : str\n}\n\nBootcamp - Python para dados/aula11-15/Aula04/Collector/datasource/api.py\n\nimport requests\nimport pandas as pd\nimport datetime\nfrom io import BytesIO\nfrom contracts.schema import GenericSchema\nfrom typing import List\nfrom tools.retry import retry\n\nclass APICollector:\n        def __init__ (self, schema, aws): \n            self._schema  schema\n            self._aws  aws\n            self._buffer  None\n            return\n        \n        def start(self, param):\n            response  self.getData(param)\n            response  self.extractData(response)\n            response  self.transformDf(response)\n            response  self.convertToParquet(response)\n\n            if self._buffer is not None:\n                 file_name  self.fileName()\n                 print(file_name)\n                 self._aws.upload_file(response, file_name)\n                 return True\n                 \n            return False\n                \n        @retry(requests.exceptions.RequestException, tries5, delay1, backoff2)\n        def getData(self, param):\n            response  None\n            if param > 1:\n                  response  requests.get(f'http://127.0.0.1:8000/gerar_compras/{param}').json()\n            else:\n                 response  requests.get('http://127.0.0.1:8000/gerar_compra').json()\n            return response\n                \n        def extractData(self, response):\n            result: List[GenericSchema]  []\n            for item in response:\n                index  {}\n                for key, value in self._schema.items():\n                    if type(item.get(key))  value:\n                        index[key]  item[key]\n                    else:\n                        index[key]  None\n                result.append(index)\n            return result\n        \n        def transformDf(self, response):\n              result  pd.DataFrame(response)\n              return result\n        \n        def convertToParquet(self, response):\n            self._buffer  BytesIO()\n            try:\n                with self._buffer as buffer:\n                    response.to_parquet(buffer)\n                    return buffer\n            except Exception as e:\n                    print(f\"Error converting DataFrame to Parquet: {e}\")\n                    self._buffer  None\n\n        def fileName(self):\n             data_atual  datetime.datetime.now().isoformat()\n             match  data_atual.split(\".\")\n             return f\"api/api-reponse-compra{match[0]}.parquet\"\n\nBootcamp - Python para dados/aula11-15/Aula04/Collector/tools/retry.py\n\nimport time\nfrom functools import wraps\n\ndef retry(exception_to_check, tries3, delay1, backoff2):\n    \"\"\"\n    Decorator que retenta a função várias vezes em caso de exceção.\n    \n    :param exception_to_check: A exceção (ou tuple de exceções) que deve ser capturada.\n    :param tries: O número máximo de tentativas.\n    :param delay: O tempo de espera inicial entre as tentativas.\n    :param backoff: O fator pelo qual o atraso deve aumentar após cada tentativa.\n    \"\"\"\n    def decorator_retry(func):\n        @wraps(func)\n        def wrapper_retry(*args, **kwargs):\n            _tries, _delay  tries, delay\n            while _tries > 1:\n                try:\n                    return func(*args, **kwargs)\n                except exception_to_check as e:\n                    print(f\"{func.__name__} falhou, tentando novamente em {_delay} segundos. Tentativas restantes: {_tries - 1}\")\n                    time.sleep(_delay)\n                    _tries - 1\n                    _delay * backoff\n            return func(*args, **kwargs)\n        return wrapper_retry\n    return decorator_retry\n\nBootcamp - Python para dados/aula11-15/Aula04/Collector/tools/aws/client.py\n\nimport boto3\nfrom botocore.exceptions import NoCredentialsError\nimport sys\nimport os\n\n# Suponha que você tenha uma variável de ambiente chamada \"MINHA_VARIAVEL\"\n# Você pode acessar seu valor usando a função os.environ.get()\n\n\nclass S3Client:\n\n    def __init__(self):\n        \n        self._envs  {\n            \"aws_access_key_id\": os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n            \"aws_secret_access_key\": os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n            \"region_name\": os.environ.get(\"AWS_REGION\", \"us-west-1\"),  # Usando um valor padrão se a variável não estiver definida\n            \"s3_bucket\": os.environ.get(\"S3_BUCKET_NAME\"),\n            \"datalake\" : os.environ.get(\"DELTA_LAKE_S3_PATH\")\n        }\n\n        for var in self._envs:\n            if self._envs[var] is None:\n                print(f\"A variável de ambiente {var} não está definida.\")\n                sys.exit(1)\n\n        self.s3  boto3.client('s3', aws_access_key_idself._envs[\"aws_access_key_id\"], aws_secret_access_keyself._envs[\"aws_secret_access_key\"], region_nameself._envs[\"region_name\"])\n\n    def upload_file(self, data, s3_key):\n        try:\n            self.s3.put_object(Bodydata.getvalue(), Bucketself._envs[\"s3_bucket\"], Keys3_key)\n        except NoCredentialsError:\n            print(\"Credenciais não encontradas. Certifique-se de configurar suas credenciais AWS corretamente.\")\n\n    def download_file(self, s3_key):\n        try:\n            file  self.s3.get_object(Bucketself._envs[\"s3_bucket\"], Keys3_key)\n            print(f\"Download bem-sucedido para {s3_key}\")\n            return file\n        except NoCredentialsError:\n            print(\"Credenciais não encontradas. Certifique-se de configurar suas credenciais AWS corretamente.\")\n        except FileNotFoundError:\n            print(f\"Arquivo {s3_key} não encontrado no bucket {self._envs['s3_bucket']}.\")\n        except Exception as e:\n            print(f\"Ocorreu um erro durante o download: {e}\")\n\n    def list_object(self, prefix):\n        return self.s3.list_objects(Bucketself._envs[\"s3_bucket\"], Prefixprefix)['Contents']\n\nBootcamp - Python para dados/aula11-15/Aula04/FakeApi/start.py\n\nfrom fastapi import FastAPI\nfrom faker import Faker\nimport pandas as pd\nimport random\n\n\napp  FastAPI(debugTrue)\nfake  Faker()\n\nfile_name  'data/products.csv'\ndf  pd.read_csv(file_name)\ndf['indice']  range(1, len(df) +1)\ndf.set_index('indice', inplaceTrue)\n\nlojapadraoonline  11\n\n@app.get(\"/\")\nasync def hello_world():\n    return 'Coca-Cola me patrocina!'\n\n@app.get(\"/gerar_compra\")\nasync def gerar_compra():\n    index  random.randint(1, len(df)-1)\n    tuple  df.iloc[index]\n    return [{\n            \"client\": fake.name(),\n            \"creditcard\": fake.credit_card_provider(),\n            \"product\": tuple[\"Product Name\"],\n            \"ean\": int(tuple[\"EAN\"]),\n            \"price\":  round(float(tuple[\"Price\"])*1.2,2),\n            \"clientPosition\": fake.location_on_land(),\n            \"store\": lojapadraoonline,\n            \"dateTime\": fake.iso8601()\n        }]\n\n@app.get(\"/gerar_compras/{numero_registro}\")\nasync def gerar_compra(numero_registro: int):\n    \n    if numero_registro < 1:\n        return {\"error\" : \"O número deve ser maior que 1\"}\n \n    respostas  []\n    for _ in range(numero_registro):\n        try:\n            index  random.randint(1, len(df)-1)\n            tuple  df.iloc[index]\n            compra  {\n                    \"client\": fake.name(),\n                    \"creditcard\": fake.credit_card_provider(),\n                    \"product\": tuple[\"Product Name\"],\n                    \"ean\": int(tuple[\"EAN\"]),\n                    \"price\":  round(float(tuple[\"Price\"])*1.2,2),\n                    \"clientPosition\": fake.location_on_land(),\n                    \"store\": lojapadraoonline,\n                    \"dateTime\": fake.iso8601()\n                    }\n            respostas.append(compra)\n        except IndexError as e:\n            print(f\"Erro de índice: {e}\")\n        except ValueError as e:\n            print(f\"Erro inesperado: {e}\")\n            compra  {\n                    \"client\": fake.name(),\n                    \"creditcard\": fake.credit_card_provider(),\n                    \"product\": \"error\",\n                    \"ean\": 0,\n                    \"price\":  0.0,\n                    \"clientPosition\": fake.location_on_land(),\n                    \"store\": lojapadraoonline,\n                    \"dateTime\": fake.iso8601()\n                    }\n            respostas.append(compra)\n        except Exception as e:\n            print(f\"Erro inesperado: {e}\")\n    return respostas\n\nBootcamp - Python para dados/aula11-15/basics/func_csv.py\n\nimport pandas as pd\n\n\ndef carregar_csv_e_filtrar(arquivo_csv, estado):\n    # Carregar o arquivo CSV em um DataFrame\n    df  pd.read_csv(arquivo_csv)\n    \n    # Verificar e remover células vazias\n    df  df.dropna()\n    \n    # Filtrar as linhas pela coluna estado\n    df_filtrado  df[df['estado']  estado]\n    \n    return df_filtrado\n\narquivo_csv  './exemplo.csv'  # substitua 'dados.csv' pelo caminho do seu arquivo CSV\nestado_filtrado  'SP'  # estado que você quer filtrar\ndf_filtrado  carregar_csv_e_filtrar(arquivo_csv, estado_filtrado)\n\nprint(df_filtrado)\n\n",
        "Bootcamp - Python para dados/aula11-15/basics/pessoa-class.py\n\nfrom datetime import datetime\n\nclass Pessoa:\n    def __init__(self, nome, idade, profissao):\n        self.nome  nome\n        self.idade  idade\n        self.profissao  profissao\n\n    def ola(self):\n        return f'Olá {self.nome}'\n\n    def ano_nascimento(self):\n        ano_atual  datetime.now().year\n        idade  int(self.idade)\n        ano_nascimento  ano_atual - idade\n        return f'Você nasceu em: {ano_nascimento}'\n\n    def tem_emprego(self):\n        if self.profissao  \"Data Eng\":\n            return f'Não há vagas para: {self.profissao}'\n        return f'Vou te arranjar um emprego de {self.profissao}'\n\n# Exemplo de uso:\npessoa  Pessoa(\"Fabio\", \"35\", \"Data Eng\")\npessoa2  Pessoa(\"Luciano\", \"33\", \"Data Product Manager\")\nprint(pessoa.ola())\nprint(pessoa.ano_nascimento())\nprint(pessoa.tem_emprego())\n\nprint(pessoa2.ola())\nprint(pessoa2.ano_nascimento())\nprint(pessoa2.tem_emprego())\n\n\nBootcamp - Python para dados/aula11-15/basics/pessoa.py\n\nfrom datetime import datetime\n\ndef ola_Pessoa(pessoa):\n    return f'Olá {pessoa[\"nome\"]}'\n\ndef ano_nascimento_Pessoa(pessoa):\n    ano_atual  datetime.now().year\n    idade  int(pessoa[\"idade\"])\n    ano_nascimento  ano_atual - idade\n    return f'você nasceu em: {ano_nascimento}'\n\ndef tem_emprego(pessoa):\n    if pessoa[\"profissao\"]  \"Data Eng\":\n        return f'Não há vagas para: {pessoa[\"profissao\"]}'\n    \n    return f'Vou te arranjar um emprego de {pessoa[\"profissao\"]}'\n        \n\npessoa  {\n    \"nome\" : \"Fabio\",\n    \"idade\" : \"35\",\n    \"profissao\" : \"Data Eng\"\n}\n\npessoa2  {\n    \"nome\" : \"Luciano\",\n    \"idade\" : \"33\",\n    \"profissao\" : \"Data Product Manager\"\n}\n\nprint(pessoa)    \nprint(ola_Pessoa(pessoa))\nprint(ano_nascimento_Pessoa(pessoa))\nprint(tem_emprego(pessoa))\n\nprint(pessoa2)    \nprint(ola_Pessoa(pessoa2))\nprint(ano_nascimento_Pessoa(pessoa2))\nprint(tem_emprego(pessoa2))\n\nBootcamp - Python para dados/aula16/README.md\n\n# aula_16_aovivo\n\n\nBootcamp - Python para dados/aula16/create.py\n\nfrom typing import Optional\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, tableTrue):\n    id: Optional[int]  Field(defaultNone, primary_keyTrue)\n    name: str\n    secret_name: str\n    age: Optional[int]  None\n\n\nhero_1  Hero(name\"Deadpond\", secret_name\"Dive Wilson\")\nhero_2  Hero(name\"Spider-Boy\", secret_name\"Pedro Parqueador\")\nhero_3  Hero(name\"Rusty-Man\", secret_name\"Tommy Sharp\", age48)\n\n\nengine  create_engine(\"sqlite:///database.db\", echoTrue)\n\n\nSQLModel.metadata.create_all(engine)\n\nwith Session(engine) as session:\n    session.add(hero_1)\n    session.add(hero_2)\n    session.add(hero_3)\n    session.commit()\n\n\n\nBootcamp - Python para dados/aula16/desafio.py\n\nfrom sqlmodel import SQLModel, Field, create_engine, Session, select\nfrom typing import Optional\n\nclass Livro(SQLModel, tableTrue):\n    id: Optional[int]  Field(defaultNone, primary_keyTrue)\n    titulo: str\n    autor: str\n    ano_publicacao: int\n    disponivel: bool  True\n\n# Conexão com o banco de dados (SQLite para simplicidade)\nengine  create_engine(\"sqlite:///biblioteca.db\")\n\n# Criação da tabela\nSQLModel.metadata.create_all(engine)\n\ndef adicionar_livro(livro: Livro):\n    with Session(engine) as session:\n        session.add(livro)\n        session.commit()\n\ndef buscar_livros_por_autor(autor: str):\n    with Session(engine) as session:\n        livros  session.exec(select(Livro).where(Livro.autor  autor)).all()\n        return livros\n\ndef atualizar_disponibilidade_livro(id_livro: int, disponivel: bool):\n    with Session(engine) as session:\n        livro  session.get(Livro, id_livro)\n        livro.disponivel  disponivel\n        session.add(livro)\n        session.commit()\n\ndef remover_livro(id_livro: int):\n    with Session(engine) as session:\n        livro  session.get(Livro, id_livro)\n        session.delete(livro)\n        session.commit()\n\n# Demonstração\nif __name__  \"__main__\":\n    adicionar_livro(Livro(titulo\"Dom Casmurro\", autor\"Machado de Assis\", ano_publicacao1899))\n    adicionar_livro(Livro(titulo\"O Pequeno Príncipe\", autor\"Antoine de Saint-Exupéry\", ano_publicacao1943))\n\n    print(\"Livros de Machado de Assis:\", buscar_livros_por_autor(\"Machado de Assis\"))\n    atualizar_disponibilidade_livro(1, False)  # Supondo que o ID do \"Dom Casmurro\" seja 1\n    remover_livro(2)  # Supondo que o ID de \"O Pequeno Príncipe\" seja 2\n\n\nBootcamp - Python para dados/aula16/main.py\n\nfrom typing import Optional\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\nclass Hero(SQLModel, tableTrue):\n    id: Optional[int]  Field(defaultNone, primary_keyTrue)\n    name: str\n    secret_name: str\n    age: Optional[int]  None\n\nengine  create_engine(\"sqlite:///database.db\", echoTrue)\n\nSQLModel.metadata.create_all(engine)\n\nhero_1  Hero(name\"Spider-Boy\", secret_name\"Pedro Parqueador\")\nhero_2  Hero(name\"Rusty-Man\", secret_name\"Tommy Sharp\", age48)\n\nwith Session(engine) as session:\n    session.add(hero_1)\n    session.add(hero_2)\n    session.commit()\n\nBootcamp - Python para dados/aula16/pyproject.toml\n\n[tool.poetry]\nname  \"aula-16-ao-vivo\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"lvgalvaofilho <lvgalvaofilho@gmail.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\nsqlmodel  \"^0.0.16\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\nBootcamp - Python para dados/aula16/read.py\n\nfrom typing import Optional\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, tableTrue):\n    id: Optional[int]  Field(defaultNone, primary_keyTrue)\n    name: str\n    secret_name: str\n    age: Optional[int]  None\n\n\nengine  create_engine(\"sqlite:///database.db\")\n\nwith Session(engine) as session:\n    statement  select(Hero).where(Hero.name  \"Spider-Boy\")\n    hero  session.exec(statement).first()\n    print(hero)\n\nBootcamp - Python para dados/aula16/read_ops.py\n\nfrom sqlmodel import SQLModel, create_engine, Session\nfrom sqlalchemy import text\n\n\n# Supondo que o banco de dados seja SQLite e esteja no arquivo `database.db`\nengine  create_engine(\"sqlite:///database.db\")\n\n# Criação da sessão\nwith Session(engine) as session:\n    # Sua consulta SQL\n    statement  text(\"SELECT * FROM hero;\")\n    \n    # Executando a consulta\n    results  session.exec(statement)\n    \n    # Fetch dos resultados\n    heroes  results.fetchall()\n    \n    # Imprimindo os resultados\n    for hero in heroes:\n        print(hero)\n\n\nBootcamp - Python para dados/aula16/read_ops_2.py\n\nfrom sqlmodel import SQLModel, create_engine, Session\nfrom sqlalchemy import text\n\n\n# Supondo que o banco de dados seja SQLite e esteja no arquivo `database.db`\nengine  create_engine(\"sqlite:///database.db\")\n\n# Criação da sessão\nwith Session(engine) as session:\n    # Sua consulta SQL\n    statement  text(\"DROP TABLE hero;\")\n    \n    # Executando a consulta\n    results  session.exec(statement)\n    \n    # Fetch dos resultados\n    heroes  results.fetchall()\n    \n    # Imprimindo os resultados\n    for hero in heroes:\n        print(hero)\n\n\nBootcamp - Python para dados/aula16/read_sql.sql\n\nSELECT id, name, secret_name, age\nFROM hero\nWHERE name  \"Spider-Boy\"\n\nBootcamp - Python para dados/aula16/planejado/exemplo_00_dict.py\n\nvenda  {\n    \"id\": 1,\n    \"produto\": \"Notebook Gamer\",\n    \"valor\": 5000.00,  # Float positivo\n    \"quantidade\": 2,   # Int positivo\n    \"data\": \"2024-03-18\",\n    \"email_comprador\": \"cliente@example.com\"\n}\n\nprint(f\"Venda de {venda['produto']} no valor de {venda['valor']} para {venda['email_comprador']}.\")\n\nBootcamp - Python para dados/aula16/planejado/exemplo_01_classe.py\n\nfrom datetime import date\n\nclass Venda:\n    def __init__(self, id: int, produto: str, valor: float, quantidade: int, data: date, email_comprador: str):\n        self.id: int  id\n        self.produto: str  produto\n        self.valor: float  valor\n        self.quantidade: int  quantidade\n        self.data: date  data\n        self.email_comprador: str  email_comprador\n\n    def __repr__(self) -> str:\n        return f\"Venda(id{self.id}, produto{self.produto}, valor{self.valor}, quantidade{self.quantidade}, data{self.data}, email_comprador{self.email_comprador})\"\n\n# Exemplo de uso\nvenda  Venda(1, \"Notebook Gamer\", 5000.00, 2, date(2024, 3, 18), \"cliente@example.com\")\nprint(venda)\n\n\nBootcamp - Python para dados/aula16/planejado/exemplo_02_dataclasse.py\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass VendaDataClass:\n    id: int\n    produto: str\n    valor: float\n    quantidade: int\n    data: str\n    email_comprador: str\n\n# Exemplo de uso\nvenda_dc  VendaDataClass(1, \"Notebook Gamer\", 5000.00, 2, \"2024-03-18\", \"cliente@example.com\")\nprint(venda_dc)\n\n\n\nBootcamp - Python para dados/aula16/planejado/exemplo_03_classe_com_validador.py\n\nimport re\nfrom datetime import date\n\nclass VendaValidada:\n    def __init__(self, id: int, produto: str, valor: float, quantidade: int, data: date, email_comprador: str):\n        if not isinstance(valor, float) or valor < 0:\n            raise ValueError(\"Valor deve ser um float positivo.\")\n        if not isinstance(quantidade, int) or quantidade < 0:\n            raise ValueError(\"Quantidade deve ser um int positivo.\")\n        if not re.match(r\"[^@]+@[^@]+\\.[^@]+\", email_comprador):\n            raise ValueError(\"E-mail do comprador inválido.\")\n\n        self.id: int  id\n        self.produto: str  produto\n        self.valor: float  valor\n        self.quantidade: int  quantidade\n        self.data: date  data\n        self.email_comprador: str  email_comprador\n\n    def __repr__(self) -> str:\n        return f\"VendaValidada(id{self.id}, produto{self.produto}, valor{self.valor}, quantidade{self.quantidade}, data{self.data}, email_comprador{self.email_comprador})\"\n\n# Exemplo de uso\nvenda_validada  VendaValidada(1, \"Notebook Gamer\", 5000.00, 2, date(2024, 3, 18), \"cliente@example.com\")\nprint(venda_validada)\n\ntry:\n    venda_invalidada  VendaValidada(1, \"Notebook Gamer\", -5000.00, 2, date(2024, 3, 18), \"cliente\")\n    print(venda_invalidada)\nexcept Exception as e:\n    print(e)\n\n\nBootcamp - Python para dados/aula16/planejado/exemplo_04_dataclasse_com_validador.py\n\nfrom dataclasses import dataclass\nimport re\nfrom datetime import date\n\ndef validar_valor(valor: float) -> float:\n    if valor < 0:\n        raise ValueError(\"Valor deve ser um float positivo.\")\n    return valor\n\ndef validar_quantidade(quantidade: int) -> int:\n    if quantidade < 0:\n        raise ValueError(\"Quantidade deve ser um int positivo.\")\n    return quantidade\n\ndef validar_email(email: str) -> str:\n    if not re.match(r\"[^@]+@[^@]+\\.[^@]+\", email):\n        raise ValueError(\"E-mail do comprador inválido.\")\n    return email\n\n@dataclass\nclass VendaValidada:\n    id: int\n    produto: str\n    valor: float\n    quantidade: int\n    data: date\n    email_comprador: str\n\n    def __post_init__(self):\n        self.valor  validar_valor(self.valor)\n        self.quantidade  validar_quantidade(self.quantidade)\n        self.email_comprador  validar_email(self.email_comprador)\n\n# Exemplo de uso com try-except\ntry:\n    venda_validada  VendaValidada(id1, produto\"Notebook Gamer\", valor5000.00, quantidade2, datadate(2024, 3, 18), email_comprador\"cliente@example.com\")\n    print(venda_validada)\nexcept ValueError as e:\n    print(e)\n\ntry:\n    venda_invalida  VendaValidada(id2, produto\"Mouse Sem Fio\", valor-30.00, quantidade3, datadate(2024, 3, 19), email_comprador\"clienteinvali.do\")\n    print(venda_invalida)\nexcept ValueError as e:\n    print(e)\n\n\nBootcamp - Python para dados/aula16/planejado/exemplo_05_pydantic.py\n\nfrom pydantic import BaseModel, EmailStr, ValidationError, PositiveInt, PositiveFloat\nfrom datetime import date\n\nclass VendaPydantic(BaseModel):\n    id: int\n    produto: str\n    valor: PositiveFloat\n    quantidade: PositiveInt\n    data: date\n    email_comprador: EmailStr\n\n# Exemplo de uso\ntry:\n    venda_pydantic  VendaPydantic(id1, produto\"Notebook Gamer\", valor5000.00, quantidade2, datadate(2024, 3, 18), email_comprador\"cliente@example.com\")\n    print(venda_pydantic)\nexcept ValidationError as e:\n    print(e.json())\n\n# Exemplo de uso\ntry:\n    venda_pydantic  VendaPydantic(id1, produto\"Notebook Gamer\", valor-5000.00, quantidade2, datadate(2024, 3, 18), email_comprador\"cliente@example.com\")\n    print(venda_pydantic)\nexcept ValidationError as e:\n    print(e)\n\n\n\nBootcamp - Python para dados/aula16/planejado/exemplo_06_orm.py\n\nfrom datetime import date\nfrom sqlalchemy import create_engine, Column, Integer, Float, String, Date\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom pydantic import BaseModel, EmailStr\nfrom typing import List\n\n# Definição do modelo SQLAlchemy\nBase  declarative_base()\n\nclass VendaModel(Base):\n    __tablename__  'vendas'\n    id  Column(Integer, primary_keyTrue)\n    produto  Column(String)\n    valor  Column(Float)\n    quantidade  Column(Integer)\n    data  Column(Date)\n    email_comprador  Column(String)\n\n# Configuração do banco de dados SQLAlchemy\nengine  create_engine('sqlite:///vendas.db', echoTrue)\nSessionLocal  sessionmaker(autocommitFalse, autoflushFalse, bindengine)\n\nBase.metadata.create_all(bindengine)\n\n# Modelos Pydantic\nclass VendaBase(BaseModel):\n    produto: str\n    valor: int  # Garante que o valor seja maior que 0\n    quantidade: int\n    data: date\n    email_comprador: EmailStr\n\nclass VendaCreate(VendaBase):\n    pass\n\nclass Venda(VendaBase):\n    id: int\n\n    class Config:\n        from_attributesTrue\n\n# Funções para interagir com o banco de dados\ndef create_venda(db: Session, venda: VendaCreate) -> VendaModel:\n    db_venda  VendaModel(**venda.model_dump())\n    db.add(db_venda)\n    db.commit()\n    db.refresh(db_venda)\n    return db_venda\n\ndef get_vendas(db: Session, skip: int  0, limit: int  100) -> List[VendaModel]:\n    return db.query(VendaModel).offset(skip).limit(limit).all()\n\n# Uso dos modelos e interação com o banco de dados\nif __name__  \"__main__\":\n    db  SessionLocal()\n\n    # Criando uma nova venda\n    # venda_data  {\n    #     \"produto\": \"Notebook Ultra\",\n    #     \"valor\": 4500.00,\n    #     \"quantidade\": 1,\n    #     \"data\": date.today(),\n    #     \"email_comprador\": \"comprador@example.com\"\n    # }\n    # nova_venda  VendaCreate(**venda_data)\n    # venda_criada  create_venda(dbdb, vendanova_venda)\n    # print(f\"Venda criada: {venda_criada}\")\n\n    # Recuperando vendas\n    vendas  get_vendas(dbdb)\n    print(\"Vendas recuperadas:\")\n    for venda_model in vendas:\n        # Deserializando para o modelo Pydantic\n        venda_pydantic  Venda.from_orm(venda_model)\n        print(venda_pydantic)\n\nBootcamp - Python para dados/aula16/planejado/exemplo_07_sql_model.py\n\nfrom datetime import date\nfrom sqlalchemy import create_engine, Column, Integer, Float, String, Date\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom pydantic import BaseModel, EmailStr\nfrom typing import List\n\n# Definição do modelo SQLAlchemy\nBase  declarative_base()\n\nclass VendaModel(Base):\n    __tablename__  'vendas'\n    id  Column(Integer, primary_keyTrue)\n    produto  Column(String)\n    valor  Column(Float)\n    quantidade  Column(Integer)\n    data  Column(Date)\n    email_comprador  Column(String)\n\n# Configuração do banco de dados SQLAlchemy\nengine  create_engine('sqlite:///vendas.db', echoTrue)\nSessionLocal  sessionmaker(autocommitFalse, autoflushFalse, bindengine)\n\nBase.metadata.create_all(bindengine)\n\n# Modelos Pydantic\nclass VendaBase(BaseModel):\n    produto: str\n    valor: int  # Garante que o valor seja maior que 0\n    quantidade: int\n    data: date\n    email_comprador: EmailStr\n\nclass VendaCreate(VendaBase):\n    pass\n\nclass Venda(VendaBase):\n    id: int\n\n    class Config:\n        from_attributesTrue\n\n# Funções para interagir com o banco de dados\ndef create_venda(db: Session, venda: VendaCreate) -> VendaModel:\n    db_venda  VendaModel(**venda.model_dump())\n    db.add(db_venda)\n    db.commit()\n    db.refresh(db_venda)\n    return db_venda\n\ndef get_vendas(db: Session, skip: int  0, limit: int  100) -> List[VendaModel]:\n    return db.query(VendaModel).offset(skip).limit(limit).all()\n\n# Uso dos modelos e interação com o banco de dados\nif __name__  \"__main__\":\n    db  SessionLocal()\n\n    # Criando uma nova venda\n    # venda_data  {\n    #     \"produto\": \"Notebook Ultra\",\n    #     \"valor\": 4500.00,\n    #     \"quantidade\": 1,\n    #     \"data\": date.today(),\n    #     \"email_comprador\": \"comprador@example.com\"\n    # }\n    # nova_venda  VendaCreate(**venda_data)\n    # venda_criada  create_venda(dbdb, vendanova_venda)\n    # print(f\"Venda criada: {venda_criada}\")\n\n    # Recuperando vendas\n    vendas  get_vendas(dbdb)\n    print(\"Vendas recuperadas:\")\n    for venda_model in vendas:\n        # Deserializando para o modelo Pydantic\n        venda_pydantic  Venda.from_orm(venda_model)\n        print(venda_pydantic)\n\nBootcamp - Python para dados/aula16/planejado/exemplo_classe_validador.py\n\nfrom datetime import datetime\nimport re\n\nclass Venda:\n    def __init__(self, id, produto, valor, quantidade, data, email_do_comprador):\n        self.id  id\n        self.produto  produto\n        self.valor  self.validar_valor(valor)\n        self.quantidade  self.validar_quantidade(quantidade)\n        self.data  data\n        self.email_do_comprador  self.validar_email(email_do_comprador)\n\n    def __repr__(self):\n        return f\"Venda(id{self.id}, produto{self.produto}, valor{self.valor}, quantidade{self.quantidade}, data{self.data}, email_do_comprador{self.email_do_comprador})\"\n\n    def validar_valor(self, valor):\n        if valor < 0:\n            raise ValueError(\"O valor deve ser positivo.\")\n        return valor\n\n    def validar_quantidade(self, quantidade):\n        if quantidade < 0:\n            raise ValueError(\"A quantidade deve ser positiva.\")\n        return quantidade\n\n    def validar_email(self, email):\n        if not re.match(r\"[^@]+@[^@]+\\.[^@]+\", email):\n            raise ValueError(\"Email inválido.\")\n        return email\n\n\n",
        "Bootcamp - Python para dados/aula17/README.md\n\n# Aula 17: SQLAlchemy - Conjunto de ferramentas para manipular SQL em \n\n![imagem_01](./pics/1.jpg)\n\nBem-vindo à décima sétima aula do bootcamp!\n\nMapeamento Objeto-Relacional (ORM) é uma técnica que permite consultar e manipular dados de um banco de dados usando um paradigma orientado a objetos. Ao falar sobre ORM, a maioria das pessoas está se referindo a uma biblioteca que implementa a técnica de Mapeamento Objeto-Relacional, daí a frase \"um ORM\".\n\n[Excalidraw:](https://link.excalidraw.com/l/8pvW6zbNUnD/3tmGeQYjxeG)\n\n## Introdução ao SQL Alchemy\n\nUma biblioteca ORM é uma biblioteca completamente comum escrita na linguagem de sua escolha que encapsula o código necessário para manipular os dados, então você não usa mais SQL; você interage diretamente com um objeto na mesma linguagem que está usando.\n\n### Por que devemos usar ORM?\n\n- DRY: Você escreve seu modelo de dados em apenas um lugar, e é mais fácil atualizar, manter e reutilizar o código.\n\n- Você não precisa escrever seu SQL zoado (a maioria dos programadores não são bons nisso, porque SQL é tratado como uma \"sub\" linguagem, quando na realidade é uma linguagem muito poderosa e complexa).\n\n- Sanitização; usar declarações preparadas ou transações é tão fácil quanto chamar um método.\n\n- Ela se encaixa na sua maneira natural no seu código Python.\n\n- Ela abstrai o sistema de BD, então você pode mudá-lo sempre que quiser.\n\n![imagem_02](./pics/2.jpg)\n\n### Instalação\n\nPrimeiro, certifique-se de que o SQLAlchemy esteja instalado. Se não estiver, você pode instalá-lo usando pip:\n\n```bash\npip install sqlalchemy\n```\n\n### Conectando ao SQLite (Hello world!)\n\nSQLite é um banco de dados leve que é ótimo para aprender os fundamentos do SQLAlchemy. Aqui está um exemplo básico de como criar uma engine de conexão com um banco de dados SQLite em memória:\n\n```python\nfrom sqlalchemy import create_engine\n\n# Conectar ao SQLite em memória\nengine  create_engine('sqlite:///meubanco.db', echoTrue)\n\nprint(\"Conexão com SQLite estabelecida.\")\n```\n\n![engine](./pics/engine.png)\n\n[Atende diferentes \"Dialect\"](https://docs.sqlalchemy.org/en/20/core/engines.html)  \n\n### Criando nosso MAPPING\n\n![engine](./pics/mapping.png)\n\n\nAntes de inserir ou consultar dados, precisamos definir os modelos e criar tabelas correspondentes no banco de dados:\n\n```python\nfrom sqlalchemy.orm import declarative_base\nfrom sqlalchemy import Column, Integer, String\n\nBase  declarative_base()\n\nclass Usuario(Base):\n    __tablename__  'usuarios'\n    \n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String)\n    idade  Column(Integer)\n\n# Criar as tabelas no banco de dados\nBase.metadata.create_all(engine)\n```\n\n### Criando Sessões e Inserindo Dados\n\nAs sessões no SQLAlchemy são usadas para manter um 'workspace' de todas as operações de objetos que você deseja sincronizar com o banco de dados:\n\n```python\nfrom sqlalchemy.orm import sessionmaker\n\nSession  sessionmaker(bindengine)\nsession  Session()\n\nnovo_usuario  Usuario(nome'João', idade28)\nsession.add(novo_usuario)\nsession.commit()\n\nprint(\"Usuário inserido com sucesso.\")\n```\n\n### Consultando Dados\n\nAgora, vamos consultar os dados para verificar a inserção:\n\n```python\nusuario  session.query(Usuario).filter_by(nome'João').first()\nprint(f\"Usuário encontrado: {usuario.nome}, Idade: {usuario.idade}\")\n```\n\n### Utilizando Session com o With\n\nO gerenciador de contexto `with` em Python, especialmente quando usado com SQLAlchemy, é uma maneira elegante e segura de garantir que os recursos, como conexões de banco de dados e sessões, sejam apropriadamente gerenciados. Ao usar o `with`, você se beneficia da entrada e saída automática de contextos, o que significa que ao final do bloco `with`, o SQLAlchemy automaticamente fecha a sessão ou faz o commit/rollback, dependendo do resultado da operação. Isso ajuda a prevenir vazamentos de conexão e garante que as transações sejam devidamente gerenciadas.\n\n### Vantagens do Uso do `with` com SQLAlchemy\n\n* **Gerenciamento automático de transações**: As transações são automaticamente commitadas ou revertidas dependendo se exceções foram lançadas dentro do bloco.\n* **Fechamento automático de sessões**: Isso garante que os recursos sejam liberados de maneira oportuna, evitando vazamentos de conexão.\n\n### Exemplo Sem Usar `with`\n\nSem utilizar o gerenciador de contexto, você precisa manualmente gerenciar a sessão, incluindo commits, rollbacks e o fechamento da sessão:\n\n```python\nfrom sqlalchemy.orm import sessionmaker\n# assumindo que engine já foi criado\n\nSession  sessionmaker(bindengine)\nsession  Session()\n\ntry:\n    novo_usuario  Usuario(nome'Ana', idade25)\n    session.add(novo_usuario)\n    session.commit()\nexcept:\n    session.rollback()\n    raise\nfinally:\n    session.close()\n```\n\nNeste exemplo, todos os passos para garantir que a sessão seja devidamente gerida são explícitos: comitar as alterações, lidar com exceções, fazer rollback se algo der errado, e, por fim, fechar a sessão.\n\n### Exemplo Usando `with`\n\nQuando você utiliza o gerenciador de contexto `with`, muitas dessas etapas são automatizadas:\n\n```python\nfrom sqlalchemy.orm import sessionmaker, Session\n# assumindo que engine já foi criado\n\nSession  sessionmaker(bindengine)\n\nwith Session() as session:\n    novo_usuario  Usuario(nome'Ana', idade25)\n    session.add(novo_usuario)\n    # O commit é feito automaticamente aqui, se não houver exceções\n    # O rollback é automaticamente chamado se uma exceção ocorrer\n    # A sessão é fechada automaticamente ao sair do bloco with\n```\n\nNo exemplo acima, o SQLAlchemy lida com o commit, rollback e fechamento da sessão automaticamente. Se uma exceção ocorrer dentro do bloco `with`, um rollback é chamado. Quando o bloco `with` é concluído sem erros, o commit é realizado, e em ambos os casos, a sessão é fechada automaticamente no final.\n\n### Conclusão\n\nA principal vantagem de usar o gerenciador de contexto `with` com SQLAlchemy (ou qualquer outro recurso que necessite de gerenciamento de estado e liberação de recursos) é reduzir a verbosidade do código e minimizar a chance de erros, como esquecer de fechar uma sessão ou fazer rollback de uma transação falha. Ele promove um código mais limpo, seguro e legível.\n\n### Desafio\n\n![imagem_03](./pics/3.jpg)\n\n### Desafio Intermediário de SQLAlchemy: Tabelas de Produto e Fornecedor\n\nEste desafio focará na criação de duas tabelas relacionadas, `Produto` e `Fornecedor`, utilizando SQLAlchemy. Cada produto terá um fornecedor associado, demonstrando o uso de chaves estrangeiras para estabelecer relações entre tabelas. Além disso, você realizará inserções nessas tabelas para praticar a manipulação de dados.\n\n#### Passo 1: Configuração Inicial\n\nPrimeiro, certifique-se de ter o SQLAlchemy instalado. Se não, instale-o usando pip:\n\n```bash\npip install sqlalchemy\n```\n\n#### Passo 2: Definição dos Modelos\n\n```python\nfrom sqlalchemy import create_engine, Column, Integer, String, ForeignKey\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship, sessionmaker\n\nBase  declarative_base()\n\nclass Fornecedor(Base):\n    __tablename__  'fornecedores'\n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String(50), nullableFalse)\n    telefone  Column(String(20))\n    email  Column(String(50))\n    endereco  Column(String(100))\n\nclass Produto(Base):\n    __tablename__  'produtos'\n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String(50), nullableFalse)\n    descricao  Column(String(200))\n    preco  Column(Integer)\n    fornecedor_id  Column(Integer, ForeignKey('fornecedores.id'))\n    \n    # Estabelece a relação entre Produto e Fornecedor\n    fornecedor  relationship(\"Fornecedor\")\n```\n\n#### Passo 3: Criando o Banco de Dados e as Tabelas\n\n```python\nengine  create_engine('sqlite:///:memory:', echoTrue)\nBase.metadata.create_all(engine)\n\nSession  sessionmaker(bindengine)\nsession  Session()\n```\n\n#### Passo 4: Inserções nas Tabelas\n\nPrimeiro, vamos inserir alguns fornecedores:\n\n```python\nfornecedores  [\n    Fornecedor(nome\"Fornecedor A\", telefone\"12345678\", email\"contato@a.com\", endereco\"Endereço A\"),\n    Fornecedor(nome\"Fornecedor B\", telefone\"87654321\", email\"contato@b.com\", endereco\"Endereço B\"),\n    Fornecedor(nome\"Fornecedor C\", telefone\"12348765\", email\"contato@c.com\", endereco\"Endereço C\"),\n    Fornecedor(nome\"Fornecedor D\", telefone\"56781234\", email\"contato@d.com\", endereco\"Endereço D\"),\n    Fornecedor(nome\"Fornecedor E\", telefone\"43217865\", email\"contato@e.com\", endereco\"Endereço E\")\n]\n\nsession.add_all(fornecedores)\nsession.commit()\n```\n\nEm seguida, inserimos alguns produtos, cada um vinculado a um fornecedor:\n\n```python\nprodutos  [\n    Produto(nome\"Produto 1\", descricao\"Descrição do Produto 1\", preco100, fornecedor_id1),\n    Produto(nome\"Produto 2\", descricao\"Descrição do Produto 2\", preco200, fornecedor_id2),\n    Produto(nome\"Produto 3\", descricao\"Descrição do Produto 3\", preco300, fornecedor_id3),\n    Produto(nome\"Produto 4\", descricao\"Descrição do Produto 4\", preco400, fornecedor_id4),\n    Produto(nome\"Produto 5\", descricao\"Descrição do Produto 5\", preco500, fornecedor_id5)\n]\n\nsession.add_all(produtos)\nsession.commit()\n```\n\n#### Passo 5: Consulta dos Dados\n\nPara verificar se tudo correu como esperado, você pode fazer uma consulta simples para listar todos os produtos e seus fornecedores:\n\n```python\nfor produto in session.query(Produto).all():\n    print(f\"Produto: {produto.nome}, Fornecedor: {produto.fornecedor.nome}\")\n```\n\nEste desafio cobre conceitos intermediários como a criação de tabelas relacionadas, inserção de dados com chaves estrangeiras e consultas básicas no SQLAlchemy. Ele oferece uma boa prática para quem está aprendendo a manipular relações entre tabelas em um contexto de banco de dados relacional usando ORM.\n\nVamos criar um exemplo prático que demonstra a utilização de `JOIN` e `GROUP BY` tanto em SQL puro quanto usando o SQLAlchemy (ORM). O objetivo é obter a soma dos preços dos produtos agrupados por fornecedor.\n\n### Cenário\n\nTemos duas tabelas: `fornecedores` e `produtos`. Cada produto tem um `fornecedor_id` que o vincula a um fornecedor específico na tabela `fornecedores`.\n\n### SQL Puro\n\nPara realizar essa operação em SQL puro, você pode usar a seguinte query:\n\n```sql\nSELECT fornecedores.nome, SUM(produtos.preco) AS total_preco\nFROM produtos\nJOIN fornecedores ON produtos.fornecedor_id  fornecedores.id\nGROUP BY fornecedores.nome;\n```\n\nEsta query junta as tabelas `produtos` e `fornecedores` através do `fornecedor_id`, agrupa os resultados pelo nome do fornecedor e, para cada grupo, calcula a soma dos preços dos produtos associados a esse fornecedor.\n\n### SQLAlchemy (ORM)\n\nPara realizar a mesma operação usando SQLAlchemy, você seguiria estes passos:\n\n```python\nfrom sqlalchemy import func\nfrom sqlalchemy.orm import sessionmaker\n# Supondo que engine já foi definido anteriormente e os modelos Produto e Fornecedor foram definidos conforme o exemplo anterior.\n\nSession  sessionmaker(bindengine)\nsession  Session()\n\nresultado  session.query(\n    Fornecedor.nome,\n    func.sum(Produto.preco).label('total_preco')\n).join(Produto, Fornecedor.id  Produto.fornecedor_id\n).group_by(Fornecedor.nome).all()\n\nfor nome, total_preco in resultado:\n    print(f\"Fornecedor: {nome}, Total Preço: {total_preco}\")\n```\n\nNo exemplo acima com SQLAlchemy, utilizamos o método `query()` para construir uma consulta que seleciona o nome do fornecedor e a soma dos preços dos produtos. Usamos `join()` para juntar as tabelas `Produto` e `Fornecedor` baseadas na chave estrangeira. `group_by()` é utilizado para agrupar os resultados pelo nome do fornecedor, e `func.sum()` calcula a soma dos preços dos produtos para cada grupo.\n\n### Conclusão\n\nAmbos os métodos, SQL puro e SQLAlchemy, alcançam o mesmo resultado: agrupam os produtos por fornecedor e calculam a soma dos preços dos produtos para cada fornecedor. A principal diferença está na abordagem: enquanto o SQL puro é mais direto e requer que você escreva a query explicitamente, o SQLAlchemy abstrai a construção da query, permitindo que você utilize métodos Python e relações entre modelos para definir a consulta. A escolha entre um ou outro dependerá das suas necessidades específicas, preferências de desenvolvimento e o contexto do seu projeto.\n\nBootcamp - Python para dados/aula17/desafio.py\n\nfrom sqlalchemy import create_engine, Column, Integer, String, ForeignKey\nfrom sqlalchemy.orm import declarative_base\nfrom sqlalchemy.orm import relationship, sessionmaker\nfrom sqlalchemy.exc import SQLAlchemyError  # Importa exceções do SQLAlchemy\n\nBase  declarative_base()\n\nclass Fornecedor(Base):\n    __tablename__  'fornecedores'\n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String(50), nullableFalse)\n    telefone  Column(String(20))\n    email  Column(String(50))\n    endereco  Column(String(100))\n\nclass Produto(Base):\n    __tablename__  'produtos'\n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String(50), nullableFalse)\n    descricao  Column(String(200))\n    preco  Column(Integer)\n    fornecedor_id  Column(Integer, ForeignKey('fornecedores.id'))\n    fornecedor  relationship(\"Fornecedor\")  # Relação entre Produto e Fornecedor\n\nengine  create_engine('sqlite:///desafio.db', echoTrue)\nBase.metadata.create_all(engine)\n\nSession  sessionmaker(bindengine)\n\n# Inserindo fornecedores\ntry:\n    with Session() as session:  # Usando a sessão corretamente com o gerenciador de contexto\n        fornecedores  [\n            Fornecedor(nome\"Fornecedor A\", telefone\"12345678\", email\"contato@a.com\", endereco\"Endereço A\"),\n            Fornecedor(nome\"Fornecedor B\", telefone\"87654321\", email\"contato@b.com\", endereco\"Endereço B\"),\n            Fornecedor(nome\"Fornecedor C\", telefone\"12348765\", email\"contato@c.com\", endereco\"Endereço C\"),\n            Fornecedor(nome\"Fornecedor D\", telefone\"56781234\", email\"contato@d.com\", endereco\"Endereço D\"),\n            Fornecedor(nome\"Fornecedor E\", telefone\"43217865\", email\"contato@e.com\", endereco\"Endereço E\")\n        ]\n        session.add_all(fornecedores)\n        session.commit()\nexcept SQLAlchemyError as e:  # Capturando exceções do SQLAlchemy\n    print(f\"Erro ao inserir fornecedores: {e}\")\n\n# Inserindo produtos\ntry:\n    with Session() as session:  # Corrigindo a utilização da sessão\n        produtos  [\n            Produto(nome\"Produto 1\", descricao\"Descrição do Produto 1\", preco100, fornecedor_id1),\n            Produto(nome\"Produto 2\", descricao\"Descrição do Produto 2\", preco200, fornecedor_id2),\n            Produto(nome\"Produto 3\", descricao\"Descrição do Produto 3\", preco300, fornecedor_id3),\n            Produto(nome\"Produto 4\", descricao\"Descrição do Produto 4\", preco400, fornecedor_id4),\n            Produto(nome\"Produto 5\", descricao\"Descrição do Produto 5\", preco500, fornecedor_id5)\n        ]\n        session.add_all(produtos)\n        session.commit()\nexcept SQLAlchemyError as e:\n    print(f\"Erro ao inserir produtos: {e}\")\n\nfrom sqlalchemy import func\nfrom sqlalchemy.orm import sessionmaker\n# Supondo que engine já foi definido anteriormente e os modelos Produto e Fornecedor foram definidos conforme o exemplo anterior.\n\nSession  sessionmaker(bindengine)\nsession  Session()\n\nresultado  session.query(\n    Fornecedor.nome,\n    func.sum(Produto.preco).label('total_preco')\n).join(Produto, Fornecedor.id  Produto.fornecedor_id\n).group_by(Fornecedor.nome).all()\n\nfor nome, total_preco in resultado:\n    print(f\"Fornecedor: {nome}, Total Preço: {total_preco}\")\n\n\nBootcamp - Python para dados/aula17/desafio_query.py\n\nfrom sqlalchemy import func\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy import create_engine, Column, Integer, String, ForeignKey\nfrom sqlalchemy.orm import declarative_base\nfrom sqlalchemy.orm import relationship, sessionmaker\nfrom sqlalchemy.exc import SQLAlchemyError  # Importa exceções do SQLAlchemy\n\n# Supondo que engine já foi definido anteriormente e os modelos Produto e Fornecedor foram definidos conforme o exemplo anterior.\n\nBase  declarative_base()\n\nclass Fornecedor(Base):\n    __tablename__  'fornecedores'\n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String(50), nullableFalse)\n    telefone  Column(String(20))\n    email  Column(String(50))\n    endereco  Column(String(100))\n\nclass Produto(Base):\n    __tablename__  'produtos'\n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String(50), nullableFalse)\n    descricao  Column(String(200))\n    preco  Column(Integer)\n    fornecedor_id  Column(Integer, ForeignKey('fornecedores.id'))\n    fornecedor  relationship(\"Fornecedor\")  # Relação entre Produto e Fornecedor\n\nengine  create_engine('sqlite:///desafio.db', echoTrue)\nBase.metadata.create_all(engine)\n\nSession  sessionmaker(bindengine)\nsession  Session()\n\nresultado  session.query(\n    Fornecedor.nome,\n    func.sum(Produto.preco).label('total_preco')\n).join(Produto, Fornecedor.id  Produto.fornecedor_id\n).group_by(Fornecedor.nome).all()\n\nfor nome, total_preco in resultado:\n    print(f\"Fornecedor: {nome}, Total Preço: {total_preco}\")\n\nBootcamp - Python para dados/aula17/exercicio_01.py\n\nfrom sqlalchemy import create_engine\n\n# Conectar ao SQLite em memória\nengine  create_engine('sqlite:///meubanco.db', echoTrue)\n\n## dialetos\n## engine  create_engine(\"postgresql+psycopg2://scott:tiger@localhost:5432/mydatabase\")\n\n\nprint(\"Conexão com SQLite estabelecida.\")\n\nBootcamp - Python para dados/aula17/exercicio_02.py\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import declarative_base\nfrom sqlalchemy import Column, Integer, String\n\n\n# Conectar ao SQLite em memória\nengine  create_engine('sqlite:///meubanco.db', echoTrue)\n\nprint(\"Conexão com SQLite estabelecida.\")\n\nBase  declarative_base()\n\nclass Usuario(Base):\n    __tablename__  'usuarios'\n    \n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String, nullableFalse)\n    idade  Column(Integer, nullableFalse)\n\nBase.metadata.create_all(engine)\n\nprint(\"Tabela Criada com SQLite estabelecida.\")\n\nfrom sqlalchemy.orm import sessionmaker\n\nSession  sessionmaker(bindengine)\nsession  Session()\n\nnovo_usuario  Usuario(nome'João', idade28)\nsession.add(novo_usuario)\nsession.commit()\n\nprint(\"Usuário inserido com sucesso.\")\n\nusuario  session.query(Usuario).filter_by(nome'João').first()\nprint(f\"Usuário encontrado: {usuario.nome}, Idade: {usuario.idade}\")\n\nBootcamp - Python para dados/aula17/exercicio_03.py\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import declarative_base\nfrom sqlalchemy import Column, Integer, String\n\n\n# Conectar ao SQLite em memória\nengine  create_engine('sqlite:///meubanco.db', echoTrue)\n\nprint(\"Conexão com SQLite estabelecida.\")\n\nBase  declarative_base()\n\nclass Usuario(Base):\n    __tablename__  'usuarios'\n    \n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String, nullableFalse)\n    idade  Column(Integer, nullableFalse)\n\nBase.metadata.create_all(engine)\n\nprint(\"Tabela Criada com SQLite estabelecida.\")\n\nfrom sqlalchemy.orm import sessionmaker\n\nSession  sessionmaker(bindengine)\nsession  Session()\n\nprint(\"Usuário inserido com sucesso.\")\n\nusuarios  session.query(Usuario).all()\n\nfor usuario in usuarios:\n    print(f\"ID: {usuario.id}, Nome: {usuario.nome}\")\n\nBootcamp - Python para dados/aula17/exercicio_04.py\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import declarative_base\nfrom sqlalchemy import Column, Integer, String\n\n\n# Conectar ao SQLite em memória\nengine  create_engine('sqlite:///meubanco.db', echoTrue)\n\nprint(\"Conexão com SQLite estabelecida.\")\n\nBase  declarative_base()\n\nclass Usuario(Base):\n    __tablename__  'usuarios'\n    \n    id  Column(Integer, primary_keyTrue)\n    nome  Column(String, nullableTrue)\n    idade  Column(Integer, nullableTrue)\n\nBase.metadata.create_all(engine)\n\nprint(\"Tabela Criada com SQLite estabelecida.\")\n\nfrom sqlalchemy.orm import sessionmaker\n\nfrom sqlalchemy.orm import sessionmaker\n# assumindo que engine já foi criado\n\nSession  sessionmaker(bindengine)\nsession  Session()\n\ntry:\n    with Session() as session:\n        novo_usuario  Usuario(cadeira40)\n        session.add(novo_usuario)\n        # O commit é feito automaticamente aqui, se não houver exceções\n        # O rollback é automaticamente chamado se uma exceção ocorrer\n        # A sessão é fechada automaticamente ao sair do bloco with\nexcept TypeError as e:\n    print(e)\n\nBootcamp - Python para dados/aula17/pyproject.toml\n\n[tool.poetry]\nname  \"sqlachemyexemplo\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"luciano\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\nsqlalchemy  \"^2.0.28\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\nBootcamp - Python para dados/aula18/README.md\n\n# Aula 18: API - Conjunto de ferramentas para manipular SQL em \n\n![imagem_01](./pics/1.jpg)\n\nBem-vindo à décima oitava aula do bootcamp!\n\n# O que é uma API?\n\nUma API (Application Programming  Interface) é um conjunto de rotinas e padrões (contratos) estabelecidos por uma aplicação, para que outras aplicações possam utilizar as funcionalidades dessa aplicação.\n\n![Imagem](assets/server-server.png)\n\n# Por que usar uma API?\n\nNos últimos anos, a Internet se transformou de uma rede de servidores web que serviam principalmente páginas estáticas para navegadores de internet...\n\n![Internet](https://thefloppydisk.files.wordpress.com/2013/05/web10.png?w1248)\n\n...em uma arquitetura cliente-servidor, onde aplicativos web e mobile se comunicam com diferentes aplicações, cada vez mais por meio de APIs RESTful simples, mas poderosas.\n\n![Imagem](https://thefloppydisk.files.wordpress.com/2013/05/web20.png?w1245)\n\n# As regras do jogo\n\nBasicamente uma API é um contrato que define como uma aplicação vai se comunicar com a outra. Como os dados serão enviados e recebidos.\n\n![Contrato](pics/contract.png)\n\n# O que é uma API REST?\n\nREST é um acrônimo para REpresentational STATE Transfer, que é um estilo de arquitetura para sistemas distribuídos.\n\n![Rest](pics/apirest.png)\n\n# Como se comunicar com ela?\n\n- Nosso protocolo (ex: https)\n\n- Nosso servidor tem um endereço (ex: pokeapi.co)\n\n- Nosso servidor tem uma porta (ex: 8080 para http e 443 para https)\n\n- E precisamos acessar um recurso ou como constumamos chamar, endpoint ou rota (ex: /api/character)\n\n``` \nhttps://pokeapi.co/api/v2/pokemon/15\n```\n\n# Nossos verbos\n\nO protocolo HTTP é a base usada por trás das APIs REST e as \"requisita\" utilizando diversos \"tipos\". Os mais comuns são:\n\n## O que é o CRUD? \n\nCreate, Read, Update e Delete\n\n- POST: (Create) Criar um recurso\n- GET: (Read) Obter um recurso\n- PUT: (Update) Atualizar um recurso\n- DELETE: Remover um recurso\n\n# Qual a diferença entre REST e RESTful?\n\nREST é um estilo de arquitetura para sistemas distribuídos, enquanto RESTful é a implementação desse estilo.\n\n# Vamos para a prática?\n\nVamos usar o VScode e o terminal para conectar e salvar os dados de uma API em um banco de dados.\n\nBootcamp - Python para dados/aula18/exemplo_00.py\n\nimport requests\n\nresponse  requests.get(f\"https://pokeapi.co/api/v2/pokemon/15\")\ndata  response.json()\ndata_types  data['types']  # Supondo que 'data' é o dicionário com os dados do Pokémon\ntypes_list  []\nfor type_info in data_types:\n    types_list.append(type_info['type']['name'])\ntypes  ', '.join(types_list)\nprint(data['name'], types)\n\nlista_exemplo  [\"luciano\", \"fabio\", \"bruno\"]\nlista_unica  ', '.join(lista_exemplo)\nprint(lista_unica)\n\n",
        "Bootcamp - Python para dados/aula18/exemplo_01.py\n\nimport requests\n\nfrom pydantic import BaseModel\n\nclass PokemonSchema(BaseModel):\n    name: int\n    type: str\n\n    class Config:\n        orm_mode  True\n\ndef pegar_pokemon(id: int) -> PokemonSchema:\n    response  requests.get(f\"https://pokeapi.co/api/v2/pokemon/{id}\")\n    data  response.json()\n    print(data)\n    data_types  data['types']  # Supondo que 'data' é o dicionário com os dados do Pokémon\n    types_list  []\n    for type_info in data_types:\n        types_list.append(type_info['type']['name'])\n    types  ', '.join(types_list)\n    return PokemonSchema(namedata['name'], typetypes)\n\nfrom pydantic import BaseModel\n\npokemon  pegar_pokemon(24)\nprint(pokemon)\n\nBootcamp - Python para dados/aula18/exemplo_02_json.py\n\nimport requests\nimport json\n\nfrom pydantic import BaseModel\n\nclass PokemonSchema(BaseModel):\n    name: int\n    type: str\n\n    class Config:\n        orm_mode  True\n\ndef pegar_pokemon(id: int) -> PokemonSchema:\n    response  requests.get(f\"https://pokeapi.co/api/v2/pokemon/{id}\")\n    data  response.json()\n    with open(f\"{data['name']}.json\", 'w') as f:\n            json.dump(data, f)\n    data_types  data['types']  # Supondo que 'data' é o dicionário com os dados do Pokémon\n    types_list  []\n    for type_info in data_types:\n        types_list.append(type_info['type']['name'])\n    types  ', '.join(types_list)\n    return PokemonSchema(namedata['name'], typetypes)\n\nfrom pydantic import BaseModel\n\npokemon  pegar_pokemon(24)\nprint(pokemon)\n\nBootcamp - Python para dados/aula18/json_exemplo.py\n\njson_pokemon  {\n    \n}\n\nBootcamp - Python para dados/aula18/pyproject.toml\n\n[tool.poetry]\nname  \"aula-18\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"luciano\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\nrequests  \"^2.31.0\"\nsqlalchemy  \"^2.0.28\"\npydantic  \"^2.6.4\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\nBootcamp - Python para dados/aula18/src/controller.py\n\nimport requests\nfrom db import SessionLocal, engine, Base\nfrom models import Pokemon\nfrom schema import PokemonSchema\n\nBase.metadata.create_all(bindengine)\n\ndef fetch_pokemon_data(pokemon_id: int):\n    response  requests.get(f\"https://pokeapi.co/api/v2/pokemon/{pokemon_id}\")\n    print(response)\n    if response.status_code  200:\n        data  response.json()\n        types  ', '.join(type['type']['name'] for type in data['types'])\n        return PokemonSchema(namedata['name'], typetypes)\n    else:\n        return None\n\ndef add_pokemon_to_db(pokemon_schema: PokemonSchema) -> Pokemon:\n    with SessionLocal() as db:\n        db_pokemon  Pokemon(namepokemon_schema.name, typepokemon_schema.type)\n        db.add(db_pokemon)\n        db.commit()\n        db.refresh(db_pokemon)\n    return db_pokemon\n\n\n\nBootcamp - Python para dados/aula18/src/db.py\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nSQLALCHEMY_DATABASE_URL  \"sqlite:///./pokemon.db\"\nengine  create_engine(SQLALCHEMY_DATABASE_URL)\nSessionLocal  sessionmaker(autocommitFalse, autoflushFalse, bindengine)\nBase  declarative_base()\n\n\nBootcamp - Python para dados/aula18/src/main.py\n\nimport time\nimport random\nfrom controller import fetch_pokemon_data, add_pokemon_to_db\n\ndef main():\n    while True:\n        pokemon_id  random.randint(1, 350)  # Gera um ID aleatório entre 1 e 350\n        pokemon_schema  fetch_pokemon_data(pokemon_id)\n        if pokemon_schema:\n            print(f\"Adicionando {pokemon_schema.name} ao banco de dados.\")\n            add_pokemon_to_db(pokemon_schema)\n        else:\n            print(f\"Não foi possível obter dados para o Pokémon com ID {pokemon_id}.\")\n        time.sleep(10)\n\nif __name__  \"__main__\":\n    main()\n\nBootcamp - Python para dados/aula18/src/models.py\n\nfrom sqlalchemy import Column, Integer, String, DateTime\nfrom sqlalchemy.sql import func\nfrom db import Base\n\nclass Pokemon(Base):\n    __tablename__  'pokemons'\n    id  Column(Integer, primary_keyTrue, indexTrue)\n    name  Column(String)\n    type  Column(String)\n    created_at  Column(DateTime, defaultfunc.now())  # Campo adicionado\n\n\nBootcamp - Python para dados/aula18/src/schema.py\n\nfrom pydantic import BaseModel\n\nclass PokemonSchema(BaseModel):\n    name: str\n    type: str\n\n    class Config:\n        orm_mode  True\n\n\nBootcamp - Python para dados/aula19/README.md\n\n# Aula 19: Fazendo nossa API\n\n## O que é FastAPI?\n\nFastAPI é uma estrutura (framework) web de alto desempenho para construir APIs com Python 3.6+ baseada em tipos de dados declarativos (graças ao Pydantic) e no padrão ASGI (Asynchronous Server Gateway Interface). Ele é projetado para ser fácil de usar, rápido para aprender e altamente eficiente em termos de desempenho, oferecendo suporte nativo a tipos de dados Python, tipagem de dados, validação automática de entrada e documentação interativa automática (gerada automaticamente pelo Swagger UI e ReDoc).\n\nPrincipais características:\n\n1. **Rápido**: Utiliza Python assíncrono e técnicas de otimização para alto desempenho.\n2. **Fácil de usar**: Possui uma sintaxe declarativa e intuitiva, permitindo uma rápida prototipação.\n3. **Tipagem de dados**: Utiliza a tipagem de dados Python para garantir a segurança e a consistência dos dados.\n4. **Documentação automática**: Gera automaticamente documentação interativa para sua API.\n5. **Suporte a OpenAPI e Swagger**: Total compatibilidade com esses padrões, permitindo integração com outras ferramentas.\n\n## Exemplos de uso do FastAPI:\n\n### 1. Criando uma API básica:\n\n```python\nfrom typing import Union\n\nfrom fastapi import FastAPI\n\napp  FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None]  None):\n    return {\"item_id\": item_id, \"q\": q}\n```\n\n### 2. Definindo modelos de dados com Pydantic:\n\n```python\nfrom typing import Union\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp  FastAPI()\n\n\nclass Item(BaseModel):\n    name: str\n    price: float\n    is_offer: Union[bool, None]  None\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None]  None):\n    return {\"item_id\": item_id, \"q\": q}\n\n\n@app.put(\"/items/{item_id}\")\ndef update_item(item_id: int, item: Item):\n    return {\"item_name\": item.name, \"item_id\": item_id}\n```\n\n### Desafio\n\nCriar nosso primeiro CRUD\n\n1. **`POST /items/`: Cria um novo item**\n    \n    Esta rota permite criar um novo item no banco de dados. O cliente envia os dados do novo item no corpo da solicitação HTTP e o servidor adiciona esse item ao banco de dados. Aqui está como funciona:\n    \n    * **Verbo HTTP**: POST\n    * **Endpoint**: `/items/`\n    * **Ação**: Cria um novo item no banco de dados.\n    * **Requisitos**: O corpo da solicitação deve conter os dados do novo item.\n    * **Resposta**: Retorna o novo item criado.\n2. **`GET /items/`: Retorna uma lista paginada de itens**\n    \n    Esta rota permite recuperar uma lista paginada de itens do banco de dados. O cliente pode especificar opcionalmente os parâmetros `skip` (quantos itens pular) e `limit` (quantos itens retornar) para paginação. Aqui está como funciona:\n    \n    * **Verbo HTTP**: GET\n    * **Endpoint**: `/items/`\n    * **Ação**: Retorna uma lista paginada de itens do banco de dados.\n    * **Parâmetros de consulta**: `skip` (opcional, padrão  0) e `limit` (opcional, padrão  10).\n    * **Resposta**: Retorna uma lista de itens conforme especificado pelos parâmetros de consulta.\n3. **`GET /items/{item_id}`: Retorna um item específico com base no ID**\n    \n    Esta rota permite recuperar um item específico do banco de dados com base no ID fornecido. Aqui está como funciona:\n    \n    * **Verbo HTTP**: GET\n    * **Endpoint**: `/items/{item_id}`\n    * **Ação**: Retorna um item específico com base no ID fornecido.\n    * **Parâmetros de caminho**: `item_id` (ID do item a ser recuperado).\n    * **Resposta**: Retorna o item correspondente ao ID fornecido.\n4. **`PUT /items/{item_id}`: Atualiza um item existente com base no ID**\n    \n    Esta rota permite atualizar os dados de um item existente no banco de dados com base no ID fornecido. O cliente envia os novos dados do item no corpo da solicitação HTTP. Aqui está como funciona:\n    \n    * **Verbo HTTP**: PUT\n    * **Endpoint**: `/items/{item_id}`\n    * **Ação**: Atualiza um item existente com base no ID fornecido.\n    * **Parâmetros de caminho**: `item_id` (ID do item a ser atualizado).\n    * **Requisitos**: O corpo da solicitação deve conter os novos dados do item.\n    * **Resposta**: Retorna o item atualizado.\n5. **`DELETE /items/{item_id}`: Exclui um item existente com base no ID**\n    \n    Esta rota permite excluir um item existente no banco de dados com base no ID fornecido. Aqui está como funciona:\n    \n    * **Verbo HTTP**: DELETE\n    * **Endpoint**: `/items/{item_id}`\n    * **Ação**: Exclui um item existente com base no ID fornecido.\n    * **Parâmetros de caminho**: `item_id` (ID do item a ser excluído).\n    * **Resposta**: Retorna o item excluído.\n\nEssas operações fornecem uma API completa para gerenciar itens no banco de dados, permitindo criar, recuperar, atualizar e excluir itens de forma eficiente e segura. Certifique-se de que as operações estejam de acordo com os requisitos do seu projeto e que você implemente a lógica necessária para garantir a consistência e a segurança dos dados.\n\nBootcamp - Python para dados/aula19/Dockerfile\n\nFROM python:3.12\n\n# Instalando o Poetry\nRUN pip install poetry\n\n# Copiar o conteúdo do diretório atual para o contêiner\nCOPY . /src\n\n# Definir o diretório de trabalho\nWORKDIR /src\n\n# Instalar as dependências do projeto com Poetry\nRUN poetry install\n\n# Expor a porta em que a aplicação estará escutando\nEXPOSE 8501\n\n# Definir o entrypoint para executar o servidor Uvicorn\nENTRYPOINT [\"poetry\", \"run\", \"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8501\"]\n\n\nBootcamp - Python para dados/aula19/src/database.py\n\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\n\nSQLALCHEMY_DATABASE_URL  \"sqlite:///./test.db\"\nengine  create_engine(SQLALCHEMY_DATABASE_URL)\nSessionLocal  sessionmaker(autocommitFalse, autoflushFalse, bindengine)\n\nBase  declarative_base()\n\ndef get_db():\n    db  SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n\n\nBootcamp - Python para dados/aula19/src/main.py\n\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom sqlalchemy.orm import Session\nimport models\nimport database\nfrom typing import List, Union\nfrom schema import Item, ItemBase, ItemCreate\n\napp  FastAPI()\n\nmodels.Base.metadata.create_all(binddatabase.engine)\n\n@app.post(\"/items/\", response_modelItem)\ndef create_item(item: ItemCreate, db: Session  Depends(database.get_db)):\n    db_item  models.Item(**item.dict())\n    db.add(db_item)\n    db.commit()\n    db.refresh(db_item)\n    return db_item\n\n@app.get(\"/items/\", response_modelList[Item])\ndef read_items(skip: int  0, limit: int  10, db: Session  Depends(database.get_db)):\n    items  db.query(models.Item).offset(skip).limit(limit).all()\n    return items\n\n@app.get(\"/items/{item_id}\", response_modelItem)\ndef read_item(item_id: int, db: Session  Depends(database.get_db)):\n    db_item  db.query(models.Item).filter(models.Item.id  item_id).first()\n    if db_item is None:\n        raise HTTPException(status_code404, detail\"Item not found\")\n    return db_item\n\n@app.put(\"/items/{item_id}\", response_modelItem)\ndef update_item(item_id: int, item: ItemCreate, db: Session  Depends(database.get_db)):\n    db_item  db.query(models.Item).filter(models.Item.id  item_id).first()\n    if db_item is None:\n        raise HTTPException(status_code404, detail\"Item not found\")\n    for key, value in item.dict().items():\n        setattr(db_item, key, value)\n    db.commit()\n    db.refresh(db_item)\n    return db_item\n\n@app.delete(\"/items/{item_id}\", response_modelItem)\ndef delete_item(item_id: int, db: Session  Depends(database.get_db)):\n    db_item  db.query(models.Item).filter(models.Item.id  item_id).first()\n    if db_item is None:\n        raise HTTPException(status_code404, detail\"Item not found\")\n    db.delete(db_item)\n    db.commit()\n    return db_item\n\n\nBootcamp - Python para dados/aula19/src/models.py\n\nfrom sqlalchemy import Column, Integer, String, Float\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase  declarative_base()\n\nclass Item(Base):\n    __tablename__  'items'\n\n    id  Column(Integer, primary_keyTrue, indexTrue)\n    name  Column(String, indexTrue)\n    price  Column(Float)\n    is_offer  Column(String, nullableTrue)\n\n\nBootcamp - Python para dados/aula19/src/schema.py\n\nfrom pydantic import BaseModel\nfrom typing import Union\n\nclass ItemBase(BaseModel):\n    name: str\n    price: float\n    is_offer: Union[bool, None]  None\n\nclass ItemCreate(ItemBase):\n    pass\n\nclass Item(ItemBase):\n    id: int\n\n",
        "Bootcamp - Python para dados/aula20/README.md\n\n# CRUD FASTAPI POSTGRES STREAMLIT\n\nVocê sabe o que é CRUD?\n\n![Imagem CRUD](assets/crud.jpeg)\n\nA BlackFriday ta chegando. Você sabe como que o Iphone fica mais barato? Você sabe como que o vídeo game é cadastrado? Você sabia que quando abre o seu navegador, nada mais é do que o seu browser fazendo um SELECT no banco do Mercado Livre 🤯\n\nVocê precisa conhecer o CRUD.\n\nO principal responsável por tornar isso possível é o ORM\n\n![Imagem ORM](assets/orm.png)\n\n## Instalação via docker\n\n```bash\ndocker-compose up -d --build\n```\n\n### Uso\n\nFrontend:\nAcesse o endereço http://localhost:8501\n\n### Documentação\n\nBackend:\nAcesse o endereço http://localhost:8000/docs\n\n## Nossa estrutura de pastas e arquivos\n\n```bash\n├── README.md # arquivo com a documentação do projeto\n├── backend # pasta do backend (FastAPI, SQLAlchemy, Uvicorn, Pydantic)\n├── frontend # pasta do frontend (Streamlit, Requests, Pandas)\n├── docker-compose.yml # arquivo de configuração do docker-compose (backend, frontend, postgres)\n├── poetry.lock # arquivo de lock do poetry\n└── pyproject.toml # arquivo de configuração do poetry\n```\n\n## Nosso Backend\n\nNosso backend vai ser uma API, que será responsável por fazer a comunicação entre o nosso frontend com o banco de dados. Vamos detalhar cada uma das pastas e arquivos do nosso backend.\n\n### FastAPI\n\nO FastAPI é um framework web para construir APIs com Python. Ele é baseado no Starlette, que é um framework assíncrono para construir APIs. O FastAPI é um framework que está crescendo muito, e que tem uma curva de aprendizado muito baixa, pois ele é muito parecido com o Flask.\n\n### Uvicorn\n\nO Uvicorn é um servidor web assíncrono, que é baseado no ASGI, que é uma especificação para servidores web assíncronos. O Uvicorn é o servidor web recomendado pelo FastAPI, e é o servidor que vamos utilizar nesse projeto.\n\n### SQLAlchemy\n\nO SQLAlchemy é uma biblioteca para fazer a comunicação com o banco de dados. Ele é um ORM (Object Relational Mapper), que é uma técnica de mapeamento objeto-relacional que permite fazer a comunicação com o banco de dados utilizando objetos.\n\nUma das principais vantagens de trabalhar com o SQLAlchemy é que ele é compatível com vários bancos de dados, como MySQL, PostgreSQL, SQLite, Oracle, Microsoft SQL Server, Firebird, Sybase e até mesmo o Microsoft Access.\n\nAlém disso, ele realiza a sanitização dos dados, evitando ataques de SQL Injection.\n\n![imagem](assets/sqlinjection.jpeg)\n\nOutro ponto, é que você pode trabalhar com métodos nativos do Python, como por exemplo o filter, que é muito utilizado para fazer filtros em listas. Isso facilita muito a nossa vida, pois não precisamos aprender uma nova linguagem para fazer a comunicação com o banco de dados. Quem tiver familidade com Pandas, vai se sentir em casa.\n\n### Pydantic\n\nO Pydantic é uma biblioteca para fazer a validação de dados. Ele é utilizado pelo FastAPI para fazer a validação dos dados que são recebidos na API, e também para definir os tipos de dados que são retornados pela API.\n\n## docker-compose.yml\n\nEsse arquivo `docker-compose.yml` define uma aplicação composta por três serviços: `postgres`, `backend` e `frontend`, e cria uma rede chamada `mynetwork`. Vou explicar cada parte em detalhes:\n\n### Services:\n\n#### Postgres:\n\n* `image: postgres:latest`: Esse serviço utiliza a imagem mais recente do PostgreSQL disponível no Docker Hub.\n* `volumes`: Mapeia o diretório `/var/lib/postgresql/data` dentro do contêiner do PostgreSQL para um volume chamado `postgres_data` no sistema hospedeiro. Isso permite que os dados do banco de dados persistam mesmo quando o contêiner é desligado.\n* `environment`: Define variáveis de ambiente para configurar o banco de dados PostgreSQL, como nome do banco de dados (`POSTGRES_DB`), nome de usuário (`POSTGRES_USER`) e senha (`POSTGRES_PASSWORD`).\n* `networks`: Define que este serviço está na rede chamada `mynetwork`.\n\n#### Backend:\n\n* `build`: Especifica que o Docker deve construir uma imagem para esse serviço, usando um Dockerfile localizado no diretório `./backend`.\n* `volumes`: Mapeia o diretório `./backend` (no sistema hospedeiro) para o diretório `/app` dentro do contêiner. Isso permite que as alterações no código fonte do backend sejam refletidas no contêiner em tempo real.\n* `environment`: Define a variável de ambiente `DATABASE_URL`, que especifica a URL de conexão com o banco de dados PostgreSQL.\n* `ports`: Mapeia a porta `8000` do sistema hospedeiro para a porta `8000` do contêiner, permitindo que o serviço seja acessado através da porta `8000`.\n* `depends_on`: Indica que este serviço depende do serviço `postgres`, garantindo que o banco de dados esteja pronto antes que o backend seja iniciado.\n* `networks`: Também define que este serviço está na rede `mynetwork`.\n\n#### Frontend:\n\n* `build`: Similar ao backend, especifica que o Docker deve construir uma imagem para este serviço, usando um Dockerfile localizado no diretório `./frontend`.\n* `volumes`: Mapeia o diretório `./frontend` (no sistema hospedeiro) para o diretório `/app` dentro do contêiner, permitindo alterações em tempo real.\n* `ports`: Mapeia a porta `8501` do sistema hospedeiro para a porta `8501` do contêiner, permitindo acesso ao frontend através da porta `8501`.\n* `networks`: Define que este serviço também está na rede `mynetwork`.\n\n### Networks:\n\n* `mynetwork`: Define uma rede personalizada para os serviços se comunicarem entre si.\n\n### Volumes:\n\n* `postgres_data`: Define um volume para armazenar os dados do banco de dados PostgreSQL.\n\n### Comando `docker-compose up`:\n\nQuando você executa `docker-compose up`, o Docker Compose lerá o arquivo `docker-compose.yml`, criará os serviços conforme as definições especificadas e os iniciará. Isso significa que os contêineres para o banco de dados PostgreSQL, o backend e o frontend serão criados e conectados à rede `mynetwork`. O banco de dados será configurado com os detalhes fornecidos (nome do banco de dados, usuário e senha), e as imagens para os serviços de backend e frontend serão construídas a partir dos Dockerfiles fornecidos. Uma vez iniciados, você poderá acessar o backend através de `http://localhost:8000` e o frontend através de `http://localhost:8501`. Os dados do banco de dados serão persistidos no volume `postgres_data`.\n\n## Nossa estrutura de pastas e arquivos\n\n```bash\n├── backend\n│   ├── Dockerfile # arquivo de configuração do Docker\n│   ├── crud.py # arquivo com as funções de CRUD utilizando o SQL Alchemy ORM\n│   ├── database.py # arquivo com a configuração do banco de dados utilizando o SQL Alchemy \n│   ├── main.py\n│   ├── models.py\n│   ├── requirements.txt\n│   ├── router.py\n│   └── schemas.py\n```\n\n## Arquivo `database.py`\n\nO arquivo `database.py` é responsável por fazer a configuração do banco de dados utilizando o SQLAlchemy. Ele é responsável por criar a conexão com o banco de dados, e também por criar a sessão do banco de dados.\n\nCaso queira mudar de banco de dados, você só precisa mudar a URL de conexão, que está na variável SQLALCHEMY_DATABASE_URL. o SQLAlchemy é compatível com vários bancos de dados, como MySQL, PostgreSQL, SQLite, Oracle, Microsoft SQL Server, Firebird, Sybase e até mesmo o Microsoft Access.\n\nOs principais pontos desse arquivo é a engine, que é a conexão com o banco de dados, e o SessionLocal, que é a sessão do banco de dados. O SessionLocal é quem executada as queries no banco de dados.\n\nLembrar sempre de:\n\n1) Declarar a URL do banco\n2) Criar a engine usando o 'create_engine'\n3) Criar a sessão do banco\n4) Criar a Base do ORM (nosso Model vai herdar ele)\n5) Criar um gerador de sessão para ser reutilizado\n\n## Arquivo `models.py`\n\nO arquivo `models.py` é responsável por definir os modelos do SQLAlchemy, que são as classes que definem as tabelas do banco de dados. Esses modelos são utilizados para fazer a comunicação com o banco de dados.\n\nÉ aqui que definimos o nome da tabela, os campos e os tipos de dados. Conseguimos incluir campos gerados aleatoriamente, como o id e o created_at. Para o id, ao incluir o campo Integer, com o parâmetro primary_keyTrue, o SQLAlchemy já entende que esse campo é o id da tabela. Para o created_at, ao incluir o campo DateTime, com o parâmetro defaultdatetime, o SQLAlchemy já entende que esse campo é a data de criação da tabela.\n\nLembrar:\n\n1) O models é agnóstico ao banco, ele não sabe qual é o banco que é criado! Ele vai importar o base do database!\n\n2) Declarar sua Tabela\n\n## Arquivo `schemas.py`\n\nO arquivo `schemas.py` é responsável por definir os schemas do Pydantic, que são as classes que definem os tipos de dados que serão utilizados na API. Esses schemas são utilizados para fazer a validação dos dados que são recebidos na API, e também para definir os tipos de dados que são retornados pela API.\n\nO pydantic é a principal biblioteca para validação de dados em Python. Ela é utilizada pelo FastAPI para fazer a validação dos dados recebidos na API, e também para definir os tipos de dados que são retornados pela API.\n\nAlém disso, ela possui uma integração muito boa com o SQLAlchemy, que é a biblioteca que utilizamos para fazer a comunicação com o banco de dados.\n\nOutra vantagem são os seus tipos pré-definidos, que facilitam muito a nossa vida. Por exemplo, se você quer definir um campo que aceita apenas números positivos, você pode utilizar o PositiveInt. Se você quer definir um campo que aceita apenas determinadas categorias, você pode utilizar o construtor constrains.\n\nDetalhe que criamos schemas diferentes para os retornos da nossa API. Isso é uma boa prática, pois permite que você tenha mais flexibilidade para alterar os schemas no futuro.\n\nTemos o schema `ProductBase`, que é o schema base para o cadastro de produtos. Esse schema é utilizado para fazer a validação dos dados que são recebidos na API, e também para definir os tipos de dados que são retornados pela API.\n\nTemos o schema `ProductCreate`, que é o schema que é retornado pela API. Ele é uma classe que herda do schema `ProductBase`, e possui um campo a mais, que é o id. Esse campo é utilizado para identificar o produto no banco de dados.\n\nTemos o schema `ProductResponse`, que é o schema que é retornado pela API. Ele é uma classe que herda do schema `ProductBase`, e possui dois campos a mais, que é o id e o created_at. Esses campos são gerados pelo nosso banco de dados.\n\nTemos o schema `ProductUpdate`, que é o schema que é recebido pela API para update. Ele possui os campos opcionais, pois não é necessário enviar todos os campos para fazer o update.\n\n## Arquivo `crud.py`\n\nO arquivo `crud.py` é responsável por definir as funções de CRUD utilizando o SQLAlchemy ORM. Essas funções são utilizadas para fazer a comunicação com o banco de dados. É nele que definimos as funções de listagem, criação, atualização e remoção de produtos. É onde os dados são persistidos no banco de dados.\n\n## Arquivo `router.py`\n\nO arquivo `router.py` é responsável por definir as rotas da API utilizando o FastAPI. É aqui que definimos as rotas, e também as funções que serão executadas em cada rota. Todas as funções definidas aqui recebem um parâmetro, que é o parâmetro request, que é o objeto que contém os dados da requisição.\n\nOs principais parametros são o path, que é o caminho da rota, o methods, que são os métodos HTTP que a rota aceita, e o response_model, que é o schema que é retornado pela rota.\n\n```python\n@router.post(\"/products/\", response_modelProductResponse)\n```\nImportante destacar que o FastAPI utiliza o conceito de type hints, que são as anotações de tipos. Isso permite que o FastAPI faça a validação dos dados que são recebidos na API, e também para definir os tipos de dados que são retornados pela API. Por exemplo, ao definir o parâmetro product do tipo ProductResponse, o FastAPI já entende que os dados recebidos nesse parâmetro devem ser do tipo ProductResponse.\n\nConseguimos também retornar parâmetros pelo nosso path, no caso do delete, por exemplo, precisamos passar o id do produto que queremos deletar. Para isso, utilizamos o path /products/{product_id}, e definimos o parâmetro product_id na função delete_product.\n\n```python\n@router.get(\"/products/{product_id}\", response_modelProductResponse)\ndef read_product_route(product_id: int, db: Session  Depends(get_db)):\n    db_product  get_product(db, product_idproduct_id)\n    if db_product is None:\n        raise HTTPException(status_code404, detail\"Product not found\")\n    return db_product\n```\n\n## Arquivo `main.py`\n\nO arquivo `main.py` é responsável por definir a aplicação do FastAPI, e também por definir o servidor web Uvicorn. É aqui que definimos o servidor web, e também as configurações do servidor web, como o host e a porta.\n\n\n## Nosso Frontend\n\nNosso frontend vai ser uma aplicação que vai consumir a nossa API, e vai ser responsável por fazer o cadastro, alteração e remoção de produtos. Vamos detalhar cada uma das pastas e arquivos do nosso frontend.\n\n### Streamlit\n\nO Streamlit é uma biblioteca para construir aplicações web com Python. Ele é muito utilizado para construir dashboards, e também para construir aplicações que consomem APIs.\n\n### Requests\n\nO Requests é uma biblioteca para fazer requisições HTTP com Python. Ele é muito utilizado para consumir APIs, e também para fazer web scraping.\n\n### Pandas\n\nO Pandas é uma biblioteca para manipulação de dados com Python. Ele é muito utilizado para fazer análise de dados, e também para construir dashboards.\n\n\n\n## Deploy <> Em construção\n\n\n\n\n\n### AWS ECS\n\nAlém disso, nesse projeto vamos apresentar como colocar em produção um projeto utilizando containers Docker, utilizando o AWS ECS (Amazon Elastic Container Service).\n\nSe você quer ter toda a facilidade do Docker, garantir que o seu ambiente de desenvolvimento e de produção são idênticos, e ainda ter a possibilidade de escalar a sua aplicação, esse projeto é para você.\n\nA AWS ECS é um serviço de orquestração de containers, que permite que você execute containers Docker de forma escalável e altamente disponível. Com ele, você não precisa se preocupar com a infraestrutura, pois a AWS cuida de tudo para você.\n\n### AMAZON ECS\n\nÉ um serviço de orquestração de containers, que permite que você execute containers Docker de forma escalável e altamente disponível. A vantagem principal é que você não precisa se preocupar com a orquestração dos containers (Kubernetes) mas tenha todas as vantagens de utilizar containers Docker.\n\n### AMAZON ECS FARGATE\n\nO ECS Fargate é um serviço que permite que você execute containers Docker sem precisar gerenciar servidores. Ou seja, todo o gerenciamento de servidores, balanceamento de carga, auto scaling, etc, é feito pela AWS. É um serviço ainda mais gerenciado que o ECS, pois você não precisa se preocupar com a infraestrutura.\n\n### Conceitos\n\n[Imagem arquitetura](assets/arquitetura.png)\n\n#### Cluster\n\nUm cluster é um grupo de instâncias EC2 (máquinas) que executam as suas tarefas. Ou seja, as máquinas onde os meus containers vão ser executados.\n\n#### Task Definition\n\nUma task definition é um arquivo de configuração (com a formatação JSON) que define como a sua aplicação vai ser executada. Nesse arquivo você define qual imagem Docker vai ser utilizada, qual o poder computacional necessário, qual o volume que vai ser utilizado, etc.\n\n#### Task\n\nUma task é uma instância de uma task definition. Ou seja, é uma execução da sua aplicação. Por exemplo, se você tem uma task definition que define que a sua aplicação vai ser executada com 2 instâncias, você terá 2 tasks executando a sua aplicação. Aplicado ao Airflow que vimos no Workshop 02, podemos subir mais de uma instância do Airflow, para garantir que a nossa aplicação vai estar sempre disponível. Além disso, podemos configurar para subir mais instâncias quando a CPU estiver alta, por exemplo.\n\n#### Service\n\nUm service é um grupo de tasks que são executadas juntas. Por exemplo, se você tem uma task definition que define que a sua aplicação vai ser executada com 2 instâncias, você terá 2 tasks executando a sua aplicação. Essas 2 tasks formam um service. Se alguma tarefa falhar, o service vai garantir que ela vai ser executada novamente. O service também pode ser utilizado para balancear a carga entre as tasks.\n\n\n\nBootcamp - Python para dados/aula20/docker-compose.yml\n\nversion: '3.8'\nservices:\n  postgres:\n    image: postgres:latest\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_DB: mydatabase\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    networks:\n      - mynetwork\n\n  backend:\n    build: \n      context: ./backend\n      dockerfile: Dockerfile\n    volumes:\n      - ./backend:/app\n    environment:\n      DATABASE_URL: postgresql://user:password@postgres/mydatabase\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - postgres\n    networks:\n      - mynetwork\n\n  frontend:\n    build: \n      context: ./frontend\n      dockerfile: Dockerfile\n    volumes:\n      - ./frontend:/app\n    ports:\n      - \"8501:8501\"\n    networks:\n      - mynetwork\n\nnetworks:\n  mynetwork:\n\nvolumes:\n  postgres_data:\n\n\nBootcamp - Python para dados/aula20/pyproject.toml\n\n[tool.poetry]\nname  \"crud-fastapi-postgres-streamlit\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"Luciano Filho <lvgalvaofilho@gmail.com>\"]\nreadme  \"README.md\"\npackages  [{ include  \"crud_fastapi_postgres_streamlit\" }]\n\n[tool.poetry.dependencies]\npython  \"^3.11.5\"\nfastapi  \"^0.104.1\"\nuvicorn  \"^0.24.0.post1\"\nstreamlit  \"^1.28.1\"\nsqlalchemy  \"^2.0.23\"\nplotly  \"^5.18.0\"\n\n\n[tool.poetry.group.docs.dependencies]\nmkdocs  \"^1.5.3\"\n\n\n[tool.poetry.group.dev.dependencies]\npytest  \"^7.4.3\"\ntaskipy  \"^1.12.0\"\nisort  \"5.12.0\"\nruff  \"0.1.5\"\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n[tool.taskipy.tasks]\nlint  \"ruff . & isort .\"\nkill  \"kill -9 $(lsof -t -i:8000)\"\nrun  \"uvicorn backend.main:app --reload & streamlit run frontend/app.py\"\n\n\nBootcamp - Python para dados/aula20/.dockerignore\n\n# Include any files or directories that you don't want to be copied to your\n# container here (e.g., local build artifacts, temporary files, etc.).\n#\n# For more help, visit the .dockerignore file reference guide at\n# https://docs.docker.com/engine/reference/builder/#dockerignore-file\n\n**/.DS_Store\n**/__pycache__\n**/.venv\n**/.classpath\n**/.dockerignore\n**/.env\n**/.git\n**/.gitignore\n**/.project\n**/.settings\n**/.toolstarget\n**/.vs\n**/.vscode\n**/*.*proj.user\n**/*.dbmdl\n**/*.jfm\n**/bin\n**/charts\n**/docker-compose*\n**/compose*\n**/Dockerfile*\n**/node_modules\n**/npm-debug.log\n**/obj\n**/secrets.dev.yaml\n**/values.dev.yaml\nLICENSE\nREADME.md\n\n\nBootcamp - Python para dados/aula20/.python-version\n\n3.11\n\n\nBootcamp - Python para dados/aula20/backend/Dockerfile\n\n# Dockerfile-backend\n\n# Imagem base\nFROM python:3.9\n\n# Definir o diretório de trabalho no container\nWORKDIR /app\n\n# Copiar os arquivos de dependências e instalar\nCOPY requirements.txt /app/requirements.txt\nRUN pip install --no-cache-dir --upgrade -r /app/requirements.txt\n\n# Copiar o restante dos arquivos do projeto\nCOPY . /app\n\n# Comando para executar a aplicação\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\nBootcamp - Python para dados/aula20/backend/crud.py\n\nfrom sqlalchemy.orm import Session\nfrom schemas import ProductUpdate, ProductCreate\nfrom models import ProductModel\n\n\ndef get_product(db: Session, product_id: int):\n    \"\"\"\n    funcao que recebe um id e retorna somente ele\n    \"\"\"\n    return db.query(ProductModel).filter(ProductModel.id  product_id).first()\n\n\ndef get_products(db: Session):\n    \"\"\"\n    funcao que retorna todos os elementos\n    \"\"\"\n    return db.query(ProductModel).all()\n\n\ndef create_product(db: Session, product: ProductCreate):\n    db_product  ProductModel(**product.model_dump())\n    db.add(db_product)\n    db.commit()\n    db.refresh(db_product)\n    return db_product\n\n\ndef delete_product(db: Session, product_id: int):\n    db_product  db.query(ProductModel).filter(ProductModel.id  product_id).first()\n    db.delete(db_product)\n    db.commit()\n    return db_product\n\n\ndef update_product(db: Session, product_id: int, product: ProductUpdate):\n    db_product  db.query(ProductModel).filter(ProductModel.id  product_id).first()\n\n    if db_product is None:\n        return None\n\n    if product.name is not None:\n        db_product.name  product.name\n    if product.description is not None:\n        db_product.description  product.description\n    if product.price is not None:\n        db_product.price  product.price\n    if product.categoria is not None:\n        db_product.categoria  product.categoria\n    if product.email_fornecedor is not None:\n        db_product.email_fornecedor  product.email_fornecedor\n\n    db.commit()\n    return db_product\n\n\nBootcamp - Python para dados/aula20/backend/database.py\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nSQLALCHEMY_DATABASE_URL  \"postgresql://user:password@postgres/mydatabase\"\n\n# Cria o motor do banco de dados, é o conecta com o banco\nengine  create_engine(SQLALCHEMY_DATABASE_URL)\n\n# Sessão de banco de dados, é quem vai executar as queries\nSessionLocal  sessionmaker(autocommitFalse, autoflushFalse, bindengine)\n\n# Base para os modelos declarativos\nBase  declarative_base()\n\n\ndef get_db():\n    db  SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n\nBootcamp - Python para dados/aula20/backend/main.py\n\nfrom fastapi import FastAPI\nfrom database import engine\nimport models\nfrom router import router\n\nmodels.Base.metadata.create_all(bindengine)\n\napp  FastAPI()\napp.include_router(router)\n\n\nBootcamp - Python para dados/aula20/backend/models.py\n\nfrom sqlalchemy import Column, Integer, String, Float, DateTime\nfrom sqlalchemy.sql import func\nfrom database import Base\n\n\nclass ProductModel(Base):\n    __tablename__  \"products\"  # esse será o nome da tabela\n\n    id  Column(Integer, primary_keyTrue, indexTrue)\n    name  Column(String, indexTrue)\n    description  Column(String, indexTrue)\n    price  Column(Float, indexTrue)\n    categoria  Column(String, indexTrue)\n    email_fornecedor  Column(String, indexTrue)\n    created_at  Column(DateTime(timezoneTrue), defaultfunc.now(), indexTrue)\n\n\nBootcamp - Python para dados/aula20/backend/requirements.txt\n\nfastapi\nuvicorn\nSQLAlchemy\nemail-validator\npsycopg2-binary\n\n",
        "Bootcamp - Python para dados/aula20/backend/router.py\n\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm import Session\nfrom database import SessionLocal, get_db\nfrom schemas import ProductResponse, ProductUpdate, ProductCreate\nfrom typing import List\nfrom crud import (\n    create_product,\n    get_products,\n    get_product,\n    delete_product,\n    update_product,\n)\n\nrouter  APIRouter()\n\n\n@router.post(\"/products/\", response_modelProductResponse)\ndef create_product_route(product: ProductCreate, db: Session  Depends(get_db)):\n    return create_product(dbdb, productproduct)\n\n\n@router.get(\"/products/\", response_modelList[ProductResponse])\ndef read_all_products_route(db: Session  Depends(get_db)):\n    products  get_products(db)\n    return products\n\n\n@router.get(\"/products/{product_id}\", response_modelProductResponse)\ndef read_product_route(product_id: int, db: Session  Depends(get_db)):\n    db_product  get_product(db, product_idproduct_id)\n    if db_product is None:\n        raise HTTPException(status_code404, detail\"Product not found\")\n    return db_product\n\n\n@router.delete(\"/products/{product_id}\", response_modelProductResponse)\ndef detele_product_route(product_id: int, db: Session  Depends(get_db)):\n    db_product  delete_product(db, product_idproduct_id)\n    if db_product is None:\n        raise HTTPException(status_code404, detail\"Product not found\")\n    return db_product\n\n\n@router.put(\"/products/{product_id}\", response_modelProductResponse)\ndef update_product_route(\n    product_id: int, product: ProductUpdate, db: Session  Depends(get_db)\n):\n    db_product  update_product(db, product_idproduct_id, productproduct)\n    if db_product is None:\n        raise HTTPException(status_code404, detail\"Product not found\")\n    return db_product\n\n\nBootcamp - Python para dados/aula20/backend/schemas.py\n\nfrom pydantic import BaseModel, PositiveFloat, EmailStr, validator, Field\nfrom enum import Enum\nfrom datetime import datetime\nfrom typing import Optional\n\n\nclass CategoriaBase(Enum):\n    categoria1  \"Eletrônico\"\n    categoria2  \"Eletrodoméstico\"\n    categoria3  \"Móveis\"\n    categoria4  \"Roupas\"\n    categoria5  \"Calçados\"\n\n\nclass ProductBase(BaseModel):\n    name: str\n    description: Optional[str]  None\n    price: PositiveFloat\n    categoria: str\n    email_fornecedor: EmailStr\n\n    @validator(\"categoria\")\n    def check_categoria(cls, v):\n        if v in [item.value for item in CategoriaBase]:\n            return v\n        raise ValueError(\"Categoria inválida\")\n\n\nclass ProductCreate(ProductBase):\n    pass\n\n\nclass ProductResponse(ProductBase):\n    id: int\n    created_at: datetime\n\n    class Config:\n        orm_mode  True\n\n\nclass ProductUpdate(BaseModel):\n    name: Optional[str]  None\n    description: Optional[str]  None\n    price: Optional[PositiveFloat]  None\n    categoria: Optional[str]  None\n    email_fornecedor: Optional[EmailStr]  None\n\n    @validator(\"categoria\", preTrue, alwaysTrue)\n    def check_categoria(cls, v):\n        if v is None:\n            return v\n        if v in [item.value for item in CategoriaBase]:\n            return v\n        raise ValueError(\"Categoria inválida\")\n\n\nBootcamp - Python para dados/aula20/backend/.dockerignore\n\n# Include any files or directories that you don't want to be copied to your\n# container here (e.g., local build artifacts, temporary files, etc.).\n#\n# For more help, visit the .dockerignore file reference guide at\n# https://docs.docker.com/engine/reference/builder/#dockerignore-file\n\n**/.DS_Store\n**/__pycache__\n**/.venv\n**/.classpath\n**/.dockerignore\n**/.env\n**/.git\n**/.gitignore\n**/.project\n**/.settings\n**/.toolstarget\n**/.vs\n**/.vscode\n**/*.*proj.user\n**/*.dbmdl\n**/*.jfm\n**/bin\n**/charts\n**/docker-compose*\n**/compose*\n**/Dockerfile*\n**/node_modules\n**/npm-debug.log\n**/obj\n**/secrets.dev.yaml\n**/values.dev.yaml\nLICENSE\nREADME.md\n\n\nBootcamp - Python para dados/aula20/frontend/Dockerfile\n\n# Dockerfile-frontend\n\n# Imagem base\nFROM python:3.9\n\n# Definir o diretório de trabalho no container\nWORKDIR /app\n\n# Copiar os arquivos de dependências e instalar\nCOPY requirements.txt /app/requirements.txt\nRUN pip install --no-cache-dir --upgrade -r /app/requirements.txt\n\n# Copiar o restante dos arquivos do projeto\nCOPY . /app\n\n# Comando para executar a aplicação\nCMD [\"streamlit\", \"run\", \"app.py\", \"--server.port8501\", \"--server.address0.0.0.0\"]\n\n\nBootcamp - Python para dados/aula20/frontend/app.py\n\nimport streamlit as st\nimport requests\nimport pandas as pd\n\nst.set_page_config(layout\"wide\")\n\nst.image(\"logo.png\", width200)\n\nst.title(\"Gerenciamento de Produtos\")\n\n\n# Função auxiliar para exibir mensagens de erro detalhadas\ndef show_response_message(response):\n    if response.status_code  200:\n        st.success(\"Operação realizada com sucesso!\")\n    else:\n        try:\n            data  response.json()\n            if \"detail\" in data:\n                # Se o erro for uma lista, extraia as mensagens de cada erro\n                if isinstance(data[\"detail\"], list):\n                    errors  \"\\n\".join([error[\"msg\"] for error in data[\"detail\"]])\n                    st.error(f\"Erro: {errors}\")\n                else:\n                    # Caso contrário, mostre a mensagem de erro diretamente\n                    st.error(f\"Erro: {data['detail']}\")\n        except ValueError:\n            st.error(\"Erro desconhecido. Não foi possível decodificar a resposta.\")\n\n\n# Adicionar Produto\nwith st.expander(\"Adicionar um Novo Produto\"):\n    with st.form(\"new_product\"):\n        name  st.text_input(\"Nome do Produto\")\n        description  st.text_area(\"Descrição do Produto\")\n        price  st.number_input(\"Preço\", min_value0.01, format\"%f\")\n        categoria  st.selectbox(\n            \"Categoria\",\n            [\"Eletrônico\", \"Eletrodoméstico\", \"Móveis\", \"Roupas\", \"Calçados\"],\n        )\n        email_fornecedor  st.text_input(\"Email do Fornecedor\")\n        submit_button  st.form_submit_button(\"Adicionar Produto\")\n\n        if submit_button:\n            response  requests.post(\n                \"http://backend:8000/products/\",\n                json{\n                    \"name\": name,\n                    \"description\": description,\n                    \"price\": price,\n                    \"categoria\": categoria,\n                    \"email_fornecedor\": email_fornecedor,\n                },\n            )\n            show_response_message(response)\n# Visualizar Produtos\nwith st.expander(\"Visualizar Produtos\"):\n    if st.button(\"Exibir Todos os Produtos\"):\n        response  requests.get(\"http://backend:8000/products/\")\n        if response.status_code  200:\n            product  response.json()\n            df  pd.DataFrame(product)\n\n            df  df[\n                [\n                    \"id\",\n                    \"name\",\n                    \"description\",\n                    \"price\",\n                    \"categoria\",\n                    \"email_fornecedor\",\n                    \"created_at\",\n                ]\n            ]\n\n            # Exibe o DataFrame sem o índice\n            st.write(df.to_html(indexFalse), unsafe_allow_htmlTrue)\n        else:\n            show_response_message(response)\n\n# Obter Detalhes de um Produto\nwith st.expander(\"Obter Detalhes de um Produto\"):\n    get_id  st.number_input(\"ID do Produto\", min_value1, format\"%d\")\n    if st.button(\"Buscar Produto\"):\n        response  requests.get(f\"http://backend:8000/products/{get_id}\")\n        if response.status_code  200:\n            product  response.json()\n            df  pd.DataFrame([product])\n\n            df  df[\n                [\n                    \"id\",\n                    \"name\",\n                    \"description\",\n                    \"price\",\n                    \"categoria\",\n                    \"email_fornecedor\",\n                    \"created_at\",\n                ]\n            ]\n\n            # Exibe o DataFrame sem o índice\n            st.write(df.to_html(indexFalse), unsafe_allow_htmlTrue)\n        else:\n            show_response_message(response)\n\n# Deletar Produto\nwith st.expander(\"Deletar Produto\"):\n    delete_id  st.number_input(\"ID do Produto para Deletar\", min_value1, format\"%d\")\n    if st.button(\"Deletar Produto\"):\n        response  requests.delete(f\"http://backend:8000/products/{delete_id}\")\n        show_response_message(response)\n\n# Atualizar Produto\nwith st.expander(\"Atualizar Produto\"):\n    with st.form(\"update_product\"):\n        update_id  st.number_input(\"ID do Produto\", min_value1, format\"%d\")\n        new_name  st.text_input(\"Novo Nome do Produto\")\n        new_description  st.text_area(\"Nova Descrição do Produto\")\n        new_price  st.number_input(\n            \"Novo Preço\",\n            min_value0.01,\n            format\"%f\",\n        )\n        new_categoria  st.selectbox(\n            \"Nova Categoria\",\n            [\"Eletrônico\", \"Eletrodoméstico\", \"Móveis\", \"Roupas\", \"Calçados\"],\n        )\n        new_email  st.text_input(\"Novo Email do Fornecedor\")\n\n        update_button  st.form_submit_button(\"Atualizar Produto\")\n\n        if update_button:\n            update_data  {}\n            if new_name:\n                update_data[\"name\"]  new_name\n            if new_description:\n                update_data[\"description\"]  new_description\n            if new_price > 0:\n                update_data[\"price\"]  new_price\n            if new_email:\n                update_data[\"email_fornecedor\"]  new_email\n            if new_categoria:\n                update_data[\"categoria\"]  new_categoria\n\n            if update_data:\n                response  requests.put(\n                    f\"http://backend:8000/products/{update_id}\", jsonupdate_data\n                )\n                show_response_message(response)\n            else:\n                st.error(\"Nenhuma informação fornecida para atualização\")\n\n\nBootcamp - Python para dados/aula20/frontend/requirements.txt\n\nstreamlit\nrequests\npandas\n\nBootcamp - Python para dados/aula20/frontend/.dockerignore\n\n# Include any files or directories that you don't want to be copied to your\n# container here (e.g., local build artifacts, temporary files, etc.).\n#\n# For more help, visit the .dockerignore file reference guide at\n# https://docs.docker.com/engine/reference/builder/#dockerignore-file\n\n**/.DS_Store\n**/__pycache__\n**/.venv\n**/.classpath\n**/.dockerignore\n**/.env\n**/.git\n**/.gitignore\n**/.project\n**/.settings\n**/.toolstarget\n**/.vs\n**/.vscode\n**/*.*proj.user\n**/*.dbmdl\n**/*.jfm\n**/bin\n**/charts\n**/docker-compose*\n**/compose*\n**/Dockerfile*\n**/node_modules\n**/npm-debug.log\n**/obj\n**/secrets.dev.yaml\n**/values.dev.yaml\nLICENSE\nREADME.md\n\n\nBootcamp - Python para dados/aula20/frontend/.streamlit/config.toml\n\n[theme]\nprimaryColor  \"#1E2250\"             # Amarelo - similar ao botão do Mercado Livre\nbackgroundColor  \"#ffffff\"          # Amarelo claro para o fundo\nsecondaryBackgroundColor  \"#f5f5f5\" # Azul claro para fundo dos widgets\ntextColor  \"#2d377a\"                # Azul - similar ao texto do Mercado Livre\nfont  \"sans serif\"\n\n\n"
    ],
    "01-como-estruturar-projetos-e-processos-de-dados-do-zero": [
        "01-como-estruturar-projetos-e-processos-de-dados-do-zero/README.md\n\n### Instalação e Configuração\n\nNosso projeto:\n\nhttps://lvgalvao-workshop-aberto-aovivo-srcapp-508jxg.streamlit.app/\n\n1. Clone o repositório:\n```bash\ngit clone https://github.com/lvgalvao/Workshop-aberto-aovivo\ncd Workshop-aberto-aovivo\n```\n2. Configure a versão correta do Python com `pyenv`:\n```bash\npyenv install 3.11.5\npyenv local 3.11.5\n```\n3. Instale as dependências do projeto:\n```bash\npython -m venv .venv\n# O padrao é utilizar .venv\nsource .venv/bin/activate\n# Usuários Linux e mac\n.venv\\Scripts\\Activate\n# Usuários Windows\npip install -r requirements.txt  \n```\n\n4. Rode o projeto\n```bash\ntask run\n```\n\n5. Rode os testes\n```bash\ntask test\n```\n\n6. Rode a documentação\n```bash\ntask docs\n```\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/exemplo.env\n\nPOSTGRES_USERseu_usuario\nPOSTGRES_PASSWORDsua_senha\nPOSTGRES_HOSTlocalhost\nPOSTGRES_PORT5432\nPOSTGRES_DBnome_do_seu_banco_de_dados\nSENTRY_DNSseu_dns\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/mkdocs.yml\n\nsite_name: \"My Library\"\n\ntheme:\n  name: \"material\"\n\nplugins:\n- search\n- mkdocstrings\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/pyproject.toml\n\n[tool.taskipy.tasks]\nrun  \"lsof -ti :8501 | xargs kill -9 | streamlit run src/app.py\"\ntest  \"lsof -ti :8501 | xargs kill -9 | pytest tests -v\"\n\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/requirements.txt\n\naltair5.2.0\nannotated-types0.6.0\nattrs23.2.0\nBabel2.14.0\nblinker1.7.0\ncachetools5.3.2\ncertifi2023.11.17\ncharset-normalizer3.3.2\nclick8.1.7\ncolorama0.4.6\ndnspython2.5.0\nemail-validator2.1.0.post1\net-xmlfile1.1.0\nghp-import2.1.0\ngitdb4.0.11\nGitPython3.1.41\ngriffe0.39.1\nh110.14.0\nidna3.6\nimportlib-metadata7.0.1\niniconfig2.0.0\nJinja23.1.3\njsonschema4.21.1\njsonschema-specifications2023.12.1\nMarkdown3.5.2\nmarkdown-it-py3.0.0\nMarkupSafe2.1.4\nmdurl0.1.2\nmergedeep1.3.4\nmkdocs1.5.3\nmkdocs-autorefs0.5.0\nmkdocs-material9.5.5\nmkdocs-material-extensions1.3.1\nmkdocstrings0.24.0\nmkdocstrings-python1.8.0\nnumpy1.26.3\nopenpyxl3.1.2\noutcome1.3.0.post0\npackaging23.2\npaginate0.5.6\npandas2.2.0\npathspec0.12.1\npillow10.2.0\nplatformdirs4.1.0\npluggy1.4.0\nprotobuf4.25.2\npsutil5.9.8\npsycopg2-binary2.9.9\npyarrow15.0.0\npydantic2.5.3\npydantic_core2.14.6\npydeck0.8.1b0\nPygments2.17.2\npymdown-extensions10.7\nPySocks1.7.1\npytest7.4.4\npython-dateutil2.8.2\npython-dotenv1.0.1\npytz2023.3.post1\nPyYAML6.0.1\npyyaml_env_tag0.1\nreferencing0.32.1\nregex2023.12.25\nrequests2.31.0\nrich13.7.0\nrpds-py0.17.1\nselenium4.17.2\nsentry-sdk1.39.2\nsix1.16.0\nsmmap5.0.1\nsniffio1.3.0\nsortedcontainers2.4.0\nSQLAlchemy2.0.25\nstreamlit1.30.0\ntaskipy1.12.2\ntenacity8.2.3\ntoml0.10.2\ntomli2.0.1\ntoolz0.12.1\ntornado6.4\ntrio0.24.0\ntrio-websocket0.11.1\ntyping_extensions4.9.0\ntzdata2023.4\ntzlocal5.2\nurllib32.1.0\nvalidators0.22.0\nwatchdog3.0.0\nwsproto1.2.0\nzipp3.17.0\n\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/.python-version\n\n3.11.5\n\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/docs/index.md\n\n# Welcome to MkDocs\n\nFor full documentation visit [mkdocs.org](https://www.mkdocs.org).\n\n## Contrato\n\n::: src.contrato.Vendas\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/sql/create.sql\n\nCREATE TABLE vendas (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255) NOT NULL,\n    data TIMESTAMP NOT NULL,\n    valor NUMERIC(10, 2) NOT NULL CHECK (valor > 0),\n    quantidade INTEGER NOT NULL CHECK (quantidade > 0),\n    produto VARCHAR(255) NOT NULL,\n    categoria VARCHAR(50) NOT NULL\n);\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/sql/insert.sql\n\n-- Exemplo com dados valido\n\nINSERT INTO vendas (email, data, valor, produto, quantidade, categoria)\nVALUES (\n    'comprador@example.com', \n    '2023-09-15 12:00:00',  -- Substitua pela data atual formatada como string\n    100.50, \n    'Produto X', \n    3, \n    'categoria3'\n);\n\n-- Exemplo com dado invalido\n\nINSERT INTO vendas (email, data, valor, produto, quantidade, categoria)\nVALUES (\n    'comprador', \n    '2023-09-15 12:00:00',  -- Substitua pela data atual formatada como string\n    100.50, \n    'Produto X', \n    3, \n    'categoria3'\n);\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/src/app.py\n\nfrom frontend import ExcelValidadorUI\nfrom backend import process_excel, save_dataframe_to_sql\nfrom dotenv import load_dotenv\nimport sentry_sdk\nimport os\nimport logging\n\nload_dotenv(\".env\")\n\nsentry_sdk.init(\n    dsnos.getenv('SENTRY_DNS'),\n    traces_sample_rate1.0,\n    profiles_sample_rate1.0,\n)\n\ndef main():\n    ui  ExcelValidadorUI()\n    ui.display_header()\n\n    uploaded_file  ui.upload_file()\n\n    if uploaded_file:\n        df, result, errors  process_excel(uploaded_file)\n        ui.display_results(result, errors)\n\n        if errors:\n            ui.display_wrong_message()\n            sentry_sdk.capture_message(\"Erro ao subir excel\")\n            logging.error(\"Test\")\n        elif ui.display_save_button():\n            # Se não houver erros e o botão for exibido, exibir o botão e fazer o log\n            save_dataframe_to_sql(df)\n            ui.display_success_message()\n            sentry_sdk.capture_message(\"Banco de dados foi atualizado\")\n            logging.error(\"Test\")\n\nif __name__  \"__main__\":\n    main()\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/src/backend.py\n\nimport pandas as pd\nfrom contrato import Vendas\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")\n\n# Lê as variáveis de ambiente\nPOSTGRES_USER  os.getenv('POSTGRES_USER')\nPOSTGRES_PASSWORD  os.getenv('POSTGRES_PASSWORD')\nPOSTGRES_HOST  os.getenv('POSTGRES_HOST')\nPOSTGRES_PORT  os.getenv('POSTGRES_PORT')\nPOSTGRES_DB  os.getenv('POSTGRES_DB')\n\n# Cria a URL de conexão com o banco de dados\nDATABASE_URL  f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n\n# Carrega as variáveis de ambiente\nload_dotenv()\n\ndef process_excel(uploaded_file):\n    try:\n        df  pd.read_excel(uploaded_file)\n        erros  []\n        # Verificar se há colunas extras no DataFrame\n        extra_cols  set(df.columns) - set(Vendas.model_fields.keys())\n        if extra_cols:\n            return False, f\"Colunas extras detectadas no Excel: {', '.join(extra_cols)}\"\n\n        # Validar cada linha com o schema escolhido\n        for index, row in df.iterrows():\n            try:\n                _  Vendas(**row.to_dict())\n            except Exception as e:\n                erros.append(f\"Erro na linha {index + 2}: {e}\")\n\n        # Retorna tanto o resultado da validação, os erros, quanto o DataFrame\n        return df, True, erros\n\n    except Exception as e:\n        # Se houver exceção, retorna o erro e um DataFrame vazio\n        return pd.DataFrame(), f\"Erro inesperado: {str(e)}\"\n    \ndef save_dataframe_to_sql(df):\n    # Salva o DataFrame no banco de dados\n    df.to_sql('vendas', conDATABASE_URL, if_exists'replace', indexFalse)\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/src/contrato.py\n\nfrom pydantic import BaseModel, EmailStr, PositiveFloat, PositiveInt, field_validator\nfrom datetime import datetime\nfrom enum import Enum\n\nclass CategoriaEnum(str, Enum):\n    categoria1  \"categoria1\"\n    categoria2  \"categoria2\"\n    categoria3  \"categoria3\"\n\nclass Vendas(BaseModel):\n\n    email: EmailStr\n    data: datetime\n    valor: PositiveFloat\n    quantidade: PositiveInt\n    produto: str\n    categoria: CategoriaEnum\n\n    @field_validator('categoria')\n    def categoria_deve_estar_no_enum(cls, error):\n        return error\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/src/frontend.py\n\nimport streamlit as st\n\nclass ExcelValidadorUI:\n\n    def __init__(self):\n        self.set_page_config()\n\n    def set_page_config(self):\n        st.set_page_config(\n            page_title\"Validador de schema excel\"\n        )\n\n    def display_header(self):\n        st.title(\"Validador de schema excel\")\n\n    def upload_file(self):\n        return st.file_uploader(\"Carregue seu arquivo Excel aqui\", type[\"xlsx\"])\n\n    def display_results(self, result, errors):\n        if errors:\n            for error in errors:\n                st.error(f\"Erro na validação: {error}\")\n        else:\n            st.success(\"O schema do arquivo Excel está correto!\")\n\n    def display_save_button(self):\n        return st.button(\"Salvar no Banco de Dados\")\n    \n    def display_wrong_message(self):\n        return st.error(\"Necessário corrigir a planilha!\")\n    \n    def display_success_message(self):\n        return st.success(\"Dados salvos com sucesso no banco de dados!\")\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/tests/test_app.py\n\nfrom selenium import webdriver\nfrom time import sleep\nimport pytest\nimport subprocess\nfrom selenium.webdriver.common.by import By\nimport os\nfrom selenium.webdriver.firefox.options import Options\n\n@pytest.fixture\ndef driver():\n    # Iniciar o Streamlit em background\n    process  subprocess.Popen([\"streamlit\", \"run\", \"src/app.py\"])\n    options  Options()\n    options.headless  True  # Executar em modo headless\n    driver  webdriver.Firefox(optionsoptions)\n    # Iniciar o WebDriver usando GeckoDriver\n    driver.set_page_load_timeout(5)\n    yield driver\n\n    # Fechar o WebDriver e o Streamlit após o teste\n    driver.quit()\n    process.kill()\n\ndef test_app_opens(driver):\n    # Verificar se a página abre\n    driver.get(\"http://localhost:8501\")\n    sleep(2)\n\ndef test_check_title_is(driver):\n    # Verificar se a página abre\n    driver.get(\"http://localhost:8501\")\n    # Verifica se o titulo de página é\n    sleep(2)\n    # Capturar o título da página\n    page_title  driver.title\n\n    # Verificar se o título da página é o esperado\n    expected_title  \"Validador de schema excel\"  # Substitua com o título real esperado\n    assert page_title  expected_title\n\ndef test_check_streamlit_h1(driver):\n    # Acessar a página do Streamlit\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar para garantir que a página foi carregada\n    sleep(2)  # Espera 5 segundos\n\n    # Capturar o primeiro elemento <h1> da página\n    h1_element  driver.find_element(By.TAG_NAME, \"h1\")\n\n    # Verificar se o texto do elemento <h1> é o esperado\n    expected_text  \"Insira o seu excel para validação\"\n    assert h1_element.text  expected_text\n\ndef test_check_usuario_pode_inserir_um_excel_e_receber_uma_mensagem(driver):\n    # Acessar a página do Streamlit\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar para garantir que a página foi carregada\n    sleep(3)  # Espera 5 segundos\n\n    # Realizar o upload do arquivo de sucesso\n    success_file_path  os.path.abspath(\"data/correto.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(success_file_path)\n\n    # Aguardar a mensagem de sucesso\n    sleep(3)\n    assert \"O schema do arquivo Excel está correto!\" in driver.page_source\n\ndef test_check_mais_de_uma_mensagem_de_erro(driver):\n    # Acessar a página do Streamlit\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar para garantir que a página foi carregada\n    sleep(3)  # Espera 5 segundos\n\n    # Realizar o upload do arquivo de sucesso\n    multiple_erros_file_path  os.path.abspath(\"data/multiplos_erros.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(multiple_erros_file_path)\n\n    # Aguardar a mensagem de sucesso\n    sleep(3)\n    # Localizar todas as ocorrências da mensagem de erro\n    error_messages  driver.find_elements(By.XPATH, \"//*[contains(text(), 'Erro na validação')]\")\n\n    # Verificar se existem pelo menos duas mensagens de erro\n    assert len(error_messages)  2, f\"Quantidade de mensagens de erro encontradas: {len(error_messages)}\"\n\ndef test_check_usuario_insere_um_excel_valido_e_aparece_um_botao(driver):\n    # Acessar a página do Streamlit\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar para garantir que a página foi carregada\n    sleep(3)  # Espera 3 segundos\n\n    # Realizar o upload do arquivo de sucesso\n    success_file_path  os.path.abspath(\"data/correto.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(success_file_path)\n\n    # Aguardar a mensagem de sucesso\n    sleep(3)\n    assert \"O schema do arquivo Excel está correto!\" in driver.page_source\n    # Verificar se o botão \"Salvar no Banco de Dados\" está presente\n    buttons  driver.find_elements(By.XPATH, \"//button\")\n    save_button  None\n    for button in buttons:\n        if button.text  \"Salvar no Banco de Dados\":\n            save_button  button\n            break\n\n    assert save_button is not None and save_button.is_displayed()\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/tests/test_integration.py\n\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv(\".env\")\n\n# Lê as variáveis de ambiente\nPOSTGRES_USER  os.getenv('POSTGRES_USER')\nPOSTGRES_PASSWORD  os.getenv('POSTGRES_PASSWORD')\nPOSTGRES_HOST  os.getenv('POSTGRES_HOST')\nPOSTGRES_PORT  os.getenv('POSTGRES_PORT')\nPOSTGRES_DB  os.getenv('POSTGRES_DB')\n\n# Cria a URL de conexão com o banco de dados\nDATABASE_URL  f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n\ndef test_read_data_and_check_schema():\n    df  pd.read_sql('SELECT * FROM vendas', conDATABASE_URL)\n\n    # Verificar se o DataFrame não está vazio\n    assert not df.empty, \"O DataFrame está vazio.\"\n\n    # Verificar o schema (colunas e tipos de dados)\n    expected_dtype  {\n        'id': 'int64',\n        'email': 'object',  # object em Pandas corresponde a string em SQL\n        'data': 'datetime64[ns]',\n        'valor': 'float64',\n        'produto': 'object',\n        'quantidade': 'int64',\n        'categoria': 'object'\n    }\n\n    assert df.dtypes.to_dict()  expected_dtype, \"O schema do DataFrame não corresponde ao esperado.\"\n\n\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/tests/test_unit.py\n\nimport pytest\nfrom datetime import datetime\nfrom src.contrato import Vendas\nfrom pydantic import ValidationError\n\n# Testes com dados válidos\ndef test_vendas_com_dados_validos():\n    dados_validos  {\n        \"email\": \"comprador@example.com\",\n        \"data\": datetime.now(),\n        \"valor\": 100.50,\n        \"produto\": \"Produto X\",\n        \"quantidade\": 3,\n        \"categoria\": \"categoria3\",\n    }\n\n    # A sintaxe **dados_validos é uma forma de desempacotamento de dicionários em Python. \n    # O que isso faz é passar os pares chave-valor no dicionário dados_validos como argumentos nomeados para o construtor da classe Vendas.\n\n    venda  Vendas(**dados_validos)\n\n    assert venda.email  dados_validos[\"email\"]\n    assert venda.data  dados_validos[\"data\"]\n    assert venda.valor  dados_validos[\"valor\"]\n    assert venda.produto  dados_validos[\"produto\"]\n    assert venda.quantidade  dados_validos[\"quantidade\"]\n    assert venda.categoria  dados_validos[\"categoria\"]\n\n# Testes com dados inválidos\ndef test_vendas_com_dados_invalidos():\n    dados_invalidos  {\n        \"email\": \"comprador\",\n        \"data\": \"não é uma data\",\n        \"valor\": -100,\n        \"produto\": \"\",\n        \"quantidade\": -1,\n        \"categoria\": \"categoria3\"\n    }\n\n    with pytest.raises(ValidationError):\n        Vendas(**dados_invalidos)\n\n# Teste de validação de categoria\ndef test_validacao_categoria():\n    dados  {\n        \"email\": \"comprador@example.com\",\n        \"data\": datetime.now(),\n        \"valor\": 100.50,\n        \"produto\": \"Produto Y\",\n        \"quantidade\": 1,\n        \"categoria\": \"categoria inexistente\",\n    }\n\n    with pytest.raises(ValidationError):\n        Vendas(**dados)\n\n01-como-estruturar-projetos-e-processos-de-dados-do-zero/.github/workflows/ci.yml\n\nname: ci\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.9\",\"3.10\",\"3.11\",\"3.12\"]\n\n    steps:\n      - name: Copia os arquivos do repositório\n        uses: actions/checkout@v4\n\n      - name: Instalar o Python  \n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version}} \n\n      - name: Instalar Poetry\n        run: pip install -r requirements-test.txt\n\n      - name: Rodar os testes\n        run: pytest tests/test_unit.py\n\n      - name: Rodar os testes\n        run: echo hello world!        \n\n      - name: Segundo Passo\n        run: echo $(ls)\n\n      - name: Terceiro passo\n        run: echo $(pwd)\n\n"
    ],
    "Bootcamp - Python com Laennder": [
        "Bootcamp - Python com Laennder/README.md\n\n## Calendário Bootcamp - Python do Zero\n\n| Aula  | Workshop                                                                 | Horário |\n|-------|--------------------------------------------------------------------------|---------|\n| Aula 01 | Python do Zero                                    | 26/09/2024 - 19h    |\n| Aula 02 | Git, Github e bibliotecas externas                | 03/10/2024 - 19h    |\n| Aula 03 | ETL         | 10/10/2024 - 19h    |\n| Aula 04 | API | 17/10/2024 - 19h    |\n| Aula 05 | Estruturando um projeto do zero   | 24/10/2024 - 19h    |\n\nBootcamp - Python com Laennder/Aula_02/readme.md\n\n## Aula 02: Métodos, Funções, Pandas e Cálculos Estatísticos\n\n### 1. Introdução aos Métodos e Funções\n\n- **Objetivo**: Entender a diferença entre métodos e funções em Python e aprender a criar funções personalizadas.\n\n#### O que são Métodos?\n\nMétodos são ações associadas a um objeto em Python. Eles são definidos dentro de uma classe e operam sobre os dados do objeto. Por exemplo, o método `.upper()` transforma uma string em letras maiúsculas:\n\n```python\ntexto  \"hello\"\nprint(texto.upper())  # Saída: \"HELLO\"\n```\n\n#### O que são Funções?\n\nFunções são blocos de código independentes que realizam uma tarefa específica e podem retornar valores. Exemplo de uma função que soma dois números:\n\n```python\ndef somar(a, b):\n    return a + b\n\nresultado  somar(3, 5)\nprint(f\"O resultado da soma é: {resultado}\")  # Saída: O resultado da soma é: 8\n```\n\n### 2. Função de Cast (Conversão de Tipos)\n\nCasting é a conversão de um valor de um tipo para outro, como transformar uma `string` em `int` ao receber dados de um usuário via `input`:\n\n```python\nentrada  input(\"Digite sua idade: \")  # A entrada é uma string\nidade  int(entrada)  # Converte a string para inteiro\nprint(f\"Sua idade é: {idade}\")\n```\n\n### 3. Criando Funções Personalizadas\n\nAgora, vamos criar funções para calcular **média**, **variância** e **desvio padrão** de uma lista de números.\n\n#### Função para Calcular a Média:\n\nA média é o valor central de um conjunto de números:\n\n```python\ndef calcular_media(lista):\n    soma  0\n    for valor in lista:\n        soma + valor\n    media  soma / len(lista)\n    return media\n```\n\n#### Função para Calcular a Variância:\n\nA variância mede a dispersão dos valores em relação à média:\n\n```python\ndef calcular_variancia(lista):\n    media  calcular_media(lista)\n    soma_diferencas  0\n    for valor in lista:\n        diferenca  valor - media\n        soma_diferencas + diferenca ** 2\n    variancia  soma_diferencas / len(lista)\n    return variancia\n```\n\n#### Função para Calcular o Desvio Padrão:\n\nO desvio padrão é a raiz quadrada da variância, medindo a dispersão dos valores:\n\n```python\ndef calcular_desvio_padrao(lista):\n    variancia  calcular_variancia(lista)\n    desvio_padrao  variancia ** 0.5\n    return desvio_padrao\n```\n\n### 4. Aplicação Prática\n\nVamos aplicar essas funções a uma lista de idades:\n\n```python\nidades  [23, 29, 31, 25]\n\nmedia  calcular_media(idades)\nvariancia  calcular_variancia(idades)\ndesvio_padrao  calcular_desvio_padrao(idades)\n\nprint(f\"Média: {media}\")\nprint(f\"Variância: {variancia}\")\nprint(f\"Desvio Padrão: {desvio_padrao}\")\n```\n\n---\n\n## Simplificando com pandas\n\nAgora que calculamos as estatísticas manualmente, podemos simplificar o processo usando o **pandas**, uma biblioteca poderosa para manipulação de dados. Vamos explorar como usar pandas para realizar esses cálculos automaticamente.\n\n### 1. **Média** (`mean`)\n\nA **média** é o valor central de um conjunto de dados.\n\n```python\nmedia  idades_series.mean()\nprint(f\"Média: {media}\")\n```\n\n### 2. **Mediana** (`median`)\n\nA **mediana** é o valor central de um conjunto ordenado de dados.\n\n```python\nmediana  idades_series.median()\nprint(f\"Mediana: {mediana}\")\n```\n\n### 3. **Desvio Padrão** (`std`)\n\nO **desvio padrão** mede o quão dispersos os valores estão em relação à média.\n\n```python\ndesvio_padrao  idades_series.std()\nprint(f\"Desvio Padrão: {desvio_padrao}\")\n```\n\n### 4. **Variância** (`var`)\n\nA **variância** é a medida da dispersão ao quadrado dos valores em relação à média.\n\n```python\nvariancia  idades_series.var()\nprint(f\"Variância: {variancia}\")\n```\n\n### 5. **Moda** (`mode`)\n\nA **moda** é o valor que ocorre com mais frequência.\n\n```python\nmoda  idades_series.mode()[0]\nprint(f\"Moda: {moda}\")\n```\n\n### 6. **Quartis** (`quantile`)\n\nOs **quartis** dividem os dados em quatro partes iguais.\n\n```python\nprimeiro_quartil  idades_series.quantile(0.25)\nterceiro_quartil  idades_series.quantile(0.75)\nprint(f\"Primeiro Quartil: {primeiro_quartil}\")\nprint(f\"Terceiro Quartil: {terceiro_quartil}\")\n```\n\n### 7. Outros Métodos Estatísticos\n\n#### Mínimo e Máximo:\n\n```python\nminimo  idades_series.min()\nmaximo  idades_series.max()\nprint(f\"Mínimo: {minimo}\")\nprint(f\"Máximo: {maximo}\")\n```\n\n#### Soma e Contagem:\n\n```python\nsoma  idades_series.sum()\ncontagem  idades_series.count()\nprint(f\"Soma: {soma}\")\nprint(f\"Contagem: {contagem}\")\n```\n\n---\n\n## Explorando Outros Métodos e Atributos do pandas\n\n### 1. Método `describe`\n\nO método `describe` fornece um resumo estatístico da série.\n\n```python\nresumo  idades_series.describe()\nprint(resumo)\n```\n\n### 2. Atributo `shape`\n\nO atributo `shape` retorna as dimensões da série.\n\n```python\nforma  idades_series.shape\nprint(f\"Forma da Série: {forma}\")\n```\n\n### 3. Índices e `iloc`\n\nCada elemento em uma série tem um índice associado, que pode ser acessado diretamente:\n\n```python\n# Acessando valor pelo índice\nvalor_no_indice_2  idades_series[2]\nprint(f\"Valor no índice 2: {valor_no_indice_2}\")\n\n# Usando iloc para acessar pela posição\nvalor_posicao_1  idades_series.iloc[1]\nprint(f\"Valor na posição 1: {valor_posicao_1}\")\n```\n\n### 4. Nomeando Séries\n\nPodemos dar um nome à série usando o atributo `name`:\n\n```python\nidades_series.name  \"Idades dos Participantes\"\nprint(idades_series)\n```\n\n---\n\n## Conclusão: DataFrame é um Conjunto de Séries\n\nUm **DataFrame** no pandas é uma coleção de **Séries**. Cada coluna de um DataFrame é uma Série, e as linhas compartilham o mesmo índice. Vamos criar um DataFrame a partir de Séries:\n\n```python\nserie_nomes  pd.Series([\"Ana\", \"Bruno\", \"Carla\", \"David\"])\nserie_sobrenomes  pd.Series([\"Silva\", \"Oliveira\", \"Santos\", \"Costa\"])\nserie_idades  pd.Series([23, 29, 31, 25])\n\ndf_pessoas  pd.DataFrame({\n    \"nome\": serie_nomes,\n    \"sobrenome\": serie_sobrenomes,\n    \"idade\": serie_idades\n})\n\nprint(df_pessoas)\n```\n\n### Acessando Séries de um DataFrame\n\nPodemos acessar uma coluna específica (Série) de um DataFrame da seguinte forma:\n\n```python\nserie_nomes_do_df  df_pessoas[\"nome\"]\nprint(serie_nomes_do_df)\n```\n\n### Aula: Manipulação de DataFrames com Pandas e Integração de Múltiplos Arquivos CSV\n\nNesta aula, vamos trabalhar com três arquivos CSV de **vendas** e um arquivo CSV de **clientes**, aplicando as operações mais comuns do pandas, como **leitura**, **join**, **agregação**, **filtragem** e **salvamento**. O foco será entender como integrar múltiplas fontes de dados e fazer análises.\n\n#### Arquivos CSV:\n- **clientes.csv**: Arquivo contendo a dimensão dos clientes, com 10 linhas e informações pessoais dos clientes.\n- **vendas.csv**: Arquivo contendo 100 vendas.\n- **vendas_2.csv**: Segundo arquivo de vendas com mais 100 vendas.\n- **vendas_3.csv**: Terceiro arquivo de vendas com 100 vendas adicionais.\n\n### Estrutura dos Arquivos\n\n#### 1. **clientes.csv** (Dimensão)\nColunas:\n- `cliente_id`: Chave primária.\n- `nome`: Nome do cliente.\n- `sobrenome`: Sobrenome do cliente.\n- `cidade`: Cidade de residência.\n- `idade`: Idade do cliente.\n- `email`: Endereço de email.\n\n#### 2. **vendas.csv**, **vendas_2.csv**, **vendas_3.csv** (Fato)\nColunas:\n- `venda_id`: ID único da venda.\n- `cliente_id`: Chave estrangeira para relacionar com a tabela de clientes.\n- `produto`: Produto vendido.\n- `quantidade`: Quantidade vendida.\n- `valor_unitario`: Preço por unidade do produto.\n- `data_venda`: Data da venda.\n\n### Passo a Passo: Manipulação de DataFrames com Pandas\n\n#### Passo 1: Leitura dos Arquivos CSV\n\nVamos começar lendo os três arquivos de vendas e o arquivo de clientes usando `pd.read_csv()`.\n\n```python\nimport pandas as pd\n\n# Leitura dos CSVs de clientes e vendas\ndf_clientes  pd.read_csv('clientes.csv')\ndf_vendas  pd.read_csv('vendas.csv')\ndf_vendas_2  pd.read_csv('vendas_2.csv')\ndf_vendas_3  pd.read_csv('vendas_3.csv')\n\n# Exibindo as primeiras linhas de cada DataFrame\nprint(df_clientes.head())\nprint(df_vendas.head())\nprint(df_vendas_2.head())\nprint(df_vendas_3.head())\n```\n\n#### Passo 2: Unindo os Arquivos de Vendas\n\nAgora, vamos combinar os três DataFrames de vendas em um único DataFrame usando `pd.concat()`. Isso vai consolidar todas as 300 vendas.\n\n```python\n# Unindo as três tabelas de vendas\ndf_todas_vendas  pd.concat([df_vendas, df_vendas_2, df_vendas_3])\n\n# Exibindo o DataFrame combinado\nprint(df_todas_vendas.shape)  # Verificar o número total de vendas (deve ser 300)\nprint(df_todas_vendas.head())\n```\n\n#### Passo 3: Fazendo o Join entre Vendas e Clientes\n\nAgora que temos um DataFrame com todas as vendas, vamos juntar (fazer o **join**) com a tabela de clientes usando a coluna `cliente_id`, que está presente em ambos os DataFrames.\n\n```python\n# Fazendo o join entre o DataFrame de vendas e o de clientes\ndf_completo  pd.merge(df_todas_vendas, df_clientes, on\"cliente_id\")\n\n# Exibindo o DataFrame resultante\nprint(df_completo.head())\n```\n\n#### Passo 4: Análise Exploratória\n\nAgora que temos o DataFrame completo, vamos fazer algumas análises:\n\n- **Número total de vendas**:\n```python\ntotal_vendas  df_completo['venda_id'].count()\nprint(f\"Total de vendas: {total_vendas}\")\n```\n\n- **Valor total vendido**:\nVamos calcular o valor total das vendas (quantidade * valor_unitário):\n\n```python\ndf_completo['valor_total']  df_completo['quantidade'] * df_completo['valor_unitario']\nvalor_total  df_completo['valor_total'].sum()\nprint(f\"Valor total vendido: {valor_total}\")\n```\n\n- **Média do valor por venda**:\n```python\nmedia_venda  df_completo['valor_total'].mean()\nprint(f\"Média por venda: {media_venda}\")\n```\n\n#### Passo 5: Agrupamento e Resumo de Vendas\n\nVamos agrupar as vendas por cliente e calcular o total de vendas por cada cliente:\n\n- **Total de vendas por cliente**:\n```python\nvendas_por_cliente  df_completo.groupby('nome')['valor_total'].sum()\nprint(vendas_por_cliente)\n```\n\n- **Número de vendas por produto**:\n```python\nvendas_por_produto  df_completo.groupby('produto')['venda_id'].count()\nprint(vendas_por_produto)\n```\n\n#### Passo 6: Filtragem dos Dados\n\nAgora, vamos filtrar o DataFrame para analisar as vendas de um cliente específico ou as vendas realizadas em uma cidade específica.\n\n- **Vendas de clientes da cidade de São Paulo**:\n```python\nvendas_sp  df_completo[df_completo['cidade']  'São Paulo']\nprint(vendas_sp)\n```\n\n- **Vendas de um cliente específico (por exemplo, 'Ana')**:\n```python\nvendas_ana  df_completo[df_completo['nome']  'Ana']\nprint(vendas_ana)\n```\n\n#### Passo 7: Ordenação das Vendas\n\nVamos ordenar as vendas pelo maior valor total de venda:\n\n```python\nvendas_ordenadas  df_completo.sort_values(by'valor_total', ascendingFalse)\nprint(vendas_ordenadas.head())\n```\n\n#### Passo 8: Salvando o DataFrame Resultante\n\nApós todas as manipulações, podemos salvar o DataFrame resultante em um novo arquivo CSV para análise futura:\n\n```python\ndf_completo.to_csv('vendas_completo.csv', indexFalse)\n```\n\n### Resumo das Operações Realizadas:\n\n- **Leitura de CSVs**: Importação dos dados de clientes e vendas.\n- **Concatenação**: Unindo múltiplos DataFrames de vendas em um único DataFrame.\n- **Merge/Join**: Juntando os dados de clientes com as vendas através da coluna `cliente_id`.\n- **Cálculos**: Realizando operações de soma, média e agregações sobre as vendas.\n- **Agrupamento**: Agrupando vendas por cliente e por produto.\n- **Filtragem**: Selecionando dados com base em critérios, como cidade ou cliente.\n- **Ordenação**: Ordenando os dados para identificar as maiores vendas.\n- **Salvamento**: Exportando o DataFrame resultante para um novo CSV.\n\nSim, o pandas permite salvar os dados diretamente em um banco de dados relacional usando a função **`to_sql()`**, que possibilita gravar os dados de um **DataFrame** em tabelas de um banco de dados. Para isso, você pode utilizar bibliotecas como **SQLAlchemy** para criar uma conexão com o banco de dados.\n\nAqui está um exemplo básico de como você pode salvar um DataFrame em um banco de dados usando pandas:\n\n### 1. Instale as Dependências\n\nPrimeiro, você precisa instalar o **SQLAlchemy** e o conector específico para o banco de dados que você está usando (por exemplo, **psycopg2** para PostgreSQL, **PyMySQL** para MySQL, etc.).\n\n```bash\npip install sqlalchemy\npip install psycopg2  # Para PostgreSQL\npip install pymysql   # Para MySQL\n```\n\n### 2. Código para Salvar o DataFrame em um Banco de Dados\n\nVamos usar o **SQLAlchemy** para criar uma conexão com o banco de dados e, em seguida, usar a função **`to_sql()`** do pandas para salvar os dados.\n\n#### Exemplo: Salvando em um Banco de Dados PostgreSQL\n\n```python\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n# Supondo que df_completo seja o DataFrame resultante que você quer salvar\n# Configurar a conexão com o banco de dados\nengine  create_engine('postgresql+psycopg2://username:password@localhost:5432/nome_do_banco')\n\n# Salvando o DataFrame 'df_completo' na tabela 'vendas_completo' do banco de dados\ndf_completo.to_sql('vendas_completo', engine, if_exists'replace', indexFalse)\n\nprint(\"Dados salvos no banco de dados com sucesso!\")\n```\n\n### Explicação:\n\n- **`create_engine()`**: Cria a conexão com o banco de dados usando a string de conexão. Você precisa ajustar o `username`, `password`, `localhost`, `5432` (porta) e `nome_do_banco` para refletir suas credenciais e configurações do banco.\n  \n- **`to_sql()`**:\n  - O primeiro argumento é o nome da tabela onde os dados serão inseridos (`vendas_completo`).\n  - O segundo argumento é a conexão (`engine`).\n  - `if_exists'replace'` indica que, se a tabela já existir, ela será substituída. Você pode usar `'append'` para adicionar dados a uma tabela já existente.\n  - `indexFalse` evita que o índice do DataFrame seja salvo como uma coluna na tabela.\n\nBootcamp - Python com Laennder/Aula_02/csv_to_parquet.py\n\nimport pandas as pd\n\n# Leitura do arquivo CSV\ndf  pd.read_csv('vendas.csv')\n\n# Salvando o DataFrame como arquivo Parquet\ndf.to_parquet('vendas.parquet')\n\n\nBootcamp - Python com Laennder/Aula_02/parquet_to_csv.py\n\nimport pandas as pd\n\n# Leitura do arquivo Parquet\ndf  pd.read_parquet('vendas.parquet')\n\n# Salvando o DataFrame como arquivo CSV\ndf.to_csv('vendas.csv', indexFalse)\n\n\nBootcamp - Python com Laennder/Aula_02/requirements.txt\n\ncramjam2.8.4\nfastparquet2024.5.0\nfsspec2024.9.0\ngreenlet3.1.1\nnumpy2.1.1\npackaging24.1\npandas2.2.3\npsycopg22.9.9\npython-dateutil2.9.0.post0\npytz2024.2\nsix1.16.0\nSQLAlchemy2.0.35\ntyping_extensions4.12.2\ntzdata2024.2\n\n\nBootcamp - Python com Laennder/Aula_02/exercicios/readme.md\n\n### Exercício: Cálculo de Estatísticas com Python Puro e pandas\n\nNeste exercício, você vai trabalhar com uma lista de 20 itens e calcular **média**, **desvio padrão**, **máximo** e **mínimo** tanto usando Python puro quanto usando **pandas**.\n\n#### Lista de números:\n```python\nnumeros  [23, 45, 67, 89, 12, 34, 56, 78, 90, 21, 43, 65, 87, 32, 54, 76, 98, 10, 30, 50]\n```\n\n### Passo 1: Cálculos com Python Puro\n\n1. **Média**: Soma dos elementos dividida pelo número de elementos.\n2. **Desvio Padrão**: Raiz quadrada da variância.\n3. **Máximo**: Maior valor da lista.\n4. **Mínimo**: Menor valor da lista.\n\n#### Cálculos com Python Puro:\n\n```python\n\nnumeros  [23, 45, 67, 89, 12, 34, 56, 78, 90, 21, 43, 65, 87, 32, 54, 76, 98, 10, 30, 50]\n\n# Função para calcular a média\ndef calcular_media(lista):\n    soma  0\n    for valor in lista:\n        soma + valor\n    media  soma / len(lista)\n    return media\n\n# Função para calcular a variância\ndef calcular_variancia(lista):\n    media  calcular_media(lista)\n    soma_diferencas  0\n    for valor in lista:\n        diferenca  valor - media\n        soma_diferencas + diferenca ** 2\n    variancia  soma_diferencas / len(lista)\n    return variancia\n\n# Função para calcular o desvio padrão\ndef calcular_desvio_padrao(lista):\n    variancia  calcular_variancia(lista)\n    desvio_padrao  variancia ** 0.5\n    return desvio_padrao\n\n# Cálculos\nmedia  calcular_media(numeros)\ndesvio_padrao  calcular_desvio_padrao(numeros)\nmaximo  max(numeros)\nminimo  min(numeros)\n\n# Exibindo os resultados\nprint(f\"Média: {media}\")\nprint(f\"Desvio Padrão: {desvio_padrao}\")\nprint(f\"Máximo: {maximo}\")\nprint(f\"Mínimo: {minimo}\")\n```\n\n### Saída Esperada:\n\n```plaintext\nMédia: 52.95\nDesvio Padrão: 26.715776024038615\nMáximo: 98\nMínimo: 10\n```\n\n### Passo 2: Cálculos com pandas Series\n\nAgora, vamos realizar os mesmos cálculos usando uma **Série** do pandas. A vantagem do pandas é que essas operações já são implementadas como métodos da Série, o que simplifica o código.\n\n#### Cálculos com pandas:\n\n```python\nimport pandas as pd\n\n# Criando uma Série com os números\nserie_numeros  pd.Series(numeros)\n\n# Cálculos usando pandas\nmedia_pandas  serie_numeros.mean()\ndesvio_padrao_pandas  serie_numeros.std()\nmaximo_pandas  serie_numeros.max()\nminimo_pandas  serie_numeros.min()\n\n# Exibindo os resultados\nprint(f\"Média (pandas): {media_pandas}\")\nprint(f\"Desvio Padrão (pandas): {desvio_padrao_pandas}\")\nprint(f\"Máximo (pandas): {maximo_pandas}\")\nprint(f\"Mínimo (pandas): {minimo_pandas}\")\n```\n\n### Saída Esperada:\n\n```plaintext\nMédia (pandas): 52.95\nDesvio Padrão (pandas): 27.209487879199906\nMáximo (pandas): 98\nMínimo (pandas): 10\n```\n\n### Comparação dos Resultados\n\n- **Média** e **Máximo** e **Mínimo** devem ser iguais em ambos os casos.\n- O **desvio padrão** pode diferir ligeiramente, porque o pandas, por padrão, calcula o desvio padrão amostral (dividido por `n-1`), enquanto o cálculo manual faz o desvio padrão populacional (dividido por `n`). Você pode ajustar o pandas para calcular o desvio padrão populacional usando o argumento `ddof0`:\n\n```python\ndesvio_padrao_pandas  serie_numeros.std(ddof0)\n```\n\nEste exercício demonstra a flexibilidade do pandas para realizar cálculos estatísticos rapidamente e a importância de entender como as fórmulas são aplicadas tanto manualmente quanto com bibliotecas especializadas.\n\n",
        "Bootcamp - Python com Laennder/Aula_05/README.md\n\n# Projeto ETL com Python e Scrapy - Coleta de Dados do Mercado Livre\n\nEste projeto demonstra como construir um pipeline ETL utilizando Python e Scrapy para coletar dados do Mercado Livre, processá-los e armazená-los em um banco de dados SQL.\n\n## **Índice**\n\n- [Projeto ETL com Python e Scrapy - Coleta de Dados do Mercado Livre](#projeto-etl-com-python-e-scrapy---coleta-de-dados-do-mercado-livre)\n  - [**Índice**](#índice)\n  - [**Pré-requisitos**](#pré-requisitos)\n  - [**Instalação**](#instalação)\n    - [**1. Instale o Python**](#1-instale-o-python)\n    - [**2. Crie um Ambiente Virtual**](#2-crie-um-ambiente-virtual)\n    - [**3. Instale o Scrapy**](#3-instale-o-scrapy)\n  - [**Inicialização do Projeto Scrapy**](#inicialização-do-projeto-scrapy)\n  - [**Criação do Spider para o Mercado Livre**](#criação-do-spider-para-o-mercado-livre)\n    - [**Editar o Spider**](#editar-o-spider)\n  - [**Execução do Spider**](#execução-do-spider)\n  - [**Processamento e Armazenamento dos Dados**](#processamento-e-armazenamento-dos-dados)\n    - [**1. Configurar o Pipeline**](#1-configurar-o-pipeline)\n    - [**2. Ativar o Pipeline**](#2-ativar-o-pipeline)\n  - [**Recursos Adicionais**](#recursos-adicionais)\n  - [**Observações Finais**](#observações-finais)\n\n---\n\n## **Pré-requisitos**\n\n- **Python 3.12+**\n- **Pip** (gerenciador de pacotes do Python)\n- **Bibliotecas Python:**\n  - Scrapy\n  - Pandas\n  - Streaming\n\n## **Instalação**\n\n### **1. Instale o Python**\n\nVerifique se o Python está instalado:\n\n```bash\npython --version\n```\n\nSe não estiver instalado, faça o download e instale a partir do [site oficial do Python](https://www.python.org/downloads/).\n\n### **2. Crie um Ambiente Virtual**\n\nÉ recomendado usar um ambiente virtual para isolar as dependências do projeto.\n\n```bash\npython -m venv venv\n```\n\nAtive o ambiente virtual:\n\n- **Windows:**\n\n  ```bash\n  venv\\Scripts\\activate\n  ```\n\n- **Linux/MacOS:**\n\n  ```bash\n  source venv/bin/activate\n  ```\n\n### **3. Instale o Scrapy**\n\nCom o ambiente virtual ativado, instale o Scrapy usando o pip:\n\n```bash\npip install scrapy\n```\n\nVerifique se a instalação foi bem-sucedida:\n\n```bash\nscrapy --version\n```\n\n---\n\n## **Inicialização do Projeto Scrapy**\n\nDentro do diretório do seu projeto, inicialize um novo projeto Scrapy:\n\n```bash\nscrapy startproject mercado_livre\n```\n\nIsso criará a seguinte estrutura de diretórios:\n\n```\nmercado_livre/\n    scrapy.cfg\n    mercado_livre/\n        __init__.py\n        items.py\n        middlewares.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n```\n\n---\n\n## **Criação do Spider para o Mercado Livre**\n\nNavegue até o diretório `mercado_livre/spiders`:\n\n```bash\ncd mercado_livre/mercado_livre/spiders\n```\n\nCrie um novo Spider chamado `mercado_spider`:\n\n```bash\nscrapy genspider mercado_spider mercadolivre.com.br\n```\n\n### **Editar o Spider**\n\nAbra o arquivo `mercado_spider.py` e edite-o conforme necessário.\n\n**Exemplo de código:**\n\n```python\nimport scrapy\n\nclass MercadoSpider(scrapy.Spider):\n    name  'mercado_spider'\n    allowed_domains  ['mercadolivre.com.br']\n    start_urls  ['https://lista.mercadolivre.com.br/livros#D[A:livros]']\n\n    def parse(self, response):\n        for product in response.css('li.ui-search-layout__item'):\n            yield {\n                'titulo': product.css('h2.ui-search-item__title::text').get(),\n                'preco': product.css('span.price-tag-fraction::text').get(),\n                'link': product.css('a.ui-search-link::attr(href)').get(),\n            }\n\n        next_page  response.css('a.andes-pagination__link--next::attr(href)').get()\n        if next_page:\n            yield response.follow(next_page, self.parse)\n```\n\n**Notas:**\n\n- Certifique-se de que os seletores CSS correspondam à estrutura atual do site do Mercado Livre.\n- A estrutura do site pode mudar, portanto, verifique os elementos usando o inspetor do navegador.\n\n---\n\n## **Execução do Spider**\n\nPara executar o spider e salvar os dados coletados em um arquivo JSON:\n\n```bash\nscrapy crawl mercado_spider -o produtos.json\n```\n\n---\n\n## **Processamento e Armazenamento dos Dados**\n\n### **1. Configurar o Pipeline**\n\nAbra o arquivo `pipelines.py` e edite-o para processar e armazenar os dados.\n\n**Exemplo de código:**\n\n```python\nimport sqlite3\n\nclass MercadoLivrePipeline:\n\n    def open_spider(self, spider):\n        self.connection  sqlite3.connect('mercado_livre.db')\n        self.cursor  self.connection.cursor()\n        self.cursor.execute('''\n            CREATE TABLE IF NOT EXISTS produtos (\n                titulo TEXT,\n                preco TEXT,\n                link TEXT\n            )\n        ''')\n        self.connection.commit()\n\n    def close_spider(self, spider):\n        self.connection.close()\n\n    def process_item(self, item, spider):\n        self.cursor.execute('''\n            INSERT INTO produtos (titulo, preco, link) VALUES (?, ?, ?)\n        ''', (\n            item.get('titulo'),\n            item.get('preco'),\n            item.get('link')\n        ))\n        self.connection.commit()\n        return item\n```\n\n### **2. Ativar o Pipeline**\n\nNo arquivo `settings.py`, ative o pipeline:\n\n```python\nITEM_PIPELINES  {\n    'mercado_livre.pipelines.MercadoLivrePipeline': 300,\n}\n```\n\n---\n\n## **Recursos Adicionais**\n\n- [Documentação Oficial do Scrapy](https://docs.scrapy.org/en/latest/)\n- [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)\n- [Seletores CSS](https://www.w3schools.com/cssref/css_selectors.asp)\n- [Uso de XPath no Scrapy](https://docs.scrapy.org/en/latest/topics/selectors.html#working-with-xpath)\n\n---\n\n## **Observações Finais**\n\n- Certifique-se de respeitar os termos de uso do site do Mercado Livre.\n- Evite sobrecarregar o servidor com muitas requisições simultâneas.\n- Use `DOWNLOAD_DELAY` no `settings.py` para adicionar um atraso entre as requisições:\n\n```python\nDOWNLOAD_DELAY  1  # atraso de 1 segundo\n```\n\n"
    ],
    "00-automacao-data-qualiy-excel-etl": [
        "00-automacao-data-qualiy-excel-etl/README.md\n\n# Automação Data Quality Excel Etl\n\nSeja bem vindo ao Projeto Automação Data Quality Excel Etl\n\nNão perca [nosso workshop aberto](https://www.jornadadedados2024.com.br/) no dia 24/01\n\nVamos fazer todo o código desse projeto ao vivo\n\n## Conteúdo\n\nEsse repositório faz parte da Jornada de Dados 2024 \n\nPara saber mais [acesse aqui](../README.md)\n\n- [Automação Data Quality Excel Etl](#automação-data-quality-excel-etl)\n  - [Conteúdo](#conteúdo)\n  - [Você sabe o que faz um engenheiro de dados?](#você-sabe-o-que-faz-um-engenheiro-de-dados)\n  - [Ele ainda trabalha com todas essas tecnologias](#ele-ainda-trabalha-com-todas-essas-tecnologias)\n  - [Isso explica melhor](#isso-explica-melhor)\n  - [Te convido a criar uma solução](#te-convido-a-criar-uma-solução)\n  - [Mais 500 alunos em 2023 aprovaram com 92% de satisfação](#mais-500-alunos-em-2023-aprovaram-com-92-de-satisfação)\n  - [Não perca](#não-perca)\n  - [Objetivo](#objetivo)\n  - [Rodando o projeto](#rodando-o-projeto)\n  - [Possui dúvidas?](#possui-dúvidas)\n  - [Agora para o projeto?](#agora-para-o-projeto)\n\n## Você sabe o que faz um engenheiro de dados?\n\n![Figura01](./pics/radar-600x333.png)\n\nÉ o responsável por criar plataformas e pipeline de dados com qualidade\n\n## Ele ainda trabalha com todas essas tecnologias\n\n![Figura01](./pics/lista_de_tecnologias.png)\n\nMas acho que…\n\n## Isso explica melhor\n\n![Figura01](./pics/meme_barbie.png)\n\n## Te convido a criar uma solução\n\nDe engenharia de dados ao vivo comigo dia 24/01 às 20h: “Do Zero ao Deploy”\n\n![Figura01](./pics/arquitetura.png)\n\nVamos iniciar o seu portfólio?\n\n## Mais 500 alunos em 2023 aprovaram com 92% de satisfação\n\n![Figura01](./pics/workshop.png)\n\n## Não perca \n\n[Nosso workshop aberto](https://www.jornadadedados2024.com.br/) no dia 24/01\n\n![Figura01](./pics/cadastro.png)\n\n## Objetivo\n\n* Testes com Pytest e Selenium\n* Documentando com Mkdcos\n* O resto é codando em Python e tomando Coca-Cola\n\n## Rodando o projeto\n\n```bash\ngit clone\ncd \n```\n\n## Possui dúvidas? \n\n- Fale comigo [Link do Linkedin](https://www.linkedin.com/in/lucianovasconcelosf/)\n\n## Agora para o projeto?\n\n- Toda a documentação do projeto é feita usando Mkdocs\n\n\n\n\n00-automacao-data-qualiy-excel-etl/mkdocs.yml\n\nsite_name: \"My Library\"\n\ntheme:\n  name: \"material\"\n\nplugins:\n- search\n- mkdocstrings\n\n00-automacao-data-qualiy-excel-etl/pyproject.toml\n\n[tool.poetry]\nname  \"00-automacao-data-qualiy-excel-etl\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"lvgalvaofilho\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\npydantic  {extras  [\"email\"], version  \"^2.8.2\"}\nstreamlit  \"^1.37.0\"\nopenpyxl  \"^3.1.5\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n[tool.taskipy.tasks]\ndocs  \"lsof -ti :8000 | xargs kill -9 | mkdocs serve\"\nrun  \"lsof -ti :8501 | xargs kill -9 | streamlit run src/app.py\"\ntest  \"lsof -ti :8501 | xargs kill -9 | pytest tests\"\n\n00-automacao-data-qualiy-excel-etl/requirements.txt\n\nmkdocs1.5.3\nstreamlit1.30.0\ntaskipy1.12.2\npytest7.4.4\nselenium4.16.0\nopenpyxl3.1.2\npydantic[email]2.5.3\n\n00-automacao-data-qualiy-excel-etl/.python-version\n\n3.12\n\n\n00-automacao-data-qualiy-excel-etl/docs/app.md\n\n## Módulo `app.py`\n\n**Função `main`**\n\n::: src.app.main\n    options:\n      show_root_heading: false\n\n00-automacao-data-qualiy-excel-etl/docs/backend.md\n\n## Módulo `backend.py`\n\n**Função `process_excel`**\n\n::: src.backend.process_excel\n\n00-automacao-data-qualiy-excel-etl/docs/frontend.md\n\n## Módulo frontend.py\n\n::: src.frontend.ExcelValidatorUI\n    handler: python\n    options:\n      members:\n        - set_page_config\n      show_root_heading: false\n      show_source: false\n\n\n",
        "00-automacao-data-qualiy-excel-etl/docs/index.md - Parte (1/2)\n00-automacao-data-qualiy-excel-etl/docs/index.md\n\n# Fluxo do Workshop\n\nFique calmo, tudo vai dar certo.\n\n![Imagem](./pics/config_01.png)\n\nTem tudo em detalhes aqui no [Data Project Starter Kit](https://github.com/lvgalvao/DataProjectStarterKit)\n\n## 1) Configuração inicial\n\n#### 1) Vamos criar um novo projeto no Git e Github\n  \n- Acessar site Github e criar nossa pasta\n- Fazendo um teste simples para ver se tudo está bem\n- Criando um README\n\n```bash\ntouch README.md\n```\n\n- Salvando ele\n\n```bash\ngit add .\ngit commit -m \"docs: adicionando arquivo README\"\ngit push --set-upstream origin main\n```\n- Deletando ele\n```bash\ngit add .\ngit commit -m \"docs: remover arquivo README\"\ngit push\n```\n\n- Recuperando ele\n\n```bash\ngit log\ngit checkout 3eab9123874b4ec51b0ab6d103a9483f2250c23f -- README.md\ngit add .\ngit push\n```\n  \n#### 2) Vamos definir nossa versão do Python usando o Pyenv\n\n```bash\npython --versions\npyenv versions\npyenv local 3.11.5\n```\n\n#### 3) Vamos criar nosso ambiente virtual\nPara criar o ambiente virtual, abra o terminal dentro da pasta criada e faça:\n\n```bash\npython -m venv nome_do_ambiente_virtual \n# O padrao é utilizar .venv\nsource nome_do_ambiente_virtual/bin/activate\n# Usuários Linux e mac\nnome_do_ambiente_virtual\\Scripts\\Activate\n# Usuários Windows\n```\n\n#### 4) Instalando uma biblioteca\n\n```bash\npip install selenium\n```\n\n#### 4) Replicando ambientes\n\nAgora, se quisermos rodar o nosso projeto em outra máquina, não será necessário baixar as dependências uma a uma, basta fazer:\n\n```bash\npip freeze > requirements.txt\npip install -r requirements.txt  \n```\n\n#### 5) Desativando o ambiente virtual\n\nE por fim, para desativar o ambiente virtual:\n```bash\ndeactive\n```\n\n#### 6) Criando o .gitignore\n\n```bash\ntouch .gitignore\n```\n\n[Site com exemplo de arquivo](https://www.toptal.com/developers/gitignore/api/python)\n\n#### 7) Melhorando nosso README\n\n```md\n\n    ### Instalação e Configuração\n\n    1. Clone o repositório:\n\n    ```bash\n    git clone https://github.com/lvgalvao/dataprojectstarterkit.git\n    cd dataprojectstarterkit\n    ```\n\n    2. Configure a versão correta do Python com `pyenv`:\n\n    ```bash\n    pyenv install 3.11.5\n    pyenv local 3.11.5\n    ```\n\n    3. Instale as dependências do projeto:\n\n    ```bash\n    python -m venv .venv\n    # O padrao é utilizar .venv\n    source .venv/bin/activate\n    # Usuários Linux e mac\n    .venv\\Scripts\\Activate\n    # Usuários Windows\n    pip install -r requirements.txt  \n    ```\n```\n\n## 2) Precisamos falar de testes\n\nDiferença entre fases de teste, tipos de teste e formas de execução. Hoje em dia há muita confusão quando se fala em fases de teste, tipos de teste e formas de execução. \n\nSe você, assim como eu, já ouviu as frases a seguir várias vezes, então esse [artigo é para você!](https://www.zup.com.br/blog/tipos-de-teste)\n\n- “Fulano sabe teste funcional e não automatizado”;\n- “desenvolva o teste unitário antes dos funcionais”; \n- “precisamos que os testes sejam 100% automatizados”; \n- “cadê a massa de dados para os testes de contrato?”\n\n### Pirâmide de teste\nUma maneira mais visual de exemplificar um pouco sobre as fases de teste e os tipos de teste que cada fase contempla, é a pirâmide de automação de teste.\n\n![Imagem](./pics/testes.png)\n\n#### 1) Criando nosso primeiro teste\n\n```bash\npip install pytest\npip install selenium\n```\n\n[Como instalar o webdriver](https://medium.com/@wmonteiro/executando-o-selenium-com-o-python-em-windows-c876bc60bf99)\n\n![Imagem](./pics/TDD_1.jpg)\n\nVamos criar nosso arquivo de teste\n\n```bash\nmkdir tests\ncd tests\ntouch test_app.py\n```\n\narquivo `test_app.py`\n```python\nfrom selenium import webdriver\nfrom selenium.common.exceptions import TimeoutException\nfrom time import sleep\n\n# Precismaos definir qual driver vamos utilizar\ndriver  webdriver.Firefox()\n\n# Define um timeout implícito\ndriver.set_page_load_timeout(5)  # 5 segundos\n\n# Vamos fazer uma tratativa de try-except de entrar na nossa página\ntry:\n    driver.get(\"http://localhost:8501\")\n    sleep(5)\n    print(\"Acessou a página com sucesso\")\nexcept TimeoutException:\n    print(\"Tempo de carregamento da página excedeu o limite.\")\nfinally:\n    driver.quit()\n```\n\nAgora que já temos nosso teste vamos desenvolver nosso primeiro código\n\nPara isso vamos trabalhar com o *streamlit*\n\n![streamlit](./pics/streamlit.png)\n\nInstalando o streamlit\n\n```bash\npip install streamlit\n```\n\nVamos fazer o nosso Hello World\n\n```bash\nmkdir src\ncd src\ntouch app.py\n```\n\narquivo `app.py`\n```python\nimport streamlit as st\n\n# Título do App\nst.title('Nosso Primeiro App com Streamlit')\n\n# Escrevendo um Hello World com markdown\nst.markdown('**Hello world!** 🌍')\n\n# Escrevendo texto\nst.write('Esta é uma demonstração de algumas funcionalidades do Streamlit.')\n\n# Input de texto do usuário\ninput_texto  st.text_input('Digite algo aqui:')\n\n# Mostrando o texto digitado\nst.write(f'Você digitou: {input_texto}')\n\n# Slider para números\nnumero  st.slider('Escolha um número', 0, 100, 50)\n\n# Exibir o número escolhido\nst.write(f'Você escolheu o número: {numero}')\n\n# Gráfico de barras simples\nimport pandas as pd\nimport numpy as np\n\n# Criando dados aleatórios\ndados  pd.DataFrame({\n  'colunas': ['A', 'B', 'C', 'D', 'E'],\n  'valores': np.random.randn(5)\n})\n```\n\n### Temos nosso frontend /o/\n\n### Agora vamos para uma tangente\n\nTemos um problema com nosso processo que muda de porta\n\nSempre que subimos uma nova aplicação ele está usando uma outra porta\n\nPrecisamos \"matar\" esse processo e reutilizar a porta 8501\n\nUsamos o comando lsof (List Open Files) para verificar os processos que estão conectados nessa porta\n\n```bash\nlsof -i :8501\n```\n\nDepois usamos o comando kill para matar esse processo\n\n```bash\nkill -9 [PID]\n```\n\nPodemos simplificar usando somente uma linha\n\n```bash\nlsof -ti :8501 | xargs kill -9\n```\n\nNo Windows, o comando `lsof` (List Open Files), que é comum em sistemas baseados em Unix como Linux e macOS, não está disponível. No entanto, você pode realizar uma tarefa similar para verificar quais processos estão usando uma porta específica (por exemplo, a porta 8501) usando o Resource Monitor ou comandos no Prompt de Comando. Aqui estão duas maneiras de fazer isso:\n\n### 1. Usando o Resource Monitor\n\n1. Pressione `Ctrl + Shift + Esc` para abrir o Gerenciador de Tarefas.\n2. Vá para a aba \"Desempenho\" e clique em \"Monitor de Recursos\" na parte inferior.\n3. No Resource Monitor, vá para a aba \"Rede\".\n4. Olhe na seção \"Portas de Escuta\" para encontrar a porta 8505 e veja quais processos estão associados a ela.\n\n### 2. Usando o Prompt de Comando\n\n1. Abra o Prompt de Comando como administrador (isso é necessário para executar comandos que acessam informações de rede).\n    \n2. Digite o seguinte comando:\n    \n    ```cmd\n    netstat -ano | findstr :8501\n    ```\n    \n    Esse comando lista todas as conexões e portas de escuta (`netstat -ano`) e filtra os resultados para mostrar apenas as entradas relacionadas à porta 8505 (`findstr :8501`).\n    \n3. Você verá uma lista de entradas, se houver alguma, mostrando o protocolo, endereço local, endereço estrangeiro, estado, e o PID (ID do Processo) associado à porta 8505.\n    \n4. Se você quiser saber qual aplicativo está associado a um PID específico, você pode encontrar este PID na aba \"Detalhes\" do Gerenciador de Tarefas.\nPara finalizar um processo em uma linha de comando no Windows, combinando a busca do processo pela porta e o encerramento do processo, você pode usar o PowerShell. O PowerShell é mais poderoso e flexível do que o Prompt de Comando tradicional para este tipo de operação. Aqui está como você pode fazer isso:\n\nAbra o PowerShell como administrador e execute o seguinte comando:\n\n```powershell\nGet-NetTCPConnection -LocalPort 8501 | Select-Object -ExpandProperty OwningProcess | ForEach-Object {Stop-Process -Id $_ -Force}\n```\n\nEste comando faz o seguinte:\n\n1. `Get-NetTCPConnection -LocalPort 8501`: Obtém todas as conexões TCP que estão escutando na porta 8501.\n    \n2. `Select-Object -ExpandProperty OwningProcess`: Seleciona os IDs dos processos (PID) que estão escutando naquela porta.\n    \n3. `ForEach-Object {Stop-Process -Id $_ -Force}`: Para cada PID encontrado, usa o `Stop-Process` para encerrar o processo. A opção `-Force` é usada para garantir que o processo seja encerrado.\n\n### Taskipy - Para não ficar toda essa quantidade de código, vamos usar o Taskipy\n\n![Taskipy](./pics/taskipy.png)\n\nBasicamente o Taskipy é um short de comandos\n\nVamos instalar ele com o comando\n\n```bash\npip install taskipy\n```\n\nCriar um arquivo de configuração\n\n```bash\ntouch pyproject.toml\n```\n\nE dentro desse arquivo `pyproject.toml` incluir os comandos que queremos\n\n```toml\n[tool.taskipy.tasks]\nrun  \"lsof -ti :8501 | xargs kill -9 | streamlit run src/app.py\"\n```\n\nAgora conseguimos simplificar e tornar nosso processo de rodar nossa aplicação mais rápido\n\n### Saindo da tangente\n\n```bash\npython tests/test_app.py\n```\n\nE temos o nosso primeiro teste passando!\n\nAgora temos duas opções.\n\n### Escrever um novo teste ou refatorar.\n\n## 3) Nossa primeira refatoração\n\nVamos melhorar os nossos testes usando o pytest\n\n1) Vamos criar uma função que inicia o nosso driver\n\n2) Vamos criar uma função que testa se o site está online\n\nPara nossa função vamos usar o módulo fixture do pytest\n\n```python\nimport pytest\nimport subprocess\nfrom selenium import webdriver\n\n@pytest.fixture\ndef driver():\n    # Iniciar o Streamlit em background\n    process  subprocess.Popen([\"streamlit\", \"run\", \"src/app.py\"])\n\n    # Iniciar o WebDriver usando GeckoDriver\n    driver  webdriver.Firefox()\n    driver.set_page_load_timeout(5)\n    yield driver\n\n    # Fechar o WebDriver e o Streamlit após o teste\n    driver.quit()\n    process.kill()\n\ndef test_app_opens(driver):\n    # Verificar se a página abre\n    driver.get(\"http://localhost:8501\")\n```\n\nAlém disso,\nPodemos incluir um comando novo no task\n\n```pyproject.toml\ntest  \"lsof -ti :8501 | xargs kill -9 | pytest tests -v\"\n```\n\n#### Nosso segundo teste\n\n\nVamos escrever um teste que cheque se o title ta página é `validador de schema excel`\n\nPara isso vamos criar mais um teste\n\n```python\ndef test_check_title_is(driver):\n    # Verificar se a página abre\n    driver.get(\"http://localhost:8501\")\n    # Verifica se o titulo de página é\n    sleep(5)\n    # Capturar o título da página\n    page_title  driver.title\n\n    # Verificar se o título da página é o esperado\n    expected_title  \"Validador de schema excel\"  # Substitua com o título real esperado\n    assert page_title  expected_title, f\"O título da página era '{page_title}', mas esperava-se '{expected_title}'\"\n```   \n\nVamos revisitar nossa aplicação também\n\n```python\nimport streamlit as st\n\n# Título do App\nst.title('Validador de schema excel')\n```\n\nNosso teste não passa (\n\nO motivo? \n\n**O streamlit e o selenium chamam coisas diferentes com o mesmo nome!**\n\n```python\nimport streamlit as st\n\nst.set_page_config(\n    page_title\"Validador de schema excel\"\n)\n```\n\n### 3) Terceira Feature\n\n#### Adicionar um texto no h1\n\ntest_app.py\n```python\nfrom selenium.webdriver.common.by import By\n\ndef test_check_streamlit_h1(driver):\n    # Acessar a página do Streamlit\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar para garantir que a página foi carregada\n    sleep(5)  # Espera 5 segundos\n\n    # Capturar o primeiro elemento <h1> da página\n    h1_element  driver.find_element(By.TAG_NAME, \"h1\")\n\n    # Verificar se o texto do elemento <h1> é o esperado\n    expected_text  \"Insira o seu excel para validação\"\n    assert h1_element.text  expected_text\n\n```\n\napp.py\n```python\nst.title(\"Insira o seu excel para validação\")\n```\n\n### 4) Agora vamos criar um teste que o usuário pode inserir um excel, e vai aparecer uma mensagem de sucesso\n\nVamos criar nossa nova função\n\n\ntest_app.py\n```python\ndef test_check_usuario_pode_inserir_um_excel_e_receber_uma_mensagem(driver):\n    # Acessar a página do Streamlit\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar para garantir que a página foi carregada\n    sleep(5)  # Espera 5 segundos\n\n    # Realizar o upload do arquivo de sucesso\n    success_file_path  os.path.abspath(\"data/arquivo_excel.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(success_file_path)\n\n    # Aguardar a mensagem de sucesso\n    sleep(5)\n    assert \"O schema do arquivo Excel está correto!\" in driver.page_source\n```\n\napp.py\n```python\narquivo  st.file_uploader(\"Carregue seu arquivo Excel aqui\", type[\"xlsx\"])\n\nif arquivo:\n    st.success(\"O schema do arquivo Excel está correto!\")\n```\n\nAgora vamos parar com nosso frontend e ir para a parte do backend\n\n## Pydantic\n\nUm exemplo de KPI\n\nVamos instalar o Pydantic\n\n```bash\npip install \"pydantic[email]\" openpyxl\n```\n\nCriar um arquivo de estes unitários `test_unit.py`\n\n```python\nimport pytest\nfrom datetime import datetime\nfrom src.contrato import Vendas, CategoriaEnum\nfrom pydantic import ValidationError\n\n# Testes com dados válidos\ndef test_vendas_com_dados_validos():\n    dados_validos  {\n        \"email\": \"comprador@example.com\",\n        \"data\": datetime.now(),\n        \"valor\": 100.50,\n        \"produto\": \"Produto X\",\n        \"quantidade\": 3,\n        \"categoria\": \"categoria3\",\n    }\n    \n    # A sintaxe **dados_validos é uma forma de desempacotamento de dicionários em Python. \n    # O que isso faz é passar os pares chave-valor no dicionário dados_validos como argumentos nomeados para o construtor da classe Vendas.\n\n    venda  Vendas(**dados_validos)\n    \n    assert venda.email  dados_validos[\"email\"]\n    assert venda.data  dados_validos[\"data\"]\n    assert venda.valor  dados_validos[\"valor\"]\n    assert venda.produto  dados_validos[\"produto\"]\n    assert venda.quantidade  dados_validos[\"quantidade\"]\n    assert venda.categoria  dados_validos[\"categoria\"]\n\n# Testes com dados inválidos\ndef test_vendas_com_dados_invalidos():\n    dados_invalidos  {\n        \"email\": \"comprador\",\n        \"data\": \"não é uma data\",\n        \"valor\": -100,\n        \"produto\": \"\",\n        \"quantidade\": -1,\n        \"categoria\": \"categoria3\"\n    }\n\n    with pytest.raises(ValidationError):\n        Vendas(**dados_invalidos)\n\n# Teste de validação de categoria\ndef test_validacao_categoria():\n    dados  {\n        \"email\": \"comprador@example.com\",\n        \"data\": datetime.now(),\n        \"valor\": 100.50,\n        \"produto\": \"Produto Y\",\n        \"quantidade\": 1,\n        \"categoria\": \"categoria inexistente\",\n    }\n\n    with pytest.raises(ValidationError):\n        Vendas(**dados)\n```\n\nCriar nosso arquivo de contrato `contrato.py`\n\nPorque temos contrato de software\"\n\nPydantic é um serializador de ORM + Json também\n\n```python\nprimeira_venda  {\n    \"email\": \"lvgalvaofilho\",\n    \"valor\": 50.50,\n}\n\ndef validador_email(venda):\n    if not isinstance(venda, str) or '@' not in venda:\n        raise ValueError(f\"Email invalido: {venda}\")\n    \ndef validador_de_valor(venda):\n    if not isinstance(venda, float) and venda > 0:\n        raise ValueError(f\"Valor não é valido: {venda}\")\n    \ntry:\n    validador_email(primeira_venda[\"email\"])\n    validador_de_valor(primeira_venda[\"valor\"])\n    print(\"Todos os dados são validos.\")\nexcept ValueError as e:\n    print(f\"Erro na validação {e}\")\n```\n\n```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any\n\nclass CategoriaEnum(Enum):\n    ELETRONICOS  'eletronicos'\n    ALIMENTOS  'alimentos'\n    VESTUARIO  'vestuario'\n\ndef validar_email(valor: Any) -> None:\n    if not isinstance(valor, str) or \"@\" not in valor:\n        raise ValueError(f\"Email inválido: {valor}\")\n\ndef validar_positive_float(valor: Any) -> None:\n    if not isinstance(valor, float) or valor < 0:\n        raise ValueError(f\"Valor inválido (deve ser um float positivo): {valor}\")\n\ndef validar_positive_int(valor: Any) -> None:\n    if not isinstance(valor, int) or valor < 0:\n        raise ValueError(f\"Quantidade inválida (deve ser um int positivo): {valor}\")\n\n@dataclass\nclass Vendas:\n    email: str  field(metadata{\"validate\": validar_email})\n    data: datetime\n    valor: float  field(metadata{\"validate\": validar_positive_float})\n    quantidade: int  field(metadata{\"validate\": validar_positive_int})\n    produto: str\n    categoria: CategoriaEnum\n\n    def __post_init__(self):\n        for field_name, field_def in self.__dataclass_fields__.items():\n            if 'validate' in field_def.metadata:\n                validator  field_def.metadata['validate']\n                valor  getattr(self, field_name)\n                validator(valor)\n                \n```\n\n```python\nfrom pydantic import BaseModel, EmailStr, Positi",
        "00-automacao-data-qualiy-excel-etl/docs/index.md - Parte (2/2)\nveFloat, PositiveInt, validator\nfrom datetime import datetime\nfrom enum import Enum\n\nclass CategoriaEnum(str, Enum):\n    categoria1  \"categoria1\"\n    categoria2  \"categoria2\"\n    categoria3  \"categoria3\"\n\n\nclass Vendas(BaseModel):\n\n    \"\"\"\n    Modelo de dados para as vendas.\n\n    Args:\n        email (str): email do comprador\n        data (datetime): data da compra\n        valor (int): valor da compra\n        produto (str): nome do produto\n        quantidade (int): quantidade de produtos\n        categoria (str): categoria do produto\n\n    \"\"\"\n    email: EmailStr\n    data: datetime\n    valor: PositiveFloat\n    quantidade: PositiveInt\n    categoria: CategoriaEnum\n\n    @validator('categoria')\n    def categoria_deve_estar_no_enum(cls, error):\n        return errore\n```\n\n# Nossos testes já passam /o/\n\n# Vamos refatorar nossa aplicação\n\nVamos segregar a lógica do frontend (streamlit)\n\nDo app.py\n\nVamos sair disso\n\n```python\nimport streamlit as st\n\nst.set_page_config(\n    page_title\"Validador de schema excel\"\n)\n\nst.title(\"Insira o seu excel para validação\")\n\narquivo  st.file_uploader(\"Carregue seu arquivo Excel aqui\", type[\"xlsx\"])\n\nif arquivo:\n    st.success(\"O schema do arquivo Excel está correto!\")\n```\npara isso\n\n```python\nfrom frontend import ExcelValidadorUI\nfrom backend import process_excel\n\ndef main():\n    ui  ExcelValidadorUI()\n    ui.display_header()\n\n    upload_file  ui.upload_file()\n\n    if upload_file:\n        result, error  process_excel(upload_file)\n        ui.display_results(result, error)\n\nif __name__  \"__main__\":\n    main()\n```\n# Vamos criar nosso backend\n\n```python\nimport pandas as pd\nfrom contrato import Vendas\n\ndef process_excel(uploaded_file):\n    try:\n        df  pd.read_excel(uploaded_file)\n\n        # Verificar se há colunas extras no DataFrame\n        extra_cols  set(df.columns) - set(Vendas.model_fields.keys())\n        if extra_cols:\n            return False, f\"Colunas extras detectadas no Excel: {', '.join(extra_cols)}\"\n\n        # Validar cada linha com o schema escolhido\n        for index, row in df.iterrows():\n            try:\n                _  Vendas(**row.to_dict())\n            except Exception as e:\n                raise ValueError(f\"Erro na linha {index + 2}: {e}\")\n\n        return True, None\n\n    except ValueError as ve:\n        return False, str(ve)\n    except Exception as e:\n        return False, f\"Erro inesperado: {str(e)}\"\n```\n\n# Vamos para o noss último teste!\n\nArquivo `test_app.py`\n```python\ndef test_failed_upload(driver):\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar um tempo para a aplicação carregar\n    sleep(5)\n\n    # Realizar o upload do arquivo de falha\n    failure_file_path  os.path.abspath(\"data/failure.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(failure_file_path)\n\n    # Aguardar a mensagem de erro\n    sleep(5)\n    assert \"Erro na validação\" in driver.page_source\n```\n\nVamos precisar mudar o nosso frontend\n\n```\n    def display_results(self):\n        return st.success(\"O schema do arquivo Excel está correto!\")        \n```\n\n```python\ndef display_results(self, result, error):\n    if error:\n        st.error(f\"Erro na validação: {error}\")\n    else:\n        st.success(\"O schema do arquivo Excel está correto!\")\n```\n\n# Nossa documentação\n\n```\nbash\npip install mkdocs mkdocstrings\n``` \n\nVamos inserir nossa documentação\n\nVamos revisitar nosso código e inserir as docstrings\n\n\nbackend.py\n```python\n\"\"\"\nProcessa um arquivo Excel, validando-o contra um esquema específico.\nArgs:\n    uploaded_file: Um arquivo Excel carregado pelo usuário.\nReturns:\n    Uma tupla (resultado, erro), onde 'resultado' é um booleano indicando se a validação\n    foi bem-sucedida e 'erro' é uma mensagem de erro se a validação falhar.\n\"\"\" \n```\n\nfrontend.py\n```python\n\"\"\"\nClasse responsável por gerar a interface de usuário para o validador de arquivos Excel.\n\"\"\"\n```\n\ncontrato.py\n```python\n\"\"\"\nModelo de dados para as vendas.\nArgs:\n    email (str): email do comprador\n    data (datetime): data da compra\n    valor (int): valor da compra\n    produto (str): nome do produto\n    quantidade (int): quantidade de produtos\n    categoria (str): categoria do produto\n\"\"\"\n```\n\n```bash\npip install mkdocs \"mkdocstrings[python]\" mkdocs-material\n```\n\n```bash\nmkdocs new\n```\n\nmkdocs.yml\n```\nsite_name: \"My Library\"\n\ntheme:\n  name: \"material\"\n\nplugins:\n- search\n- mkdocstrings\n```\n\n```\nmkdocs gh-deploy\n```\n\n\n# Aula 01\n\nO que é CI/CD?\n\nComeçando com nossa primeira pipeline\n\n```yml\nname: ci\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Primeiro passo\n        run: echo hello world!\n\n      - name: Segundo Passo\n        run: echo $(ls)\n\n      - name: Terceiro passo\n        run: echo $(pwd)\n```\n\nLegal né?\n\nRepara que no ls, não temos nada!\n\nAgora vamos \"puxar nosso código\"\n\nMostrar github actions marketplace\n\n```yml\n    steps:\n      - name: Copia os arquivos do repositório\n        uses: actions/checkout@v4\n```\n\nAgora vamos instalar Python\n\nMostrar github actions marketplace\n\n```yml\n    steps:\n      - name: Copia os arquivos do repositório\n        uses: actions/checkout@v4\n```\n\nInstalando o nosso Python\n\n```yml\n      - name: Instalar o Python  \n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11.5' \n```\n\nE se eu tenho mais de um Python?\n\nExemplo, O pandas ele roda do 3.9 até o 3.12\n\nComo fazer isso?\n\n```yml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.6, 3.7, 3.8, 3.9, 3.10, 3.11.5]\n\n    - name: Configurar Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ matrix.python-version }}\n```\n\nTornando nosso build mais rápido\n\n```yml\n      - name: Instalar Dependencias somente de testes\n        run: pip install -r requirements-test.txt\n```\n\nRodando os nossos testes\n\n```yml\n      - name: Instalar Poetry\n        run: pytest tests/test_unit.py\n```\n\nPosso definir branchs específicas\n\n```yml\non:\n  push:\n    branches: [ main ]\n  \n  pull_request:\n    branches: [ main ]\n```\n\n## 2) Removendo o push de main\n\n# Temos 1 bug\n\n# Precisamos visualizar mais de 2 erros\n\nVamos fazer nosso test\n\n```\ndef test_check_mais_de_uma_mensagem_de_erro(driver):\n    # Acessar a página do Streamlit\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar para garantir que a página foi carregada\n    sleep(3)  # Espera 5 segundos\n\n    # Realizar o upload do arquivo de sucesso\n    success_file_path  os.path.abspath(\"data/multiplos_erros.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(success_file_path)\n\n    # Aguardar a mensagem de sucesso\n    sleep(3)\n    # Localizar todas as ocorrências da mensagem de erro\n    error_messages  driver.find_elements(By.XPATH, \"//*[contains(text(), 'Erro na validação')]\")\n```\n\nmudando no UI\n```\n    def display_results(self, result, errors):\n        if errors:\n            for error in errors:\n                st.error(error)  # Exibe cada erro em uma linha separada\n        else:\n            st.success(\"O schema do arquivo Excel está correto!\")\n```\n\n```\n        # Validar cada linha com o schema escolhido\n        for index, row in df.iterrows():\n            try:\n                _  Vendas(**row.to_dict())\n            except Exception as e:\n                erros.append(f\"Erro na linha {index + 2}: {e}\")\n\n        return True, erros\n```\n\n```python\n```\nPara isso precisamos guardar os erros em uma lista\n\n# Salvando no banco\n\n- O que não vamos falar agora.\n- Docker e ORM\n\n-> Usar SQL direto e vou usar PANDAS para salvar no SQL\n\n- Quais testes quero fazer?\n\nVamos falar de um teste de integração\n\nÉ um teste que vai validar se algo fora do nosso sistema está funcionando corretamente\n\nalguns pontos\n\n`dotenv`\n\n`.env`\n\n`pandas.read_sql()`\n\n`test_integration.py`\n\n```python\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv(\".env\")\n\n# Lê as variáveis de ambiente\nPOSTGRES_USER  os.getenv('POSTGRES_USER')\nPOSTGRES_PASSWORD  os.getenv('POSTGRES_PASSWORD')\nPOSTGRES_HOST  os.getenv('POSTGRES_HOST')\nPOSTGRES_PORT  os.getenv('POSTGRES_PORT')\nPOSTGRES_DB  os.getenv('POSTGRES_DB')\n\n# Cria a URL de conexão com o banco de dados\nDATABASE_URL  f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n\ndef test_read_data_and_check_schema():\n    df  pd.read_sql('SELECT * FROM vendas', conDATABASE_URL)\n\n    # Verificar se o DataFrame não está vazio\n    assert not df.empty, \"O DataFrame está vazio.\"\n\n    # Verificar o schema (colunas e tipos de dados)\n    expected_dtype  {\n        'id': 'int64',\n        'email': 'object',  # object em Pandas corresponde a string em SQL\n        'data': 'datetime64[ns]',\n        'valor': 'float64',\n        'produto': 'object',\n        'quantidade': 'int64',\n        'categoria': 'object'\n    }\n\n    assert df.dtypes.to_dict()  expected_dtype, \"O schema do DataFrame não corresponde ao esperado.\"\n\n```\n\num teste_funcional\n\n`test_app.py`\n\n```python\ndef test_check_usuario_insere_um_excel_valido_e_aparece_um_botao(driver):\n    # Acessar a página do Streamlit\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar para garantir que a página foi carregada\n    sleep(3)  # Espera 3 segundos\n\n    # Realizar o upload do arquivo de sucesso\n    success_file_path  os.path.abspath(\"data/correto.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(success_file_path)\n\n    # Aguardar a mensagem de sucesso\n    sleep(3)\n    assert \"O schema do arquivo Excel está correto!\" in driver.page_source\n    # Verificar se o botão \"Salvar no Banco de Dados\" está presente\n    buttons  driver.find_elements(By.XPATH, \"//button\")\n    save_button  None\n    for button in buttons:\n        if button.text  \"Salvar no Banco de Dados\":\n            save_button  button\n            break\n\n    assert save_button is not None and save_button.is_displayed()\n```\n\n## Vamos criar um banco de dados\n\nSerá um postgres.\n\nVamos usar o render\n\nVamos usar o dbeaver\n\ncreate.sql\n```sql\nCREATE TABLE vendas (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255) NOT NULL,\n    data TIMESTAMP NOT NULL,\n    valor NUMERIC(10, 2) NOT NULL CHECK (valor > 0),\n    quantidade INTEGER NOT NULL CHECK (quantidade > 0),\n    produto VARCHAR(255) NOT NULL,\n    categoria VARCHAR(50) NOT NULL\n);\n```\n\ndelete.sql\n```sql\n-- Exemplo com dados valido\n\nINSERT INTO vendas (email, data, valor, produto, quantidade, categoria)\nVALUES (\n    'comprador@example.com', \n    '2023-09-15 12:00:00',  -- Substitua pela data atual formatada como string\n    100.50, \n    'Produto X', \n    3, \n    'categoria3'\n);\n\n-- Exemplo com dado invalido\n\nINSERT INTO vendas (email, data, valor, produto, quantidade, categoria)\nVALUES (\n    'comprador', \n    '2023-09-15 12:00:00',  -- Substitua pela data atual formatada como string\n    100.50, \n    'Produto X', \n    3, \n    'categoria3'\n);\n```\n\n`app.py`\n\nVamos ter que criar um novo fluxo -> Desenhar na tela\n\n```python\n    if uploaded_file:\n        df, result, errors  process_excel(uploaded_file)\n        ui.display_results(result, errors)\n\n        if errors:\n            ui.display_wrong_message()\n        elif ui.display_save_button():\n            # Se não houver erros e o botão for exibido, exibir o botão e fazer o log\n            save_dataframe_to_sql(df)\n            ui.display_success_message()\n```\n\nbackend.py\n```python\nimport pandas as pd\nfrom contrato import Vendas\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")\n\n# Lê as variáveis de ambiente\nPOSTGRES_USER  os.getenv('POSTGRES_USER')\nPOSTGRES_PASSWORD  os.getenv('POSTGRES_PASSWORD')\nPOSTGRES_HOST  os.getenv('POSTGRES_HOST')\nPOSTGRES_PORT  os.getenv('POSTGRES_PORT')\nPOSTGRES_DB  os.getenv('POSTGRES_DB')\n\n# Cria a URL de conexão com o banco de dados\nDATABASE_URL  f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n\n# Carrega as variáveis de ambiente\nload_dotenv()\n\ndef process_excel(uploaded_file):\n    try:\n        df  pd.read_excel(uploaded_file)\n        erros  []\n        # Verificar se há colunas extras no DataFrame\n        extra_cols  set(df.columns) - set(Vendas.model_fields.keys())\n        if extra_cols:\n            return False, f\"Colunas extras detectadas no Excel: {', '.join(extra_cols)}\"\n\n        # Validar cada linha com o schema escolhido\n        for index, row in df.iterrows():\n            try:\n                _  Vendas(**row.to_dict())\n            except Exception as e:\n                erros.append(f\"Erro na linha {index + 2}: {e}\")\n\n        # Retorna tanto o resultado da validação, os erros, quanto o DataFrame\n        return df, True, erros\n\n    except Exception as e:\n        # Se houver exceção, retorna o erro e um DataFrame vazio\n        return pd.DataFrame(), f\"Erro inesperado: {str(e)}\"\n    \ndef save_dataframe_to_sql(df):\n    # Salva o DataFrame no banco de dados\n    df.to_sql('vendas', conDATABASE_URL, if_exists'replace', indexFalse)\n```\n\n`frontend.py`\n```python\nimport streamlit as st\n\nclass ExcelValidadorUI:\n\n    def __init__(self):\n        self.set_page_config()\n\n    def set_page_config(self):\n        st.set_page_config(\n            page_title\"Validador de schema excel\"\n        )\n\n    def display_header(self):\n        st.title(\"Insira o seu excel para validação\")\n\n    def upload_file(self):\n        return st.file_uploader(\"Carregue seu arquivo Excel aqui\", type[\"xlsx\"])\n\n    def display_results(self, result, errors):\n        if errors:\n            for error in errors:\n                st.error(f\"Erro na validação: {error}\")\n        else:\n            st.success(\"O schema do arquivo Excel está correto!\")\n\n    def display_save_button(self):\n        return st.button(\"Salvar no Banco de Dados\")\n\n    def display_wrong_message(self):\n        return st.error(\"Necessário corrigir a planilha!\")\n    \n    def display_success_message(self):\n        return st.success(\"Dados salvos com sucesso no banco de dados!\")\n```\n\nComo criar uma PR?\n\n```\nDescrição\nObjetivo da PR: Descreva qual é o objetivo principal desta PR. O que ela pretende alcançar? Isso pode incluir a resolução de um problema específico, a implementação de uma nova funcionalidade ou qualquer outra coisa relevante.\n\nContexto: Explique o contexto por trás desta PR. Isso pode incluir informações adicionais sobre o problema que está sendo resolvido ou a motivação por trás da nova funcionalidade.\n\nCliente:\n\nTicket:\n\nTestes Realizados: Descreva os testes que foram realizados para garantir que as alterações funcionam conforme o esperado. Certifique-se de mencionar se foram adicionados novos testes unitários ou se os testes existentes foram atualizados.\n\nDocumentação:\n\n```\n\nfrom logging import debug, info, error, warning, critical\n\n```Log\nfrom frontend import ExcelValidadorUI\nfrom backend import process_excel, save_dataframe_to_sql\nfrom logging import warning\nfrom datetime import datetime\n\ndef main():\n    ui  ExcelValidadorUI()\n    ui.display_header()\n\n    uploaded_file  ui.upload_file()\n\n    if uploaded_file:\n        df, result, errors  process_excel(uploaded_file)\n        ui.display_results(result, errors)\n\n        if errors:\n            ui.display_wrong_message()\n        elif ui.display_save_button():\n            # Se não houver erros e o botão for exibido, exibir o botão e fazer o log\n            save_dataframe_to_sql(df)\n            ui.display_success_message()\n\nif __name__  \"__main__\":\n    main()\n\n```\n\nfrom logging import basicConfig\n\nbasicConfig\n\n```\nfrom frontend import ExcelValidadorUI\nfrom backend import process_excel, save_dataframe_to_sql\nfrom logging import warning, info\nfrom datetime import datetime\n\nfrom logging import basicConfig\n\nbasicConfig (\n    filename'meus_logs.txt',\n    filemode'a',\n    encoding'utf-8',\n    format'%(levelname)s:%(asctime)s:%message)s'\n)\n\ndef main():\n    ui  ExcelValidadorUI()\n    ui.display_header()\n\n    uploaded_file  ui.upload_file()\n\n    if uploaded_file:\n        df, result, errors  process_excel(uploaded_file)\n        ui.display_results(result, errors)\n\n        if errors:\n            ui.display_wrong_message()\n            warning(\"erro ao subir excel\")\n        elif ui.display_save_button():\n            # Se não houver erros e o botão for exibido, exibir o botão e fazer o log\n            save_dataframe_to_sql(df)\n            ui.display_success_message()\n            info(\"excel subiu para o banco ás 14h\")\n\nif __name__  \"__main__\":\n    main()\n    \n```\n\nTem como colocar no terminal e no arquivo?\n\nPara isso vamos usar o Loguru\n\n```\npip install loguru\n```\n\n",
        "00-automacao-data-qualiy-excel-etl/src/app.py\n\nfrom frontend import ExcelValidatorUI\nfrom backend import process_excel\nfrom contrato import Vendas\n\ndef main():\n\n    ui  ExcelValidatorUI()\n    ui.display_header()\n\n    model_choice  Vendas\n    uploaded_file  ui.upload_file()\n\n    if uploaded_file is not None:\n        result, error  process_excel(uploaded_file, \n                                      model_choice)\n        ui.display_results(result, error)\n\nif __name__  \"__main__\":\n    main()\n\n00-automacao-data-qualiy-excel-etl/src/backend.py\n\nimport pandas as pd\nfrom contrato import Vendas\n\ndef process_excel(uploaded_file, model_name):\n    \"\"\"\n    Processa um arquivo Excel, validando-o contra um esquema específico.\n\n    Args:\n        uploaded_file: Um arquivo Excel carregado pelo usuário.\n        model_name: Nome do modelo de dados a ser usado para validação.\n\n    Returns:\n        Uma tupla (resultado, erro), onde 'resultado' é um booleano indicando se a validação\n        foi bem-sucedida e 'erro' é uma mensagem de erro se a validação falhar.\n    \"\"\"    \n    try:\n        df  pd.read_csv(uploaded_file)\n\n        # Verificar se há colunas extras no DataFrame\n        extra_cols  set(df.columns) - set(Vendas.model_fields.keys())\n        if extra_cols:\n            return False, f\"Colunas extras detectadas no Excel: {', '.join(extra_cols)}\"\n\n        # Validar cada linha com o schema escolhido\n        for index, row in df.iterrows():\n            try:\n                _  Vendas(**row.to_dict())\n            except Exception as e:\n                raise ValueError(f\"Erro na linha {index + 2}: {e}\")\n\n        return True, None\n\n    except ValueError as ve:\n        return False, str(ve)\n    except Exception as e:\n        return False, f\"Erro inesperado: {str(e)}\"\n\n00-automacao-data-qualiy-excel-etl/src/contrato.py\n\nfrom pydantic import BaseModel, EmailStr, PositiveFloat, PositiveInt, validator\nfrom datetime import datetime\nfrom enum import Enum\n\nclass CategoriaEnum(str, Enum):\n    categoria1  \"categoria1\"\n    categoria2  \"categoria2\"\n    categoria3  \"categoria3\"\n\n\nclass Vendas(BaseModel):\n\n    \"\"\"\n    Modelo de dados para as vendas.\n\n    Args:\n        email (str): email do comprador\n        data (datetime): data da compra\n        valor (int): valor da compra\n        produto (str): nome do produto\n        quantidade (int): quantidade de produtos\n        categoria (str): categoria do produto\n\n    \"\"\"\n    email: EmailStr\n    data: datetime\n    valor: PositiveFloat\n    quantidade: PositiveInt\n    categoria: CategoriaEnum\n\n    @validator('categoria')\n    def categoria_deve_estar_no_enum(cls, error):\n        return error\n\n00-automacao-data-qualiy-excel-etl/src/frontend.py\n\nimport streamlit as st\n\nclass ExcelValidatorUI:\n    \"\"\"\n    Classe responsável por gerar a interface de usuário para o validador de arquivos Excel.\n    \"\"\"\n    def __init__(self):\n        self.set_page_config()\n\n    def set_page_config(self):\n        st.set_page_config(page_title\"Validador de Schema de Excel\", layout\"wide\")\n\n    def display_header(self):\n        st.title(\"Validador de Schema de Excel\")\n\n    def upload_file(self):\n        return st.file_uploader(\"Carregue seu arquivo Excel aqui\", type[\"csv\"])\n\n    def display_results(self, result, error):\n        if error:\n            st.error(f\"Erro na validação: {error}\")\n        else:\n            st.success(\"O schema do arquivo Excel está correto!\")\n\n\n00-automacao-data-qualiy-excel-etl/tests/test_app.py\n\nimport os\nimport pytest\nimport subprocess\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException  # Importando TimeoutException\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.keys import Keys\n\n\n#\n\n@pytest.fixture(scope\"module\")\ndef driver():\n    # Iniciar o Streamlit em background\n    process  subprocess.Popen([\"streamlit\", \"run\", \"src/app.py\"])\n\n    # Iniciar o WebDriver usando GeckoDriver\n    driver  webdriver.Firefox()\n    yield driver\n\n    # Fechar o WebDriver e o Streamlit após o teste\n    driver.quit()\n    process.kill()\n\ndef test_app_opens(driver):\n    driver.get(\"http://localhost:8501\")\n\n    # Aguardar um tempo para a aplicação carregar\n    time.sleep(5)\n\n    # Verificar se o título da página é o esperado\n\n# def test_successful_upload(driver):\n#     driver.get(\"http://localhost:8501\")\n\n#     # Aguardar um tempo para a aplicação carregar\n#     time.sleep(5)\n\n    # Realizar o upload do arquivo de sucesso\n    success_file_path  os.path.abspath(\"data/success.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(success_file_path)\n\n    # Aguardar a mensagem de sucesso\n    time.sleep(5)\n    assert \"O schema do arquivo Excel está correto!\" in driver.page_source\n\n# def test_failed_upload(driver):\n#     driver.get(\"http://localhost:8501\")\n\n#     # Aguardar um tempo para a aplicação carregar\n#     time.sleep(5)\n\n#     # Realizar o upload do arquivo de falha\n#     failure_file_path  os.path.abspath(\"data/failure.xlsx\")\n#     driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(failure_file_path)\n\n#     # Aguardar a mensagem de erro\n#     time.sleep(5)\n#     assert \"Erro na validação\" in driver.page_source\n\ndef test_successful_upload_using_select(driver):\n    driver.get(\"http://localhost:8501\")\n    time.sleep(5)  # Aguarda a aplicação carregar\n\n    try:\n        select_box  driver.find_element(By.CLASS_NAME, \"stSelectbox\")\n        ActionChains(driver).move_to_element(select_box).click().perform()\n        time.sleep(2)  # Aguarda o selectbox abrir\n\n        ActionChains(driver).send_keys(\"Usuario\").send_keys(Keys.ENTER).perform()\n        time.sleep(2)  # Aguarda a seleção ser feita\n\n    except Exception as e:\n        print(f\"Erro durante a interação com o selectbox: {e}\")\n        return\n\n    # Realizar o upload do arquivo de sucesso\n    success_file_path  os.path.abspath(\"data/success.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(success_file_path)\n    time.sleep(5)  # Aguarda o upload do arquivo\n\n    # Aguardar a mensagem de sucesso\n    success_message  \"O schema do arquivo Excel está correto!\"\n    assert success_message in driver.page_source\n\ndef test_failed_upload_using_select(driver):\n    driver.get(\"http://localhost:8501\")\n    time.sleep(5)  # Aguarda a aplicação carregar\n\n    try:\n        select_box  driver.find_element(By.CLASS_NAME, \"stSelectbox\")\n        ActionChains(driver).move_to_element(select_box).click().perform()\n        time.sleep(2)  # Aguarda o selectbox abrir\n\n        ActionChains(driver).send_keys(\"Usuario\").send_keys(Keys.ENTER).perform()\n        time.sleep(2)  # Aguarda a seleção ser feita\n\n    except Exception as e:\n        print(f\"Erro durante a interação com o selectbox: {e}\")\n        return\n\n    # Realizar o upload do arquivo de falha\n    failure_file_path  os.path.abspath(\"data/failure.xlsx\")\n    driver.find_element(By.CSS_SELECTOR, 'input[type\"file\"]').send_keys(failure_file_path)\n    time.sleep(5)  # Aguarda o upload do arquivo\n\n    # Aguardar a mensagem de sucesso\n    failure_message  \"Erro na validação\"\n    assert failure_message in driver.page_source\n\n"
    ],
    "05-redis-mongodb-esse-tal-de-nosql": [
        "05-redis-mongodb-esse-tal-de-nosql/README.md\n\n# Workshop Crawler\n\n# Projeto de Web Scraping com Redis e MongoDB\n\n## Visão Geral\n\nEste projeto é focado em técnicas de Web Scraping usando Python. Utilizamos Redis e MongoDB como nossos bancos de dados para armazenamento e gerenciamento de dados coletados durante o processo de raspagem de dados.\n\n### 1. Redis\n\nRedis é um armazenamento de estrutura de dados em memória, usado como banco de dados, cache e message broker. Ele suporta estruturas de dados como strings, hashes, listas, sets, sorted sets com consultas de intervalo, bitmaps, hyperloglogs, geospatial indexes e streams. Redis tem um desempenho excepcional por manter os dados em memória.\n\n### 2. MongoDB\n\nMongoDB é um banco de dados NoSQL orientado a documentos. É escalável e flexível, permitindo que você armazene dados em um formato semelhante ao JSON com esquemas dinâmicos. Isso torna a integração de dados em certos tipos de aplicações mais fácil e rápida.\n\n### 3. Web Scraping\n\nWeb Scraping é a técnica de extrair dados de websites. Este processo envolve fazer requisições HTTP para o servidor do site desejado, coletar dados da página e, em seguida, analisar esses dados para extrair informações úteis. Utilizamos Python e suas bibliotecas como BeautifulSoup e Scrapy para realizar estas tarefas.\n\n\n\n05-redis-mongodb-esse-tal-de-nosql/pyproject.toml\n\n[tool.poetry]\nname  \"poetry new workshop-crawler\"\nversion  \"0.1.0\"\ndescription  \"Projeto para o Workshop de Crawler\"\nauthors  [\"Fabio Cantarim <fabio.cantarim@gmail.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.12\"\nrequests  \"^2.31.0\"\nselenium  \"^4.19.0\"\nbeautifulsoup4  \"^4.12.3\"\npandas  \"^2.2.2\"\nredis  \"^5.0.3\"\npymongo  \"^4.6.3\"\nmitmproxy  \"^10.2.4\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\n05-redis-mongodb-esse-tal-de-nosql/.env-example\n\nREDIS_HOST\"\"\nREDIS_PORT\"\"\n\nUSE_PROXYFalse\nPROXY_USER\"\"\nPROXY_PASSWORD\"\"\nPROXY_URL\"\"\nPROXY_PORT\"\"\n\nMONGO_HOST\"\"\nMONGO_PORT\"\"\nMONGO_DATABASE\"\"\nMONGO_COLLECTION\"\"\n\nHEADLESS\"False\"\n\nDEFAULT_PROXY_USERNAME\"\"\nDEFAULT_PROXY_PASSWORD\"\"\n\nARG1\"\"\nARG2\"\"\n\n05-redis-mongodb-esse-tal-de-nosql/docker/docker-compose.yml\n\nversion: '3.8'\n\nservices:\n  redis:\n    image: redis\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  mongodb:\n    image: mongo\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - mongodb_data:/data/db\n\nvolumes:\n  redis_data:\n  mongodb_data:\n\n\n05-redis-mongodb-esse-tal-de-nosql/src/start.py\n\nfrom browser.generic_crawler import GenericBrowserCrawler\nfrom request.generic_crawler import GenericRequestCrawler\n\n\nml  GenericBrowserCrawler(\"Ml\").crawl('Nintendo Switch')\naz  GenericBrowserCrawler(\"Amazon\").crawl('Playstation')\n\nml2  GenericRequestCrawler(\"Ml\").crawl('Xbox')\naz2  GenericRequestCrawler(\"Amazon\").crawl('Sega')\n\nprint(az2)\n\n05-redis-mongodb-esse-tal-de-nosql/src/browser/b_ml_simple.py\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\n\nclass MercadoLivreCrawler:\n    def __init__(self):\n        # Configurando as opções do Chrome para rodar headless\n        self.chrome_options  Options()\n        # self.chrome_options.add_argument(\"--headless\")  # Rodar o Chrome em modo headless\n        self.chrome_options.add_argument(\"--no-sandbox\")  # Evitar problemas de sandbox\n        self.chrome_options.add_argument(\"--disable-gpu\")\n        self.chrome_options.add_argument(\"--disable-setuid-sandbox\")\n        self.chrome_options.add_argument(\"--disable-web-security\")\n        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n        self.chrome_options.add_argument(\"--memory-pressure-off\")\n        self.chrome_options.add_argument(\"--ignore-certificate-errors\")\n        self.chrome_options.add_argument(\"--disable-featuressite-per-process\")\n\n        # Inicializando o driver do Chrome\n        self.driver  webdriver.Chrome(optionsself.chrome_options)\n\n    def execute_command(self, query):\n        # Navegando para a página de pesquisa do Mercado Livre\n        self.driver.get(f\"https://lista.mercadolivre.com.br/{query.replace(' ', '-')}\")\n        \n        # Aguardando um momento para a página carregar completamente\n        time.sleep(5)\n\n        # Obtendo o HTML da página\n        html  self.driver.page_source\n\n        # Analisando o HTML com BeautifulSoup\n        soup  BeautifulSoup(html, \"html.parser\")\n\n        # Extraindo os dados que você deseja\n        results  soup.find_all(\"div\", class_\"ui-search-result\")\n\n        data  []\n        for result in results:\n            # Aqui você pode extrair informações específicas de cada resultado, como título, preço, etc.\n            link  None\n            title  result.find(\"h2\", class_\"ui-search-item__title\").text.strip()\n            price  result.find(\"span\", class_\"andes-money-amount__fraction\").text.strip()\n            link_tag   result.find(\"a\", class_\"ui-search-link\")\n            if link_tag:\n                link  link_tag.get(\"href\")\n            data.append({\"Produto\": title, \"Preço\": price, \"URL\": link})\n\n        # Fechando o navegador\n        self.driver.quit()\n\n        return data\n\n    def send_dataframe(self, query):\n        data  self.execute_command(query)\n        df  pd.DataFrame(data)\n        return df\n\n# Exemplo de utilização\ncrawler  MercadoLivreCrawler()\ndataframe  crawler.send_dataframe(\"fire emblem warriros\")\nprint(dataframe)\n\n\n05-redis-mongodb-esse-tal-de-nosql/src/browser/generic_crawler.py\n\n##https://json-ld.org/learn.html\nimport time\nimport json\nimport pandas as pd\nfrom datetime import datetime\nfrom bs4 import BeautifulSoup\nfrom browser.crawlers.default_crawler import AbstractCrawler\nfrom browser.provider.actions.dict import action_dict\n\nclass GenericBrowserCrawler(AbstractCrawler):\n    def __init__(self, type):\n        super().__init__()\n        self.type  type\n        self.steps  json.loads(self.get_steps(self.type))\n        if self.steps is None:\n            raise(\"Crawler Não configurado!\")\n\n    def crawl(self, query):\n        self.query  query\n        self.execute_before()\n        df  self.execute_main()\n        self.execute_after()\n        self.mongo.save_dataframe(df)\n        print(\"Wait\")\n        \n    def execute_main(self):\n        self.browser.get(f\"{self.steps[\"link\"][\"path\"]}{self.query.replace(' ', self.steps[\"link\"][\"connector\"])}\")\n        time.sleep(5)\n        self.content  self.extraction()\n        self.browser.quit()\n        return self.transform_to_df_and_improve(self.content)    \n    \n    def execute_before(self):\n        before  self.steps[\"script\"][\"before\"]\n        if before:\n            for action in before:\n                if action_dict[action] is None:\n                    raise(\"Script não definido\")\n                action_dict[action](self.browser, before[action])\n            return\n\n    def execute_after(self):\n        after  self.steps[\"script\"][\"after\"]\n        if after:\n            for action in after:\n                if action_dict[action] is None:\n                    raise(\"Script não definido\")\n                action_dict[action](self.browser, after[action])\n            return\n\n    def extraction(self):\n        self.html  self.browser.page_source\n        \n        soup  BeautifulSoup(self.html, \"html.parser\")\n        \n        if self.steps[\"search\"][\"custom\"]:\n            results  soup.find_all(self.steps[\"search\"][\"tag\"], self.steps[\"search\"][\"custom\"])\n        else:\n            results  soup.find_all(self.steps[\"search\"][\"tag\"], class_self.steps[\"search\"][\"class\"])\n\n        data  []\n        for result in results:\n            product  {}\n            for step in self.steps[\"product\"]:\n                value  self.steps[\"product\"][step]\n                try:\n                    content  eval(value)\n                except:\n                    content  None\n                product[step]  content\n            data.append(product)\n        return data\n\n    def transform_to_df_and_improve(self, data):\n        df  pd.DataFrame(data)\n        df  df.assign(keywordself.query)\n        df  df.assign(ecommerce  self.type)\n        df  df.assign(dateTimeReferencedatetime.now().isoformat())\n        df  df.assign(crawlerType  \"Browser\")\n        return df\n\n\n\n05-redis-mongodb-esse-tal-de-nosql/src/browser/crawlers/default_crawler.py\n\nfrom abc import ABC, abstractmethod\n\nfrom browser.provider.generic_b_crawler import GenericBrowserCrawler\nfrom tools.redis import RedisClient\nfrom tools.mongodb import MongoConnection\n\n\nclass AbstractCrawler(ABC):\n    def __init__(self):\n        self.browser  GenericBrowserCrawler().get_browser()\n        self.redis  RedisClient.get()\n        self.mongo  MongoConnection()\n\n    @abstractmethod\n    def crawl(self):\n        pass\n    \n    @abstractmethod\n    def execute_main(self):\n        pass\n    \n    @abstractmethod\n    def execute_before(self):\n        pass\n    \n    @abstractmethod\n    def execute_before(self):\n        pass\n\n    def get_steps(self, site):\n        return self.redis.get(site)\n    \n    def save_data(self, data):\n        try:\n            self.mongo.save_dataframe(data)\n        except:\n            raise(\"Não foi possível salvar os dados no Mongo\")\n\n05-redis-mongodb-esse-tal-de-nosql/src/browser/provider/generic_b_crawler.py\n\nimport os\nfrom selenium import webdriver\n\nclass GenericBrowserCrawler:\n\n    browser: None\n    options  webdriver.ChromeOptions()\n\n    default_options   [\n        \"--no-sandbox\",\n        \"--disable-gpu\",\n        \"--disable-setuid-sandbox\",\n        \"--disable-web-security\",\n        \"--disable-dev-shm-usage\",\n        \"--memory-pressure-off\",\n        \"--ignore-certificate-errors\",\n        \"--disable-featuressite-per-process\",\n        \"--user-agentMozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"]\n\n    def get_browser(self, args: list[str]  None):\n        new_args  args\n        if args is None:\n            new_args  self.default_options\n        self.set_options(new_args)\n        return webdriver.Chrome(optionsself.options)\n    \n    def is_headless(self):\n        headless  os.getenv('HEADLESS')\n        if headless is None:\n            self.options.add_argument(\"--headless\")\n\n    \n    def set_options(self, args: list[str] | None):\n        self.is_headless()\n        self.set_proxy()\n        if args:\n            for arg in args:\n                self.options.add_argument(arg)\n\n    def set_proxy(self):\n        if os.getenv(\"USE_PROXY\"):\n            #Proxy url possibilities: IP or Protocol://User:Password@IP:Port\n            user   os.getenv(\"PROXY_USER\")\n            password  os.getenv(\"PROXY_PASSWORD\")\n            url  os.getenv(\"PROXY_URL\")\n            port  os.getenv(\"PROXY_PORT\")\n            proxy_provider  f'http://{user}:{password}@{url}:{port}'\n            self.options.add_argument(f'--proxy-server{proxy_provider}')\n\n\n05-redis-mongodb-esse-tal-de-nosql/src/browser/provider/actions/dict.py\n\n\ndef goto(browser, url):\n    return browser.get(url)\n\n\n\naction_dict  {}\n\n\naction_dict['goto']  goto\n\n05-redis-mongodb-esse-tal-de-nosql/src/mitm/start.py\n\nfrom mitmproxy.tools.main import mitmdump\nimport os\n\nusername  os.getenv(\"DEFAULT_PROXY_USERNAME\")\npassword  os.getenv(\"DEFAULT_PROXY_PASSWORD\")\nserver  os.getenv(\"DEFAULT_PROXY_SERVER\", 'brd.superproxy.io')\nport  os.getenv(\"DEFAULT_PROXY_PORT\", 22225)\n\n\nmitmdump(args[\n    \"-s\", \"./mitm/addon/proxy_controller.py\",\n    \"--mode\", f\"upstream:http://{server}:{port}\",\n    \"--upstream-auth\", f\"{username}:{password}\",\n    \"--set stream_large_bodies5g\",\n    \"--set connection_strategylazy\"\n    \"--set upstream_certfalse\",\n    \"--ssl-insecure\"\n])\n\n\"\"\"\n--set connection_strategylazy\n\nDetermine when server connections should be established. \nWhen set to lazy, mitmproxy tries to defer establishing an upstream connection as long as possible. \nThis makes it possible to use server replay while being offline. \nWhen set to eager, mitmproxy can detect protocols with server-side greetings, as well as accurately mirror TLS ALPN negotiation.\nDefault: eager\nChoices: eager, lazy\n\"\"\"\n\n\"\"\"\n--set stream_large_bodies5g\n\nStream data to the client if response body exceeds the given threshold. \nIf streamed, the body will not be stored in any way, and such responses cannot be modified. \nUnderstands k/m/g suffixes, i.e. 3m for 3 megabytes.\nDefault: None\n\"\"\"\n\n05-redis-mongodb-esse-tal-de-nosql/src/mitm/addon/proxy_controller.py\n\nimport logging\nimport base64\n\nfrom os import environ as env\nfrom uuid import uuid4\n\nfrom mitmproxy import ctx, http\nfrom mitmproxy.connection import Server\nfrom mitmproxy.http import HTTPFlow\nfrom mitmproxy.utils import strutils\nfrom mitmproxy.net.server_spec import ServerSpec\n# from mitmproxy.proxy.layers.http import HTTPMode\n\nLOGGING_LEVEL  env.get(\"LOGGING_LEVEL\", \"INFO\").upper()\nlogger  logging.getLogger(__name__)\nlogger.setLevel(LOGGING_LEVEL)\n\nclass ProxyControllerAddon:\n    def responseheaders(self, flow: HTTPFlow):\n        \"\"\"\n        Enables streaming for all responses.\n        This method sets `stream` attribute to True in the response, \n        allowing streaming large responses.\n\n        Equivalent to passing `--set stream_large_bodies1` to mitmproxy.\n        \"\"\"\n        flow.response.stream  True\n\n    def request(self, flow: HTTPFlow):\n        self.set_proxy_configs(flow)\n    \n    \n    def response(self, flow: HTTPFlow):\n        pass\n\n    def set_proxy_configs(self, flow: HTTPFlow):\n        if self.has_new_proxy_auth(flow):\n                flow.request.headers[\"Proxy-Authorization\"]  self.has_new_proxy_auth(flow)\n        if self.has_new_proxy_server(flow):\n            address  self.has_new_proxy_server(flow)\n            logger.info(f\"UPSTREAM_PROXY_ENDPOINT: {address}\")\n            is_proxy_change  address ! flow.server_conn.address\n            if is_proxy_change:\n                flow.server_conn  Server(addressaddress)\n                if flow.request.headers[\"x-New-Proxy-Server\"] ! \"No-Proxy\":\n                    flow.server_conn.via  ServerSpec((\"http\", address))            \n\n    def has_new_proxy_auth(self, flow: HTTPFlow):\n        \"\"\"\n        Checks if the request contains the x-New-Proxy-Auth header.\n        If found, retrieves the parameter and encodes it in base64, \n        then concatenates the string \"Basic\" with the encoded value.\n        The parameter should be in the format {username}:{password}.\n        \"\"\"\n        if \"x-New-Proxy-Auth\" in flow.request.headers:\n            user, password  flow.request.headers[\"x-New-Proxy-Auth\"].split(\":\")\n            session  15054\n            new_auth  f'{user}-session-{session}-c_tag-{session}:{password}'\n            return \"Basic \" + base64.b64encode(strutils.always_bytes(new_auth)).decode(\"utf-8\")\n        return None\n\n    def has_new_proxy_server(self, flow: HTTPFlow):\n        \"\"\"\n        Checks if the request contains the x-New-Proxy-Server header.\n        If found, retrieves the parameter in the format {host}:{port}.\n        The parameter could be an IP address or hostname.\n        Returns a tuple containing the host (string) and port (integer).\n        \"\"\"\n        if \"x-New-Proxy-Server\" in flow.request.headers:\n            if flow.request.headers[\"x-New-Proxy-Server\"]  \"No-Proxy\":\n                return {\"localhost\", 80}\n            else:\n                proxy_server  flow.request.headers[\"x-New-Proxy-Server\"]\n                host,port  proxy_server.split(\":\")\n                return (host, int(port))\n        return \n  \naddons  [ProxyControllerAddon()]\n\n05-redis-mongodb-esse-tal-de-nosql/src/mitm/test/download.py\n\nimport requests\nfrom os import environ as env\n\n# Configuração do proxy para o MITM Proxy\nproxy  {\n    'http': 'http://localhost:8080',\n    'https': 'http://localhost:8080',\n}\n\ncert_path  './mitmproxy-ca-cert.pem'\n\n\n# URL do arquivo que você deseja baixar\nurl  'http://releases.ubuntu.com/22.04.4/ubuntu-22.04.4-desktop-amd64.iso'\n\nheaders  {\n    # 'Proxy-Authorization': env.get(\"ARG1\"),\n    'Proxy-Authentication': env.get(\"ARG2\"),\n    # 'Proxy-Server' : '20.33.5.27:8888'\n}\n\n# Realiza a solicitação HTTP através do MITM Proxy\nwith requests.get(url, streamTrue, proxiesproxy, verifycert_path, headersheaders) as r:\n# with requests.get(url, streamTrue, proxiesproxy) as r:    \n# Verifica o tamanho total do arquivo, se disponível\n    total_size  r.headers.get('content-length')\n    if total_size is None:\n        print(\"O tamanho total do arquivo é desconhecido.\")\n    else:\n        total_size  int(total_size)\n\n    print(f'Tamanho do arquivo é {total_size}')\n    # Inicializa a quantidade de bytes recebidos\n    downloaded_size  0\n\n    # Trata a resposta conforme necessário\n    if r.status_code  200:\n        with open('arquivo_baixado.iso', 'wb') as f:\n            for chunk in r.iter_content(chunk_size1024):\n                if chunk: # Verifica se há dados no chunk\n                    f.write(chunk)\n                    downloaded_size + len(chunk)\n                    if total_size:\n                        progress  downloaded_size / total_size * 100\n                        print(f\"Progresso: {progress:.2f}%\")\n        print(\"Download concluído.\")\n    else:\n        print(f\"Erro: {r.status_code}\")\n\n\n05-redis-mongodb-esse-tal-de-nosql/src/mitm/test/ip.py\n\nimport requests\nfrom os import environ as env\n\nurl  \"http://lumtest.com/myip.json\"\n# url  \"http://www.globo.com\"\n\nproxy  {\n    'http': 'http://localhost:8080',\n    'https': 'http://localhost:8080',\n}\n\ncert_path  './mitmproxy-ca-cert.pem'\n\nheaders  {\n    'Proxy-Authorization': env.get(\"ARG1\"),\n    # 'x-New-Proxy-Auth': env.get(\"ARG2\")\n    # 'x-New-Proxy-Server' : '38.145.211.246:8899'\n    # 'x-New-Proxy-Server' : 'No-Proxy'\n\n}\n\n# Fazendo a requisição GET\nresponse  requests.get(url, proxiesproxy, verifycert_path, headersheaders)\n# Verifica se a requisição foi bem-sucedida (código de status 200)\nif response.status_code  200:\n    # Imprime o conteúdo da resposta (o endereço IP retornado pelo site)\n    print(\"Endereço IP retornado pelo site:\", response.json()[\"ip\"])\n    print(\"Cidade\", response.json()[\"geo\"] )\n    \nelse:\n    # Se a requisição não foi bem-sucedida, imprime o código de status\n    print(\"Falha na requisição. Código de status:\", response.status_code)\n\n05-redis-mongodb-esse-tal-de-nosql/src/mitm/test/mitmproxy-ca-cert.pem\n\n-----BEGIN CERTIFICATE-----\nMIIDNTCCAh2gAwIBAgIUdyrzXy9rHnJXw+rpyMcUopNCl50wDQYJKoZIhvcNAQEL\nBQAwKDESMBAGA1UEAwwJbWl0bXByb3h5MRIwEAYDVQQKDAltaXRtcHJveHkwHhcN\nMjQwMzEyMTcyNzQ0WhcNMzQwMzEyMTcyNzQ0WjAoMRIwEAYDVQQDDAltaXRtcHJv\neHkxEjAQBgNVBAoMCW1pdG1wcm94eTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC\nAQoCggEBALvbpilEsx8WYdV+SuA2JWYVN/7idrc3Qz3uCXg9gAuHAAyrMAYZMQfD\nIVsc1iAxVqPev3PsJZP5Y8xp3cXrNsOHNnCbnXL0i7uOFeFadYGMLe7a9VaAuDv0\niviUcse1hTKOF2r3Lb2Kkk572x7UcBErujlqOrw6+6RNKa5B03ud+pkPhTv7tT8Z\nwbJ2VAA70+64E13zv88Gnk8Ln9Jzq8Nw3yrzszhp1GuEjPrDQGRZ+0nz38XVfhv3\n6Yukfm8GZb0NTWott1/swXWzLJUI4bWeC+y9o77etBC1aREuHal8mRhmVF5z2iqN\nfO4y5TZusAGs1ILceXaQa66zb1K8VOcCAwEAAaNXMFUwDwYDVR0TAQH/BAUwAwEB\n/zATBgNVHSUEDDAKBggrBgEFBQcDATAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYE\nFPK5t017L37CqHbfwXksKewGAJwmMA0GCSqGSIb3DQEBCwUAA4IBAQBELjafhk2B\nzdzKksZVcTNYzCSIvcUE6NakjdcTk0ntt826XX37us8E3p9x8v76q7Wi9jR1TVd5\n9DuMQzzpXEsQRV5b+j1rCMjMuqNvDW1mNzHskOro//qEnBveFPAV2JYhZTrDMTn7\nHKRbMktsrFD36hOmtagzJ+NTv8dXRBGV9ZFOUhA7bdesLRJLkXtpXoJwPVslhAwq\ns42trpO09altAu+7bZ7T18KeO4lSMPZKKZVZdPDCBFOzNvhwneB3+6GYt7aaq7hA\nbqciGRRWao2ua3d2hw+r2q1ulUw7QBH/UP/2HVg0g5tTPqvX2349t0MB8gl1ZdYo\nBpX7V9gmpdKx\n-----END CERTIFICATE-----\n\n\n05-redis-mongodb-esse-tal-de-nosql/src/request/generic_crawler.py\n\nfrom datetime import datetime\nimport json\n\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport requests\nfrom request.crawlers.default_crawler import AbstractCrawler\n\n\nclass GenericRequestCrawler(AbstractCrawler):\n    def __init__(self, type):\n        super().__init__()\n        self.type  type\n\n    def crawl(self,query):\n        self.query  query\n        self.configs  json.loads(self.get_steps(self.type))\n        if self.configs is None:\n            raise(\"Crawler Não configurado!\")\n        self.get_data()\n        self.extraction()\n        self.transform_to_df_and_improve()\n        self.save_data(self.df)\n\n\n    def get_data(self):\n        url  f\"{self.configs[\"link\"][\"path\"]}{self.query.replace(' ', self.configs[\"link\"][\"connector\"])}\"\n        headers  {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n        }\n\n        response  requests.get(url, headersheaders)\n\n        if response.status_code > 400:\n            raise(\"Status Code ! 200\")    \n        \n        self.data  response.text\n        \n\n    def extraction(self):\n        soup  BeautifulSoup(self.data, \"html.parser\")\n        \n        if self.configs[\"search\"][\"custom\"]:\n            results  soup.find_all(self.configs[\"search\"][\"tag\"], self.configs[\"search\"][\"custom\"])\n        else:\n            results  soup.find_all(self.configs[\"search\"][\"tag\"], class_self.configs[\"search\"][\"class\"])\n\n        data  []\n        for result in results:\n            product  {}\n            for step in self.configs[\"product\"]:\n                value  self.configs[\"product\"][step]\n                try:\n                    content  eval(value)\n                except:\n                    content  None\n                product[step]  content\n            data.append(product)\n        self.df  data\n\n    def transform_to_df_and_improve(self):\n        df  pd.DataFrame(self.df)\n        df  df.assign(keywordself.query)\n        df  df.assign(ecommerce  self.type)\n        df  df.assign(dateTimeReferencedatetime.now().isoformat())\n        df  df.assign(crawlerType  \"Request\")\n        self.df  df\n\n\n",
        "05-redis-mongodb-esse-tal-de-nosql/src/request/r_ml_simple.py\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nclass MercadoLivreCrawler:\n    def execute_command(self, query):\n        url  f\"https://lista.mercadolivre.com.br/{query.replace(' ', '-')}\"\n        headers  {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n        }\n\n        response  requests.get(url, headersheaders)\n        if response.status_code  200:\n            soup  BeautifulSoup(response.text, 'html.parser')\n            results  soup.find_all(\"div\", class_\"ui-search-result\")\n\n            data  []\n            for result in results:\n                link  None\n                title  result.find(\"h2\", class_\"ui-search-item__title\").text.strip()\n                price  result.find(\"span\", class_\"andes-money-amount__fraction\").text.strip()\n                link_tag   result.find(\"a\", class_\"ui-search-link\")\n                if link_tag:\n                    link  link_tag.get(\"href\")\n                data.append({\"Produto\": title, \"Preço\": price, \"URL\": link})\n\n            return data\n        else:\n            print(\"Falha ao fazer a solicitação HTTP.\")\n            return None\n\n    def send_dataframe(self, query):\n        data  self.execute_command(query)\n        if data:\n            df  pd.DataFrame(data)\n            return df\n        else:\n            return None\n\n# Exemplo de utilização\ncrawler  MercadoLivreCrawler()\ndataframe  crawler.send_dataframe(\"iphone 12\")\nif dataframe is not None:\n    print(dataframe)\nelse:\n    print(\"Erro ao obter os dados.\")\n\n\n05-redis-mongodb-esse-tal-de-nosql/src/request/crawlers/default_crawler.py\n\nfrom abc import ABC, abstractmethod\n\nfrom browser.provider.generic_b_crawler import GenericBrowserCrawler\nfrom tools.redis import RedisClient\nfrom tools.mongodb import MongoConnection\n\n\nclass AbstractCrawler(ABC):\n    def __init__(self):\n        self.redis  RedisClient.get()\n        self.mongo  MongoConnection()\n\n    @abstractmethod\n    def crawl(self):\n        pass\n\n    def get_steps(self, site):\n        return self.redis.get(site)\n    \n    def save_data(self, data):\n        try:\n            self.mongo.save_dataframe(data)\n        except:\n            raise(\"Não foi possível salvar os dados no Mongo\")\n\n05-redis-mongodb-esse-tal-de-nosql/src/tools/mongodb.py\n\nimport pandas as pd\nimport os\nfrom pymongo import MongoClient\n\nclass MongoConnection:\n    _instance  None\n\n    def __new__(cls, *args, **kwargs):\n        if not cls._instance:\n            cls._instance  super().__new__(cls)\n            cls._instance._client  None\n            cls._instance._db  None\n            cls._instance._collection  None\n        return cls._instance\n\n    def __init__(self, host'localhost', port27017, database'test', collection'data'):\n        if self._client is None:\n            self.host  os.getenv(\"MONGO_HOST\", host)\n            self.port  int(os.getenv(\"MONGO_PORT\",port))\n            self.database_name  os.getenv(\"MONGO_DATABASE\",database)\n            self.collection_name  os.getenv(\"MONGO_COLLECTION\",collection)\n            self._connect()\n\n    def _connect(self):\n        self._client  MongoClient(self.host, self.port)\n        self._db  self._client[self.database_name]\n        self._collection  self._db[self.collection_name]\n\n    def save_dataframe(self, df):\n        # Convertendo o DataFrame para formato JSON\n        data  df.to_dict(orient'records')\n        # Inserindo os dados na coleção MongoDB\n        self._collection.insert_many(data)\n        print(\"DataFrame salvo no MongoDB com sucesso.\")\n\n    def close_connection(self):\n        if self._client:\n            self._client.close()\n            print(\"Conexão fechada com sucesso.\")\n\n05-redis-mongodb-esse-tal-de-nosql/src/tools/redis.py\n\nimport os\nimport redis\n\n\nclass RedisClient:\n    _instance  None\n\n    def __new__(cls, *args, **kwargs):\n        if not cls._instance:\n            cls._instance  super(RedisClient, cls).__new__(cls, *args, **kwargs)\n            cls._instance._redis_client  cls._instance._connect_to_redis()\n        return cls._instance\n\n    @staticmethod\n    def _load_config():\n        return {\n            \"host\": os.getenv(\"REDIS_HOST\",\"localhost\"),\n            \"port\": os.getenv(\"REDIS_PORT\",6379),\n            \"decode_responses\": True\n        }\n\n    @classmethod\n    def _connect_to_redis(cls):\n        config  cls._load_config()\n        redis_client  redis.StrictRedis(**config)\n        return redis_client\n\n    @classmethod\n    def get(cls):\n        return cls()._redis_client\n\n"
    ],
    "Bootcamp - Cloud para dados": [
        "Bootcamp - Cloud para dados/README.md\n\n## Próximos treinamentos\n\n![pics](/pics/bootcamp_cloud.png)\n\nPara entregar valor ao negócio, é fundamental que nossas aplicações, dashboards, bancos de dados e modelos estejam em produção, ou seja, em uso pelo cliente.\n\nSe você já tentou implementar um serviço em Cloud na AWS, Azure ou GCP, sabe que há muitos desafios envolvidos: desde padrões a seguir, passando por configurações de IAM (acesso), redes privadas, até a implantação de máquinas virtuais e instalação de Docker.\n\nMuitas vezes, profissionais focados em dados e aplicações não possuem esse conhecimento especializado.\n\nO Bootcamp de Cloud para dados foi criado para preencher essa lacuna. É um curso que fornece os elementos essenciais de Cloud para que você possa implantar suas aplicações de forma independente.\n\nComeçamos do zero, com foco em quem não possui experiência em Cloud, e seguimos passo a passo até a implantação das suas primeiras aplicações. Durante o curso, construiremos cinco aplicações em diferentes plataformas de Cloud.\n\nSe você deseja entender VPC, EC2, e toda a infraestrutura necessária para subir suas aplicações, este Bootcamp é ideal para você.\n\nO Bootcamp começará no dia 22 de agosto e terá duração até o dia 04 de outubro.\n\nAs aulas serão ao vivo, sempre às 12h (meio-dia), com duração de 1 hora e 20 minutos. Além disso, todas as aulas serão disponibilizadas na nossa plataforma no mesmo dia, para que você possa assisti-las quantas vezes desejar.\n\nPara fazer parte desse bootcamp, a inscrição será liberada no dia 20/08 ás 20h em nosso canal no Youtube\n\n[![abertura](/pics/capa_youtube_bootcamp_cloud.png)](https://www.youtube.com/watch?vGqgWGZtC-3w)\n\n## Conteúdo completo Bootcamp de cloud\n\nAqui está o calendário completo de aula (podendo sofrer alterações ao longo do curso)\n\nAqui está a tabela atualizada com os nomes dos serviços específicos do Azure e GCP nos respectivos projetos:\n\n| Acesso e início   | Tema da Aula                                            | Objetivo da Aula                                                                                      | Principais Tecnologias Abordadas                          | Carga Horária |\n|-------------------|---------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------|---------------|\n| 23/08/2024 (Quinta) | Introdução a Cloud, criando nossa conta e publicando um site na AWS | Introduzir os conceitos básicos de computação em nuvem, criar uma conta AWS e publicar um site simples. | Amazon S3, Amazon Route 53                                | 1h20          |\n| 24/08/2024 (Sexta)  | Serviços de Armazenamento na Nuvem                    | Entender e gerenciar serviços de armazenamento na nuvem, garantindo segurança e escalabilidade para os dados empresariais. | Amazon S3                                                 | 1h20          |\n| 26/08/2024 (Segunda)| Computação em Nuvem - VMs                             | Explorar a criação e uso de VMs para aplicações escaláveis na nuvem.                                    | Amazon EC2                                                | 1h20          |\n| 27/08/2024 (Terça)  | Projeto EC2 e S3                                      | Implementar um projeto integrando EC2 e S3 para armazenar e processar dados de forma eficiente.         | Amazon EC2, Amazon S3                                     | 1h20          |\n| 29/08/2024 (Quinta) | Gerenciamento de Segurança IAM                        | Configurar e gerenciar identidades e acessos com segurança usando IAM.                                  | AWS IAM                                                   | 1h20          |\n| 30/08/2024 (Sexta)  | Gerenciamento de Redes VPC                            | Configurar redes virtuais na nuvem para otimizar a comunicação entre serviços.                          | Amazon VPC                                                | 1h20          |\n| 02/09/2024 (Segunda)| Banco de Dados Relacional                             | Aprender a configurar e gerenciar bancos de dados relacionais na nuvem.                                 | Amazon RDS                                                | 1h20          |\n| 03/09/2024 (Terça)  | Projeto Frontend e RDS                                | Desenvolver um frontend que interage com um banco de dados relacional na nuvem.                         | Amazon EC2, Amazon RDS                                    | 1h20          |\n| 05/09/2024 (Quinta) | Arquitetura de Eventos Parte 1                        | Entender e criar arquiteturas baseadas em eventos para comunicação entre serviços na nuvem.             | Amazon SNS, Amazon SQS, AWS EventBridge                   | 1h20          |\n| 06/09/2024 (Sexta)  | Arquitetura de Eventos Parte 2                        | Continuar o desenvolvimento de arquiteturas baseadas em eventos, integrando serviços adicionais.        | Amazon SNS, Amazon SQS, AWS EventBridge                   | 1h20          |\n| 09/09/2024 (Segunda)| AWS Lambda Parte 1                                    | Explorar a execução de funções serverless usando AWS Lambda para automatizar processos na nuvem.        | AWS Lambda                                                | 1h20          |\n| 10/09/2024 (Terça)  | AWS Lambda Parte 2                                    | Implementar um projeto completo utilizando AWS Lambda para criar uma arquitetura serverless.            | AWS Lambda                                                | 1h20          |\n| 12/09/2024 (Quinta) | Automatizando Tudo com Terraform                      | Aprender a automatizar a criação e o gerenciamento de infraestruturas na nuvem usando Terraform.        | Terraform, AWS                                            | 1h20          |\n| 13/09/2024 (Sexta)  | Terraform - Avançado                                  | Explorar funcionalidades avançadas do Terraform para gerenciar infraestruturas complexas.               | Terraform, AWS                                            | 1h20          |\n| 16/09/2024 (Segunda)| Projeto PDF Eventos Parte 1                           | Implementar um projeto que processa PDFs em eventos usando AWS Lambda, S3 e SQS.                        | AWS Lambda, Amazon S3, Amazon SQS                         | 1h20          |\n| 17/09/2024 (Terça)  | Projeto PDF Eventos Parte 2                           | Continuar o desenvolvimento do projeto de processamento de PDFs, integrando mais funcionalidades.       | AWS Lambda, Amazon S3, Amazon SQS                         | 1h20          |\n| 19/09/2024 (Quinta) | Introdução ao Azure                                   | Entender os conceitos básicos do Azure e configurar os serviços equivalentes à AWS (Blob Storage, VMs). | Azure Blob Storage, Azure Virtual Machines (VMs)          | 1h20          |\n| 20/09/2024 (Sexta)  | Projeto Azure: PostgreSQL e VNet                      | Implementar um projeto no Azure usando serviços equivalentes ao RDS, IAM e VPC da AWS.                  | Azure Database for PostgreSQL, Azure IAM, Azure Virtual Network (VNet) | 1h20          |\n| 23/09/2024 (Segunda)| Projeto Azure: Functions e Event Grid                 | Criar uma arquitetura serverless no Azure, utilizando funções e eventos, similar ao AWS Lambda.         | Azure Functions, Azure Event Grid                         | 1h20          |\n| 24/09/2024 (Terça)  | Revisão e De-Para AWS-Azure                           | Revisar o aprendizado e discutir o mapeamento de serviços AWS para Azure.                               | AWS, Azure                                                | 1h20          |\n| 26/09/2024 (Quinta) | Introdução ao GCP                                     | Entender os conceitos básicos do Google Cloud Platform e configurar os serviços equivalentes à AWS.     | Google Cloud Storage, Google Compute Engine (VMs)         | 1h20          |\n| 27/09/2024 (Sexta)  | Projeto GCP: Cloud SQL e VPC                          | Implementar um projeto no GCP usando serviços equivalentes ao RDS, IAM e VPC da AWS.                    | Cloud SQL (PostgreSQL), Google IAM, Google Virtual Private Cloud (VPC) | 1h20          |\n| 30/09/2024 (Segunda)| Projeto GCP: Cloud Functions e Pub/Sub                | Criar uma arquitetura serverless no GCP, utilizando funções e eventos, similar ao AWS Lambda.           | Google Cloud Functions, Google Pub/Sub                    | 1h20          |\n| 01/10/2024 (Terça)  | Revisão e De-Para AWS-GCP                             | Revisar o aprendizado e discutir o mapeamento de serviços AWS para GCP.                                 | AWS, Google Cloud Platform (GCP)                          | 1h20          |\n| 03/10/2024 (Quinta) | Containers na AWS Parte 1                             | Introdução ao uso de containers na AWS, utilizando Amazon ECS e Docker.                                 | Amazon ECS (Elastic Container Service), Docker            | 1h20          |\n| 04/10/2024 (Sexta)  | Containers na AWS Parte 2                             | Implementar um projeto utilizando containers na AWS com Amazon ECS e Docker.                            | Amazon ECS, Docker                                        | 1h20          |\n| 07/10/2024 (Segunda)| Containers na AWS Parte 3                             | Introdução ao Kubernetes na AWS, utilizando Amazon EKS.                                                 | Amazon EKS (Elastic Kubernetes Service), Kubernetes       | 1h20          |\n| 08/10/2024 (Terça)  | Containers na AWS Parte 4                             | Implementar um projeto utilizando Kubernetes na AWS com Amazon EKS.                                     | Amazon EKS, Kubernetes                                    | 1h20          |\n| 10/10/2024 (Quinta) | Airflow em Projeto Completo Parte 1                   | Introdução ao Apache Airflow, configurando e criando um pipeline de dados na AWS.                       | Apache Airflow, AWS                                       | 1h20          |\n| 11/10/2024 (Sexta)  | Airflow em Projeto Completo Parte 2                   | Implementar um projeto completo de pipeline de dados usando Apache Airflow na AWS.                      | Apache Airflow, AWS                                       | 1h20          |\n\n",
        "Bootcamp - Cloud para dados/Aula_01/README.md\n\n# Bootcamp Cloud: Aula 01\n\n## Introdução à AWS e Cloud Computing\n\n**Objetivo:** Fornecer uma introdução prática ao uso da AWS, abordando a criação de contas, controle de custos e navegação na interface gráfica, com foco em aplicações na área de dados.\n\n---\n\n## 1. Por que Utilizar Cloud?\n\n1. **Escalabilidade:** A cloud permite escalar recursos conforme necessário, sem a necessidade de grandes investimentos iniciais.\n2. **Custo-eficiência:** Com a cloud, você paga apenas pelo que utiliza, reduzindo custos operacionais.\n3. **Acessibilidade:** Serviços de cloud podem ser acessados de qualquer lugar, facilitando a colaboração e o trabalho remoto.\n4. **Segurança:** Provedores de cloud como AWS oferecem robustos recursos de segurança, incluindo criptografia e autenticação multifator.\n\n## 2. O que Não é Cloud\n\n1. **Não é um Data Center On-Premise:** Embora a cloud envolva servidores, não é o mesmo que ter um data center físico na empresa.\n2. **Não é apenas armazenamento:** Cloud envolve muito mais do que apenas guardar dados. É um ecossistema completo de serviços.\n3. **Não é sempre barato:** Se mal gerido, o uso da cloud pode resultar em custos inesperados.\n\n## 3. Acessando o Site da AWS\n\n1. **Site da AWS:** Visite [aws.amazon.com](https://aws.amazon.com).\n2. **Trocando o Idioma:** \n   - No canto superior direito, clique no ícone de globo e selecione “English”.\n   - **Motivo:** O inglês é o idioma predominante em certificações, documentação, vagas de emprego, e na própria interface da AWS.\n\n## 4. Cadastro e Configuração Inicial da Conta AWS\n\n**Criar uma Conta na AWS:**\n1. Acesse o site oficial da AWS (aws.amazon.com) e clique em \"Create an AWS Account\".\n2. Insira as informações solicitadas, como e-mail, senha e nome da conta.\n3. Selecione o tipo de conta e forneça as informações de pagamento (cartão de crédito/débito).\n4. Verifique a conta via e-mail e complete o processo de criação.\n\n**Configurações Iniciais:**\n- Opte pelo **AWS Free Tier**, que oferece uma camada gratuita com serviços limitados por 12 meses, ideal para quem está começando.\n- Após o cadastro, revise as configurações de segurança e habilite a autenticação multifator (MFA) para proteger a conta.\n\n## 5. Controle de Custos na AWS\n\n**Evitar Custos Inesperados:**\n- **AWS Budgets:** Use o AWS Budgets para configurar alertas de custo. Acesse o serviço pelo console, defina um orçamento mensal e configure notificações por e-mail.\n- **AWS Cost Explorer:** Monitore os gastos em tempo real utilizando o AWS Cost Explorer, que permite visualizar detalhadamente onde e como os recursos estão sendo utilizados.\n\n**Dicas Práticas:**\n- Defina **limites de serviço** para evitar que recursos sejam provisionados além do necessário.\n- Revise regularmente as instâncias e serviços em execução e encerre aqueles que não estão em uso.\n\n## 6. Navegação na Interface Gráfica da AWS\n\n**AWS Management Console:**\n- O **AWS Management Console** é o painel de controle principal da AWS. Ele oferece acesso a todos os serviços disponíveis na plataforma, como S3, EC2 e RDS.\n\n**Serviços Comuns:**\n- **S3 (Simple Storage Service):** Armazenamento escalável de objetos, ideal para dados brutos, backups e arquivos de grande volume.\n- **EC2 (Elastic Compute Cloud):** Serviço que permite criar e gerenciar instâncias de máquinas virtuais na nuvem, altamente configuráveis.\n- **RDS (Relational Database Service):** Serviço gerenciado de banco de dados relacional, que suporta mecanismos como MySQL, PostgreSQL, e SQL Server.\n\n**Documentação e Suporte:**\n- A AWS oferece documentação abrangente e suporte direto pelo console. Acesse a documentação para obter informações detalhadas sobre cada serviço e suas configurações.\n\n## 7. Visão Geral dos Produtos AWS\n\n1. **Explorando Todos os Produtos:**\n   - Navegue até a seção \"Products\" no menu principal.\n   - Explore as diversas categorias: Computação, Armazenamento, Banco de Dados, etc.\n\n2. **EC2 (Elastic Compute Cloud):**\n   - **Visão Geral:** Serviço que permite criar instâncias de servidores virtuais.\n   - **Instâncias de Todos os Tipos:** De uso geral, otimizadas para memória, para computação, etc.\n   - **Democratização:** A cloud, ao permitir acesso fácil a essas tecnologias, democratiza o acesso à internet e ao conhecimento técnico.\n\n3. **Visão Geral de Vários Tipos de Banco de Dados:**\n   - **RDS (Relational Database Service):** Gerencia bancos de dados como MySQL, PostgreSQL, e SQL Server.\n   - **DynamoDB:** Banco de dados NoSQL totalmente gerenciado.\n   - **Redshift:** Armazenamento e análise de dados em larga escala.\n   \n   **Motivação para Estudo:** Ao explorar os diferentes tipos de bancos de dados, você é motivado a estudar e entender novas tecnologias.\n\n## 8. Tipos de Serviço AWS\n\n1. **Free Tier:**\n   - **12 Meses Gratuitos:** Oferece acesso gratuito a muitos serviços durante o primeiro ano.\n   - **Trial Limitado:** Alguns serviços têm um período de trial limitado a 1-3 meses.\n\n## 9. Configuração de Regiões\n\n**Regiões e Zonas de Disponibilidade:**\n- **Regiões** são locais geográficos onde a AWS mantém seus data centers. Cada região é composta por várias **Zonas de Disponibilidade**, que são grupos de data centers fisicamente separados.\n- A escolha da região influencia a latência dos serviços e pode ter implicações legais e de conformidade, dependendo dos dados armazenados.\n\n**Escolha da Região:**\n- Selecione a região mais próxima dos seus usuários ou que atenda aos requisitos específicos de conformidade do seu projeto. No console da AWS, é possível alternar facilmente entre diferentes regiões.\n\n## 10. Configuração de IAM (Identity and Access Management)\n\n**Importância do IAM:**\n- Criação de usuários e grupos com permissões específicas.\n- Segurança aprimorada com autenticação multifator (MFA).\n\n**Criar um Fator de Autenticação:**\n- Acesse o IAM no console AWS.\n- Vá para “Users” e selecione seu usuário root.\n- Clique em “Security credentials” e ative o MFA.\n\n**Uso do Authy:**\n- Utilize o Authy como aplicativo MFA para adicionar uma camada extra de segurança.\n\n**Criar um Acesso Admin no IAM:**\n- Vá para \"Users\" e clique em \"Add user\".\n- Crie um usuário com \"Programmatic access\" e \"AWS Management Console access\".\n- Defina a senha e anote a Access Key.\n- **Add User to Group:** Adicione o usuário ao grupo \"Administrators\".\n- **Attach Permissions:** Associe a permissão \"AdministratorAccess\".\n- **Tags:** Adicione tags para identificar e gerenciar o usuário.\n- **Regions:** Defina as regiões de uso para o usuário.\n\n## 11. Criando o Nosso Primeiro Serviço S3\n\nNesta seção, vamos aprender a criar e configurar um bucket S3 na AWS, além de explorar como armazenar arquivos, configurar permissões e servir um site estático.\n\n### 11.1 Criando o Bucket S3\n\n1. **Nome do Bucket:**\n   - Escolha um nome único para o seu bucket. Lembre-se que o nome deve ser globalmente único dentro da AWS.\n  \n2. **Zona (Region):**\n   - Selecione a região onde o bucket será criado. É recomendável escolher a região mais próxima dos seus usuários para reduzir latência.\n\n3. **ACLs (Access Control Lists):**\n   - Configure as listas de controle de acesso (ACLs) para definir permissões de leitura e escrita. Por padrão, é melhor manter o controle de acesso restrito.\n\n4. **Bucket Público:**\n   - Inicialmente, o bucket deve ser privado por questões de segurança. Se for necessário torná-lo público, isso pode ser feito posteriormente.\n\n5. **Bucket Versioning:**\n   - Habilite o versionamento do bucket para manter várias versões de um objeto. Isso pode ser útil para recuperação de dados e auditoria.\n\n6. **Tags:**\n   - Adicione tags para identificar e organizar seu bucket. Tags podem incluir informações como ambiente (produção, desenvolvimento), proprietário, e finalidade.\n\n7. **Default Encryption:**\n   - Configure a criptografia padrão para garantir que todos os objetos armazenados no bucket sejam criptografados automaticamente.\n\n### 11.2 Acessando o Bucket\n\n- Depois de criar o bucket, acesse-o através do console da AWS. Na interface do bucket, você pode ver e gerenciar todos os objetos armazenados.\n\n### 11.3 Colocando uma Imagem no Bucket\n\n1. **Upload de Arquivo:**\n   - Faça upload de uma imagem ou qualquer outro arquivo no bucket.\n  \n2. **Verificando o Arquivo:**\n   - Após o upload, tente acessar o arquivo diretamente. Você notará que, por padrão, o arquivo não será acessível publicamente.\n\n### 11.4 Configurando Permissões: Tornando o Arquivo Público\n\n1. **Padrão Privado:**\n   - Por padrão, todos os arquivos em um bucket S3 são privados, ou seja, somente você pode acessá-los.\n  \n2. **Tornar o Arquivo Público:**\n   - Para tornar o arquivo acessível publicamente, você pode modificar as permissões do arquivo ou configurar uma política de bucket.\n\n### 11.5 Bucket Policy: Exemplo de Permissão de Leitura para Usuário Anônimo\n\n- **Bucket Policy:**\n   - Adicione uma política ao bucket para permitir que usuários anônimos leiam os arquivos. Um exemplo de política de leitura pública para um bucket S3 pode ser adicionado através do editor de políticas no console da AWS.\n\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::nome-do-bucket/*\"\n        }\n    ]\n}\n```\n\n### 11.6 Adicionando um Arquivo `index.html`\n\n- **Upload do Arquivo `index.html`:**\n   - Adicione um arquivo `index.html` ao seu bucket. Esse arquivo servirá como a página principal do seu site estático.\n\n### 11.7 Abrindo um Website Estático\n\n1. **Configuração de Website Estático:**\n   - No console do S3, habilite a opção de hospedagem de site estático. Defina o `index.html` como o documento de índice.\n  \n2. **Acessando o Website:**\n   - Após a configuração, você poderá acessar o site via URL fornecida pelo S3.\n\n### 11.8 Retornando o `index.html` Diretamente na Raiz\n\n- **Configurando o Redirecionamento:**\n   - Configure para que o `index.html` seja servido diretamente na raiz do site, garantindo que o usuário veja o conteúdo imediatamente ao acessar o URL do bucket.\n\n### 11.9 O que é um Site Estático?\n\n- **Definição:**\n   - Um site estático é um site cujas páginas são servidas exatamente como são armazenadas, sem processamento dinâmico. Isso significa que não há interação com banco de dados ou scripts do lado do servidor, tornando-o ideal para conteúdos simples, como blogs ou páginas de portfólio.\n\n   Aqui está o passo a passo atualizado, começando com o comando `sudo su` para obter acesso root, definindo uma senha para o root, e então seguindo com a instalação do Python e Streamlit, além da execução do aplicativo.\n\n## 12. Criando e Configurando uma Instância EC2\n\nNesta seção, vamos aprender a configurar uma instância EC2, obter acesso root, definir uma senha para o root, e depois configurar o ambiente com Python e Streamlit para rodar um aplicativo básico de \"Hello World\", incluindo a renderização de uma imagem armazenada no S3.\n\n### 12.1 Conectando-se à Instância EC2 e Obtendo Acesso Root\n\n1. **Conectar-se via SSH:**\n   - Use o comando SSH no terminal para conectar-se à instância. O comando será algo assim:\n\n   ```bash\n   ssh -i \"caminho-para-sua-chave.pem\" ec2-user@seu-endereco-publico-ec2\n   ```\n\n2. **Obter Acesso Root:**\n   - Assim que estiver conectado, troque para o usuário root usando o comando:\n\n   ```bash\n   sudo su\n   ```\n\n3. **Definir uma Senha para o Root (se necessário):**\n   - Se você ainda não configurou uma senha para o root, ou deseja alterá-la, execute:\n\n   ```bash\n   passwd\n   ```\n\n   - Você será solicitado a digitar a nova senha para o root e confirmá-la.\n\n### 12.2 Instalando Python, `pip3`, e Streamlit\n\n1. **Atualizar Pacotes e Instalar Python:**\n   - Primeiro, atualize os pacotes e instale Python 3 com os seguintes comandos:\n\n   ```bash\n   yum update -y\n   yum install python3 -y\n   ```\n\n2. **Instalando o `pip3`:**\n   - Instale o `pip3`, que é o gerenciador de pacotes para Python 3:\n\n   ```bash\n   yum install python3-pip -y\n   ```\n\n3. **Verificando a Instalação do `pip3`:**\n   - Verifique se o `pip3` foi instalado corretamente:\n\n   ```bash\n   pip3 --version\n   ```\n\n4. **Instalando o Streamlit:**\n   - Agora que o `pip3` está instalado, você pode instalar o Streamlit com o comando:\n\n   ```bash\n   pip3 install streamlit\n   ```\n\n### 12.3 Criando o Código do Streamlit com a Imagem\n\n1. **Criar um Arquivo Python para o Streamlit:**\n   - Use o editor de texto `nano` para criar e editar o arquivo `app.py`. No terminal, execute:\n\n   ```bash\n   nano app.py\n   ```\n\n2. **Inserir o Código para Renderizar a Imagem:**\n   - Insira o seguinte código no arquivo `app.py`:\n\n   ```python\n   import streamlit as st\n\n   # Título do aplicativo\n   st.title(\"Hello, World!\")\n\n   # Descrição\n   st.write(\"Este é um simples aplicativo Streamlit rodando em uma instância EC2 na AWS.\")\n   ```\n\n3. **Salvar o Arquivo e Sair do Editor:**\n   - Para salvar o arquivo, pressione `Ctrl + O` e depois `Enter`.\n   - Para sair do editor `nano`, pressione `Ctrl + X`.\n\n### 12.4 Executando o Streamlit na Porta 80\n\n1. **Executar o Streamlit na Porta 80:**\n   - Agora que você está logado como root, execute o Streamlit na porta 80:\n\n   ```bash\n   streamlit run app.py --server.port 80 --server.enableCORS false\n   ```\n\n2. **Acessar a Aplicação:**\n   - Acesse o aplicativo no navegador através do endereço IP público da sua instância EC2:\n\n   ```http\n   http://seu-endereco-publico-ec2\n   ```\n\n### Resultado Esperado\n\nAo acessar o aplicativo, você verá o título \"Hello, World!\", uma descrição, e a imagem \"fotodaturma.png\" renderizada diretamente do S3, com a legenda \"Foto da Turma - Primeiro Dia\".\n\nBootcamp - Cloud para dados/Aula_01/app.py\n\n# https://docs.streamlit.io/get-started/tutorials/create-an-app\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\nst.title('Uber pickups in NYC')\n\nDATE_COLUMN  'date/time'\nDATA_URL  ('https://s3.amazonaws.com/www.bootcampdecloudjornadadedados.com/uber-raw-data-sep14.csv.gz')\n\n@st.cache_data\ndef load_data(nrows):\n    data  pd.read_csv(DATA_URL, nrowsnrows)\n    lowercase  lambda x: str(x).lower()\n    data.rename(lowercase, axis'columns', inplaceTrue)\n    data[DATE_COLUMN]  pd.to_datetime(data[DATE_COLUMN])\n    return data\n\ndata_load_state  st.text('Loading data...')\ndata  load_data(10000)\ndata_load_state.text(\"Done! (using st.cache_data)\")\n\nif st.checkbox('Show raw data'):\n    st.subheader('Raw data')\n    st.write(data)\n\nst.subheader('Number of pickups by hour')\nhist_values  np.histogram(data[DATE_COLUMN].dt.hour, bins24, range(0,24))[0]\nst.bar_chart(hist_values)\n\n# Some number in the range 0-23\nhour_to_filter  st.slider('hour', 0, 23, 17)\nfiltered_data  data[data[DATE_COLUMN].dt.hour  hour_to_filter]\n\nst.subheader('Map of all pickups at %s:00' % hour_to_filter)\nst.map(filtered_data)\n\nBootcamp - Cloud para dados/Aula_01/index.html\n\n<!DOCTYPE html>\n<html lang\"en\">\n<head>\n    <meta charset\"UTF-8\">\n    <meta name\"viewport\" content\"widthdevice-width, initial-scale1.0\">\n    <title>Primeira Aula - Bootcamp Cloud</title>\n    <style>\n        body {\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n            background-color: #f4f4f4;\n            color: #333;\n        }\n        .container {\n            max-width: 800px;\n            margin: auto;\n            background: #fff;\n            padding: 20px;\n            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n        }\n        h1 {\n            color: #0066cc;\n        }\n        img {\n            max-width: 100%;\n            height: auto;\n            display: block;\n            margin: 20px 0;\n        }\n        p {\n            font-size: 18px;\n            line-height: 1.6;\n        }\n        .summary {\n            background-color: #e6f7ff;\n            padding: 15px;\n            margin-top: 20px;\n            border-left: 5px solid #0099cc;\n        }\n        .links {\n            margin-top: 20px;\n            font-size: 18px;\n        }\n        .links a {\n            color: #0066cc;\n            text-decoration: none;\n        }\n        .links a:hover {\n            text-decoration: underline;\n        }\n    </style>\n</head>\n<body>\n    <div class\"container\">\n        <h1>Primeira Aula - Bootcamp Cloud</h1>\n        <img src\"fotodaturma.png\" alt\"Foto da Turma\">\n        <p>Hoje foi o primeiro passo de uma jornada de 2 meses! Estamos animados para ver o progresso de todos ao longo deste bootcamp, que vai cobrir os principais aspectos da cloud com foco na AWS.</p>\n        <p>Aproveite cada momento e vamos juntos construir uma base sólida em cloud computing!</p>\n        \n        <div class\"summary\">\n            <h2>Resumo do Bootcamp</h2>\n            <p>O Bootcamp de Cloud para Dados foi criado para ajudar você a implantar suas aplicações de forma independente. Durante os próximos 30 dias, vamos explorar conceitos essenciais de Cloud, incluindo VPC, EC2, IAM, e muito mais. Construiremos cinco aplicações em diferentes plataformas de Cloud, como AWS, Azure e GCP, sempre com foco em entregar valor ao negócio.</p>\n            <p>Se você deseja entender toda a infraestrutura necessária para subir suas aplicações, este Bootcamp é ideal para você. As aulas serão ao vivo, com duração de 1h20 cada, e todas estarão disponíveis na nossa plataforma para revisões. Prepare-se para uma imersão completa na nuvem!</p>\n        </div>\n\n        <div class\"links\">\n            <p>Para saber mais, visite nosso site: <a href\"https://www.suajornadadedados.com.br\" target\"_blank\">www.suajornadadedados.com.br</a></p>\n            <p>Para realizar a inscrição, clique aqui: <a href\"https://chk.eduzz.com/e2d3br2f\" target\"_blank\">Inscrição Bootcamp Cloud</a></p>\n        </div>\n    </div>\n</body>\n</html>\n\n\n",
        "Bootcamp - Cloud para dados/Aula_02/README.md\n\n# Bootcamp Cloud: Aula 02\n\n## S3: Armazenamento de Dados na AWS\n\n**Objetivo:** Explorar as diversas aplicações do Amazon S3 no contexto de engenharia, ciência e análise de dados, com um foco prático em como configurar e utilizar o serviço via script Python usando o boto3. Além disso, abordaremos a criação de um IAM exclusivo e um grupo de recursos para gerenciar acesso e segurança.\n\n---\n\n### 1. Revisão da Aula Anterior: Verificando Gastos de Ontem e Usando Tags para Filtragem\n\n**Objetivo:** Aprender a utilizar o AWS Cost Explorer para verificar os gastos do dia anterior e aplicar filtros utilizando tags criadas durante o gerenciamento dos recursos.\n\n#### Passo 1: Acessar o AWS Cost Explorer\n\n1. **Login no AWS Management Console:**\n   - Acesse sua conta AWS e no console, digite \"Cost Explorer\" na barra de pesquisa superior e selecione \"Cost Explorer\" nos resultados.\n\n2. **Ativando o Cost Explorer (se necessário):**\n   - Se esta for a primeira vez que você está usando o Cost Explorer, será necessário ativá-lo. Clique em \"Enable Cost Explorer\".\n\n#### Passo 2: Verificar os Gastos de Ontem\n\n1. **Configurar o Período de Tempo:**\n   - No Cost Explorer, selecione o intervalo de datas. No campo \"Filter by Date\", escolha a data de ontem como início e fim.\n\n2. **Visualizar Gastos:**\n   - O Cost Explorer mostrará os gastos do dia anterior, distribuídos por serviço, região, ou qualquer outro filtro padrão da AWS.\n\n#### Passo 3: Usar Tags para Filtragem de Custos\n\n1. **Aplicar Filtros de Tag:**\n   - Para ver os custos associados a um grupo específico de recursos, clique em \"Add Filter\" e selecione \"Tag\".\n\n2. **Selecionar Tags Relevantes:**\n   - Escolha a tag que você criou anteriormente, como `Environment: Production` ou `Project: DataAnalysis`. Isso permitirá que você visualize apenas os custos associados a recursos marcados com essas tags.\n\n3. **Analisar Resultados:**\n   - O Cost Explorer atualizará os resultados para mostrar apenas os gastos relacionados aos recursos que possuem a tag selecionada.\n\n#### Passo 4: Configurar Relatórios Personalizados\n\n1. **Criar um Relatório Customizado:**\n   - Se você quiser acompanhar esses custos regularmente, pode criar um relatório customizado clicando em \"Save as Report\".\n   - Dê um nome ao relatório (ex: \"Gastos Diários por Tag\") e defina as configurações para receber atualizações automáticas por e-mail.\n\n#### Passo 5: Boas Práticas ao Usar Tags e Monitorar Custos\n\n1. **Consistência na Aplicação de Tags:**\n   - Certifique-se de que todas as equipes utilizem as mesmas convenções de tags para facilitar a filtragem e monitoramento dos custos.\n\n2. **Revisão Regular de Custos:**\n   - Incorpore a revisão diária ou semanal dos custos como parte do seu workflow para evitar surpresas no faturamento.\n\n3. **Alertas de Orçamento:**\n   - Use o AWS Budgets para criar alertas de orçamento baseados em tags, garantindo que você seja notificado se os gastos superarem os limites previstos.\n\n---\n\n## 2. O que é Storage?\n\n1. **Definição:**\n   - Storage, ou armazenamento, refere-se ao espaço onde dados digitais são guardados. No contexto da AWS, o S3 é um serviço de storage de objetos, onde arquivos de todos os tipos e tamanhos podem ser armazenados e acessados com alta disponibilidade e durabilidade.\n\n2. **Tipos de Storage na AWS:**\n   - Além do S3, a AWS oferece outros tipos de storage, como EBS (Elastic Block Store) para armazenamento de blocos, e o Amazon EFS (Elastic File System) para armazenamento de arquivos. O S3 é focado no armazenamento de objetos, ideal para grandes volumes de dados não estruturados.\n\n### 2.1 Vantagem da Redundância\n\n1. **O que é Redundância?**\n   - Redundância em storage se refere à duplicação de dados em múltiplos locais para garantir a disponibilidade e durabilidade dos dados, mesmo em caso de falhas.\n\n2. **Vantagem da Redundância no S3:**\n   - O Amazon S3 armazena automaticamente os dados em pelo menos três zonas de disponibilidade (AZs) dentro de uma região, oferecendo proteção contra falhas em qualquer uma dessas zonas.\n   - Isso garante que os dados estejam sempre acessíveis e seguros, mesmo em situações de desastres naturais ou falhas em data centers.\n\n3. **Como Funciona a Redundância no S3?**\n   - Ao enviar um arquivo para um bucket S3, o dado é replicado automaticamente em múltiplas AZs. Essa replicação é transparente para o usuário, que não precisa se preocupar com a configuração ou gerenciamento desse processo.\n   - A redundância é uma das principais razões pelas quais o S3 oferece uma durabilidade de 99.999999999% (11 noves), garantindo que os dados sejam preservados a longo prazo.\n\n---\n\n## 3. Criação de um Bucket S3: Passo a Passo e Detalhes\n\n### 3.1 Criando um Bucket S3 via AWS Management Console\n\n1. **Acessando o AWS Management Console:**\n   - Faça login no [AWS Management Console](https://aws.amazon.com) e navegue até o serviço S3. Isso pode ser feito digitando \"S3\" na barra de pesquisa superior e selecionando \"S3\" nos resultados.\n\n2. **Iniciando a Criação do Bucket:**\n   - Na página do S3, clique em “Create bucket” para iniciar o processo de criação.\n\n3. **Nome do Bucket:**\n   - **Bucket name:** Insira um nome único para o bucket. O nome deve ser globalmente exclusivo em todas as regiões da AWS e seguir as regras de nomenclatura (por exemplo, sem espaços, apenas caracteres minúsculos).\n   - **Exemplo:** `meu-bucket-dados-2024`\n\n4. **Escolhendo a Região:**\n   - **Region:** Selecione a região onde o bucket será criado. A escolha da região pode afetar a latência e os custos de transferência de dados. É recomendável escolher uma região próxima dos usuários finais ou do local onde os dados serão processados.\n   - **Exemplo:** `US West (Oregon)`\n\n5. **Configurações de Controle de Acesso:**\n   - **Block Public Access settings for this bucket:** Por padrão, a AWS bloqueia o acesso público a novos buckets. Deixe essa configuração ativada para garantir a segurança dos dados.\n   - **Block all public access:** Mantendo essa opção ativada, você evita que qualquer pessoa fora da sua conta AWS tenha acesso aos dados do bucket.\n\n6. **Configurações de Versionamento:**\n   - **Bucket Versioning:** Habilitar o versionamento permite que o S3 mantenha várias versões de um objeto, o que é útil para proteção contra exclusões acidentais e para auditoria.\n   - **Opções:**\n     - **Enable:** Ativa o versionamento.\n     - **Disable:** Desativa o versionamento (padrão).\n     - **Suspend:** Suspende o versionamento, mantendo versões anteriores sem criar novas versões.\n   - **Recomendação:** Habilitar o versionamento se o bucket for usado para armazenar dados críticos ou históricos.\n\n7. **Configurações de Logs de Acesso:**\n   - **Server access logging:** Essa opção permite registrar todas as solicitações de acesso ao bucket, o que é útil para auditoria e monitoramento de segurança.\n   - **Opções:**\n     - **Enable:** Ativa o registro de logs.\n     - **Target bucket:** Especifique um bucket para armazenar os logs de acesso.\n     - **Target prefix:** Defina um prefixo para os logs, organizando-os dentro do bucket de destino.\n   - **Recomendação:** Ativar se precisar monitorar acessos ao bucket.\n\n8. **Criptografia:**\n   - **Default encryption:** A criptografia padrão garante que todos os objetos armazenados no bucket sejam criptografados automaticamente.\n   - **Opções:**\n     - **AES-256:** Criptografia usando a chave gerenciada pela AWS (AES-256).\n     - **AWS-KMS:** Criptografia com uma chave gerenciada pelo AWS Key Management Service (KMS), o que oferece maior controle sobre as chaves de criptografia.\n   - **Recomendação:** Habilitar a criptografia padrão, especialmente se você estiver armazenando dados sensíveis.\n\n9. **Etiquetas (Tags):**\n   - **Add tags:** As tags são pares chave-valor que ajudam a organizar e gerenciar seus buckets.\n   - **Exemplo:** \n     - **Key:** `Environment`\n     - **Value:** `Production`\n   - **Recomendação:** Usar tags para facilitar a identificação e gerenciamento de buckets em ambientes grandes ou complexos.\n\n10. **Gerenciamento de Ciclo de Vida:**\n    - **Object Lock:** Esse recurso protege os objetos de serem excluídos ou modificados por um período especificado. É útil para conformidade regulatória e proteção contra exclusão acidental.\n    - **Opções:**\n      - **Enable:** Ativa o Object Lock (requer habilitação de versionamento).\n    - **Recomendação:** Ativar apenas se houver necessidade específica de reten\n\nção de dados.\n\n11. **Configurações Avançadas:**\n    - **Transfer acceleration:** Acelera transferências de dados para e do bucket, usando pontos de presença da AWS globalmente.\n    - **Requester Pays:** Esta configuração faz com que o solicitante pague pelos custos de download de dados do bucket, útil em cenários de compartilhamento de dados públicos.\n    - **Intelligent-Tiering:** Automatiza o movimento de objetos entre camadas de armazenamento de custo mais baixo com base nos padrões de acesso.\n    - **Lifecycle rules:** Crie regras de ciclo de vida para transitar objetos entre diferentes classes de armazenamento (ex: de Standard para Glacier) com base em políticas de retenção.\n    - **Recomendação:** Avaliar cada uma dessas opções com base nas necessidades do projeto.\n\n12. **Revisão e Criação do Bucket:**\n    - Revise todas as configurações que você escolheu para o bucket.\n    - Clique em \"Create bucket\" para finalizar a criação.\n\n13. **Verificação e Acesso:**\n    - Após a criação, o novo bucket aparecerá na lista de buckets do S3. Clique no nome do bucket para acessar a interface de gerenciamento, onde você pode começar a carregar arquivos, configurar permissões adicionais, ou ajustar configurações.\n\n### 3.2 Recursos Avançados no Painel S3\n\n1. **Intelligent-Tiering Archive Configurations:**\n   - **Descrição:** O Intelligent-Tiering é uma classe de armazenamento que move automaticamente os objetos entre diferentes camadas de armazenamento (frequentemente acessado e raramente acessado) com base em padrões de acesso. É ideal para otimizar custos sem sacrificar a performance.\n   - **Archive Configurations:** Permite a configuração de arquivamento de objetos para camadas de armazenamento ainda mais econômicas, como Deep Archive.\n\n2. **Server Access Logging:**\n   - **Descrição:** Essa propriedade permite registrar todas as solicitações feitas ao bucket S3, incluindo acessos e modificações. Esses logs podem ser armazenados em outro bucket S3 para auditoria e análise.\n   - **Recomendação:** Habilitar logging pode ser útil para monitoramento de segurança e compliance.\n\n3. **AWS CloudTrail Data Events:**\n   - **Descrição:** AWS CloudTrail pode ser configurado para registrar eventos de dados do S3, como acessos a objetos específicos ou mudanças em permissões. **Este serviço pode ter custos associados**, especialmente se muitos eventos forem registrados.\n   - **Recomendação:** Usar o CloudTrail para monitorar eventos críticos, como alterações em buckets sensíveis.\n\n4. **Event Notifications:**\n   - **Descrição:** Configura notificações para eventos específicos no bucket, como a criação de novos objetos ou exclusão de objetos. Essas notificações podem acionar funções Lambda, filas SQS, ou tópicos SNS.\n   - **Exemplo de Uso:** Automatizar a geração de thumbnails quando uma nova imagem é carregada no bucket.\n\n5. **Object Lock:**\n   - **Descrição:** O Object Lock impede que objetos sejam excluídos ou sobrescritos por um período determinado, o que é útil para conformidade regulatória e prevenção de perda de dados.\n   - **Recomendação:** Usar em buckets que armazenam dados sensíveis que não podem ser alterados ou excluídos.\n\n6. **Static Website Hosting:**\n   - **Descrição:** Permite configurar o bucket S3 para hospedar um site estático, servindo arquivos como HTML, CSS e JavaScript diretamente da nuvem.\n   - **Exemplo de Uso:** Hospedagem de páginas web simples, como portfólios ou blogs estáticos.\n\n### 3.3 Exemplo Prático: Bucket Temporário com Regras de Ciclo de Vida\n\n1. **Cenário:** Vamos supor que você esteja gerenciando um pipeline de ETL (Extract, Transform, Load) onde os dados são extraídos de várias fontes e carregados em um bucket temporário para processamento.\n\n2. **Processo:**\n   - **Passo 1:** Criação do Bucket Temporário\n     - Nomeie o bucket como `temp-etl-bucket`.\n     - Este bucket armazena os dados extraídos antes do processamento.\n   \n   - **Passo 2:** Processamento dos Dados\n     - Uma vez que os dados estejam no bucket temporário, um job de processamento (como um script Python ou um AWS Lambda) é acionado para transformar os dados.\n     - Após o processamento, os dados transformados são movidos para um bucket final, `processed-data-bucket`.\n\n   - **Passo 3:** Aplicação de Regras de Ciclo de Vida\n     - Configure uma regra de ciclo de vida para o bucket `temp-etl-bucket`:\n       - **Transição de Armazenamento:** Após 7 dias, mova os dados para uma classe de armazenamento mais econômica, como Glacier.\n       - **Exclusão Automática:** Após 30 dias, exclua permanentemente os objetos do bucket.\n\n3. **Configuração da Regra de Ciclo de Vida:**\n   - No console do S3, vá para a aba “Management” e selecione “Lifecycle Rules”.\n   - Clique em “Create lifecycle rule”.\n   - Nomeie a regra (ex: `TempDataCleanup`).\n   - Defina as condições:\n     - **Prefix:** Se necessário, limite a regra a um determinado prefixo (por exemplo, `/temp`).\n     - **Transition:** Mova os dados para o Glacier após 7 dias.\n     - **Expiration:** Exclua os dados após 30 dias.\n\n4. **Benefícios:**\n   - **Redução de Custos:** Ao mover os dados para o Glacier e posteriormente excluí-los, você reduz custos de armazenamento.\n   - **Automação:** As regras de ciclo de vida garantem que o bucket temporário seja gerenciado automaticamente, sem necessidade de intervenção manual.\n   - **Segurança:** Garantir que os dados temporários sejam excluídos ajuda a minimizar riscos de segurança e conformidade.\n\n---\n\n## 4. Exemplo de Uso do S3 via Python (boto3)\n\n### 4.1 Instalando o boto3\n\n1. **Instalação do boto3:**\n   - Para interagir com o S3 via Python, é necessário instalar o boto3, o SDK da AWS para Python:\n\n   ```bash\n   pip install boto3\n   ```\n\n### 4.2 Configurando Credenciais AWS\n\n1. **Configuração das Credenciais:**\n   - Antes de utilizar o boto3, é necessário configurar as credenciais da AWS. Isso pode ser feito através do `aws configure` ou definindo variáveis de ambiente.\n\n   ```bash\n   aws configure\n   ```\n\n2. **Configuração Manual (Opcional):**\n   - As credenciais também podem ser configuradas manualmente:\n\n   ```python\n   import boto3\n\n   session  boto3.Session(\n       aws_access_key_id'YOUR_ACCESS_KEY',\n       aws_secret_access_key'YOUR_SECRET_KEY',\n       region_name'us-west-2'\n   )\n   s3  session.resource('s3')\n   ```\n\n### 4.3 Criando e Gerenciando Buckets\n\n1. **Criando um Bucket:**\n   - Use o boto3 para criar um novo bucket S3:\n\n   ```python\n   import boto3\n\n   s3  boto3.client('s3')\n   bucket_name  'meu-novo-bucket'\n\n   s3.create_bucket(Bucketbucket_name, CreateBucketConfiguration{\n       'LocationConstraint': 'us-west-2'})\n   ```\n\n2. **Listando Buckets:**\n   - Verifique todos os buckets existentes:\n\n   ```python\n   response  s3.list_buckets()\n\n   for bucket in response['Buckets']:\n       print(bucket['Name'])\n   ```\n\n### 4.4 Upload e Download de Arquivos\n\n1. **Upload de Arquivos:**\n   - Carregue um arquivo local para o S3:\n\n   ```python\n   s3.upload_file('local-file.txt', bucket_name, 's3-file.txt')\n   ```\n\n2. **Download de Arquivos:**\n   - Baixe um arquivo do S3 para o sistema local:\n\n   ```python\n   s3.download_file(bucket_name, 's3-file.txt', 'local-file.txt')\n   ```\n\n### 4.5 Configurando ACLs e Políticas de Bucket\n\n1. **Definindo Permissões de Acesso:**\n   - Use boto3 para definir permissões de leitura/escrita:\n\n   ```python\n   s3.put_object_acl(Bucketbucket_name, Key's3-file.txt', ACL'public-read')\n   ```\n\n2. **Políticas de Bucket:**\n   - Configure uma política de bucket para permitir acesso público:\n\n   ```python\n   bucket_policy  {\n       'Version': '2012-10-17',\n       'Statement': [{\n           'Sid': 'PublicReadGetObject',\n           'Effect': 'Allow',\n           'Principal': '*',\n           'Action': 's3:GetObject',\n           'Resource': f'arn:aws:s3:::{bucket_name}/*'\n       }]\n   }\n\n   s3.put_bucket_policy(Bucketbucket_name, Policyjson.dumps(bucket_policy))\n   ```\n\n### 4.6 Excluindo Objetos e Buckets\n\n1. **Excluir um Objeto:**\n   - Remova um arquivo do S3:\n\n   ```python\n   s3.delete_object(Bucketbucket_name, Key's3-file.txt')\n   ```\n\n2. **Excluir um Bucket:**\n   - Delete um bucket (após excluir todos os objetos):\n\n   ```python\n   s3.delete_bucket(Bucketbucket_name)\n   ```\n\n---\n\n## 5.\n\n Casos de Uso do S3 em Engenharia, Ciência e Análise de Dados\n\n### 5.1 Engenharia de Dados\n\n1. **Data Lake:**\n   - O S3 é frequentemente utilizado como uma camada de armazenamento em data lakes, onde dados brutos de diferentes fontes são armazenados antes de serem processados e analisados.\n   - Vantagens incluem escalabilidade, baixo custo, e integração com serviços como AWS Glue e Amazon Athena.\n\n2. **ETL (Extract, Transform, Load):**\n   - O S3 é essencial em pipelines ETL, onde dados são extraídos de fontes diversas, transformados, e armazenados no S3 antes de serem carregados em um data warehouse ou banco de dados.\n   - Exemplos incluem armazenar arquivos CSV ou Parquet que são processados periodicamente por ferramentas como Apache Spark ou AWS Glue.\n\n3. **Backup e Arquivamento:**\n   - O S3 é uma solução robusta para backup e arquivamento de dados críticos, com suporte para versionamento e regras de ciclo de vida para mover dados entre diferentes classes de armazenamento, como S3 Glacier.\n\n### 5.2 Ciência de Dados\n\n1. **Armazenamento de Datasets:**\n   - Cientistas de dados podem utilizar o S3 para armazenar grandes datasets que são utilizados em modelagem, treinamento de algoritmos de machine learning, e análise exploratória de dados.\n\n2. **Integração com Amazon SageMaker:**\n   - O S3 é integrado ao Amazon SageMaker, permitindo que datasets sejam carregados diretamente do S3 para notebooks e treinamentos de modelos de machine learning.\n   - Modelos treinados também podem ser armazenados no S3 para versionamento e reutilização.\n\n3. **Data Sharing e Colaboração:**\n   - O S3 permite o compartilhamento de grandes datasets com equipes distribuídas ou com outros pesquisadores, através da configuração de permissões e políticas de acesso.\n\n### 5.3 Análise de Dados\n\n1. **Análise com Amazon Athena:**\n   - O Amazon S3 pode ser usado como o local de armazenamento para dados analisados com Amazon Athena, que permite executar queries SQL diretamente sobre dados armazenados no S3 sem a necessidade de carregar esses dados para um banco de dados relacional.\n\n2. **Dashboards e Reporting:**\n   - Dados armazenados no S3 podem ser facilmente integrados a ferramentas de BI e reporting, como Amazon QuickSight ou Tableau, para criar dashboards e relatórios atualizados em tempo real.\n\n3. **Log Storage e Análise:**\n   - Logs de aplicações, servidores, e eventos podem ser armazenados no S3 e analisados usando Amazon Athena ou serviços de terceiros, ajudando a identificar padrões e insights operacionais.\n\n---\n\n**Conclusão:** Nesta aula, exploramos o Amazon S3 como um serviço fundamental para a gestão de dados na AWS, cobrindo desde a criação e configuração de buckets, até exemplos práticos de uso com Python e casos de uso em diferentes áreas. Além disso, discutimos boas práticas de segurança e gerenciamento de dados, incluindo o uso de IAM e grupos de recursos, para garantir que seus dados estejam seguros e bem gerenciados.\n\n",
        "Bootcamp - Cloud para dados/Aula_02/projeto/README.md\n\n## Projeto Python: Backup de Arquivos Locais para o S3\n\n### Introdução\n\nO objetivo deste projeto é criar um sistema automatizado de backup utilizando Python e o serviço de armazenamento de objetos Amazon S3. O projeto será responsável por ler arquivos de uma pasta local no seu computador, enviar esses arquivos para um bucket S3 na AWS e, em seguida, deletar os arquivos locais após o upload bem-sucedido.\n\nEssa solução é ideal para cenários onde é necessário garantir que os dados locais sejam salvos em um ambiente seguro e acessível, liberando espaço de armazenamento no dispositivo local e protegendo contra perdas de dados.\n\n### Diagrama do Fluxo do Projeto\n\n```mermaid\ngraph TD\n    A[Início do Backup] --> B{Listar Arquivos na Pasta Local}\n    B -->|Arquivos Encontrados| C[Upload dos Arquivos para S3]\n    B -->|Nenhum Arquivo Encontrado| F[Fim do Processo]\n    C --> D{Upload Bem-Sucedido?}\n    D -->|Sim| E[Deletar Arquivos Locais]\n    D -->|Não| G[Registrar Erro]\n    E --> F[Fim do Processo]\n    G --> F[Fim do Processo]\n```\n\n### Explicação do Fluxo:\n\n1. **Início do Backup:** O processo começa com a execução do script Python.\n2. **Listar Arquivos na Pasta Local:** O script verifica a presença de arquivos na pasta local especificada.\n   - Se arquivos forem encontrados, o processo continua.\n   - Se nenhum arquivo for encontrado, o processo termina.\n3. **Upload dos Arquivos para S3:** Os arquivos listados são enviados para o bucket S3 designado.\n4. **Upload Bem-Sucedido?:** Verifica se o upload dos arquivos foi realizado com sucesso.\n   - Se o upload for bem-sucedido, os arquivos locais são deletados.\n   - Se ocorrer algum erro durante o upload, o erro é registrado para análise posterior.\n5. **Fim do Processo:** O processo de backup é concluído, seja após o upload e deleção dos arquivos ou após a detecção de um erro.\n\n### 1. Configuração Inicial do Projeto\n\n#### Passo 1: Criação do Ambiente de Desenvolvimento\n\n1. **Criar uma Pasta para o Projeto:**\n   - Crie uma nova pasta para o seu projeto, por exemplo: `backup_s3`.\n\n2. **Criar um Ambiente Virtual (Opcional, mas Recomendado):**\n   - Navegue até a pasta do projeto e crie um ambiente virtual:\n   \n   ```bash\n   python -m venv venv\n   ```\n\n   - Ative o ambiente virtual:\n     - **Windows:** `venv\\Scripts\\activate`\n     - **Linux/MacOS:** `source venv/bin/activate`\n\n3. **Instalar Dependências Necessárias:**\n   - Instale o boto3 para interagir com o S3 e outras bibliotecas necessárias:\n   \n   ```bash\n   pip install boto3\n   ```\n\n4. **Criar um Arquivo `requirements.txt`:**\n   - Para facilitar a instalação de dependências no futuro, crie um arquivo `requirements.txt`:\n   \n   ```bash\n   boto3\n   ```\n\n   - Você pode gerar este arquivo automaticamente com:\n   \n   ```bash\n   pip freeze > requirements.txt\n   ```\n\n### 2. Configuração do AWS IAM e S3\n\n1. **Criar um Bucket S3:**\n   - Siga os passos mencionados na seção anterior para criar um bucket S3 que será utilizado para armazenar os backups.\n\n2. **Criar um Usuário IAM para o Projeto:**\n   - Crie um usuário IAM com permissões para acessar o bucket S3 específico.\n   - Obtenha as credenciais (Access Key ID e Secret Access Key) que serão usadas no código.\n\n### 3. Implementação do Código Python\n\n#### Passo 1: Configurar as Credenciais AWS\n\n1. **Criar um Arquivo de Configuração:**\n   - Crie um arquivo `.env` para armazenar as credenciais do S3 de forma segura:\n   \n   ```\n   AWS_ACCESS_KEY_IDYOUR_ACCESS_KEY\n   AWS_SECRET_ACCESS_KEYYOUR_SECRET_KEY\n   AWS_REGIONYOUR_REGION\n   BUCKET_NAMEYOUR_BUCKET_NAME\n   ```\n\n   - Instale a biblioteca `python-dotenv` para carregar essas variáveis de ambiente:\n   \n   ```bash\n   pip install python-dotenv\n   ```\n\n2. **Carregar as Credenciais no Código:**\n\n   - No arquivo principal do projeto (`backup.py`), carregue as variáveis de ambiente:\n   \n   ```python\n   import os\n   from dotenv import load_dotenv\n\n   load_dotenv()\n\n   AWS_ACCESS_KEY_ID  os.getenv('AWS_ACCESS_KEY_ID')\n   AWS_SECRET_ACCESS_KEY  os.getenv('AWS_SECRET_ACCESS_KEY')\n   AWS_REGION  os.getenv('AWS_REGION')\n   BUCKET_NAME  os.getenv('BUCKET_NAME')\n   ```\n\n#### Passo 2: Listar e Fazer Upload dos Arquivos\n\n1. **Listar Arquivos em uma Pasta Local:**\n\n   - No mesmo arquivo `backup.py`, adicione o código para listar todos os arquivos em uma pasta específica:\n   \n   ```python\n   import os\n\n   def listar_arquivos(pasta):\n       arquivos  []\n       for nome_arquivo in os.listdir(pasta):\n           caminho_completo  os.path.join(pasta, nome_arquivo)\n           if os.path.isfile(caminho_completo):\n               arquivos.append(caminho_completo)\n       return arquivos\n   ```\n\n2. **Fazer Upload dos Arquivos para o S3:**\n\n   - Utilize o boto3 para fazer o upload dos arquivos listados para o S3:\n   \n   ```python\n   import boto3\n\n   s3_client  boto3.client(\n       's3',\n       aws_access_key_idAWS_ACCESS_KEY_ID,\n       aws_secret_access_keyAWS_SECRET_ACCESS_KEY,\n       region_nameAWS_REGION\n   )\n\n   def upload_arquivos_para_s3(arquivos):\n       for arquivo in arquivos:\n           nome_arquivo  os.path.basename(arquivo)\n           s3_client.upload_file(arquivo, BUCKET_NAME, nome_arquivo)\n           print(f'{nome_arquivo} foi enviado para o S3.')\n   ```\n\n3. **Deletar Arquivos Locais Após o Upload:**\n\n   - Após o upload, delete os arquivos locais para liberar espaço:\n   \n   ```python\n   def deletar_arquivos_locais(arquivos):\n       for arquivo in arquivos:\n           os.remove(arquivo)\n           print(f'{arquivo} foi deletado do local.')\n   ```\n\n#### Passo 3: Integrar Tudo e Rodar o Backup\n\n1. **Função Principal do Projeto:**\n\n   - Combine todas as funções em uma função principal que executa o backup:\n   \n   ```python\n   def executar_backup(pasta):\n       arquivos  listar_arquivos(pasta)\n       if arquivos:\n           upload_arquivos_para_s3(arquivos)\n           deletar_arquivos_locais(arquivos)\n       else:\n           print(\"Nenhum arquivo encontrado para backup.\")\n\n   if __name__  \"__main__\":\n       PASTA_LOCAL  'caminho/para/sua/pasta'\n       executar_backup(PASTA_LOCAL)\n   ```\n\n2. **Rodar o Projeto:**\n\n   - Salve o código e execute o script:\n   \n   ```bash\n   python backup.py\n   ```\n\n### 4. Testar e Implementar Melhorias\n\n1. **Testar o Backup:**\n   - Coloque alguns arquivos na pasta local e execute o script. Verifique se os arquivos foram enviados corretamente para o S3 e se foram deletados do seu computador.\n\n2. **Melhorias Futuras:**\n   - **Logs:** Adicione logging para monitorar as operações.\n   - **Exceções:** Implemente tratamento de exceções para lidar com erros durante o upload ou a exclusão de arquivos.\n   - **Agendamento:** Considere utilizar o `cron` no Linux ou o Agendador de Tarefas no Windows para agendar execuções automáticas do script.\n\nBootcamp - Cloud para dados/Aula_02/projeto/main.py\n\nimport os\nfrom typing import List\nimport boto3\nfrom dotenv import load_dotenv\n\n# Carrega as variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Configurações da AWS a partir do .env\nAWS_ACCESS_KEY_ID: str  os.getenv('AWS_ACCESS_KEY_ID')\nAWS_SECRET_ACCESS_KEY: str  os.getenv('AWS_SECRET_ACCESS_KEY')\nAWS_REGION: str  os.getenv('AWS_REGION')\nBUCKET_NAME: str  os.getenv('BUCKET_NAME')\n\n# Configura o cliente S3\ns3_client  boto3.client(\n    's3',\n    aws_access_key_idAWS_ACCESS_KEY_ID,\n    aws_secret_access_keyAWS_SECRET_ACCESS_KEY,\n    region_nameAWS_REGION\n)\n\ndef listar_arquivos(pasta: str) -> List[str]:\n    \"\"\"Lista todos os arquivos em uma pasta local.\"\"\"\n    arquivos: List[str]  []\n    for nome_arquivo in os.listdir(pasta):\n        caminho_completo  os.path.join(pasta, nome_arquivo)\n        if os.path.isfile(caminho_completo):\n            arquivos.append(caminho_completo)\n    return arquivos\n\ndef upload_arquivos_para_s3(arquivos: List[str]) -> None:\n    \"\"\"Faz upload dos arquivos listados para o S3.\"\"\"\n    for arquivo in arquivos:\n        nome_arquivo: str  os.path.basename(arquivo)\n        s3_client.upload_file(arquivo, BUCKET_NAME, nome_arquivo)\n        print(f'{nome_arquivo} foi enviado para o S3.')\n\ndef deletar_arquivos_locais(arquivos: List[str]) -> None:\n    \"\"\"Deleta os arquivos locais após o upload.\"\"\"\n    for arquivo in arquivos:\n        os.remove(arquivo)\n        print(f'{arquivo} foi deletado do local.')\n\ndef executar_backup(pasta: str) -> None:\n    \"\"\"Executa o processo completo de backup.\"\"\"\n    arquivos: List[str]  listar_arquivos(pasta)\n    if arquivos:\n        upload_arquivos_para_s3(arquivos)\n        deletar_arquivos_locais(arquivos)\n    else:\n        print(\"Nenhum arquivo encontrado para backup.\")\n\nif __name__  \"__main__\":\n    PASTA_LOCAL: str  'caminho/para/sua/pasta'  # Substitua pelo caminho da sua pasta local\n    executar_backup(PASTA_LOCAL)\n\n\nBootcamp - Cloud para dados/Aula_02/projeto/main_v2.py\n\nimport os\nfrom typing import List\nimport boto3\nfrom dotenv import load_dotenv\n\n# Carrega as variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Configurações da AWS a partir do .env\nAWS_ACCESS_KEY_ID: str  os.getenv('AWS_ACCESS_KEY_ID')\nAWS_SECRET_ACCESS_KEY: str  os.getenv('AWS_SECRET_ACCESS_KEY')\nAWS_REGION: str  os.getenv('AWS_REGION')\nBUCKET_NAME: str  os.getenv('BUCKET_NAME')\n\n# Print para verificar se as variáveis de ambiente foram carregadas corretamente\nprint(f\"AWS_ACCESS_KEY_ID: {AWS_ACCESS_KEY_ID}\")\nprint(f\"AWS_SECRET_ACCESS_KEY: {AWS_SECRET_ACCESS_KEY}\")\nprint(f\"AWS_REGION: {AWS_REGION}\")\nprint(f\"BUCKET_NAME: {BUCKET_NAME}\")\n\n# Configura o cliente S3\ntry:\n    s3_client  boto3.client(\n        's3',\n        aws_access_key_idAWS_ACCESS_KEY_ID,\n        aws_secret_access_keyAWS_SECRET_ACCESS_KEY,\n        region_nameAWS_REGION\n    )\n    print(\"Cliente S3 configurado com sucesso.\")\nexcept Exception as e:\n    print(f\"Erro ao configurar o cliente S3: {e}\")\n    raise\n\ndef listar_arquivos(pasta: str) -> List[str]:\n    \"\"\"Lista todos os arquivos em uma pasta local.\"\"\"\n    arquivos: List[str]  []\n    try:\n        for nome_arquivo in os.listdir(pasta):\n            caminho_completo  os.path.join(pasta, nome_arquivo)\n            if os.path.isfile(caminho_completo):\n                arquivos.append(caminho_completo)\n        print(f\"Arquivos listados na pasta '{pasta}': {arquivos}\")\n    except Exception as e:\n        print(f\"Erro ao listar arquivos na pasta '{pasta}': {e}\")\n        raise\n    return arquivos\n\ndef upload_arquivos_para_s3(arquivos: List[str]) -> None:\n    \"\"\"Faz upload dos arquivos listados para o S3.\"\"\"\n    for arquivo in arquivos:\n        nome_arquivo: str  os.path.basename(arquivo)\n        try:\n            print(f\"Tentando fazer upload de '{nome_arquivo}' para o bucket '{BUCKET_NAME}'...\")\n            s3_client.upload_file(arquivo, BUCKET_NAME, nome_arquivo)\n            print(f\"{nome_arquivo} foi enviado para o S3.\")\n        except Exception as e:\n            print(f\"Erro ao enviar '{nome_arquivo}' para o S3: {e}\")\n            raise\n\ndef deletar_arquivos_locais(arquivos: List[str]) -> None:\n    \"\"\"Deleta os arquivos locais após o upload.\"\"\"\n    for arquivo in arquivos:\n        try:\n            os.remove(arquivo)\n            print(f\"{arquivo} foi deletado do local.\")\n        except Exception as e:\n            print(f\"Erro ao deletar o arquivo '{arquivo}': {e}\")\n            raise\n\ndef executar_backup(pasta: str) -> None:\n    \"\"\"Executa o processo completo de backup.\"\"\"\n    try:\n        print(f\"Iniciando o processo de backup para a pasta '{pasta}'...\")\n        arquivos: List[str]  listar_arquivos(pasta)\n        if arquivos:\n            upload_arquivos_para_s3(arquivos)\n            deletar_arquivos_locais(arquivos)\n        else:\n            print(\"Nenhum arquivo encontrado para backup.\")\n    except Exception as e:\n        print(f\"Erro no processo de backup: {e}\")\n        raise\n\nif __name__  \"__main__\":\n    PASTA_LOCAL: str  'download'  # Substitua pelo caminho da sua pasta local\n    try:\n        executar_backup(PASTA_LOCAL)\n    except Exception as e:\n        print(f\"Erro ao executar o backup: {e}\")\n\n\nBootcamp - Cloud para dados/Aula_02/projeto/.env\n\n# Credenciais AWS\nAWS_ACCESS_KEY_IDYOUR_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEYYOUR_SECRET_ACCESS_KEY\nAWS_REGIONYOUR_AWS_REGION\n\n# Nome do Bucket S3\nBUCKET_NAMEYOUR_BUCKET_NAME\n\n\n",
        "Bootcamp - Cloud para dados/Aula_03/README.md - Parte (1/2)\nBootcamp - Cloud para dados/Aula_03/README.md\n\n# Bootcamp Cloud: Aula 03\n\n## EC2: Computação Escalável na AWS\n\n**Objetivo:** Introduzir o Amazon EC2 (Elastic Compute Cloud), explorando suas funcionalidades principais, configurações e boas práticas. Durante a aula, criaremos uma instância EC2, configuraremos uma aplicação simples e discutiremos como utilizar o serviço para diferentes cenários na engenharia de dados.\n\n---\n\n### 1. Revisão da Aula Anterior: Configurando e Utilizando o S3\n\n**Objetivo:** Revisar os conceitos e práticas da aula anterior sobre o Amazon S3, focando na criação e configuração de buckets, upload e download de arquivos, e como o S3 pode ser integrado em pipelines de dados.\n\n#### Passo 1: Revisão da Criação de Buckets\n\n1. **Criar um Bucket S3:**\n   - Revisamos o processo de criação de um bucket S3, incluindo a escolha da região, configuração de controle de acesso e habilitação de versionamento.\n\n2. **Aplicar Regras de Ciclo de Vida:**\n   - Exploramos como configurar regras de ciclo de vida para mover dados entre diferentes classes de armazenamento e excluir objetos automaticamente após um período definido.\n\n#### Passo 2: Revisão do Uso do boto3 para Interagir com o S3\n\n1. **Criar e Listar Buckets:**\n   - Utilizamos o boto3 para criar um bucket e listar todos os buckets existentes na conta AWS.\n\n2. **Upload e Download de Arquivos:**\n   - Praticamos o upload e download de arquivos entre o sistema local e o S3 utilizando o boto3, além de configurar permissões de acesso.\n\n3. **Excluir Objetos e Buckets:**\n   - Exploramos como excluir objetos e buckets utilizando comandos simples do boto3.\n\n#### Passo 3: Revisão de Casos de Uso do S3\n\n1. **Data Lake e ETL:**\n   - Discutimos como o S3 é utilizado em arquiteturas de data lake e pipelines ETL, armazenando dados brutos e processados.\n\n2. **Backup e Arquivamento:**\n   - Exploramos o uso do S3 para backup e arquivamento de dados críticos, com a utilização de regras de ciclo de vida para otimizar custos.\n\n---\n\n## 2. O que é o EC2?\n\n1. **Definição:**\n   - Amazon EC2 (Elastic Compute Cloud) é um serviço de computação em nuvem que oferece capacidade de processamento escalável. Com o EC2, você pode lançar instâncias de máquinas virtuais (VMs) em questão de minutos, configurando-as de acordo com as necessidades específicas do seu projeto.\n\n2. **Componentes Principais do EC2:**\n   - **Instâncias:** Máquinas virtuais que rodam sistemas operacionais e aplicativos.\n   - **Tipos de Instância:** Variam em termos de CPU, memória, armazenamento e capacidade de rede, permitindo otimizar custos e performance para diferentes workloads.\n   - **AMIs (Amazon Machine Images):** Modelos predefinidos que incluem o sistema operacional e software inicial necessário para iniciar uma instância.\n   - **Volumes EBS (Elastic Block Store):** Armazenamento persistente que pode ser conectado às instâncias EC2.\n   - **Grupos de Segurança:** Firewalls virtuais que controlam o tráfego de entrada e saída das instâncias EC2.\n\n---\n\n## 3. Página Principal do EC2\n\n### 3.1 Nível Gratuito do EC2\n\nO Amazon EC2 oferece uma camada gratuita para novos usuários da AWS, permitindo que explorem e experimentem o serviço sem custos iniciais, dentro de determinados limites.\n\n#### 3.1.1 Uso da Oferta (Mensal)\n\n- **Instâncias do Linux do EC2:**\n  - **Utilização:** 11%\n  - **Horas Restantes:** 668 horas\n\nEssa oferta inclui até 750 horas de uso de instâncias t2.micro ou t3.micro por mês, tanto para Linux quanto para Windows, durante os primeiros 12 meses após a criação da conta.\n\n#### 3.1.2 Espaço de Armazenamento no EBS\n\n- **Espaço Disponível:** A oferta gratuita também inclui 30 GB de armazenamento em volumes EBS (Elastic Block Store) de propósito geral ou magnéticos.\n\n### 3.2 Integridade do Serviço\n\nA AWS mantém uma alta disponibilidade de seus serviços em diferentes zonas de disponibilidade (AZs) dentro de uma região.\n\n#### 3.2.1 Zonas\n\nAs zonas de disponibilidade são locais físicos isolados dentro de uma região da AWS, projetados para operar de forma independente em caso de falhas.\n\n- **Nome da Zona:** `us-east-1a`\n  - **ID da Zona:** `use1-az6`\n\n- **Nome da Zona:** `us-east-1b`\n  - **ID da Zona:** `use1-az1`\n\n- **Nome da Zona:** `us-east-1c`\n  - **ID da Zona:** `use1-az2`\n\n### 3.3 Atributos da Conta\n\nOs atributos da conta fornecem informações sobre a configuração e os recursos padrão disponíveis para sua conta AWS.\n\n#### 3.3.1 VPC Padrão\n\n- **ID da VPC:** `vpc-0cab8f7db8d745f83`\n\nCada conta AWS tem uma VPC padrão em cada região, que pode ser usada para criar e gerenciar recursos de rede.\n\n#### 3.3.2 Configurações\n\nAs configurações de proteção e segurança de dados são fundamentais para garantir que seus recursos na AWS estejam seguros e protegidos.\n\n#### 3.3.3 Console Serial do EC2\n\nO Console Serial do EC2 é uma ferramenta que permite acesso direto ao console de uma instância para solução de problemas em instâncias EC2 que não estão acessíveis pela rede.\n\n---\n\n## 4. Configuração e Lançamento de uma Instância EC2\n\n### 4.1 Criando uma Instância EC2 via AWS Management Console\n\n1. **Acessando o AWS Management Console:**\n   - Faça login no [AWS Management Console](https://aws.amazon.com) e navegue até o serviço EC2. Isso pode ser feito digitando \"EC2\" na barra de pesquisa superior e selecionando \"EC2\" nos resultados.\n\n2. **Iniciando o Lançamento de uma Instância:**\n   - Na página do EC2, clique em “Launch Instance” para iniciar o processo de criação.\n\n3. **Escolhendo uma Amazon Machine Image (AMI):**\n   - **AMI:** Selecione uma AMI que contenha o sistema operacional desejado. AMIs podem ser públicas, privadas ou compartilhadas. \n   - **Exemplo:** Escolha uma AMI Amazon Linux 2 ou Ubuntu.\n\n4. **Selecionando o Tipo de Instância:**\n   - **Tipo de Instância:** Escolha o tipo de instância que melhor se adequa ao seu workload. Tipos como t2.micro são elegíveis para o nível gratuito, ideal para testes e pequenas aplicações.\n   - **Exemplo:** `t2.micro` (1 vCPU, 1 GiB RAM).\n\n5. **Configuração da Instância:**\n   - **Número de Instâncias:** Defina quantas instâncias deseja lançar.\n   - **Network:** Selecione a VPC e sub-rede onde a instância será criada.\n   - **IAM Role:** Atribua uma IAM role se a instância precisar acessar outros serviços AWS.\n\n6. **Configuração de Armazenamento (Volumes EBS):**\n   - **Adicionar Volumes:** Configure volumes EBS adicionais se necessário. O volume root é criado automaticamente com a AMI selecionada.\n   - **Exemplo:** 8 GiB de General Purpose SSD.\n\n7. **Configuração de Grupos de Segurança:**\n   - **Criar/Selecionar Grupo de Segurança:** Configure as regras de firewall para controlar o tráfego de entrada e saída.\n   - **Exemplo:** Permitir SSH (porta 22) de um IP específico para acessar a instância.\n\n8. **Revisão e Lançamento:**\n   - Revise todas as configurações e clique em “Launch”. Será solicitado que você selecione ou crie um par de chaves para acessar a instância via SSH.\n\n### 4.2 Acessando a Instância via SSH\n\n1. **Obtenção do IP Público da Instância:**\n   - Após o lançamento, o EC2 atribui um IP público à instância. Este IP pode ser encontrado no console EC2.\n\n2. **Acessando via SSH:**\n   - Use o terminal ou um cliente SSH para acessar a instância.\n   - **Exemplo de Comando SSH:**\n     ```bash\n     ssh -i \"minha-chave.pem\" ec2-user@ec2-xx-xxx-xx-xx.compute-1.amazonaws.com\n     ```\n\n3. **Configuração Inicial:**\n   - Uma vez conectado, você pode atualizar o sistema operacional, instalar pacotes adicionais e configurar sua aplicação.\n\n---\n\n### 5. Famílias de Instâncias EC2\n\nO Amazon EC2 oferece várias famílias de instâncias, cada uma projetada para diferentes tipos de workloads. As famílias são categorizadas com base em fatores como uso geral, computação intensiva, memória intensiva e otimização com hardware especializado. Abaixo estão as principais famílias de instâncias EC2 e suas respectivas aplicações em projetos de dados.\n\n#### 5.1 General Purpose (Propósito Geral)\n\n**Descrição:**\nAs instâncias General Purpose são projetadas para fornecer um equilíbrio de recursos de computação, memória e rede, tornando-as ide\n\nais para uma ampla variedade de workloads. Elas oferecem uma combinação versátil de recursos, sendo adequadas para a maioria das aplicações.\n\n**Tipos de Instância:**\n- **t3, t3a, t4g:** Instâncias com desempenho balanceado que são ideais para aplicações que precisam de uma carga moderada de computação e memória.\n- **m6g, m6i, m5:** Oferecem uma proporção mais equilibrada de CPU e memória, sendo ideais para aplicações que exigem um pouco mais de recursos.\n\n**Aplicações em Dados:**\n- **Ambientes de Desenvolvimento e Teste:** Instâncias General Purpose são frequentemente usadas para configurar ambientes de desenvolvimento, testar pipelines de dados e executar simulações em pequena escala.\n- **Aplicações de Banco de Dados:** Para bancos de dados de uso geral que não requerem otimização extrema de computação ou memória, como pequenos clusters de banco de dados NoSQL.\n- **Servidores Web e de Aplicações:** Adequadas para hospedar servidores que processam dados e fornecem APIs para consultas e análises.\n\n#### 5.2 Compute Optimized (Otimização de Computação)\n\n**Descrição:**\nAs instâncias Compute Optimized são projetadas para workloads que exigem uma alta taxa de computação por núcleo de processador. Elas oferecem alta performance para tarefas intensivas em computação.\n\n**Tipos de Instância:**\n- **c7g, c6g, c6i, c5:** Oferecem uma alta relação de vCPU para memória, otimizando o desempenho para aplicações de computação intensiva.\n\n**Aplicações em Dados:**\n- **Processamento de Dados em Lote:** Ideal para tarefas como processamento em lote de grandes volumes de dados, onde o tempo de computação é um fator crítico.\n- **Analytics e Modelagem de Dados:** Utilizadas em análises avançadas e modelagem estatística que requerem um grande poder de processamento.\n- **Simulações e Modelagem Computacional:** Para workloads que envolvem cálculos complexos, como simulações financeiras ou científicas.\n\n#### 5.3 Memory Optimized (Otimização de Memória)\n\n**Descrição:**\nAs instâncias Memory Optimized são projetadas para workloads que exigem grandes quantidades de memória RAM, oferecendo alta capacidade de memória para processar grandes volumes de dados em memória.\n\n**Tipos de Instância:**\n- **r6g, r6i, r5, r5b:** Estas instâncias oferecem uma alta proporção de memória em relação ao número de vCPUs, sendo ideais para aplicações que precisam de grandes volumes de memória.\n- **x2gd, x2idn:** São projetadas para cargas de trabalho que necessitam de ainda mais memória, como grandes bancos de dados em memória.\n\n**Aplicações em Dados:**\n- **Bancos de Dados em Memória:** Ideal para bancos de dados que mantêm grandes quantidades de dados na RAM, como Redis, Memcached ou bancos de dados em memória do SAP HANA.\n- **Análise em Tempo Real:** Utilizadas em aplicações de análise de grandes volumes de dados em tempo real, onde a capacidade de processar dados diretamente na memória é crítica.\n- **Big Data e Data Warehousing:** Adequadas para processar grandes volumes de dados em sistemas de data warehousing, como Amazon Redshift, ou para operações de ETL complexas.\n\n#### 5.4 Accelerated Computing (Computação Acelerada)\n\n**Descrição:**\nAs instâncias Accelerated Computing são projetadas para workloads que podem se beneficiar de hardware especializado, como GPUs ou FPGAs, proporcionando um aumento significativo no desempenho para tarefas específicas.\n\n**Tipos de Instância:**\n- **p4, p3:** Instâncias com GPUs NVIDIA, otimizadas para tarefas de deep learning, inferência de machine learning e simulações científicas.\n- **g5g, g4ad:** Oferecem GPUs voltadas para renderização gráfica e codificação de vídeo.\n- **f1:** Instâncias com FPGAs (Field Programmable Gate Arrays), adequadas para tarefas como criptografia e processamento de sinais.\n\n**Aplicações em Dados:**\n- **Trein",
        "Bootcamp - Cloud para dados/Aula_03/README.md - Parte (2/2)\namento de Modelos de Machine Learning:** Instâncias com GPUs são ideais para treinar modelos complexos de deep learning, permitindo um treinamento mais rápido e eficiente.\n- **Inferência em Tempo Real:** Para aplicações de machine learning que requerem inferência em tempo real com latência mínima.\n- **Simulações Computacionais e Modelagem 3D:** Utilizadas em simulações que exigem alto poder de processamento gráfico ou operações de matemática intensiva.\n- **Análise de Vídeo e Imagem:** Ideal para processar e analisar grandes volumes de dados de vídeo e imagem em tempo real, como em sistemas de vigilância ou análise de conteúdo multimídia.\n\n---\n\nCada uma dessas famílias de instâncias EC2 é otimizada para diferentes tipos de workloads e oferece recursos específicos que podem ser usados para maximizar a eficiência e o desempenho em projetos de dados. Escolher a instância certa depende dos requisitos específicos do seu projeto, como a quantidade de dados a ser processada, a complexidade das operações e a necessidade de memória, computação ou aceleração de hardware.\n\n---\n\n## 6. Tipos de Preço do EC2\n\nO Amazon EC2 oferece diferentes modelos de precificação para atender às diversas necessidades e orçamentos dos usuários. Compreender esses modelos é essencial para otimizar custos e garantir que você está utilizando os recursos de maneira eficiente. A seguir, são apresentados os principais tipos de preço do EC2:\n\n#### 6.1 Instâncias Sob Demanda (On-Demand)\n\n**Descrição:**\nAs instâncias Sob Demanda permitem que você pague por capacidade de computação por hora ou por segundo (dependendo do tipo de instância) sem compromisso a longo prazo. Este modelo é ideal para cargas de trabalho que são imprevisíveis ou variáveis.\n\n**Vantagens:**\n- **Flexibilidade:** Você pode iniciar, parar e terminar instâncias a qualquer momento sem penalidades.\n- **Sem Compromisso Inicial:** Não há necessidade de investir em contratos de longo prazo.\n- **Escalabilidade:** Fácil de escalar conforme a demanda aumenta ou diminui.\n\n**Aplicações Comuns:**\n- Ambientes de desenvolvimento e teste.\n- Aplicações com cargas de trabalho variáveis ou imprevisíveis.\n- Projetos de curto prazo que não justificam um compromisso a longo prazo.\n\n#### 6.2 Savings Plans (Planos de Economia)\n\n**Descrição:**\nOs Savings Plans oferecem descontos significativos (até 72%) em comparação com as tarifas Sob Demanda em troca de um compromisso de uso consistente (medido em dólares por hora) por um período de 1 ou 3 anos. Existem dois tipos principais de Savings Plans:\n\n- **Compute Savings Plans:** Oferecem maior flexibilidade, aplicando-se a qualquer instância EC2 independente da região, tipo de instância, sistema operacional ou tenancy.\n- **EC2 Instance Savings Plans:** Oferecem descontos maiores, mas são aplicáveis apenas a instâncias específicas dentro de uma família de instâncias e região selecionadas.\n\n**Vantagens:**\n- **Descontos Significativos:** Economia considerável em comparação com o modelo Sob Demanda.\n- **Flexibilidade (no caso dos Compute Savings Plans):** Possibilidade de alterar tipos de instância e regiões sem perder os descontos.\n- **Previsibilidade de Custos:** Facilita o planejamento financeiro com base no compromisso de uso.\n\n**Aplicações Comuns:**\n- Cargas de trabalho estáveis e previsíveis que podem se comprometer com um nível de uso consistente.\n- Empresas que buscam otimizar custos a longo prazo.\n\n#### 6.3 Instâncias Spot\n\n**Descrição:**\nAs Instâncias Spot permitem que você aproveite a capacidade ociosa da AWS com descontos de até 90% em relação às tarifas Sob Demanda. No entanto, a AWS pode interromper essas instâncias com um aviso prévio de dois minutos quando precisar recuperar a capacidade.\n\n**Vantagens:**\n- **Custo Reduzido:** Descontos significativos tornam as Instâncias Spot extremamente econômicas.\n- **Escalabilidade:** Ideal para workloads que podem ser interrompidos e retomados sem impacto significativo.\n\n**Desvantagens:**\n- **Interrupções:** A possibilidade de interrupção a qualquer momento pode não ser adequada para todas as aplicações.\n- **Disponibilidade Variável:** A disponibilidade das Instâncias Spot pode variar conforme a demanda.\n\n**Aplicações Comuns:**\n- Processamento em lote e tarefas de ETL.\n- Treinamento de modelos de machine learning que podem ser reiniciados.\n- Aplicações distribuídas e resilientes que podem lidar com interrupções, como clusters de Hadoop ou Spark.\n\n#### 6.4 Instâncias Dedicadas (Dedicated Instances)\n\n**Descrição:**\nAs Instâncias Dedicadas são executadas em hardware físico dedicado exclusivamente para a sua conta AWS. Isso garante que suas instâncias não compartilhem hardware com instâncias de outros clientes.\n\n**Vantagens:**\n- **Isolamento Físico:** Maior segurança e conformidade para cargas de trabalho sensíveis.\n- **Controle de Hardware:** Garantia de que os recursos de hardware não serão compartilhados com outros clientes.\n\n**Desvantagens:**\n- **Custo Mais Elevado:** Geralmente, as Instâncias Dedicadas são mais caras do que as instâncias compartilhadas.\n- **Menor Flexibilidade de Preço:** Não se beneficiam tanto de descontos como os Savings Plans ou Instâncias Spot.\n\n**Aplicações Comuns:**\n- Aplicações que exigem conformidade regulatória específica.\n- Cargas de trabalho sensíveis que necessitam de isolamento físico por razões de segurança.\n- Ambientes empresariais que requerem controle total sobre o hardware subjacente.\n\n---\n\n### Comparação dos Modelos de Pre\n\nço\n\n| Modelo de Preço       | Flexibilidade | Desconto em Relação ao On-Demand | Compromisso Necessário | Adequado para                                       |\n|-----------------------|---------------|-----------------------------------|------------------------|-----------------------------------------------------|\n| Sob Demanda           | Alta          | 0%                                | Nenhum                 | Cargas de trabalho imprevisíveis ou variáveis       |\n| Savings Plans         | Média/Alta    | Até 72%                            | Sim (1 ou 3 anos)      | Cargas de trabalho estáveis e previsíveis           |\n| Instâncias Spot       | Baixa         | Até 90%                            | Não                    | Processamento em lote, machine learning, workloads resilientes |\n| Instâncias Dedicadas  | Média         | 0-30%                             | Nenhum ou contratual    | Aplicações sensíveis, conformidade regulatória      |\n\n### Considerações para Escolha do Modelo de Preço\n\n- **Natureza da Carga de Trabalho:** Se sua aplicação pode tolerar interrupções, as Instâncias Spot podem oferecer enormes economias. Para cargas de trabalho críticas que não podem ser interrompidas, as Instâncias Sob Demanda ou Savings Plans são mais adequadas.\n- **Previsibilidade de Uso:** Se você pode prever consistentemente o uso de computação, os Savings Plans proporcionam uma excelente relação custo-benefício.\n- **Requisitos de Segurança e Conformidade:** Instâncias Dedicadas são necessárias para certas aplicações que exigem isolamento físico.\n- **Orçamento e Otimização de Custos:** Avalie o equilíbrio entre economia e flexibilidade para selecionar o modelo que melhor se alinha ao seu orçamento e necessidades operacionais.\n\n### Estratégias para Otimização de Custos\n\n1. **Mix de Modelos de Preço:** Utilizar uma combinação de diferentes modelos de preço (por exemplo, Sob Demanda para cargas variáveis, Savings Plans para cargas estáveis e Spot para tarefas flexíveis) pode maximizar a economia sem comprometer a performance.\n2. **Monitoramento e Ajustes Contínuos:** Utilize ferramentas como AWS Cost Explorer e AWS Trusted Advisor para monitorar o uso e ajustar as estratégias de compra conforme necessário.\n3. **Automação com Auto Scaling:** Configure grupos de Auto Scaling para aproveitar automaticamente as Instâncias Spot quando disponíveis e mudar para instâncias Sob Demanda ou reservadas quando necessário.\n4. **Escolha de Regiões e Tipos de Instância:** Selecionar regiões com custos mais baixos e tipos de instância que melhor atendem às necessidades de desempenho pode reduzir significativamente os custos.\n\n\n### 1. **Configurar a Instância EC2**\n\nAqui está o passo a passo atualizado para configurar o Airflow em uma máquina Linux usando `apt` ao invés de `yum`, seguido por instruções sobre como garantir que sua DAG seja carregada corretamente na interface do Airflow.\n\n### 1. **Atualizar os Pacotes do Sistema**\n\nPrimeiro, certifique-se de que todos os pacotes do sistema estão atualizados:\n\n```bash\nsudo apt update\nsudo apt upgrade -y\n```\n\n### 2. **Instalar o `pip3`**\n\nInstale o gerenciador de pacotes `pip3` para Python:\n\n```bash\nsudo apt install -y python3-pip\n```\n\n### 3. **Instalar o SQLite**\n\nInstale o banco de dados SQLite, que é necessário para o Airflow funcionar corretamente:\n\n```bash\nsudo apt install -y sqlite3\n```\n\n### 4. **Instalar o Ambiente Virtual do Python**\n\nInstale o módulo `venv` para criar um ambiente virtual isolado para o Airflow:\n\n```bash\nsudo apt install -y python3.12-venv\n```\n\n### 5. **Instalar as Dependências do PostgreSQL**\n\nInstale a biblioteca `libpq-dev` que é necessária para o suporte ao PostgreSQL no Airflow:\n\n```bash\nsudo apt-get install -y libpq-dev\n```\n\n### 6. **Criar um Ambiente Virtual**\n\nCrie um ambiente virtual Python para isolar as dependências do Airflow:\n\n```bash\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\n### 7. **Instalar o Apache Airflow**\n\nInstale o Apache Airflow usando `pip` dentro do ambiente virtual:\n\n```bash\npip install \"apache-airflow[celery]2.10.0\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.0/constraints-3.8.txt\"\n```\n\n### 8. **Inicializar o Banco de Dados do Airflow**\n\nMigre o banco de dados do Airflow para garantir que todas as tabelas necessárias sejam criadas:\n\n```bash\nairflow db migrate\n```\n\n### 9. **Criar um Usuário Administrador**\n\nCrie um usuário administrador para acessar a interface web do Airflow:\n\n```bash\nairflow users create \\\n    --username admin \\\n    --firstname Peter \\\n    --lastname Parker \\\n    --role Admin \\\n    --email spiderman@superhero.org\n```\n\n### 10. **Iniciar o Servidor Web do Airflow**\n\nInicie o servidor web do Airflow em segundo plano:\n\n```bash\nairflow webserver &\n```\n\n### 11. **Iniciar o Scheduler do Airflow**\n\nPor fim, inicie o scheduler do Airflow para gerenciar a execução dos DAGs:\n\n```bash\nairflow scheduler &\n```\n\n### 12. **Adicionar uma Nova DAG**\n\nPara adicionar uma nova DAG:\n\n1. **Criar o Arquivo da DAG:**\n\n   Navegue até o diretório de DAGs do Airflow e crie um novo arquivo Python para a DAG:\n\n   ```bash\n   cd ~/airflow/dags\n   nano minha_dag.py\n   ```\n\n   **Exemplo de Conteúdo da DAG:**\n\n   ```python\n   from airflow import DAG\n   from airflow.operators.dummy import DummyOperator\n   from datetime import datetime\n\n   default_args  {\n       'owner': 'airflow',\n       'start_date': datetime(2024, 8, 26),\n   }\n\n   with DAG('minha_dag',\n            default_argsdefault_args,\n            schedule_interval'@daily',\n            catchupFalse) as dag:\n\n       start  DummyOperator(task_id'start')\n\n       start\n   ```\n\n2. **Salvar e Sair:**\n\n   Salve o arquivo no `nano` pressionando `CTRL + O`, e saia pressionando `CTRL + X`.\n\n### 13. **Verificar se a DAG Aparece na UI**\n\nSe a nova DAG não aparecer na UI:\n\n1. **Verifique o Scheduler:**\n\n   Certifique-se de que o scheduler está rodando corretamente:\n\n   ```bash\n   airflow scheduler\n   ```\n\n2. **Reinicie o Scheduler:**\n\n   Se necessário, reinicie o scheduler para carregar as novas DAGs:\n\n   ```bash\n   pkill -f \"airflow scheduler\"\n   airflow scheduler &\n   ```\n\n3. **Verifique as Configurações do Airflow:**\n\n   Verifique se o diretório `dags_folder` no `airflow.cfg` aponta para o diretório onde sua DAG está localizada (`~/airflow/dags`).\n\n### 14. **Acessar o Airflow via Interface Web**\n\nFinalmente, acesse a interface web do Airflow no navegador:\n\n",
        "Bootcamp - Cloud para dados/Aula_04/README.md - Parte (1/2)\nBootcamp - Cloud para dados/Aula_04/README.md\n\n# **Bootcamp Cloud: Aula 04: IAM na AWS**\n\n**Objetivo**: Nesta aula, vamos explorar o IAM (Identity and Access Management) da AWS. Vamos entender como proteger a conta AWS, o papel do usuário root, como criar e gerenciar usuários, grupos, políticas, e configurar o MFA.\n\n### **1. Protegendo a Conta AWS**\n\n- **Importância**: A conta AWS é o coração da sua infraestrutura na nuvem. Protegê-la é essencial para evitar acessos não autorizados e garantir a segurança dos seus recursos.\n\n### **2. Usuário Root**\n\n- **O que é o Usuário Root**: \n  - O usuário root é a conta inicial criada ao configurar a AWS. Ele tem acesso total a todos os recursos e configurações.\n  - **Riscos**: O uso contínuo do usuário root é arriscado, pois ele tem permissões ilimitadas, o que pode levar a potenciais danos em caso de comprometimento.\n\n- **Boas Práticas**:\n  - Evitar o uso diário do usuário root.\n  - Criar usuários IAM com permissões específicas para tarefas do dia a dia.\n  - Habilitar o MFA (Multi-Factor Authentication) para a conta root.\n\n### **3. IAM (Identity and Access Management)**\n\n- **O que é IAM**:\n  - IAM permite criar e gerenciar usuários, grupos, e permissões na AWS.\n  - Facilita a implementação do princípio do menor privilégio, garantindo que cada usuário tenha apenas as permissões necessárias.\n\n### **4. Configuração do MFA (Multi-Factor Authentication)**\n\n- **O que é MFA**: \n  - MFA adiciona uma camada extra de segurança, exigindo que o usuário forneça uma segunda forma de autenticação além da senha, como um código gerado por um dispositivo móvel.\n\n- **Passo a Passo para Configurar o MFA no Usuário Root**:\n  \n  1. **Acessar o Console de Gerenciamento da AWS**:\n     - Faça login como o usuário root.\n  \n  2. **Navegar até a Página de Segurança da Conta**:\n     - No canto superior direito, clique no nome da conta e selecione \"Minha Conta\".\n     - Role para baixo até \"Configurações de segurança\" e clique em \"Ativar MFA\" na seção de autenticação multifator.\n\n  3. **Escolher o Tipo de Dispositivo MFA**:\n     - Selecione \"Aplicativo autenticador\" para usar um dispositivo móvel como segundo fator de autenticação.\n\n  4. **Configurar o Aplicativo Autenticador**:\n     - Abra o aplicativo autenticador no seu dispositivo móvel (ex: Google Authenticator).\n     - Escaneie o código QR fornecido pela AWS ou insira a chave manualmente.\n  \n  5. **Verificar o Código MFA**:\n     - Insira os códigos gerados pelo aplicativo para verificar a configuração.\n  \n  6. **Salvar a Configuração**:\n     - Confirme e salve a configuração do MFA.\n\n  7. **Testar a Configuração**:\n     - Saia e faça login novamente para verificar se o MFA está funcionando corretamente.\n\n---\n\n### **5. Criando um Usuário Administrativo**\n\n**Passo a Passo para Criar um Usuário Administrativo:**\n\n1. **Acessar o Console IAM**:\n   - **Faça login** no console da AWS com o usuário root.\n   - Navegue até o serviço **IAM (Identity and Access Management)**.\n   \n2. **Adicionar Novo Usuário**:\n   - Clique em **\"Usuários\"** e selecione **\"Adicionar usuário\"**.\n   - Insira um nome para o novo usuário administrativo, como \"admin-user\".\n\n3. **Selecionar o Tipo de Acesso**:\n   - **Acesso programático**: Marque essa opção para gerar **chaves de acesso** (Access Key ID e Secret Access Key), necessárias para scripts ou automações que interagem com a AWS via API.\n   - **Acesso à AWS Management Console**: Marque essa opção para permitir que o usuário faça login no console da AWS. Defina uma senha inicial (você pode permitir que o usuário a redefina no primeiro login ou definir uma senha permanente).\n\n4. **Configurar Permissões**:\n   - **Anexar Políticas Diretamente**: \n     - **AdministratorAccess**: Selecione esta política para conceder ao usuário todas as permissões administrativas, permitindo acesso completo a todos os recursos da AWS.\n     - **PowerUserAccess** (Alternativa): Selecione esta política se desejar conceder permissões administrativas amplas, mas sem acesso ao gerenciamento de contas, como a criação de novos usuários IAM ou configuração de billing.\n\n5. **Revisar e Criar o Usuário**:\n   - Revise as configurações e clique em **\"Criar usuário\"**.\n   - **Download do CSV**: Faça o download do arquivo CSV contendo as chaves de acesso e a senha do console para uso futuro. Estas informações são essenciais para acessar a AWS programaticamente e via console.\n\n6. **Encerramento do Uso do Usuário Root**:\n   - Após a criação do usuário administrativo, saia do console e faça login novamente utilizando as credenciais do novo usuário.\n   - A partir deste ponto, o uso do usuário root deve ser restrito a tarefas críticas e configurações de segurança inicial.\n\n---\n\n### **6. Comparação: Root vs AdministratorAccess vs PowerUserAccess**\n\n| **Característica**                  | **Usuário Root**                                      | **AdministratorAccess**                          | **PowerUserAccess**                            |\n|-------------------------------------|------------------------------------------------------|-------------------------------------------------|------------------------------------------------|\n| **Acesso Total**                    | Sim, acesso total e irrestrito a todos os recursos   | Sim, acesso completo a quase todos os recursos  | Não, acesso restrito a certos recursos        |\n| **Gerenciamento de IAM**            | Sim                                                  | Sim                                              | Não, sem acesso ao IAM                        |\n| **Configuração de Billing**         | Sim                                                  | Sim                                              | Não                                            |\n| **Criação de Recursos**             | Sim                                                  | Sim                                              | Sim                                            |\n| **Modificação de Políticas de Conta**| Sim                                                  | Sim                                              | Não                                            |\n| **Gerenciamento de Faturamento e Conta** | Sim, incluindo alteração de informações de pagamento e fechamento da conta | Não                                              | Não                                            |\n| **Cancelamento de Serviços e Fechamento da Conta** | Sim | Não | Não |\n| **Alteração de Suporte AWS**        | Sim, pode alterar o plano de suporte (ex: suporte básico para empresarial) | Não | Não |\n| **Exclusão de CloudFront Key Pairs**| Sim, pode criar, gerenciar ou excluir CloudFront key pairs | Não | Não |\n| **Uso recomendado**                 | Configuração inicial e tarefas críticas de segurança | Uso diário para administração e operações gerais | Uso para administração sem acesso a IAM e Billing |\n\n---\n\n### **7. Questões Frequentes**\n\n1. **O AdministratorAccess pode alterar o Usuário Root?**\n\n   Não, um usuário com a política AdministratorAccess não pode alterar o usuário root. Somente o próprio usuário root pode alterar suas próprias configurações, como o nome de usuário, senha, chaves de acesso, ou desativar o MFA. Isso é uma medida de segurança importante para proteger a conta AWS, garantindo que apenas o usuário root tenha controle total sobre suas próprias credenciais e configurações.\n\n2. **Eu contratei uma consultoria, qual acesso criar?**\n\n   A escolha entre conceder `AdministratorAccess` ou `PowerUserAccess` à consultoria depende da natureza das tarefas que eles irão realizar e do nível de controle que você deseja manter sobre sua conta AWS.\n\n   - **Quando conceder `AdministratorAccess`**:\n     - **Cenário**: A consultoria precisa ter acesso completo para gerenciar todos os recursos da AWS, incluindo a criação e gerenciamento de usuários IAM, configuração de políticas, gerenciamento de billing, e outras tarefas administrativas completas.\n     - **Risco**: Eles terão permissão para alterar quase todos os aspectos da sua conta, incluindo ações sensíveis que podem impactar a segurança ou o faturamento.\n\n   - **Quando conceder `PowerUserAccess`**:\n     - **Cenário**: A consultoria precisa realizar tarefas administrativas gerais, como criar e gerenciar recursos, mas você deseja limitar o acesso a configurações de conta e gerenciamento de usuários IAM.\n     - **Risco**: Eles não poderão criar ou gerenciar usuários IAM, alterar configurações de faturamento, ou fazer mudanças no plano de suporte, o que oferece um nível adicional de segurança e controle sobre aspectos críticos da sua conta.\n\n   - **Recomendação**:\n     - **PowerUserAccess** pode ser mais apropriado se você quiser manter controle sobre as configurações mais sensíveis da sua conta, como gerenciamento de usuários IAM e informações de faturamento. \n     - **AdministratorAccess** deve ser concedido somente se a consultoria realmente precisar de controle total sobre todos os aspectos da sua infraestrutura AWS, e você confia plenamente que eles irão gerenciar esses recursos de forma segura e responsável. \n\n---\n\n### **8. Criando um Grupo de \"Engenheiro de Dados\" no IAM**\n\n**Passo a Passo para Criar um Grupo de \"Engenheiro de Dados\" no IAM**\n\n1. **Acessar o Console IAM:**\n   - Faça login no console da AWS com um usuário que tenha permissões suficientes para gerenciar o IAM.\n   - Navegue até o serviço **IAM (Identity and Access Management)**.\n\n2. **Criar um Novo Grupo:**\n   - No painel do IAM, selecione **\"Gr\n\nupos\"** no menu lateral.\n   - Clique em **\"Criar Novo Grupo\"**.\n   - Insira o nome do grupo como **\"EngenheiroDeDados\"**.\n\n3. **Copiar Permissões de um Grupo Existente (Opcional):**\n   - Se você já possui um grupo com permissões similares e deseja copiar suas permissões, selecione essa opção e escolha o grupo de referência.\n   - Caso contrário, pule esta etapa e prossiga para anexar políticas diretamente.\n\n4. **Anexar Políticas Diretamente:**\n   - Na tela de anexar políticas, você verá uma lista de políticas gerenciadas pela AWS.\n   - Para o grupo de \"Engenheiro de Dados\", selecione as seguintes políticas relevantes:\n\n     - **AmazonS3FullAccess**: Dá ao grupo acesso completo ao Amazon S3, permitindo criar, ler, escrever e excluir buckets e objetos.\n     - **AmazonEC2FullAccess**: Permite ao grupo gerenciar instâncias EC2, incluindo o provisionamento, configuração e encerramento de instâncias.\n     - **AmazonVPCFullAccess**: Concede ao grupo a capacidade de gerenciar redes VPC, sub-redes, roteamento e segurança de redes.\n     - **AmazonRDSFullAccess**: Concede permissão para gerenciar instâncias de bancos de dados no Amazon RDS.\n     - **AmazonGlueConsoleFullAccess**: Permite ao grupo acessar e usar o serviço de ETL Amazon Glue para operações de dados.\n     - **AmazonAthenaFullAccess**: Dá ao grupo a permissão para consultar dados armazenados no S3 usando o Amazon Athena.\n\n   - Após selecionar as políticas desejadas, clique em **\"Next Step\"**.\n\n5. **Revisar o Grupo:**\n   - Revise as permissões que serão concedidas ao grupo.\n   - Verifique se todas as políticas necessárias estão anexadas.\n\n6. **Criar o Grupo:**\n   - Clique em **\"Create Group\"** para finalizar a criação do grupo \"EngenheiroDeDados\".\n\n7. **Adicionar Usuários ao Grupo:**\n   - Após a criação do grupo, você pode adicionar usuários existentes ao grupo \"EngenheiroDeDados\" ou criar novos usuários e associá-los ao grupo.\n   - Isso garantirá que todos os membros do grupo tenham as mesmas permissões para acessar e gerenciar os recursos associados ao projeto de dados.\n\n---\n\n### **9. Tags Sugeridas para o Grupo \"Engenheiro de Dados\"**\n\nAs tags ajudam a organizar e identificar recursos relacionados ao grupo, facilitando a gestão e a automação:\n\n- **Project:** Nome do projeto específico (ex: \"DataPipeline2024\", \"CustomerAnalytics\").\n- **Department:** Nome do departamento ou equipe (ex: \"DataEngineering\", \"AnalyticsTeam\").\n- **Role:** Descrição da função (ex: \"DataEngineer\", \"DataOps\").\n- **Environment:** Ambiente onde o grupo atuará (ex: \"Production\", \"Staging\", \"Development\").\n- **Owner:** Nome do proprietário ou responsável pelo grupo (ex: \"Luciano\", \"DataTeamLead\").\n- **CostCenter:** Código ou nome do centro de custos (ex: \"CC1001\", \"MarketingData\").\n- **Compliance:** Requisitos de conformidade específicos (ex: \"GDPR\", \"HIPAA\").\n- **BusinessUnit:** Unidade de negócio relevante (ex: \"Sales\", \"ProductDevelopment\").\n- **Purpose:** Finalidade do grupo (ex: \"DataProcessing\", \"ETLTasks\").\n- **SecurityLevel:** Nível de segurança exigido (ex: \"High\", \"Confidential\").\n\n---\n\n### **10. Projeto: Criando Grupos, Usuários e Anexando Políticas no IAM**\n\n**Passo a Passo: Criando Grupos, Usuários e Anexando Políticas no IAM**\n\n",
        "Bootcamp - Cloud para dados/Aula_04/README.md - Parte (2/2)\n#### **1. Criar os Grupos no IAM**\n\n1. **Acessar o Console IAM:**\n   - Faça login no console da AWS e navegue até o serviço **IAM (Identity and Access Management)**.\n\n2. **Criar o Grupo \"EngenheiroDeDados\":**\n   - No painel do IAM, selecione **\"Grupos\"** no menu lateral.\n   - Clique em **\"Criar Novo Grupo\"**.\n   - Nome do grupo: **EngenheiroDeDados**.\n   - **Anexar Políticas Diretamente:**\n     - **AmazonS3FullAccess**\n     - **AmazonEC2FullAccess**\n     - **AmazonVPCFullAccess**\n   - Clique em **\"Criar Grupo\"**.\n\n3. **Criar o Grupo \"CientistaDeDados\":**\n   - Repita o processo anterior.\n   - Nome do grupo: **CientistaDeDados**.\n   - **Anexar Políticas Diretamente:**\n     - **AmazonAthenaFullAccess**\n     - **AmazonGlueConsoleFullAccess**\n   - Clique em **\"Criar Grupo\"**.\n\n4. **Criar o Grupo \"LambdaExecutors\":**\n   - Repita o processo novamente.\n   - Nome do grupo: **LambdaExecutors**.\n   - **Anexar Políticas Diretamente:**\n     - **AWSLambdaBasicExecutionRole**\n   - Clique em **\"Criar Grupo\"**.\n\n#### **2. Criar os Usuários no IAM**\n\n1. **Criar Usuário 1 (Engenheiro de Dados):**\n   - No IAM, selecione **\"Usuários\"** no menu lateral.\n   - Clique em **\"Adicionar Usuário\"**.\n   - Nome do usuário: **Engenheiro1**.\n   - **Selecionar o Tipo de Acesso:**\n     - Marque **\"Acesso programático\"** e **\"Acesso à AWS Management Console\"**.\n     - Defina uma senha para o console.\n   - **Atribuir ao Grupo:**\n     - Selecione o grupo **\"EngenheiroDeDados\"**.\n   - Clique em **\"Próximo\"** e **\"Criar Usuário\"**.\n\n2. **Criar Usuário 2 (Engenheiro de Dados):**\n   - Repita o processo anterior.\n   - Nome do usuário: **Engenheiro2**.\n   - Atribua ao grupo **\"EngenheiroDeDados\"**.\n\n3. **Criar Usuário 3 (Cientista de Dados):**\n   - Repita o processo anterior.\n   - Nome do usuário: **Cientista1**.\n   - Atribua ao grupo **\"CientistaDeDados\"**.\n\n4. **Criar Usuário 4 (Lambda Executor):**\n   - Repita o processo anterior.\n   - Nome do usuário: **LambdaExecutor1**.\n   - **Atribuir ao Grupo:**\n     - Selecione o grupo **\"LambdaExecutors\"**.\n\n#### **3. Anexar Usuários aos Grupos e Roles**\n\n1. **Verificar Grupos e Políticas:**\n   - Navegue até a página de cada grupo (EngenheiroDeDados, CientistaDeDados, LambdaExecutors) e verifique se as políticas apropriadas foram anexadas.\n\n2. **Anexar Roles (para Lambda):**\n   - Para o usuário **LambdaExecutor1**, vá em **\"Roles\"** no menu do IAM.\n   - Crie uma nova role chamada **LambdaExecutionRole**.\n   - Anexe a política **\"AmazonDynamoDBFullAccess\"**.\n   - Atribua a role ao usuário **LambdaExecutor1**.\n\n#### **4. Revisão e Testes**\n\n1. **Verificar Acessos:**\n   - Faça login como cada usuário criado para verificar se as permissões e acessos aos serviços AWS estão funcionando conforme o esperado.\n   - Teste a criação e o gerenciamento de recursos em S3, EC2, VPC para engenheiros de dados, Athena e Glue para cientistas de dados, e execução de funções Lambda para o executor de Lambda.\n\n2. **Documentação e Tags:**\n   - Considere adicionar tags aos usuários e grupos para facilitar a organização e a gestão dentro da AWS.\n\n---\n\n### **11. Melhores Práticas de IAM**\n\n- **Princípio do Menor Privilégio**: Conceder apenas as permissões necessárias.\n- **Senhas Fortes e MFA**: Utilizar autenticação multifator em todas as contas críticas.\n- **Monitoramento e Auditoria**: Usar ferramentas como AWS CloudTrail para monitorar atividades.\n\n---\n\n### **12. Acesso Programático na AWS**\n\n**O que é Acesso Programático?**  \nAcesso programático permite que você interaja com os serviços da AWS usando a AWS CLI, SDKs (como boto3 para Python), e ferramentas de automação. É ideal para cenários onde você precisa integrar seus aplicativos ou scripts com a AWS para gerenciar e operar recursos na nuvem.\n\n### **Cenário: Construindo um Projeto Python que Precisa de Acesso a um Projeto Específico**\n\nSe você está construindo um projeto Python que precisa de acesso a recursos específicos na AWS, como S3, DynamoDB, ou EC2, você deve criar um usuário IAM com permissões programáticas específicas para o projeto. Aqui está como fazer isso:\n\n---\n\n### **13. Passo a Passo para Criar Acesso Programático para um Projeto Específico**\n\n1. **Acessar o Console IAM:**\n   - Faça login no console da AWS.\n   - Navegue até o serviço **IAM (Identity and Access Management)**.\n\n2. **Criar um Novo Usuário:**\n   - No painel do IAM, selecione **\"Usuários\"** no menu lateral.\n   - Clique em **\"Adicionar Usuário\"**.\n   - Nome do usuário: Escolha\n\n um nome que reflita o propósito do projeto, como **\"ProjetoPythonUser\"**.\n\n3. **Selecionar o Tipo de Acesso:**\n   - Marque **\"Acesso programático\"**. Isso gerará um **Access Key ID** e **Secret Access Key**, que você usará para configurar as credenciais no seu código Python.\n\n4. **Configurar Permissões:**\n   - **Criar uma Política Personalizada** (opcional):\n     - Selecione **\"Anexar Políticas Diretamente\"** e procure pelas políticas gerenciadas que você precisa, ou crie uma política personalizada que conceda apenas as permissões necessárias para o projeto específico.\n     - Exemplo de serviços que você pode precisar:\n       - **AmazonS3FullAccess** para interagir com buckets S3.\n       - **AmazonDynamoDBFullAccess** para trabalhar com tabelas DynamoDB.\n       - **AmazonEC2FullAccess** se seu projeto gerencia instâncias EC2.\n   - **Anexar Políticas ao Usuário**: Selecione as políticas necessárias para o usuário conforme o escopo do projeto.\n\n5. **Revisar e Criar o Usuário:**\n   - Revise as configurações e clique em **\"Criar usuário\"**.\n   - **Download do CSV**: Baixe o arquivo CSV que contém o **Access Key ID** e o **Secret Access Key**. Estas credenciais serão usadas para configurar o acesso programático no seu projeto Python.\n\n6. **Configurar o Projeto Python:**\n   - No seu ambiente de desenvolvimento, configure as credenciais AWS usando o SDK boto3 ou outro SDK apropriado.\n   - Exemplo em Python usando boto3:\n     ```python\n     import boto3\n\n     # Configurando o acesso com as credenciais do IAM\n     session  boto3.Session(\n         aws_access_key_id'your-access-key-id',\n         aws_secret_access_key'your-secret-access-key',\n         region_name'your-region'  # Ex: 'us-east-1'\n     )\n\n     # Exemplo de uso do S3\n     s3  session.resource('s3')\n     for bucket in s3.buckets.all():\n         print(bucket.name)\n     ```\n\n### **14. Recomendações**\n\n- **Princípio do Menor Privilégio**: Sempre conceda apenas as permissões necessárias para o projeto específico. Se o projeto só precisa acessar o S3, evite adicionar permissões desnecessárias para outros serviços.\n- **Segurança das Credenciais**: Armazene as credenciais de forma segura e evite commitá-las em sistemas de controle de versão, como Git. Use serviços como AWS Secrets Manager ou variáveis de ambiente para gerenciar credenciais de maneira segura.\n\n### **15. Exemplo de Uso de Role no IAM: EC2 com Acesso ao S3**\n\n**Cenário Real**: Suponha que você esteja configurando uma instância EC2 na AWS que precisa acessar um bucket S3 para armazenar ou recuperar dados. Em vez de configurar credenciais de acesso diretamente na instância (o que pode ser inseguro), você pode usar uma **IAM Role** para conceder à instância EC2 as permissões necessárias para interagir com o S3. Isso é uma prática recomendada para aumentar a segurança e simplificar o gerenciamento de permissões.\n\n### **Passo a Passo para Criar uma Role para EC2 com Acesso ao S3**\n\n1. **Criar uma IAM Role**\n\n   1.1. **Acessar o Console IAM**:\n   - Faça login no console da AWS e navegue até o serviço **IAM (Identity and Access Management)**.\n   - No menu lateral, selecione **\"Roles\"** e clique em **\"Create Role\"**.\n\n   1.2. **Escolher o Tipo de Trusted Entity**:\n   - Na tela de criação da role, selecione **\"AWS Service\"** como o tipo de trusted entity.\n   - Em seguida, selecione **EC2** como o serviço que usará esta role.\n\n   1.3. **Anexar Políticas de Permissão**:\n   - Na etapa de anexar políticas, procure pela política gerenciada **AmazonS3FullAccess** ou **AmazonS3ReadOnlyAccess**, dependendo das necessidades do seu projeto.\n   - Selecione a política e clique em **\"Next\"**.\n\n   1.4. **Configurar Nome e Tags**:\n   - Dê um nome descritivo para a role, como **EC2-S3-Access-Role**.\n   - (Opcional) Adicione tags para facilitar a gestão e identificação da role.\n\n   1.5. **Revisar e Criar a Role**:\n   - Revise as configurações e clique em **\"Create Role\"**.\n\n2. **Atribuir a Role à Instância EC2**\n\n   2.1. **Criar ou Selecionar uma Instância EC2**:\n   - Navegue até o serviço **EC2** no console da AWS.\n   - Ao criar uma nova instância, na etapa de configuração, procure a seção **IAM role** e selecione a role **EC2-S3-Access-Role** criada anteriormente.\n\n   2.2. **Anexar a Role a uma Instância Existente**:\n   - Se você já possui uma instância EC2 em execução, vá para o painel de **Instâncias** no EC2.\n   - Selecione a instância desejada, clique em **Actions** > **Security** > **Modify IAM Role**.\n   - Selecione a role **EC2-S3-Access-Role** e clique em **Update IAM Role**.\n\n3. **Testar o Acesso do EC2 ao S3**\n\n   3.1. **Acessar a Instância EC2**:\n   - Conecte-se à instância EC2 via SSH.\n\n   3.2. **Verificar o Acesso ao S3**:\n   - Usando a AWS CLI, execute um comando simples para listar os buckets S3:\n     ```bash\n     aws s3 ls\n     ```\n   - Se a role foi atribuída corretamente, você verá a lista de buckets no S3.\n\n   3.3. **Exemplo de Uso no Código**:\n   - Se o seu aplicativo na EC2 precisa interagir com o S3 programaticamente, você pode usar o SDK da AWS (por exemplo, boto3 para Python) sem precisar gerenciar manualmente as credenciais:\n     ```python\n     import boto3\n\n     s3  boto3.client('s3')\n     response  s3.list_buckets()\n     for bucket in response['Buckets']:\n         print(f'Bucket Name: {bucket[\"Name\"]}')\n     ```\n\n### **Por Que Usar uma IAM Role em EC2?**\n\n- **Segurança**: Evita o uso de credenciais embutidas na instância, que podem ser comprometidas.\n- **Gerenciamento Simplificado**: As permissões são centralizadas na role e podem ser ajustadas sem necessidade de alterar a configuração da instância.\n- **Automação e Escalabilidade**: Ao usar uma role, o acesso a recursos AWS pode ser facilmente gerenciado em grandes ambientes de forma consistente e segura.\n\nEsse é um cenário muito comum em projetos na nuvem, onde a segurança e a facilidade de gerenciamento são cruciais para o sucesso e a manutenção da infraestrutura.\n\nExatamente! Essa é uma das principais vantagens de usar uma **IAM Role** para uma instância EC2.\n\n### **Vantagens da IAM Role em uma Instância EC2**\n\n1. **Eliminação da Necessidade de Credenciais Embutidas**: \n   - Quando você atribui uma IAM Role a uma instância EC2, o código ou os scripts que você executa nessa instância podem acessar diretamente os recursos da AWS (como S3, DynamoDB, etc.) sem precisar incluir manualmente as **Access Key** e **Secret Access Key** no código. Isso reduz o risco de exposição dessas credenciais, que poderiam ser comprometidas se fossem incluídas diretamente no código.\n\n2. **Segurança Aprimorada**:\n   - Ao evitar o uso de credenciais embutidas, você minimiza o risco de que essas credenciais sejam expostas ou mal utilizadas. A IAM Role é gerenciada pela AWS, e as permissões podem ser ajustadas centralmente sem necessidade de atualizar o código na instância.\n\n3. **Gerenciamento Simples e Centralizado**:\n   - Com IAM Roles, as permissões para acessar diferentes serviços AWS são definidas de maneira centralizada. Isso facilita a gestão das permissões à medida que o ambiente cresce, sem necessidade de modificar cada instância individualmente.\n\n4. **Rotação Automática de Credenciais**:\n   - A AWS automaticamente gerencia e rotaciona as credenciais temporárias associadas a uma IAM Role. Isso significa que as credenciais são continuamente atualizadas sem intervenção manual, garantindo que o acesso continue seguro.\n\n### **Como Funciona na Prática?**\n\nSim, se você subir um código Python na instância EC2 que tem uma IAM Role configurada com acesso ao S3, por exemplo, você **não precisará especificar as chaves de acesso**. O código simplesmente usará as permissões da IAM Role atribuída à instância para acessar os recursos. Aqui está um exemplo simples:\n\n```python\nimport boto3\n\n# Não é necessário especificar aws_access_key_id ou aws_secret_access_key\ns3  boto3.client('s3')\n\n# Listando os buckets no S3\nresponse  s3.list_buckets()\nfor bucket in response['Buckets']:\n    print(f'Bucket Name: {bucket[\"Name\"]}')\n```\n\nNeste exemplo, o código Python roda na instância EC2 e usa automaticamente as permissões da IAM Role para interagir com o S3, sem que você precise se preocupar com as credenciais. Isso torna o processo mais seguro, eficiente e fácil de gerenciar.\n\n",
        "Bootcamp - Cloud para dados/Aula_05/README.md - Parte (1/2)\nBootcamp - Cloud para dados/Aula_05/README.md\n\n# **Bootcamp Cloud: Aula 05: VPC na AWS**\n\n**Objetivo**: Nesta aula, vamos explorar o conceito de VPC (Virtual Private Cloud) na AWS, entender seus componentes principais, como sub-redes públicas e privadas, gateways de internet, endpoints de VPC, e aprender a configurar e utilizar esses elementos para criar uma infraestrutura de rede segura e eficiente na nuvem.\n\n### **1. O Que é VPC?**\n\n- **Definição**: A VPC (Virtual Private Cloud) é um serviço da AWS que permite criar uma rede privada virtual dentro da nuvem. Com a VPC, você tem controle total sobre o seu ambiente de rede, incluindo a escolha de seu próprio intervalo de endereços IP, a criação de sub-redes e a configuração de tabelas de roteamento e gateways de internet.\n  \n- **Benefícios**:\n  - **Isolamento de Rede**: Mantém seus recursos isolados de outros clientes da AWS.\n  - **Controle de Segurança**: Permite definir regras de segurança detalhadas para proteger seus recursos.\n  - **Flexibilidade de Configuração**: Oferece a possibilidade de personalizar a configuração de sua rede, como endereçamento IP, gateways, e roteamento.\n\n### **2. Componentes Principais de uma VPC**\n\n#### **Sub-redes (Subnets)**\n\n**O que são Sub-redes?**\n- As sub-redes são divisões dentro de uma VPC que permitem segmentar e organizar recursos. Cada sub-rede é associada a uma parte do intervalo de endereços IP da VPC e reside em uma única Zona de Disponibilidade (AZ).\n\n**Tipos de Sub-redes:**\n- **Sub-redes Públicas**: São sub-redes conectadas a um Gateway de Internet (Internet Gateway), permitindo que recursos dentro delas tenham acesso direto à internet. Ideal para recursos que precisam ser acessíveis externamente, como servidores web.\n- **Sub-redes Privadas**: São sub-redes que não possuem acesso direto à internet. Elas são usadas para recursos que devem ser isolados do acesso público, como bancos de dados. O acesso à internet a partir dessas sub-redes é feito por meio de um NAT Gateway, que roteia o tráfego de saída de forma segura.\n\n#### **Sub-redes Públicas (Public Subnets)**\n\n- **Definição**: Sub-redes públicas são configuradas para que os recursos nelas alocados possam se comunicar diretamente com a internet. Isso é feito associando a sub-rede a um Gateway de Internet (IGW).\n  \n- **Uso Comum**:\n  - **Servidores Web**: Servidores que precisam estar acessíveis ao público, como servidores web, geralmente são colocados em sub-redes públicas. Isso permite que eles recebam tráfego de entrada diretamente da internet.\n  - **Recursos que Necessitam de Acesso Direto à Internet**: Recursos como proxies, gateways, ou load balancers que devem receber ou enviar dados diretamente pela internet também são colocados em sub-redes públicas.\n\n- **Regras de Segurança**:\n  - **Security Groups**: Devem ser configurados para permitir tráfego apenas nas portas e protocolos necessários (por exemplo, HTTP na porta 80 ou HTTPS na porta 443).\n\n#### **Sub-redes Privadas (Private Subnets)**\n\n- **Definição**: Sub-redes privadas são aquelas que não têm acesso direto à internet. Elas são usadas para hospedar recursos que devem ser protegidos de acessos externos diretos, como bancos de dados ou servidores de backend. O tráfego de saída para a internet a partir dessas sub-redes é feito usando um NAT Gateway.\n\n- **Uso Comum**:\n  - **Bancos de Dados e Servidores de Aplicação**: Servidores que armazenam dados críticos, como bancos de dados, ou que executam lógica de aplicação sensível, são colocados em sub-redes privadas para proteger contra acessos diretos da internet.\n  - **Recursos Backend**: Qualquer recurso que não precisa de acesso direto da internet, mas pode precisar se conectar a outros serviços da AWS ou realizar atualizações de software pela internet.\n\n- **Acesso à Internet através de NAT**:\n  - **NAT Gateway**: Para permitir que recursos em sub-redes privadas acessem a internet de forma segura (por exemplo, para baixar atualizações ou acessar APIs), utilizamos um NAT Gateway. O NAT roteia o tráfego de saída das instâncias privadas para a internet, sem expor esses recursos diretamente.\n\nUm **NAT Gateway** (Network Address Translation Gateway) é um serviço na AWS que permite que instâncias em uma sub-rede privada façam solicitações de saída para a internet (como downloads ou atualizações de software) sem estarem diretamente acessíveis de fora da VPC. Vamos entender melhor o papel do NAT Gateway e a vantagem de usá-lo, mesmo com o custo associado.\n\n### **O Que é um NAT Gateway?**\n- **NAT Gateway** é um recurso gerenciado da AWS que permite que instâncias em sub-redes privadas tenham acesso de saída à internet (como para downloads ou atualizações), enquanto impede conexões de entrada não solicitadas vindas da internet.\n- Funciona traduzindo endereços IP privados (da sub-rede privada) em um endereço IP público (associado ao NAT Gateway) para todo o tráfego de saída, e vice-versa para o tráfego de resposta.\n\n### **Vantagem de Usar um NAT Gateway em uma Sub-rede Privada**\n\n1. **Segurança Aprimorada:**\n   - **Controle de Acesso:** Quando você coloca uma instância em uma sub-rede privada e usa um NAT Gateway, a instância não é acessível diretamente pela internet. Isso significa que ninguém pode iniciar uma conexão diretamente com a instância, reduzindo a superfície de ataque.\n   - **Isolamento da Rede:** Instâncias em sub-redes privadas são protegidas por não serem visíveis diretamente na internet. O NAT Gateway permite que apenas tráfego de saída seja permitido, não o de entrada não solicitado.\n\n2. **Limitação de Acesso Direto:**\n   - **Acesso Controlado:** As instâncias privadas que precisam de conectividade de saída (por exemplo, para acessar atualizações de software, bibliotecas, ou APIs externas) ainda podem fazê-lo sem ficarem diretamente expostas à internet. Isso oferece um equilíbrio entre segurança e funcionalidade.\n   - **Prevenção de Ataques:** Como as instâncias em sub-redes privadas não têm IPs públicos, elas não são diretamente atacáveis por scans de portas, ataques de DDoS, ou tentativas de invasão que podem ser direcionadas a IPs públicos.\n\n3. **Compliance e Conformidade:**\n   - **Requisitos de Segurança:** Algumas regulamentações de segurança e conformidade (como PCI-DSS, HIPAA, etc.) exigem que servidores de dados críticos (como bancos de dados) sejam isolados e não tenham exposição direta à internet. Usar sub-redes privadas com NAT Gateway cumpre esses requisitos.\n\n### **Por que Não Colocar a Sub-rede Direto como Pública?**\n\nSe você coloca suas instâncias diretamente em uma **sub-rede pública**:\n- **Exposição Direta à Internet:** Cada instância em uma sub-rede pública pode receber tráfego de entrada direto da internet. Mesmo que você configure regras de firewall e segurança (Security Groups e NACLs), a instância ainda estará visível e pode ser alvo de ataques.\n- **Necessidade de Endereços IP Públicos:** Cada instância precisaria de um IP público, aumentando o custo e o risco de segurança.\n- **Superfície de Ataque Maior:** Com a exposição direta, a chance de ataques, como scans de portas ou ataques de DDoS, aumenta significativamente.\n\nVamos esclarecer os conceitos de **entrada (inbound)** e **saída (outbound)** no contexto de redes e comunicação na internet, bem como entender como isso se aplica a uma infraestrutura na nuvem, como a AWS.\n\n### **Diferença entre Entrada (Inbound) e Saída (Outbound)**\n\n1. **Tráfego de Entrada (Inbound Traffic):**\n   - **Definição:** Tráfego de entrada refere-se a todas as conexões e dados que vêm de fora da rede e estão entrando em seus recursos, como instâncias de servidores ou dispositivos.\n   - **Exemplos:**\n     - Um usuário acessando um site hospedado em um servidor web na AWS.\n     - Requisições de API enviadas para um serviço hospedado na sua VPC.\n     - Conexões SSH de um administrador tentando acessar remotamente um servidor.\n\n2. **Tráfego de Saída (Outbound Traffic):**\n   - **Definição:** Tráfego de saída refere-se a todas as conexões e dados que estão saindo de seus recursos e indo para fora da rede, como para a internet ou outras redes.\n   - **Exemplos:**\n     - Um servidor em sua sub-rede privada baixando atualizações de software da internet.\n     - Envio de dados para um serviço externo, como uma API de terceiros.\n     - Conexões de um banco de dados na sua VPC para outro banco de dados em uma rede externa.\n\n### **Como Funciona o NAT Gateway no Contexto de Saída e Entrada?**\n\nUm **NAT Gateway** permite que o tráfego de saída (outbound) de instâncias em uma sub-rede privada alcance a internet sem permitir o tráfego de entrada (inbound) direto da internet para essas instâncias. Vamos ver como isso se aplica a ambos os tipos de tráfego:\n\n- **Tráfego de Saída (Outbound) com NAT Gateway:**\n  - **Exemplo de Funcionamento:**\n    - Instâncias em uma sub-rede privada que não têm endereços IP públicos enviam solicitações para a internet (como atualizações de software ou chamadas para APIs).\n    - O NAT Gateway, localizado em uma sub-rede pública, recebe essas solicitações, traduz os endereços IP privados para o IP público do NAT Gateway e encaminha o tráfego para a internet.\n    - Quando a resposta chega, o NAT Gateway traduz de volta para os endereços IP privados e entrega a resposta às instâncias na sub-rede privada.\n\n- **Tráfego de Entrada (Inbound) com NAT Gateway:**\n  - **Exemplo de Restrição:**\n    - Um NAT Gateway **não** permite conexões de entrada iniciadas da internet para as instâncias na sub-rede privada. Ele bloqueia automaticamente qualquer tentativa de conexão iniciada externamente, garantindo que apenas o tráfego de saída e suas respectivas respostas sejam permitidos.\n\n### **Por que a Distinção é Importante?**\n\n- **Segurança:**\n  - Separar o tráfego de entrada e saída é uma prática importante para segurança. Tráfego de entrada direto da internet representa um risco de segurança, pois expõe seus recursos a ataques externos, como tentativas de invasão, DDoS, etc.\n  - Tráfego de saída é necessário para muitas operações, como atualizações de software e chamadas de API. No entanto, ele não deve expor diretamente os recursos a ataques de entrada.\n\n- **Uso de NAT Gateway:**\n  - Usar um **NAT Gateway** permite que recursos em sub-redes privadas tenham conectividade de saída segura com a internet, sem expô-los a tráfego de entrada desnecessário. Isso protege seus recursos enquanto ainda permite que realizem operações necessárias.\n\n### **Outbound e Inbound: Exemplos Comuns na AWS**\n\n1. **Instância EC2 Pública:**\n   - **Inbound:** Pode receber conexões de entrada diretamente da internet (por exemplo, um servidor web recebendo requisições HTTP).\n   - **Outbound:** Pode enviar tráfego para a internet (por exemplo, fazer solicitações de saída a uma API externa).\n\n2. **Instância EC2 Privada com NAT Gateway:**\n   - **Inbound:** Não pode receber conexões de entrada diretamente da internet (somente resposta a requisições de saída previamente iniciadas).\n   - **Outbound:** Pode enviar tráfego de saída para a internet através do NAT Gateway (por exemplo, baixar pacotes de atualização).\n\n### **Conclusão: Diferença entre Entrada e Saída**\n\n- **Entrada (Inbound):** Conexões iniciadas de fora da rede em direção aos seus recursos.\n- **Saída (Outbound):** Conexões iniciadas pelos seus recursos em direção a fora da rede.\n- **NAT Gateway:** Facilita conexões de saída para recursos em sub-redes privadas, mantendo-os protegidos de conexões de entrada diretas, garantindo um ambiente mais seguro e controlado.\n\n### **Quando Usar um NAT Gateway?**\n\nUse um **NAT Gateway** quando:\n- Você precisa que instâncias em sub-redes privadas tenham acesso de saída à internet, mas sem serem diretamente acessíveis de fora da VPC.\n- A segurança é uma prioridade e você quer minimizar a exposição dos seus recursos à internet.\n- Você quer aproveitar a solução gerenciada da AWS, que oferece alta disponibilidade e manutenção automática.\n\n### **Diferença entre NAT Gateway e VPC Endpoint**\n\nAntes de detalhar onde usar um **NAT Gateway** em vez de um **VPC Endpoint**, vamos esclarecer o que cada um desses recursos faz:\n\n- **NAT Gateway**: Permite que instâncias em sub-redes privadas tenham acesso de saída à internet sem permitir conexões de entrada da internet. Ideal para casos em que os recursos na sub-rede privada precisam se comunicar com serviços fora da VPC (como APIs de terceiros ou atualizações de software).\n\n- **VPC Endpoint**: Permite que sua VPC se conecte diretamente a determinados serviços da AWS (como S3 ou DynamoDB) sem passar pela internet pública. Ideal para quando você deseja acessar serviços da AWS de forma segura e sem exposição à internet.\n\n### **Quando Usar NAT Gateway ao Invés de VPC Endpoint?**\n\nUse o **NAT Gateway** em vez de um **VPC Endpoint** quando:\n\n1. **",
        "Bootcamp - Cloud para dados/Aula_05/README.md - Parte (2/2)\nConectar a Recursos Fora da AWS:**\n   - Se seus recursos em sub-redes privadas precisam se comunicar com serviços ou recursos fora da AWS, como APIs de terceiros, sites para atualizações de software, ou servidores externos.\n   - **Exemplo:** Sua instância EC2 em uma sub-rede privada precisa baixar pacotes de software ou atualizações diretamente da internet. Nesse caso, um NAT Gateway permitirá que essa instância tenha acesso de saída sem estar diretamente exposta à internet para conexões de entrada.\n\n2. **Acesso Genérico à Internet:**\n   - Quando você precisa de acesso geral à internet para vários propósitos e não está limitado a serviços específicos da AWS.\n   - **Exemplo:** Um servidor de banco de dados em uma sub-rede privada que precisa se conectar a um repositório de pacotes para baixar atualizações ou enviar logs para um serviço de monitoramento externo.\n\n3. **Acessar Serviços Não Suportados por VPC Endpoints:**\n   - VPC Endpoints atualmente suportam apenas um subconjunto de serviços da AWS (por exemplo, S3, DynamoDB). Se você precisa acessar serviços ou destinos que não são cobertos por VPC Endpoints, você precisará usar um NAT Gateway.\n   - **Exemplo:** Conectar-se a um serviço da AWS que não possui um VPC Endpoint específico (como algumas APIs menos comuns ou serviços regionais específicos).\n\n### **Quando Usar VPC Endpoint ao Invés de NAT Gateway?**\n\nUse um **VPC Endpoint** em vez de um **NAT Gateway** quando:\n\n1. **Conectar-se a Serviços da AWS de Forma Segura:**\n   - Quando você deseja que sua VPC se conecte diretamente a serviços da AWS (como S3 ou DynamoDB) sem passar pela internet pública. Isso proporciona mais segurança e pode reduzir a latência.\n   - **Exemplo:** Um aplicativo em uma instância EC2 em uma sub-rede privada precisa acessar um bucket S3 para armazenar ou recuperar arquivos de dados. Um VPC Endpoint para S3 permitirá essa comunicação sem passar pela internet.\n\n2. **Reduzir Custos:**\n   - VPC Endpoints geralmente são mais baratos do que usar um NAT Gateway porque não há cobrança por dados de saída através de um endpoint, enquanto o NAT Gateway cobra por dados transferidos.\n   - **Exemplo:** Transferir grandes volumes de dados entre sua VPC e o S3. Um VPC Endpoint para S3 pode reduzir significativamente os custos de transferência de dados em comparação ao uso de um NAT Gateway.\n\n3. **Compliance e Conformidade:**\n   - Se suas políticas de segurança exigem que todo o tráfego permaneça dentro da rede da AWS sem passar pela internet pública, usar VPC Endpoints é ideal.\n   - **Exemplo:** Organizações que precisam estar em conformidade com regulamentações como PCI-DSS, HIPAA, ou outras normas que exigem controle estrito de onde os dados trafegam.\n\n### **Resumo da Escolha: NAT Gateway vs. VPC Endpoint**\n\n| **Cenário**                                         | **Usar NAT Gateway**                                 | **Usar VPC Endpoint**                                |\n|-----------------------------------------------------|------------------------------------------------------|------------------------------------------------------|\n| **Conectar a recursos fora da AWS**                 | Sim                                                  | Não                                                  |\n| **Acesso geral à internet (saída)**                 | Sim                                                  | Não                                                  |\n| **Acessar serviços da AWS sem expor à internet**    | Não                                                  | Sim                                                  |\n| **Reduzir custos de transferência de dados**        | Não (pode ser caro para grandes volumes)             | Sim                                                  |\n| **Conformidade com regulamentações de segurança**   | Não (passa pela internet pública)                    | Sim (tráfego permanece dentro da rede da AWS)        |\n| **Acessar serviços da AWS que não possuem Endpoint**| Sim                                                  | Não                                                  |\n\n### **Conclusão**\n\n- **Use NAT Gateway** quando você precisar que recursos em uma sub-rede privada tenham acesso de saída à internet para fins gerais ou para se comunicar com recursos fora da AWS.\n- **Use VPC Endpoints** quando você deseja que seus recursos em sub-redes privadas acessem serviços da AWS de maneira segura e econômica, sem expô-los à internet pública.\n\n### **Conclusão**\n\nEmbora o NAT Gateway tenha um custo, ele é essencial para garantir que as instâncias em sub-redes privadas possam se comunicar com a internet sem comprometer a segurança. Ele oferece uma solução balanceada entre a necessidade de conectividade de saída e a necessidade de segurança, proteção e conformidade com normas e regulamentações.\n\n#### **Internet Gateway (IGW)**\n\n- **Definição**: Um Internet Gateway (IGW) é um componente que conecta a VPC à internet. Ele permite que recursos em sub-redes públicas possam se comunicar com a internet.\n\n- **Uso**:\n  - **Acesso de Entrada e Saída**: O IGW permite que o tráfego entre a internet e os recursos da VPC flua livremente, garantindo que servidores em sub-redes públicas possam receber e enviar tráfego.\n  - **Associado a Sub-redes Públicas**: Uma sub-rede é considerada pública quando associada a um Internet Gateway através de uma tabela de roteamento que direciona o tráfego para a internet.\n\n#### **NAT Gateway**\n\n- **Definição**: Um NAT (Network Address Translation) Gateway permite que instâncias em sub-redes privadas acessem a internet para tráfego de saída, sem permitir conexões de entrada vindas da internet.\n\n- **Uso**:\n  - **Tráfego de Saída Seguro**: Permite que recursos em sub-redes privadas, como servidores de aplicação ou bancos de dados, façam solicitações de saída para a internet (por exemplo, para atualizações de software), sem expor esses recursos a acessos externos.\n  - **Alocação de IP Elástico**: O NAT Gateway é associado a um Elastic IP para gerenciar o tráfego de saída.\n\n#### **VPC Endpoints**\n\n- **Definição**: Um VPC Endpoint permite conectar sua VPC a serviços AWS diretamente, sem passar pela internet pública. Isso fornece uma maneira segura e eficiente de acessar serviços da AWS como S3 ou DynamoDB dentro da VPC.\n\n- **Tipos de VPC Endpoints**:\n  - **Gateway Endpoints**: Usados para serviços baseados em S3 e DynamoDB.\n  - **Interface Endpoints**: Usados para conectar serviços AWS mais amplamente (como SQS, SNS) diretamente dentro de sua VPC via interfaces de rede privadas (ENIs).\n\n- **Benefícios**:\n  - **Segurança**: Todo o tráfego permanece dentro da rede AWS, sem transitar pela internet pública.\n  - **Redução de Latência e Custos**: Ao evitar o tráfego pela internet, você reduz a latência e evita custos associados ao uso de largura de banda pública.\n\n### **3. Passo a Passo: Criando uma VPC na AWS**\n\n**Cenário**: Vamos criar uma VPC com uma sub-rede pública e uma privada, configurar um gateway de internet, um NAT Gateway, VPC Endpoints, e aplicar as regras de segurança necessárias.\n\n#### **1. Criar a VPC**\n\n1. **Acessar o Console VPC**:\n   - Faça login no console da AWS e navegue até o serviço **VPC (Virtual Private Cloud)**.\n   - No painel do VPC, clique em **\"Your VPCs\"** e depois em **\"Create VPC\"**.\n\n2. **Configurar a VPC**:\n   - **Nome**: Dê um nome à sua VPC, por exemplo, \"MinhaVPC\".\n   - **CIDR Block**: Defina um bloco CIDR, como \"10.0.0.0/16\", que representa o intervalo de endereços IP que sua VPC utilizará.\n   - **Tenancy**: Escolha entre \"default\" (uso compartilhado) ou \"dedicated\" (hardware dedicado).\n\n3. **Criar a VPC**:\n   - Clique em **\"Create VPC\"** para finalizar a criação da VPC.\n\n#### **2. Criar Sub-redes**\n\n1. **Criar Sub-rede Pública**:\n   - No painel VPC, selecione **\"Subnets\"** e clique em **\"Create Subnet\"**.\n   - **Nome**: Dê um nome, como \"MinhaSubnetPublica\".\n   - **VPC**: Selecione a VPC criada anteriormente (\"MinhaVPC\").\n   - **CIDR Block**: Defina um bloco CIDR, como \"10.0.1.0/24\".\n   - **Availability Zone**: Escolha uma zona de disponibilidade.\n\n2. **Criar Sub-rede Privada**:\n   - Repita o processo para criar uma sub-rede privada.\n   - **Nome**: \"MinhaSubnetPrivada\".\n   - **CIDR Block**: \"10.0.2.0/24\".\n   - Esta sub-rede não terá acesso direto à internet.\n\n#### **3. Configurar um Gateway de Internet**\n\n1. **Criar o Gateway de Internet**:\n   - No painel VPC, clique em **\"Internet Gateways\"** e depois em **\"Create Internet Gateway\"**.\n   - **Nome**: \"MeuInternetGateway\".\n   - Clique em **\"Create\"**.\n\n2. **Associar o Gateway de Internet à VPC**:\n   - Selecione o gateway de internet recém-criado.\n   - Clique em **\"Actions\"** e selecione **\"Attach to VPC\"**.\n  \n\n - Escolha \"MinhaVPC\" e clique em **\"Attach\"**.\n\n#### **4. Configurar um NAT Gateway para a Sub-rede Privada**\n\n1. **Criar um NAT Gateway**:\n   - No painel VPC, clique em **\"NAT Gateways\"** e depois em **\"Create NAT Gateway\"**.\n   - **Nome**: \"MeuNATGateway\".\n   - **Sub-rede**: Escolha a \"MinhaSubnetPublica\" para que o NAT Gateway possa ter acesso ao Internet Gateway.\n   - **Elastic IP Allocation**: Clique em \"Allocate Elastic IP\" para associar um IP Elástico ao NAT Gateway.\n\n2. **Associar o NAT Gateway à Tabela de Roteamento da Sub-rede Privada**:\n   - Vá até **\"Route Tables\"** e selecione a tabela associada à \"MinhaSubnetPrivada\".\n   - Adicione uma rota para o NAT Gateway para tráfego de saída para a internet.\n\n#### **5. Configurar Tabelas de Roteamento**\n\n1. **Criar uma Tabela de Roteamento para Sub-rede Pública**:\n   - No painel VPC, clique em **\"Route Tables\"** e depois em **\"Create Route Table\"**.\n   - **Nome**: \"TabelaPublica\".\n   - **VPC**: Selecione \"MinhaVPC\".\n   - Clique em **\"Create\"**.\n\n2. **Adicionar uma Rota para o Gateway de Internet**:\n   - Selecione a \"TabelaPublica\" criada.\n   - Clique em **\"Routes\"** > **\"Edit routes\"** > **\"Add route\"**.\n   - **Destination**: \"0.0.0.0/0\" (rota padrão para toda a internet).\n   - **Target**: Selecione \"MeuInternetGateway\".\n   - Clique em **\"Save changes\"**.\n\n3. **Associar a Tabela de Roteamento à Sub-rede Pública**:\n   - Clique em **\"Subnet associations\"** > **\"Edit subnet associations\"**.\n   - Selecione \"MinhaSubnetPublica\".\n   - Clique em **\"Save associations\"**.\n\n#### **6. Configurar VPC Endpoints**\n\n1. **Criar um VPC Endpoint para o S3**:\n   - No painel VPC, clique em **\"Endpoints\"** e depois em **\"Create Endpoint\"**.\n   - **Nome**: \"S3-Endpoint\".\n   - **Service**: Escolha \"com.amazonaws.region.s3\".\n   - **VPC**: Selecione \"MinhaVPC\".\n   - **Rota**: Selecione as sub-redes desejadas (por exemplo, \"MinhaSubnetPrivada\") e clique em **\"Create Endpoint\"**.\n\n### **4. Testando e Verificando a Configuração da VPC**\n\n1. **Lançar Instâncias EC2 em Sub-redes Públicas e Privadas**:\n   - No painel EC2, crie uma instância em \"MinhaSubnetPublica\" e outra em \"MinhaSubnetPrivada\".\n   - Verifique se a instância pública pode acessar a internet e a privada não.\n\n2. **Testar Conexões entre as Instâncias**:\n   - Conecte-se à instância na sub-rede pública usando SSH.\n   - Da instância pública, tente pingar a instância na sub-rede privada para verificar a conectividade interna.\n\n3. **Testar VPC Endpoints**:\n   - Verifique se a instância na sub-rede privada pode acessar o S3 diretamente usando o VPC Endpoint, sem precisar passar pela internet.\n\n### **5. Melhores Práticas ao Configurar VPCs**\n\n- **Segregação de Ambientes**: Use diferentes VPCs para ambientes de desenvolvimento, teste e produção.\n- **Monitoramento e Logs**: Habilite o AWS VPC Flow Logs para monitorar e registrar o tráfego de rede.\n- **Princípio do Menor Privilégio**: Configure security groups e NACLs para permitir apenas o tráfego necessário.\n- **Uso de VPC Endpoints**: Utilize VPC Endpoints para reduzir o tráfego pela internet pública e aumentar a segurança ao acessar serviços AWS.\n\n### **6. Projeto: Criando uma Rede Segura na AWS com VPC**\n\n**Objetivo do Projeto**: Construir uma infraestrutura de rede usando a VPC, garantindo que os recursos críticos estejam isolados e protegidos, enquanto mantém o acesso adequado aos recursos públicos.\n\n**Passo a Passo Resumido:**\n\n1. **Criação de VPC e Sub-redes**:\n   - Configure a VPC e divida-a em sub-redes públicas e privadas.\n2. **Implementação de Gateways e Tabelas de Roteamento**:\n   - Configure o acesso à internet para sub-redes públicas e o uso de NAT para sub-redes privadas.\n3. **Aplicação de Regras de Segurança**:\n   - Use security groups, NACLs e VPC Endpoints para proteger recursos e otimizar o tráfego.\n4. **Teste e Verificação**:\n   - Teste o acesso entre instâncias e à internet para validar a configuração.\n\nEssa aula cobre todos os componentes essenciais de uma VPC e fornece uma base sólida para a construção de redes seguras e bem configuradas na AWS.\n\n",
        "Bootcamp - Cloud para dados/Aula_06/README.md - Parte (1/2)\nBootcamp - Cloud para dados/Aula_06/README.md\n\n### **Desafio de Negócio**\n\n**Dever de casa **\n\nhttps://blog.intigriti.com/hacking-tools/hacking-misconfigured-aws-s3-buckets-a-complete-guide\n\n**Desafio**: **Proteção e Processamento Seguro de Dados Sensíveis em Ambientes Regulatórios**\n\n![image](1_C2EJ15gGamMmbWGAt7I0Rg.png)\n\nEm muitas indústrias, como a financeira, de saúde ou de tecnologia, as empresas lidam com grandes volumes de dados altamente sensíveis que devem ser protegidos rigorosamente para cumprir regulamentações como GDPR, HIPAA, ou normas internas de segurança. Um desafio comum é garantir que esses dados sejam processados de forma segura, sem exposição a ameaças externas, ao mesmo tempo em que permitem o acesso controlado para manutenção e monitoramento.\n\n### **Cenário**\n\nUma empresa de serviços financeiros precisa processar dados de transações bancárias que são armazenados em um ambiente de armazenamento seguro (S3). Esses dados são altamente sensíveis e sujeitos a rigorosas regulamentações de proteção de dados. A empresa enfrenta o desafio de processar esses dados de forma eficiente e segura, garantindo que nenhum dado seja exposto à internet durante o processamento.\n\n### **Motivação para Montar a Arquitetura**\n\n1. **Segurança e Conformidade**: A principal motivação é proteger os dados sensíveis e cumprir as exigências regulatórias. Ao isolar o processamento de dados em uma subrede privada, a arquitetura garante que os dados não sejam expostos à internet. A conexão ao bucket S3 é feita exclusivamente através de um VPC Endpoint, mantendo o tráfego de dados dentro da infraestrutura segura da AWS.\n\n2. **Acesso Controlado e Seguro**: Para manutenção e monitoramento, é necessário acesso à instância que processa os dados. No entanto, para evitar riscos, o acesso direto à instância privada não é permitido. Em vez disso, uma instância pública (Bastion Host) na subrede pública atua como um ponto de entrada seguro. Isso permite que apenas usuários autorizados, através de conexões seguras, possam acessar a instância privada, minimizando os riscos de ataques externos.\n\n3. **Gerenciamento Eficiente de Recursos**: A arquitetura permite separar responsabilidades, com a instância privada focada exclusivamente no processamento seguro dos dados, enquanto a instância pública gerencia o acesso e monitoramento. Isso facilita a manutenção, escalabilidade e otimização de recursos, garantindo que cada componente esteja configurado para desempenhar sua função da forma mais eficiente possível.\n\n### **Benefício para o Negócio**\n\nImplementar essa arquitetura permite à empresa processar dados sensíveis em conformidade com regulamentações rigorosas, ao mesmo tempo em que assegura que os dados permanecem seguros e acessíveis apenas por usuários autorizados. Isso não só reduz o risco de violações de dados, como também garante que a empresa esteja em total conformidade com normas de proteção de dados, evitando penalidades e mantendo a confiança dos clientes e parceiros.\n\n### **Estimativa de Custo para Implementação da Arquitetura**\n\nOs custos de implementação de uma arquitetura como a descrita podem variar dependendo de vários fatores, como o tamanho e o uso dos recursos, a região da AWS em que o projeto é implementado, e o nível de utilização dos serviços. Abaixo, forneço uma estimativa aproximada baseada em preços de mercado comuns para os principais componentes da arquitetura.\n\n#### **Componentes e Custos Estimados**\n\n1. **VPC (Virtual Private Cloud)**\n   - **Custo:** Geralmente, a criação e o uso de uma VPC são gratuitos. No entanto, os componentes dentro da VPC, como subredes e roteamento, podem gerar custos dependendo do tráfego.\n\n2. **Instância EC2 Pública (Bastion Host)**\n   - **Tipo de Instância:** t3.medium (2 vCPUs, 4 GB RAM)\n   - **Custo:** Aproximadamente $0,0416 por hora.\n   - **Uso Mensal (720 horas):** $29,95\n\n3. **Instância EC2 Privada (Processamento)**\n   - **Tipo de Instância:** m5.large (2 vCPUs, 8 GB RAM)\n   - **Custo:** Aproximadamente $0,096 por hora.\n   - **Uso Mensal (720 horas):** $69,12\n\n4. **S3 Storage**\n   - **Custo por GB armazenado:** $0,023 por GB.\n   - **Transferência de dados entre buckets:** Sem custos dentro da mesma região.\n   - **Estimativa de 500 GB de dados:** $11,50 por mês.\n\n5. **VPC Endpoint para S3**\n   - **Custo:** $0,01 por hora por endpoint.\n   - **Uso Mensal (720 horas):** $7,20\n\n6. **Tráfego de Dados (Internet Gateway)**\n   - **Custo de saída de dados para a internet:** $0,09 por GB.\n   - **Estimativa para 100 GB de tráfego de saída:** $9,00\n\n7. **CloudWatch Logs e Métricas**\n   - **Métricas Customizadas:** $0,30 por métrica por mês.\n   - **Logs (5 GB por mês):** $0,50 por GB.\n   - **Estimativa para logs e métricas:** $5,00 por mês.\n\n8. **IAM (Roles e Usuários)**\n   - **Custo:** A criação e gerenciamento de IAM Roles e Usuários não têm custos diretos associados.\n\n#### **Custo Total Estimado Mensal**\n\n| **Componente**                | **Custo Mensal** |\n|--------------------------------|------------------|\n| Instância EC2 Pública (t3.medium)  | $29,95           |\n| Instância EC2 Privada (m5.large)   | $69,12           |\n| Armazenamento S3 (500 GB)      | $11,50           |\n| VPC Endpoint para S3           | $7,20            |\n| Tráfego de Dados (100 GB)      | $9,00            |\n| CloudWatch (Logs e Métricas)   | $5,00            |\n| **Total Estimado**             | **$131,77**      |\n\n### **Considerações Finais**\n\n- **Custos Variáveis:** O custo final pode variar dependendo do uso efetivo dos recursos, como o tráfego de dados e o armazenamento adicional no S3.\n- **Reduções Potenciais:** Usar instâncias spot ou reservar instâncias EC2 para contratos de longo prazo pode reduzir significativamente os custos.\n- **Testes e Desenvolvimento:** Durante a fase de desenvolvimento e teste, você pode utilizar instâncias menores e reduzir a quantidade de dados processados para minimizar os custos.\n\nEssas estimativas fornecem uma visão geral dos custos mensais associados a manter essa arquitetura AWS em produção. Ajustes específicos podem ser feitos com base nas necessidades e no uso real do projeto.\n\nO custo de desenvolvimento de um projeto como esse pode variar amplamente dependendo de vários fatores, como a complexidade do projeto, a experiência e localização dos desenvolvedores, o tempo necessário para implementação, e se você contratará uma equipe interna, freelancers ou uma empresa especializada.\n\n### **Fatores a Considerar no Cálculo do Custo de Desenvolvimento**\n\n1. **Tamanho e Complexidade do Projeto**:\n   - Este projeto envolve a configuração de uma infraestrutura AWS, desenvolvimento de scripts de automação e monitoramento, além de garantir segurança e conformidade com práticas recomendadas.\n\n2. **Equipe Necessária**:\n   - **Arquiteto de Soluções AWS**: Responsável por desenhar a arquitetura e configurar os serviços AWS.\n   - **Engenheiro de DevOps**: Para implementar a infraestrutura como código (IaC) usando ferramentas como Terraform ou CloudFormation.\n   - **Desenvolvedor Backend (Python, etc.)**: Para desenvolver scripts de automação e o dashboard.\n   - **Especialista em Segurança**: Para configurar corretamente as políticas de IAM e garantir que a infraestrutura seja segura.\n   - **Gerente de Projeto**: Para coordenar o desenvolvimento e garantir que o projeto seja entregue dentro do prazo e orçamento.\n\n3. **Tempo de Desenvolvimento**:\n   - **Fase de Planejamento e Design**: 1-2 semanas\n   - **Configuração da Infraestrutura AWS**: 2-3 semanas\n   - **Desenvolvimento de Scripts e Dashboard**: 3-4 semanas\n   - **Testes e Ajustes**: 1-2 semanas\n   - **Total Estimado**: 7-11 semanas\n\n### **Estimativa de Custo de Desenvolvimento**\n\n1. **Arquiteto de Soluções AWS**:\n   - **Taxa por Hora**: $100 - $200\n   - **Horas Estimadas**: 40 - 80 horas\n   - **Custo Total**: $4,000 - $16,000\n\n2. **Engenheiro de DevOps**:\n   - **Taxa por Hora**: $80 - $150\n   - **Horas Estimadas**: 60 - 100 horas\n   - **Custo Total**: $4,800 - $15,000\n\n3. **Desenvolvedor Backend**:\n   - **Taxa por Hora**: $60 - $120\n   - **Horas Estimadas**: 80 - 120 horas\n   - **Custo Total**: $4,800 - $14,400\n\n4. **Especialista em Segurança**:\n   - **Taxa por Hora**: $100 - $200\n   - **Horas Estimadas**: 20 - 40 horas\n   - **Custo Total**: $2,000 - $8,000\n\n5. **Gerente de Projeto**:\n   - **Taxa por Hora**: $60 - $100\n   - **Horas Estimadas**: 40 - 60 horas\n   - **Custo Total**: $2,400 - $6,000\n\n### **Custo Total Estimado de Desenvolvimento**\n\n| **Função**                       | **Custo Estimado** |\n|----------------------------------|--------------------|\n| Arquiteto de Soluções AWS        | $4,000 - $16,000   |\n| Engenheiro de DevOps             | $4,800 - $15,000   |\n| Desenvolvedor Backend            | $4,800 - $14,400   |\n| Especialista em Segurança        | $2,000 - $8,000    |\n| Gerente de Projeto               | $2,400 - $6,000    |\n| **Custo Total Estimado**         | **$18,000 - $59,400** |\n\n### **Considerações Adicionais**\n\n- **Sobrecarga e Contingências**: Geralmente, é recomendado adicionar 10-20% ao orçamento total para cobrir eventuais imprevistos ou mudanças no escopo do projeto.\n- **Opções de Redução de Custo**: Se o orçamento for um problema, pode-se considerar freelancers ou empresas de desenvolvimento em regiões com taxas mais baixas. Alternativamente, parte do trabalho pode ser automatizada ou simplificada, reduzindo o tempo de desenvolvimento.\n\n### **Custo Total Estimado (Desenvolvimento + Operação)**\n\n- **Desenvolvimento**: $18,000 - $59,400\n- **Operação Mensal**: $131,77 (como estimado anteriormente)\n- **Primeiro Mês (Desenvolvimento + Operação)**: $18,131.77 - $59,531.77\n\nEssa estimativa deve fornecer uma visão abrangente dos custos envolvidos na criação e operação dessa arquitetura AWS, permitindo um planejamento financeiro mais preciso.\n\n### **Requisitos do Projeto: Arquitetura AWS para Acesso a S3 a Partir de EC2 em Rede Privada**\n\n#### **1. Objetivo do Projeto**\nO objetivo deste projeto é configurar uma arquitetura AWS onde uma instância EC2 privada acessa buckets S3 em uma rede privada usando um Gateway Endpoint. Para facilitar a gestão e o monitoramento, também será criada uma instância EC2 pública, que atuará como um bastion host, permitindo o acesso seguro à instância privada. A arquitetura será implementada dentro de uma VPC, com subredes públicas e privadas devidamente configuradas para garantir segurança e eficiência.\n\n#### **2. Requisitos do Projeto**\n\n**2.1. Infraestrutura de Rede (VPC)**\n- **VPC**: Criar uma Virtual Private Cloud (VPC) que incluirá subredes públicas e privadas.\n- **Subrede Pública**:\n  - Deve conter uma instância EC2 pública que atuará como um bastion host, permitindo o acesso SSH à instância privada na subrede privada.\n  - Deve ser conectada a um Internet Gateway para permitir o tráfego de entrada e saída da internet.\n- **Subrede Privada**:\n  - Deve conter uma instância EC2 privada, responsável por a",
        "Bootcamp - Cloud para dados/Aula_06/README.md - Parte (2/2)\ncessar e processar dados nos buckets S3.\n  - Não deve ter acesso direto à internet, garantindo que todo o tráfego externo seja gerenciado através da instância pública.\n- **Gateway Endpoint**:\n  - Criar um Gateway Endpoint para S3, associado à tabela de rotas da subrede privada, permitindo que a instância EC2 privada acesse os buckets S3 sem passar pela internet.\n\n**2.2. EC2 Instances**\n- **Instância EC2 Pública (Bastion Host)**:\n  - Deve ser configurada na subrede pública para permitir o acesso SSH à instância EC2 privada.\n  - Esta instância terá acesso à internet para downloads de pacotes e outras operações de manutenção.\n- **Instância EC2 Privada**:\n  - Localizada na subrede privada, será responsável por processar os dados nos buckets S3.\n  - Deve ser configurada para acessar os buckets S3 via o Gateway Endpoint.\n  - O acesso SSH à instância privada deve ser feito exclusivamente através do bastion host, garantindo que a instância permaneça isolada da internet.\n\n**2.3. S3 Buckets**\n- **Buckets S3**:\n  - Criar e configurar os buckets S3 necessários para o projeto.\n  - Garantir que o acesso a esses buckets seja feito exclusivamente via o Gateway Endpoint a partir da instância privada, mantendo os dados seguros dentro da VPC.\n\n**2.4. IAM Roles e Políticas**\n- **IAM Role para Instância EC2 Privada**:\n  - Criar uma IAM Role que conceda à instância EC2 privada as permissões necessárias para acessar os buckets S3 via o Gateway Endpoint.\n  - Esta role deve ser restrita apenas às ações necessárias, seguindo o princípio do menor privilégio.\n- **IAM User com Permissões Restritas**:\n  - Criar um usuário IAM com permissões específicas para gerenciar os recursos do projeto (EC2, VPC, S3).\n  - Este usuário deve ter permissões limitadas ao escopo do projeto e não deve ser capaz de criar outros usuários IAM ou modificar configurações globais de segurança.\n\n#### **3. Escopo do Projeto**\nO escopo do projeto inclui a configuração de uma VPC com subredes públicas e privadas, a criação de uma instância EC2 pública (bastion host) e uma instância EC2 privada, a configuração de um Gateway Endpoint para S3, e a implementação de políticas de segurança que garantam acesso seguro e restrito aos recursos AWS envolvidos.\n\n**Incluído no Escopo**:\n- Configuração da VPC com subredes públicas e privadas.\n- Criação e configuração de uma instância EC2 pública (bastion host) e uma instância EC2 privada.\n- Configuração de um Gateway Endpoint para S3.\n- Criação e configuração de buckets S3 com políticas de acesso apropriadas.\n- Configuração de IAM Roles e políticas para acesso seguro e restrito aos recursos.\n\n**Excluído do Escopo**:\n- Gerenciamento de outras infraestruturas não relacionadas ao projeto.\n- Configuração de segurança e políticas IAM globais não especificadas no escopo do projeto.\n\n#### **4. Entregáveis do Projeto**\n\n**4.1. Documentação de Configuração**\n- Instruções detalhadas sobre como criar e configurar cada componente da arquitetura (VPC, subredes, EC2, IAM Role, Gateway Endpoint, S3 buckets, etc.).\n- Especificações das políticas IAM personalizadas utilizadas no projeto.\n\n**4.2. Instâncias EC2 Configuradas**\n- Instância EC2 pública (bastion host) e instância EC2 privada operacionais e configuradas corretamente.\n\n**4.3. Buckets S3 Configurados**\n- Buckets S3 configurados com as políticas de acesso corretas, garantido que o acesso seja feito via Gateway Endpoint.\n\n**4.4. Usuário IAM com Permissões Restritas**\n- Criação do usuário IAM com a política personalizada, garantindo que ele possa gerenciar os recursos deste projeto de forma segura e controlada.\n\n**4.5. VPC Configurada**\n- VPC configurada com subredes públicas e privadas, e um Gateway Endpoint para S3 configurado corretamente.\n\nCom base nesse documento, o usuário deve ser capaz de criar e configurar toda a infraestrutura necessária para o projeto de transferência segura de dados entre buckets S3 na AWS utilizando uma instância EC2 privada e um bastion host.\n\nAqui está o tutorial revisado, incluindo todos os passos mencionados, para configurar um VPC Endpoint na AWS e acessar o S3 de forma segura:\n\nAqui está a versão revisada do tutorial com sugestões para nomes de recursos mais adequados para um ambiente corporativo:\n\n---\n\n**Tutorial Completo: Configuração de VPC Endpoint na AWS**\n\n**0) Criar um Usuário IAM com Permissão PowerUser**  \nAntes de iniciar, crie um usuário IAM com a permissão PowerUser para gerenciar todos os recursos na AWS de forma segura. Nomeie o usuário de forma clara, como `project-admin-user`.\n\n**1) Criar uma VPC**  \nNo console da AWS, vá para \"VPC\".  \nClique em \"Create VPC\".  \nEscolha a opção \"VPC Only\".  \nNomeie a VPC como `corp-vpc` e defina o range CIDR como `10.0.0.0/16`.  \nClique em \"Create VPC\".\n\n**2) Criar Subnets**  \nNo painel da VPC, vá para \"Subnets\".  \nClique em \"Create Subnet\" e selecione a VPC criada (`corp-vpc`).  \nCrie duas sub-redes:  \n- **Subrede Pública**: Nome: `public-subnet-az1`, Zona de Disponibilidade: `eu-west-1a`, CIDR: `10.0.1.0/24`.  \n- **Subrede Privada**: Nome: `private-subnet-az1`, Zona de Disponibilidade: `eu-west-1a`, CIDR: `10.0.2.0/24`.\n\n**3) Criar um Internet Gateway**  \nNo painel da VPC, vá para \"Internet Gateways\".  \nClique em \"Create Internet Gateway\" e nomeie como `corp-igw`.  \nSelecione o Internet Gateway criado e clique em \"Attach to VPC\". Selecione a VPC (`corp-vpc`).\n\n**4) Criar Tabelas de Rotas**  \nVá para \"Route Tables\" e clique em \"Create Route Table\".  \nCrie duas tabelas de rotas:  \n- **Tabela de Rota Pública**: Nome: `public-route-table`, associada à VPC `corp-vpc`.  \n- **Tabela de Rota Privada**: Nome: `private-route-table`, associada à VPC `corp-vpc`.\n\n**5) Associar a Tabela de Rota Pública à Subrede Pública**  \nSelecione a tabela de rota pública (`public-route-table`).  \nVá para \"Subnet Associations\" e associe-a à `public-subnet-az1`.\n\n**6) Editar Rotas da Tabela Pública para o Internet Gateway**  \nNa tabela de rota pública, vá para \"Routes\" e clique em \"Edit Routes\".  \nAdicione uma rota com destino `0.0.0.0/0` e selecione o Internet Gateway `corp-igw`.\n\n**7) Associar a Tabela de Rota Privada à Subrede Privada**  \nSelecione a tabela de rota privada (`private-route-table`).  \nVá para \"Subnet Associations\" e associe-a à `private-subnet-az1`.\n\n**8) Configurar Instância EC2 Pública**  \nVá para \"EC2\" e clique em \"Launch Instance\".  \nNomeie como `public-ec2-instance`, selecione Amazon Linux e o tipo `t2.micro`.  \nEscolha a sub-rede pública (`public-subnet-az1`) e habilite o IP público.  \nCrie um par de chaves (`corp-key-pair`) para SSH e salve.  \nEdite as configurações de segurança para liberar o acesso SSH.  \nAcesse a instância pública via terminal com o comando SSH usando o arquivo `.pem`.\n\n**9) Configurar Instância EC2 Privada**  \nClique em \"Launch Instance\" novamente.  \nNomeie como `private-ec2-instance`, selecione Amazon Linux e o tipo `t2.micro`.  \nEscolha a sub-rede privada (`private-subnet-az1`) e desabilite o IP público.  \nUse a mesma chave privada gerada anteriormente (`corp-key-pair`).  \nEdite as configurações de segurança para permitir o acesso SSH da instância pública.  \nNa instância pública, use o comando `vi corp-key.pem` para copiar a chave privada e acessar a instância privada via SSH.\n\n**10) Criar um Bucket S3**  \nNo console AWS, vá para \"S3\" e crie um bucket, nomeando-o como `corp-data-bucket`.  \nConfigure as credenciais AWS na instância privada com:  \n`aws configure`  \nExecute o comando `aws s3 ls` para listar os buckets.\n\n**11) Criar um VPC Endpoint**  \nNo painel da VPC, vá para \"Endpoints\".  \nClique em \"Create Endpoint\" e escolha o tipo Gateway.  \nSelecione o serviço `com.amazonaws.eu-west-1.s3` e escolha a VPC (`corp-vpc`).  \nAssocie o endpoint à tabela de rota privada (`private-route-table`).\n\n**12) Associar o VPC Endpoint à Subrede Privada**  \nGaranta que o VPC Endpoint está associado corretamente com a sub-rede privada (`private-subnet-az1`).\n\n**13) Testar Operações no S3**  \nNa instância privada, teste a conectividade com o S3:  \nPara inserir um arquivo:  \n`aws s3 cp arquivo.txt s3://corp-data-bucket/`  \nPara deletar um arquivo:  \n`aws s3 rm s3://corp-data-bucket/arquivo.txt`\n\n**Conclusão**  \nSeguindo estes passos, você configurou um ambiente seguro na AWS onde as instâncias em sub-rede privada acessam o S3 sem sair do ambiente da AWS, utilizando o VPC Endpoint. \n\n### Diagrama em Mermaid\n\nAqui está o diagrama Mermaid da solução de configuração da VPC com VPC Endpoint para acesso ao S3:\n\n```mermaid\ngraph TD\n    A[Usuário IAM com Permissão PowerUser] --> B[VPC - corp-vpc]\n    B --> C[Subrede Pública - public-subnet-az1]\n    B --> D[Subrede Privada - private-subnet-az1]\n    B --> E[Internet Gateway - corp-igw]\n    C --> F[Tabela de Rota Pública - public-route-table]\n    D --> G[Tabela de Rota Privada - private-route-table]\n    F --> E\n    H[EC2 Instância Pública - public-ec2-instance] --> C\n    I[EC2 Instância Privada - private-ec2-instance] --> D\n    J[VPC Endpoint - S3 Gateway] --> G\n    G --> D\n    I --> |Acesso SSH| H\n    K[S3 Bucket - corp-data-bucket] --> J\n    I --> |Acesso ao S3| K\n```\n\n### Checklist de Desenvolvimento\n\n#### Pré-Configuração\n- [ ] Criar um usuário IAM com permissão PowerUser (`project-admin-user`).\n\n#### Configuração da VPC e Subnets\n- [ ] Criar uma VPC (`corp-vpc`) com o range CIDR `10.0.0.0/16`.\n- [ ] Criar Subrede Pública (`public-subnet-az1`) com CIDR `10.0.1.0/24`.\n- [ ] Criar Subrede Privada (`private-subnet-az1`) com CIDR `10.0.2.0/24`.\n\n#### Configuração de Internet Gateway e Rotas\n- [ ] Criar um Internet Gateway (`corp-igw`) e associá-lo à VPC (`corp-vpc`).\n- [ ] Criar Tabela de Rota Pública (`public-route-table`) e associar à sub-rede pública (`public-subnet-az1`).\n- [ ] Criar Tabela de Rota Privada (`private-route-table`) e associar à sub-rede privada (`private-subnet-az1`).\n- [ ] Editar rotas da Tabela de Rota Pública para direcionar o tráfego `0.0.0.0/0` para o Internet Gateway (`corp-igw`).\n\n#### Configuração das Instâncias EC2\n- [ ] Configurar EC2 Instância Pública (`public-ec2-instance`) na sub-rede pública com IP público e criar um par de chaves (`corp-key-pair`).\n- [ ] Configurar EC2 Instância Privada (`private-ec2-instance`) na sub-rede privada sem IP público, usando o mesmo par de chaves.\n\n#### Configuração do S3 e VPC Endpoint\n- [ ] Criar um bucket S3 (`corp-data-bucket`).\n- [ ] Criar um VPC Endpoint do tipo Gateway para o S3 (`com.amazonaws.eu-west-1.s3`) e associar à tabela de rota privada (`private-route-table`).\n- [ ] Testar a conectividade da instância privada com o S3, incluindo operações de copiar e remover arquivos.\n\n#### Teste e Validação\n- [ ] Verificar acesso SSH entre a instância pública e privada.\n- [ ] Confirmar que a instância privada consegue acessar o S3 através do VPC Endpoint.\n\nEsse checklist cobre todos os passos do desenvolvimento e validação da configuração descrita, assegurando que o ambiente esteja seguro e funcional conforme as especificações.\n\n",
        "Bootcamp - Cloud para dados/Aula_07/README.md\n\n# **Bootcamp Cloud: Aula 07: Gerenciando Bancos de Dados na Amazon com RDS**\n\n**Objetivo**: Nesta aula, vamos explorar o Amazon RDS (Relational Database Service), entender suas principais funcionalidades, tipos de bancos de dados suportados, e aprender a configurar, gerenciar e otimizar bancos de dados na AWS.\n\n### **1. O Que é o Amazon RDS?**\n\n- **Definição**: O Amazon RDS é um serviço gerenciado de banco de dados relacional na nuvem que facilita a configuração, operação e escalabilidade de bancos de dados relacionais. Ele oferece suporte para diversos mecanismos de banco de dados, como MySQL, PostgreSQL, MariaDB, Oracle, SQL Server e Amazon Aurora.\n\n- **Benefícios**:\n  - **Automação de Tarefas**: Automatiza tarefas como backups, atualizações de software, e recuperação de falhas.\n  - **Escalabilidade**: Permite ajustar a capacidade de acordo com a demanda de maneira rápida e fácil, sem precisar de grandes investimentos em hardware.\n  - **Segurança**: Suporte para encriptação de dados em repouso e em trânsito, além de controles de acesso granulares.\n  - **Alta Disponibilidade**: Oferece replicação multi-AZ (Availability Zones) para garantir alta disponibilidade e durabilidade.\n\n### **2. Principais Funcionalidades do Amazon RDS**\n\n#### **Multi-AZ Deployment**\n\n- **O que é?**: Uma configuração que permite implantar instâncias de banco de dados em múltiplas zonas de disponibilidade, proporcionando alta disponibilidade e tolerância a falhas.\n- **Benefícios**:\n  - **Failover Automático**: Se a instância primária falhar, o RDS promove automaticamente uma réplica para minimizar o tempo de inatividade.\n  - **Melhor Desempenho de Leitura e Escrita**: Separação de leituras e escritas entre instâncias primárias e réplicas.\n\n#### **Read Replicas**\n\n- **O que são?**: Réplicas de leitura permitem replicar dados de uma instância RDS para melhorar a performance de leitura.\n- **Uso Comum**:\n  - **Escalabilidade de Leitura**: Descarrega o tráfego de leitura da instância primária.\n  - **Disaster Recovery**: Pode ser usada para recuperar dados rapidamente em caso de falhas.\n\n#### **Backup e Restore Automático**\n\n- **Backup Automático**: Realiza backups automáticos dos dados e dos logs de transações.\n- **Snapshots Manuais**: Permitem capturar o estado do banco de dados a qualquer momento.\n- **Restore Point-in-Time**: Restaurar o banco de dados para um ponto específico no tempo com precisão.\n\n#### **Segurança no Amazon RDS**\n\n- **Encryption**: Criptografa dados em repouso e em trânsito, utilizando chaves gerenciadas pelo AWS KMS (Key Management Service).\n- **Controle de Acesso**: Usando VPC Security Groups, IAM Roles e políticas para restringir o acesso ao banco de dados.\n- **Monitoramento**: CloudWatch e Event Subscriptions para monitorar métricas e receber alertas sobre o status do banco de dados.\n\n### **3. Configurando o Amazon RDS: Passo a Passo**\n\n#### **1. Criando uma Instância de Banco de Dados no RDS**\n\n1. **Acessar o Console do RDS**:\n   - Faça login no console da AWS e navegue até o serviço **RDS (Relational Database Service)**.\n   - Clique em **\"Create Database\"** para iniciar o processo de configuração.\n\n2. **Escolher o Motor de Banco de Dados**:\n   - Selecione o mecanismo de banco de dados desejado (MySQL, PostgreSQL, etc.).\n   - Escolha a versão do mecanismo com base nas necessidades de compatibilidade e recursos.\n\n3. **Configuração da Instância**:\n   - **Tipo de Instância**: Escolha o tipo de instância que atende à sua carga de trabalho (db.t3.micro para testes ou db.m5.large para produção).\n   - **Multi-AZ Deployment**: Ative se desejar alta disponibilidade.\n\n4. **Configuração de Armazenamento**:\n   - **Tipo de Armazenamento**: Escolha entre armazenamento SSD padrão, provisionado ou magnético.\n   - **Auto Scaling de Armazenamento**: Ative para permitir que o RDS ajuste automaticamente o armazenamento conforme necessário.\n\n5. **Configuração de Conectividade**:\n   - **VPC**: Selecione a VPC onde a instância será criada.\n   - **Sub-redes**: Escolha as sub-redes dentro da VPC (pública ou privada).\n   - **Grupos de Segurança**: Configure Security Groups para controlar o acesso de entrada e saída.\n\n6. **Configuração de Autenticação e Backup**:\n   - **Autenticação IAM**: Opcional, para usar credenciais IAM para conectar ao banco de dados.\n   - **Backup Automático**: Configure a retenção de backups automáticos e os horários de backup.\n\n7. **Criação da Instância**:\n   - Clique em **\"Create Database\"** para finalizar a criação da instância.\n\n#### **2. Gerenciando Backups e Restaurações**\n\n- **Configurar Backups Automáticos**:\n  - Defina o período de retenção dos backups e o horário preferido para realização.\n- **Snapshots Manuais**:\n  - Faça snapshots manuais antes de realizar mudanças críticas no banco de dados.\n- **Restauração**:\n  - Utilize a funcionalidade de **Restore to Point in Time** para restaurar o banco a um estado anterior.\n\n#### **3. Monitoramento e Escalabilidade**\n\n- **CloudWatch Metrics**:\n  - Monitore o uso de CPU, memória, IOPS, e outras métricas críticas através do CloudWatch.\n- **Auto Scaling**:\n  - Configure para ajustar automaticamente a capacidade da instância com base na demanda.\n\n### **4. Melhores Práticas ao Utilizar o Amazon RDS**\n\n- **Isolamento de Sub-redes Privadas**: Sempre que possível, implante bancos de dados em sub-redes privadas para evitar a exposição direta à internet.\n- **Regras de Segurança e Autenticação**: Use Security Groups restritivos e a autenticação IAM para aumentar a segurança.\n- **Multi-AZ e Read Replicas**: Utilize Multi-AZ para alta disponibilidade e Read Replicas para escalar leituras.\n- **Testes de Backup e Restauração**: Realize testes periódicos de restauração para garantir que seus backups funcionam conforme o esperado.\n- **Monitoramento e Alertas**: Configure alertas para monitorar o desempenho e a integridade do banco de dados.\n\n### **5. Projeto Prático: Criando e Gerenciando um Banco de Dados com Amazon RDS**\n\n**Objetivo do Projeto**: Criar uma instância RDS para um ambiente de produção, configurando backups, segurança e escalabilidade.\n\n**Passo a Passo Resumido**:\n\n1. **Criar a Instância RDS**:\n   - Escolha o tipo de banco de dados e configure conforme as necessidades da aplicação.\n2. **Configurar Multi-AZ e Read Replicas**:\n   - Implemente uma configuração Multi-AZ para alta disponibilidade e uma Read Replica para melhorar o desempenho de leitura.\n3. **Aplicar Regras de Segurança**:\n   - Use Security Groups para restringir o acesso ao banco de dados, garantindo que apenas instâncias autorizadas possam conectar-se.\n4. **Monitorar e Ajustar a Instância**:\n   - Utilize o CloudWatch para monitorar métricas de desempenho e ajuste a instância conforme necessário.\n5. **Testar Backup e Restauração**:\n   - Realize testes para garantir que o processo de backup e restauração está funcionando corretamente.\n\nEssa aula oferece uma visão abrangente sobre o gerenciamento de bancos de dados na AWS com o Amazon RDS, capacitando você a configurar, proteger e escalar seus bancos de dados na nuvem.\n\n### **6. Criando uma VPC do Zero e Configurando o RDS em uma Rede Pública**\n\nPara configurar o Amazon RDS em uma rede pública, vamos criar uma VPC do zero, configurar suas sub-redes, habilitar DNS, e então criar uma instância de banco de dados RDS. Essa configuração é especialmente útil para fins de desenvolvimento ou teste, onde o acesso público ao banco de dados é necessário. \n\n#### **Passo a Passo para Criar uma VPC com RDS**\n\n**1. Criando a VPC e Sub-rede Pública**\n\n1. **Acessar o Console VPC**:\n   - Acesse o console da AWS e navegue até o serviço **VPC**.\n   - Clique em **\"Create VPC\"**.\n\n2. **Configurar a VPC**:\n   - **Nome**: Dê um nome à sua VPC, por exemplo, \"MinhaVPC-RDS\".\n   - **CIDR Block**: Defina um bloco CIDR, como \"10.0.0.0/16\", para o intervalo de endereços IP.\n   - **Habilitar DNS**: Marque as opções para habilitar DNS hostnames e DNS resolution para garantir que as instâncias dentro da VPC possam resolver nomes de DNS.\n   - Clique em **\"Create VPC\"** para finalizar a criação.\n\n3. **Criar uma Sub-rede Pública**:\n   - No painel VPC, selecione **\"Subnets\"** e clique em **\"Create Subnet\"**.\n   - **Nome**: \"MinhaSubnetPublica\".\n   - **VPC**: Selecione a VPC recém-criada (\"MinhaVPC-RDS\").\n   - **CIDR Block**: Defina um bloco CIDR, como \"10.0.1.0/24\".\n   - **Availability Zone**: Escolha uma zona de disponibilidade para a sub-rede.\n   - Clique em **\"Create Subnet\"**.\n\n4. **Configurar o Gateway de Internet**:\n   - No painel VPC, clique em **\"Internet Gateways\"** e depois em **\"Create Internet Gateway\"**.\n   - **Nome**: \"MeuInternetGateway\".\n   - Clique em **\"Create\"**, depois associe o gateway à VPC (\"MinhaVPC-RDS\") clicando em **\"Attach to VPC\"**.\n\n5. **Configurar a Tabela de Roteamento**:\n   - Vá para **\"Route Tables\"** e selecione a tabela associada à sua VPC.\n   - **Adicionar Rota**: Clique em **\"Edit Routes\"**, adicione uma rota para \"0.0.0.0/0\" e selecione o Internet Gateway criado anteriormente.\n   - **Associar à Sub-rede Pública**: Vá para **\"Subnet Associations\"** e associe a tabela à \"MinhaSubnetPublica\".\n\n**2. Criando o RDS na Rede Pública**\n\n1. **Acessar o Console do RDS**:\n   - Navegue para o serviço **RDS** no console da AWS e clique em **\"Create Database\"**.\n\n2. **Escolher o Motor de Banco de Dados**:\n   - Selecione o mecanismo **MySQL** ou outro que desejar.\n   - Escolha a versão mais recente que suporte o **Free Tier**.\n\n3. **Configurações da Instância**:\n   - **Tipo de Instância**: Escolha **t4g.micro** (Free Tier).\n   - **Multi-AZ Deployment**: Desative para um ambiente de teste ou desenvolvimento.\n   - **Master Username**: Defina o usuário mestre, por exemplo, \"admin\".\n   - **Master Password**: Crie uma senha segura para o administrador.\n\n4. **Configuração de Conectividade**:\n   - **VPC**: Selecione \"MinhaVPC-RDS\".\n   - **Sub-rede Pública**: Escolha a sub-rede pública criada anteriormente.\n   - **Public Access**: Ative para permitir que o RDS seja acessado publicamente (para ambientes de teste).\n   - **Security Group**: Configure para permitir acesso ao banco (por exemplo, na porta 3306 para MySQL).\n\n5. **Configuração de Backup**:\n   - **Backup Retention Period**: Defina o período de retenção de backups automáticos, que pode variar de 1 a 35 dias.\n   - **Backup Window**: Escolha um horário de manutenção para os backups automáticos; isso deve ser durante um período de menor carga.\n\n6. **Criar o Banco de Dados**:\n   - Revise as configurações e clique em **\"Create Database\"**. O processo de criação pode levar alguns minutos.\n\n**3. Detalhes de Criação e Configuração do Amazon RDS**\n\n- **Tipos de Armazenamento**:\n  - **General Purpose SSD (gp2)**: Para a maioria dos usos comuns, com desempenho balanceado.\n  - **Provisioned IOPS (io1)**: Para cargas de trabalho de I/O intensivo, como grandes aplicações OLTP.\n  - **Magnetic Storage**: Armazenamento mais econômico, mas menos performático.\n\n- **Opções de Backup**:\n  - **Backup Automático**: Realiza backups completos diariamente e backups incrementais dos logs de transações.\n  - **Snapshots Manuais**: Criação manual de snapshots para capturar o estado atual do banco de dados.\n  - **Point-in-Time Restore**: Permite restaurar o banco de dados para um ponto específico no tempo.\n\n- **Segurança**:\n  - **IAM Authentication**: Autenticação de usuários via IAM, para controle mais granular.\n  - **VPC Security Groups**: Controle de acesso baseado em IPs e portas, para proteger o banco de dados.\n\n- **Janela de Manutenção**:\n  - **Configuração de Manutenção**: Escolha um período de manutenção para realizar updates de software que afetam a instância, garantindo o mínimo de interrupções possíveis.\n\n### **Conclusão**\n\nEssa abordagem de criar uma VPC pública para configurar um RDS permite que você gerencie e acesse seu banco de dados com facilidade, mantendo um nível básico de segurança e controle. Para ambientes de produção, recomenda-se uma configuração mais restritiva, como o uso de sub-redes privadas e a desativação do acesso público direto ao RDS.\n\nEntendido! Vamos configurar o WordPress na instância EC2 usando o AWS Systems Manager Session Manager (AWS Connect) para acesso, sem a necessidade de chaves PEM. Segue o passo a passo completo:\n\n### **Passo a Passo Completo: Instalando o WordPress em uma Instância EC2 Usando Amazon RDS**\n\n#### **1. Preparando o Ambiente na AWS**\n\n**1.1 Criar a VPC e Configurar Rede**\n\n1. **Acessar o Console VPC**:\n   - No console da AWS, navegue até **VPC** e clique em **\"Create VPC\"**.\n\n2. **Configurar a VPC**:\n   - **Nome**: \"MinhaVPC-WordPress\".\n   - **CIDR Block**: \"10.0.0.0/16\".\n   - Habilitar DNS hostnames e DNS resolution.\n   - Clique em **\"Create VPC\"**.\n\n3. **Criar Sub-rede Pública**:\n   - No painel VPC, selecione **\"Subnets\"** e clique em **\"Create Subnet\"**.\n   - **Nome**: \"MinhaSubnetPublica\".\n   - **VPC**: Selecione \"MinhaVPC-WordPress\".\n   - **CIDR Block**: \"10.0.1.0/24\".\n   - Escolha uma zona de disponibilidade (ex: us-east-1a).\n   - Clique em **\"Create Subnet\"**.\n\n4. **Configurar o Gateway de Internet**:\n   - No painel VPC, clique em **\"Internet Gateways\"** e em **\"Create Internet Gateway\"**.\n   - **Nome**: \"MeuInternetGateway\".\n   - Clique em **\"Create\"**, depois associe o gateway à VPC criada clicando em **\"Attach to VPC\"**.\n\n5. **Configurar a Tabela de Roteamento**:\n   - Vá para **\"Route Tables\"**, selecione a tabela associada à VPC.\n   - Clique em **\"Edit Routes\"** > **\"Add route\"**, adicione \"0.0.0.0/0\" e selecione o Internet Gateway.\n   - Associe a tabela à sub-rede pública criada.\n\n#### **2. Criar a Instância do Banco de Dados com Amazon RDS**\n\n1. **Acessar o Console do RDS**:\n   - Navegue até **RDS** e clique em **\"Create Database\"**.\n\n2. **Configurar o Banco de Dados**:\n   - **Engine**: Selecione **MySQL**.\n   - **Version**: Escolha uma versão compatível com WordPress.\n   - **Template**: Escolha **Free Tier** se aplicável.\n   - **DB Instance Class**: Selecione **db.t4g.micro** (Free Tier).\n   - **Multi-AZ**: Desative para ambientes de teste.\n\n3. **Configurações de Identificação e Autenticação**:\n   - **DB Instance Identifier**: Nome da instância, ex: \"WordPressDB\".\n   - **Master Username**: \"admin\".\n   - **Master Password**: Defina uma senha segura.\n\n4. **Configurações de Conectividade**:\n   - **VPC**: Selecione \"MinhaVPC-WordPress\".\n   - **Subnets**: Escolha a sub-rede pública.\n   - **Public Access**: Ative para permitir conexão da instância EC2.\n   - **Security Group**: Crie ou selecione um grupo que permita acesso na porta 3306 (MySQL).\n\n5. **Backup e Manutenção**:\n   - Configure backups automáticos e defina a janela de manutenção para horários de baixa demanda.\n\n6. **Criar o Banco de Dados**:\n   - Revise as configurações e clique em **\"Create Database\"**.\n\n",
        "Bootcamp - Cloud para dados/Aula_08/README.md\n\n# Bootcamp Cloud: Aula 08 - Integração entre EC2 e RDS para Processamento de Requisições de API e Armazenamento de Dados\n\n**Objetivo**: Nesta aula, vamos aprender a configurar uma instância EC2 para rodar uma aplicação de API utilizando Docker e integrar essa aplicação com um banco de dados Amazon RDS. Vamos explorar as vantagens dessa arquitetura, as melhores práticas de segurança, e como configurar os recursos necessários na AWS.\n\n### **1. Introdução à Integração EC2 e RDS**\n\nA combinação de instâncias EC2 com o Amazon RDS é amplamente utilizada para construir aplicações robustas que precisam processar requisições de APIs e armazenar dados em um banco de dados relacional gerenciado. EC2 oferece flexibilidade e controle sobre a execução do código da aplicação, enquanto o RDS cuida da gestão do banco de dados, garantindo alta disponibilidade, backups automatizados e segurança.\n\n### **2. Benefícios da Integração entre EC2 e RDS**\n\n- **Escalabilidade e Flexibilidade**: A instância EC2 permite escalabilidade horizontal da aplicação, enquanto o RDS escala automaticamente o banco de dados de acordo com a demanda.\n- **Gerenciamento Simplificado**: O RDS cuida de atualizações, backups, e manutenção do banco de dados, liberando tempo para focar no desenvolvimento da aplicação.\n- **Segurança Avançada**: Com o uso de Security Groups, podemos restringir o acesso ao banco de dados, garantindo que apenas a instância EC2 tenha permissão para se conectar.\n- **Alto Desempenho**: O uso combinado de EC2 e RDS permite que aplicações sejam otimizadas para lidar com grandes volumes de requisições e operações de banco de dados.\n\n### **3. Passo a Passo para Configuração da Instância EC2**\n\n#### **3.1. Configurando a Instância EC2**\n\n1. **Acessar o Console EC2**:\n   - No console da AWS, navegue até **EC2** e clique em **\"Launch Instance\"**.\n\n2. **Escolher a AMI (Amazon Machine Image)**:\n   - Selecione uma AMI com Ubuntu Server (última versão LTS).\n\n3. **Escolher o Tipo de Instância**:\n   - Escolha um tipo de instância que atenda às necessidades do projeto (ex.: t2.micro para desenvolvimento ou t3.medium para produção).\n\n4. **Configuração de Rede**:\n   - Selecione a VPC e Sub-rede desejadas.\n   - Configure o Security Group permitindo acesso necessário, como SSH na porta 22 para administração e outras portas conforme a aplicação.\n\n5. **Configuração de Armazenamento**:\n   - Configure o armazenamento de acordo com o necessário para a aplicação (ex.: 30GB SSD).\n\n6. **Launch**:\n   - Revise as configurações e clique em **\"Launch\"**. Escolha um par de chaves para acesso SSH ou crie um novo.\n\n#### **3.2. Configurando a Aplicação na Instância EC2**\n\n1. **Acessar a Instância via SSH**:\n   - Conecte-se à instância EC2 usando o terminal:\n     ```bash\n     ssh -i \"caminho/para/sua-chave.pem\" ubuntu@seu-endereco-ec2\n     ```\n\n2. **Atualizar Pacotes e Instalar Docker e Git**:\n   - Execute os seguintes comandos:\n     ```bash\n     sudo su\n     sudo apt-get update\n     sudo apt install -y git docker.io docker-compose\n     ```\n\n3. **Clonar o Repositório do Projeto**:\n   - Clone o repositório do projeto:\n     ```bash\n     git clone https://github.com/lvgalvao/api-scheduler-python-rds.git /app\n     ```\n\n4. **Construir e Executar o Contêiner Docker**:\n   - Navegue para o diretório do projeto e construa a imagem Docker:\n     ```bash\n     cd /app\n     sudo docker build -t api-schedule-app .\n     ```\n   - Execute o contêiner com as variáveis de ambiente para integração com o RDS:\n     ```bash\n     sudo docker run -d \\\n     --name api-schedule-app-container \\\n     -e DB_HOST<endereco-rds> \\\n     -e DB_USER<usuario> \\\n     -e DB_PASS<senha> \\\n     -e DB_NAME<nome-do-banco> \\\n     api-schedule-app\n     ```\n\n### **4. Configuração do Banco de Dados Amazon RDS**\n\n#### **4.1. Criando uma Instância RDS**\n\n1. **Acessar o Console do RDS**:\n   - Navegue até o serviço **RDS** no console da AWS e clique em **\"Create Database\"**.\n\n2. **Escolher o Mecanismo do Banco de Dados**:\n   - Selecione o banco de dados desejado (ex.: PostgreSQL, MySQL).\n   - Escolha a versão de acordo com os requisitos do projeto.\n\n3. **Configurações da Instância**:\n   - **DB Instance Class**: Escolha uma classe de instância compatível com a carga esperada (ex.: db.t3.micro para teste).\n   - **Multi-AZ Deployment**: Ative se precisar de alta disponibilidade.\n\n4. **Configurações de Conectividade**:\n   - **VPC**: Selecione a mesma VPC da instância EC2.\n   - **Sub-redes**: Escolha sub-redes privadas para o banco de dados.\n   - **Public Access**: Desative para um ambiente de produção, habilite apenas para desenvolvimento com restrições de segurança.\n\n5. **Configurações de Autenticação e Backup**:\n   - Configure backups automáticos e defina as políticas de retenção e manutenção.\n\n6. **Criação da Instância**:\n   - Clique em **\"Create Database\"** para finalizar.\n\n### **5. Criando e Configurando Security Groups**\n\n#### **5.1. Criando um Security Group para a Instância EC2**\n\n1. **Acessar o Console EC2**:\n   - Navegue até **Security Groups** e clique em **\"Create Security Group\"**.\n\n2. **Configurar o Security Group**:\n   - **Nome**: Nomeie seu grupo (ex.: SG-EC2-API).\n   - **Descrição**: Descreva a função (ex.: Security Group para instância EC2 da API).\n   - **Regras de Entrada**:\n     - **SSH (Porta 22)**: Permita o acesso somente do seu IP (para acesso seguro).\n     - **HTTP/HTTPS**: Habilite conforme necessário para a aplicação.\n\n#### **5.2. Criando um Security Group para o Banco de Dados RDS**\n\n1. **Acessar o Console RDS**:\n   - No painel do RDS, acesse **Security Groups** e clique em **\"Create Security Group\"**.\n\n2. **Configurar o Security Group do RDS**:\n   - **Nome**: Nomeie seu grupo (ex.: SG-RDS-Database).\n   - **Regras de Entrada**:\n     - **Banco de Dados (Porta 5432 para PostgreSQL, ou outra conforme o banco)**.\n     - **Origem**: Especifique o Security Group da instância EC2 (ex.: SG-EC2-API) para garantir que apenas a instância EC2 possa se conectar ao banco.\n\n### **6. Vantagens e Oportunidades da Integração EC2 + RDS para Engenharia de Dados**\n\n1. **Desempenho e Escalabilidade**: A combinação EC2 e RDS permite processar grandes volumes de dados e escalar conforme a demanda, ideal para pipelines de dados e processamento intensivo.\n\n2. **Facilidade de Manutenção**: Com o RDS gerenciado, as tarefas complexas de manutenção do banco de dados, como backups e patches, são automatizadas, liberando os engenheiros para focar em otimizações de processamento.\n\n3. **Segurança e Controle de Acesso**: A configuração de Security Groups restritivos garante que o tráfego de dados entre a aplicação e o banco seja seguro, minimizando riscos de acesso não autorizado.\n\n4. **Implementação Ágil de Projetos de Dados**: Essa arquitetura facilita a rápida implementação de APIs que coletam, processam e armazenam dados, otimizando fluxos de trabalho de ETL (Extração, Transformação e Carga) com acesso seguro ao banco de dados.\n\n5. **Monitoramento e Otimização**: Utilizando ferramentas da AWS como CloudWatch, é possível monitorar métricas críticas de desempenho tanto da instância EC2 quanto do RDS, ajustando conforme necessário para garantir alta performance.\n\n### **Conclusão**\n\nNesta aula, aprendemos como configurar e integrar uma instância EC2 com o Amazon RDS, criando uma arquitetura robusta para aplicações que processam requisições de APIs e armazenam dados de forma eficiente e segura. Essa integração é um pilar essencial para qualquer projeto de engenharia de dados, permitindo que você construa soluções escaláveis e de alto desempenho na nuvem.\n\nBootcamp - Cloud para dados/Aula_08/Dockerfile\n\n# Usar uma imagem base do Python\nFROM python:3.12\n\n# Definir o diretório de trabalho\nWORKDIR /app\n\n# Copiar o arquivo requirements.txt para o container\nCOPY requirements.txt .\n\n# Instalar as dependências listadas no requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copiar todo o conteúdo do projeto para o diretório de trabalho do container\nCOPY . .\n\n# Comando para rodar o script fetch.py localizado na pasta src\nCMD [\"python\", \"src/fetch.py\"]\n\n\nBootcamp - Cloud para dados/Aula_08/requirements.txt\n\nannotated-types0.7.0\ncertifi2024.8.30\ncharset-normalizer3.3.2\nidna3.8\npsycopg2-binary2.9.9\npydantic2.8.2\npydantic_core2.20.1\npython-dotenv1.0.1\nrequests2.32.3\nschedule1.2.2\ntyping_extensions4.12.2\nurllib32.2.2\n\n\nBootcamp - Cloud para dados/Aula_08/src/fetch.py\n\nfrom requests import Session\nfrom requests.exceptions import ConnectionError, Timeout, TooManyRedirects\nfrom dotenv import load_dotenv\nimport schedule\nimport json\nimport os\nimport time\nimport psycopg2\nfrom psycopg2 import sql\n\n# Carregar variáveis do arquivo .env\nload_dotenv()\n\n# URL da API de Produção para obter a última cotação do Bitcoin\nurl  'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'\n\n# Parâmetros da requisição para obter a cotação do Bitcoin\nparameters  {\n    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo\n    'convert': 'USD'  # Convertendo a cotação para USD\n}\n\n# Headers com a chave da API obtida do arquivo .env\nheaders  {\n    'Accepts': 'application/json',\n    'X-CMC_PRO_API_KEY': os.getenv('CMC_API_KEY'),  # Obtendo a chave do .env\n}\n\n# Criar uma sessão\nsession  Session()\nsession.headers.update(headers)\n\n# Configuração do banco de dados RDS\nDB_HOST  os.getenv(\"DB_HOST\")\nDB_NAME  os.getenv(\"DB_NAME\")\nDB_USER  os.getenv(\"DB_USER\")\nDB_PASS  os.getenv(\"DB_PASS\")\n\n# Função para criar a tabela caso ainda não exista\ndef criar_tabela():\n    try:\n        conn  psycopg2.connect(\n            hostDB_HOST,\n            databaseDB_NAME,\n            userDB_USER,\n            passwordDB_PASS\n        )\n        cursor  conn.cursor()\n        \n        # Criação da tabela\n        create_table_query  '''\n        CREATE TABLE IF NOT EXISTS bitcoin_quotes (\n            id SERIAL PRIMARY KEY,\n            price NUMERIC,\n            volume_24h NUMERIC,\n            market_cap NUMERIC,\n            last_updated TIMESTAMP\n        );\n        '''\n        cursor.execute(create_table_query)\n        conn.commit()\n        cursor.close()\n        conn.close()\n        print(\"Tabela criada ou já existente.\")\n    except Exception as e:\n        print(f\"Erro ao criar a tabela: {e}\")\n\n# Função para salvar os dados no banco de dados\ndef salvar_no_rds(usd_quote):\n    try:\n        conn  psycopg2.connect(\n            hostDB_HOST,\n            databaseDB_NAME,\n            userDB_USER,\n            passwordDB_PASS\n        )\n        cursor  conn.cursor()\n        \n        # Inserção dos dados na tabela\n        insert_query  sql.SQL(\n            \"INSERT INTO bitcoin_quotes (price, volume_24h, market_cap, last_updated) VALUES (%s, %s, %s, %s)\"\n        )\n        cursor.execute(insert_query, (\n            usd_quote['price'],\n            usd_quote['volume_24h'],\n            usd_quote['market_cap'],\n            usd_quote['last_updated']\n        ))\n        conn.commit()\n        cursor.close()\n        conn.close()\n        print(\"Dados salvos com sucesso!\")\n    except Exception as e:\n        print(f\"Erro ao salvar dados no RDS: {e}\")\n\n# Função que faz a requisição à API e salva a última cotação do Bitcoin\ndef consultar_cotacao_bitcoin():\n    try:\n        response  session.get(url, paramsparameters)\n        data  json.loads(response.text)\n        \n        # Verificar se os dados do Bitcoin estão presentes na resposta\n        if 'data' in data and 'BTC' in data['data']:\n            bitcoin_data  data['data']['BTC']\n            usd_quote  bitcoin_data['quote']['USD']\n            \n            # Salvar os dados no banco de dados\n            salvar_no_rds(usd_quote)\n        else:\n            print(\"Erro ao obter a cotação do Bitcoin:\", data['status'].get('error_message', 'Erro desconhecido'))\n\n    except (ConnectionError, Timeout, TooManyRedirects) as e:\n        print(f\"Erro na requisição: {e}\")\n\n# Criar a tabela no banco de dados\ncriar_tabela()\n\n# Agendar a função para rodar a cada 15 segundos\nschedule.every(15).seconds.do(consultar_cotacao_bitcoin)\n\n# Loop principal para manter o agendamento ativo\nif __name__  \"__main__\":\n    print(\"Iniciando o agendamento para consultar a API a cada 15 segundos...\")\n    while True:\n        schedule.run_pending()\n        time.sleep(1)\n\n\nBootcamp - Cloud para dados/Aula_08/src/fetch_1.py\n\nimport urllib3\nimport json\nimport os\n\n# Carregar variáveis de ambiente\nurl  os.getenv('API_URL')  # URL da API obtida das variáveis de ambiente\napi_key  os.getenv('CMC_API_KEY')  # Chave da API obtida das variáveis de ambiente\n\n# Parâmetros da requisição para obter a cotação do Bitcoin\nparameters  {\n 'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo\n 'convert': 'USD'  # Convertendo a cotação para USD\n}\n\n# Headers com a chave da API\nheaders  {\n 'Accept': 'application/json',\n 'X-CMC_PRO_API_KEY': api_key  # Obtendo a chave da API das variáveis de ambiente\n}\n\n# Criar um PoolManager para gerenciar conexões\nhttp  urllib3.PoolManager()\n\n# Função Lambda\ndef lambda_handler(event, context):\n try:\n     # Converte os parâmetros para o formato de query string\n     query_string  '&'.join([f'{key}{value}' for key, value in parameters.items()])\n     full_url  f\"{url}?{query_string}\"\n     \n     # Fazendo o request GET para a API\n     response  http.request('GET', full_url, headersheaders)\n     data  json.loads(response.data.decode('utf-8'))\n     \n     # Verificar se os dados do Bitcoin estão presentes na resposta\n     if 'data' in data and 'BTC' in data['data']:\n         bitcoin_data  data['data']['BTC']\n         usd_quote  bitcoin_data['quote']['USD']\n         \n         # Log da resposta\n         print(f\"Cotação do Bitcoin obtida: {usd_quote}\")\n     else:\n         print(\"Erro ao obter a cotação do Bitcoin:\", data.get('status', {}).get('error_message', 'Erro desconhecido'))\n\n except urllib3.exceptions.HTTPError as e:\n     print(f\"Erro na requisição: {e}\")\n\n\nBootcamp - Cloud para dados/Aula_08/src/fetch_2.py\n\nfrom requests import Session\nfrom requests.exceptions import ConnectionError, Timeout, TooManyRedirects\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, ValidationError, Field\nimport json\nimport os\n\n# Carregar variáveis do arquivo .env\nload_dotenv()\n\n# URL da API de Produção para obter a última cotação do Bitcoin\nurl  'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'\n\n# Parâmetros da requisição para obter a cotação do Bitcoin\nparameters  {\n    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo\n    'convert': 'USD'  # Convertendo a cotação para USD\n}\n\n# Headers com a chave da API obtida do arquivo .env\nheaders  {\n    'Accepts': 'application/json',\n    'X-CMC_PRO_API_KEY': os.getenv('CMC_API_KEY'),  # Obtendo a chave do .env\n}\n\n# Criar uma sessão\nsession  Session()\nsession.headers.update(headers)\n\n# Modelo Pydantic para o Parsing da Resposta\nclass QuoteModel(BaseModel):\n    price: float\n    volume_24h: float  Field(alias'volume_24h')\n    market_cap: float  Field(alias'market_cap')\n    last_updated: str  Field(alias'last_updated')\n\nclass BitcoinDataModel(BaseModel):\n    symbol: str\n    quote: dict\n\n    def get_usd_quote(self) -> QuoteModel:\n        return QuoteModel(**self.quote['USD'])\n\nclass ApiResponseModel(BaseModel):\n    data: dict\n    status: dict\n\n    def get_bitcoin_data(self) -> BitcoinDataModel:\n        return BitcoinDataModel(**self.data['BTC'])\n\n# Função que faz a requisição à API e processa os dados usando Pydantic\ndef consultar_cotacao_bitcoin():\n    try:\n        response  session.get(url, paramsparameters)\n        data  json.loads(response.text)\n        \n        # Parsing da resposta usando Pydantic\n        api_response  ApiResponseModel(**data)\n        bitcoin_data  api_response.get_bitcoin_data()\n        quote  bitcoin_data.get_usd_quote()\n\n        # Imprimir os dados da cotação\n        print(f\"Última cotação do Bitcoin: ${quote.price:.2f} USD\")\n        print(f\"Volume 24h: ${quote.volume_24h:.2f} USD\")\n        print(f\"Market Cap: ${quote.market_cap:.2f} USD\")\n        print(f\"Última atualização: {quote.last_updated}\")\n\n    except (ConnectionError, Timeout, TooManyRedirects) as e:\n        print(f\"Erro na requisição: {e}\")\n    except ValidationError as e:\n        print(f\"Erro ao validar a resposta da API: {e}\")\n\n# Executa a função para consultar a cotação do Bitcoin\nconsultar_cotacao_bitcoin()\n\n\nBootcamp - Cloud para dados/Aula_08/src/fetch_3.py\n\nfrom requests import Session\nfrom requests.exceptions import ConnectionError, Timeout, TooManyRedirects\nfrom dotenv import load_dotenv\nimport schedule\nimport json\nimport os\nimport time\n\n# Carregar variáveis do arquivo .env\nload_dotenv()\n\n# URL da API de Produção para obter a última cotação do Bitcoin\nurl  'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'\n\n# Parâmetros da requisição para obter a cotação do Bitcoin\nparameters  {\n    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo\n    'convert': 'USD'  # Convertendo a cotação para USD\n}\n\n# Headers com a chave da API obtida do arquivo .env\nheaders  {\n    'Accepts': 'application/json',\n    'X-CMC_PRO_API_KEY': os.getenv('CMC_API_KEY'),  # Obtendo a chave do .env\n}\n\n# Criar uma sessão\nsession  Session()\nsession.headers.update(headers)\n\n# Função que faz a requisição à API e imprime a última cotação do Bitcoin\ndef consultar_cotacao_bitcoin():\n    try:\n        response  session.get(url, paramsparameters)\n        data  json.loads(response.text)\n        \n        # Verificar se os dados do Bitcoin estão presentes na resposta\n        if 'data' in data and 'BTC' in data['data']:\n            bitcoin_data  data['data']['BTC']\n            usd_quote  bitcoin_data['quote']['USD']\n            \n            # Imprimir os dados da cotação\n            print(f\"Última cotação do Bitcoin: ${usd_quote['price']:.2f} USD\")\n            print(f\"Volume 24h: ${usd_quote['volume_24h']:.2f} USD\")\n            print(f\"Market Cap: ${usd_quote['market_cap']:.2f} USD\")\n            print(f\"Última atualização: {usd_quote['last_updated']}\")\n        else:\n            print(\"Erro ao obter a cotação do Bitcoin:\", data['status'].get('error_message', 'Erro desconhecido'))\n\n    except (ConnectionError, Timeout, TooManyRedirects) as e:\n        print(f\"Erro na requisição: {e}\")\n\n# Agendar a função para rodar a cada 15 segundos\nschedule.every(15).seconds.do(consultar_cotacao_bitcoin)\n\n# Loop principal para manter o agendamento ativo\nif __name__  \"__main__\":\n    print(\"Iniciando o agendamento para consultar a API a cada 15 segundos...\")\n    while True:\n        schedule.run_pending()\n        time.sleep(1)\n\n\nBootcamp - Cloud para dados/Aula_08/src/fetch_4.py\n\nfrom requests import Session\nfrom requests.exceptions import ConnectionError, Timeout, TooManyRedirects\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, ValidationError, Field\nimport json\nimport os\n\n# Carregar variáveis do arquivo .env\nload_dotenv()\n\n# URL da API de Produção para obter a última cotação do Bitcoin\nurl  'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'\n\n# Parâmetros da requisição para obter a cotação do Bitcoin\nparameters  {\n    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo\n    'convert': 'USD'  # Convertendo a cotação para USD\n}\n\n# Headers com a chave da API obtida do arquivo .env\nheaders  {\n    'Accepts': 'application/json',\n    'X-CMC_PRO_API_KEY': os.getenv('CMC_API_KEY'),  # Obtendo a chave do .env\n}\n\n# Criar uma sessão\nsession  Session()\nsession.headers.update(headers)\n\n# Modelo Pydantic para o Parsing da Resposta\nclass QuoteModel(BaseModel):\n    price: float\n    volume_24h: float  Field(alias'volume_24h')\n    market_cap: float  Field(alias'market_cap')\n    last_updated: str  Field(alias'last_updated')\n\nclass BitcoinDataModel(BaseModel):\n    symbol: str\n    quote: dict\n\n    def get_usd_quote(self) -> QuoteModel:\n        return QuoteModel(**self.quote['USD'])\n\nclass ApiResponseModel(BaseModel):\n    data: dict\n    status: dict\n\n    def get_bitcoin_data(self) -> BitcoinDataModel:\n        return BitcoinDataModel(**self.data['BTC'])\n\n# Função que faz a requisição à API e processa os dados usando Pydantic\ndef consultar_cotacao_bitcoin():\n    try:\n        response  session.get(url, paramsparameters)\n        data  json.loads(response.text)\n        \n        # Parsing da resposta usando Pydantic\n        api_response  ApiResponseModel(**data)\n        bitcoin_data  api_response.get_bitcoin_data()\n        quote  bitcoin_data.get_usd_quote()\n\n        # Imprimir os dados da cotação\n        print(f\"Última cotação do Bitcoin: ${quote.price:.2f} USD\")\n        print(f\"Volume 24h: ${quote.volume_24h:.2f} USD\")\n        print(f\"Market Cap: ${quote.market_cap:.2f} USD\")\n        print(f\"Última atualização: {quote.last_updated}\")\n\n    except (ConnectionError, Timeout, TooManyRedirects) as e:\n        print(f\"Erro na requisição: {e}\")\n    except ValidationError as e:\n        print(f\"Erro ao validar a resposta da API: {e}\")\n\n# Executa a função para consultar a cotação do Bitcoin\nconsultar_cotacao_bitcoin()\n\n\n",
        "Bootcamp - Cloud para dados/Aula_09/README.md\n\n### Aula 09: AWS Lambda e Eventos na AWS\n\n**Objetivo**: Nesta aula, exploraremos o AWS Lambda e seu papel dentro de arquiteturas serverless na AWS. Vamos entender as diferenças entre o AWS Lambda e o EC2, discutir suas vantagens e desafios, e aprender como eventos podem ser aproveitados para criar sistemas escaláveis e eficientes.\n\n### 3. **Arquitetura Serverless com AWS Lambda**\n\nEste diagrama mostra uma arquitetura típica serverless, onde múltiplos serviços da AWS interagem com o Lambda para automatizar processos.\n\n```mermaid\ngraph TB\n    S3[S3] --> |Upload de Arquivo| Lambda1[AWS Lambda]\n    APIGateway[API Gateway] --> |Requisição HTTP| Lambda2[AWS Lambda]\n    DynamoDB[DynamoDB Streams] --> |Alteração na Tabela| Lambda3[AWS Lambda]\n    CloudWatch[CloudWatch Events] --> |Alerta| Lambda4[AWS Lambda]\n    SQS[SQS] --> |Mensagem na Fila| Lambda5[AWS Lambda]\n\n    Lambda1 --> Process1[Processa Arquivo]\n    Lambda2 --> Process2[API Backend]\n    Lambda3 --> Process3[Sincroniza Dados]\n    Lambda4 --> Process4[Automatiza Resposta]\n    Lambda5 --> Process5[Processa Mensagem]\n```\n\n### **1. Introdução ao AWS Lambda**\n\nAWS Lambda é um serviço de computação serverless que executa código sem que você precise gerenciar servidores. Ele é ideal para processos pontuais que respondem automaticamente a eventos, como uploads no S3, alterações em bancos de dados, e requisições HTTP através do API Gateway. Embora o termo \"serverless\" sugira a ausência de servidores, na prática, isso significa que o código é executado em servidores gerenciados pela AWS, não por você.\n\n### **2. Diferença entre AWS Lambda e EC2**\n\n#### **2.1. AWS EC2 (Elastic Compute Cloud)**\n- **Definição**: Serviço de computação que permite criar e gerenciar instâncias de servidores virtuais na nuvem, oferecendo controle total sobre o ambiente.\n- **Vantagens**:\n  - **Controle Completo**: Customização do ambiente de software e hardware.\n  - **Ambientes Persistentes**: Ideal para aplicações que exigem disponibilidade contínua.\n  - **Escalabilidade Flexível**: Permite ajustes manuais ou automáticos de recursos.\n- **Desafios**:\n  - **Gerenciamento**: Requer manutenção contínua e configuração detalhada.\n  - **Custo**: Pagamento contínuo pelo tempo de execução, independente do uso.\n  - **Complexidade de Configuração**: Requer atenção aos detalhes de rede, segurança e capacidade.\n\n#### **2.2. AWS Lambda**\n- **Definição**: Serviço que executa código em resposta a eventos sem necessidade de gerenciar servidores, cobrando apenas pelo tempo de execução.\n- **Vantagens**:\n  - **Serverless**: Reduz a complexidade operacional.\n  - **Escalabilidade Automática**: Ajusta automaticamente com base na carga de eventos.\n  - **Custo-Eficiência**: Ideal para cargas intermitentes com pagamento por uso.\n- **Desafios**:\n  - **Limitações de Execução**: Tempo máximo de 15 minutos, com restrições de memória e armazenamento.\n  - **Cold Starts**: Pequenos atrasos iniciais quando funções são ativadas após inatividade.\n  - **Configuração de Permissões**: Requer configurações cuidadosas para garantir segurança.\n\n### **3. Principais Casos de Uso do AWS Lambda**\n- **Processamento de Arquivos**: Automatiza tarefas como redimensionamento de imagens e análise de dados em S3.\n- **ETL em Tempo Real**: Transformação de dados em tempo real a partir de streams de dados.\n- **APIs Serverless**: Gerenciamento de APIs com API Gateway e Lambda.\n- **Eventos de IoT**: Resposta a dados de dispositivos IoT.\n- **Automação de Infraestrutura**: Tarefas automatizadas como limpeza de recursos e monitoramento.\n\n### **4. Eventos na AWS e Integração com Lambda**\nLambda pode ser acionado por diversos eventos na AWS, permitindo respostas dinâmicas a mudanças nos serviços.\n- **Eventos S3**: Código executado ao upload de arquivos.\n- **Eventos DynamoDB Streams**: Funções disparadas por alterações em tabelas.\n- **Eventos API Gateway**: Requisições HTTP acionam funções Lambda.\n- **Eventos CloudWatch**: Ações baseadas em alertas e agendamentos.\n- **Eventos SQS**: Mensagens processadas de filas SQS.\n\n### **5. Configurando uma Função AWS Lambda**\n1. **Criar Função no Console AWS Lambda**: Selecione “Author from scratch” e configure nome, runtime e permissões.\n2. **Escrever Código**: Escreva o código da função no editor integrado, definindo a lógica do lambda handler.\n3. **Adicionar Trigger**: Configure o evento que irá acionar a função, como upload no S3 ou requisição HTTP.\n4. **Testar Função**: Utilize o console para testes com eventos simulados e ajuste o código conforme necessário.\n\n### **6. Comparação de Cenários de Uso: AWS Lambda vs. EC2**\n\n| **Critério**           | **AWS EC2**                                  | **AWS Lambda**                              |\n|------------------------|----------------------------------------------|---------------------------------------------|\n| **Controle**           | Total sobre ambiente e recursos             | Limitado ao código da função                |\n| **Persistência**       | Executa continuamente                       | Executa sob demanda, de forma intermitente  |\n| **Escalabilidade**     | Manual ou automática                         | Automática com base em eventos              |\n| **Gerenciamento**      | Requer configuração e manutenção            | Automação pela AWS                          |\n| **Custo**              | Contínuo enquanto ativo                     | Paga apenas pelo uso                        |\n| **Tempo de Resposta**  | Latência menor em execução contínua         | Cold starts podem aumentar a latência       |\n\n### **7. Desafios ao Usar AWS Lambda e EC2**\n- **AWS Lambda**: \n  - Limitações de tempo e recursos, cold starts e desafios de debug.\n- **AWS EC2**: \n  - Requer gerenciamento contínuo, configuração de escalabilidade e custos constantes.\n\n### **8. Motivação para Escolher AWS Lambda**\n- **Economia**: Pagamento por uso sem custos fixos.\n- **Escalabilidade**: Ajustes automáticos com base na demanda.\n- **Foco no Código**: Sem preocupações com infraestrutura.\n\n### **Conclusão**\nAWS Lambda oferece uma abordagem serverless que simplifica a execução de código na nuvem, focando em escalabilidade e economia de custos. Comparado ao EC2, o Lambda reduz a complexidade de gerenciamento, mas exige um entendimento claro de suas limitações. A escolha entre Lambda e EC2 deve considerar o caso de uso específico, pesando controle, custos e necessidades operacionais.\n\nVou criar alguns diagramas usando Mermaid para ilustrar os conceitos apresentados na aula sobre AWS Lambda e EC2. Esses diagramas ajudarão a visualizar como o AWS Lambda funciona, suas integrações com eventos e uma comparação com o EC2.\n\n### **Exemplo: Teste Meu Primeiro Lambda**\n\nEste exemplo de função Lambda vai simplesmente retornar uma mensagem de teste quando invocada. Isso ajudará você a entender o básico de como criar, configurar e testar uma função Lambda.\n\n#### **Passo a Passo para Criar a Função Lambda**\n\n1. **Acesse o Console do AWS Lambda:**\n   - Vá para o [AWS Management Console](https://aws.amazon.com/console/) e selecione **Lambda** no menu de serviços.\n\n2. **Criar a Função Lambda:**\n   - Clique em **Create Function**.\n   - Escolha **Author from scratch**.\n   - Configure os seguintes detalhes:\n     - **Function name**: `TesteMeuPrimeiroLambda`.\n     - **Runtime**: Selecione `Python 3.9` ou outra versão que você prefira.\n     - **Permissions**: Selecione **Create a new role with basic Lambda permissions** para permitir que a função grave logs no CloudWatch.\n\n3. **Adicionar o Código da Função:**\n\n   Após criar a função, adicione o seguinte código no editor do console do Lambda:\n\n   ```python\n   def lambda_handler(event, context):\n       # Função básica que retorna uma mensagem de teste\n       return {\n           'statusCode': 200,\n           'body': 'Olá! Este é o meu primeiro teste com AWS Lambda.'\n       }\n   ```\n\n4. **Testar a Função Lambda:**\n\n   - Clique em **Deploy** para salvar o código.\n   - Clique em **Test** para criar um evento de teste:\n     - Dê um nome ao evento de teste, como `EventoTeste`.\n     - Use o payload padrão ou um JSON simples, como:\n       ```json\n       {\n         \"mensagem\": \"Teste de invocação\"\n       }\n       ```\n   - Clique em **Test** novamente para executar a função.\n\n   Você verá o resultado da execução na parte inferior da página, mostrando algo semelhante a:\n\n   ```json\n   {\n     \"statusCode\": 200,\n     \"body\": \"Olá! Este é o meu primeiro teste com AWS Lambda.\"\n   }\n   ```\n\n### **Explicação do Código**\n\n- **`lambda_handler(event, context)`**: É a função principal que o AWS Lambda executa. Ela recebe dois parâmetros:\n  - **`event`**: Contém os dados que você envia quando aciona a função, como um JSON com informações específicas.\n  - **`context`**: Inclui informações de contexto sobre a execução da função, como o tempo de execução restante.\n\n- **Retorno da Função**:\n  - **`statusCode`**: Código HTTP 200 indicando sucesso.\n  - **`body`**: Uma mensagem de resposta simples que é retornada para quem acionou a função.\n\n### **Vantagens deste Exemplo Simples:**\n\n- **Sem Dependências Externas**: Nenhuma instalação de bibliotecas é necessária, o que simplifica o deploy.\n- **Facilidade de Configuração**: Com apenas alguns cliques e um pequeno trecho de código, você pode experimentar o AWS Lambda.\n- **Ideal para Primeiros Testes**: Um bom ponto de partida para entender o funcionamento básico do AWS Lambda e como ele responde a eventos.\n\nEste exemplo proporciona uma introdução ao uso do AWS Lambda de forma simples, sem complicações adicionais, sendo ideal para iniciantes que desejam entender como começar com funções serverless na AWS.\n\nVamos ajustar o exemplo para que a função Lambda receba uma mensagem através de um API Gateway, salve essa mensagem em um arquivo JSON e armazene o arquivo em um bucket S3. Esse exemplo será mais prático e útil para demonstrar o fluxo completo de integração entre o Lambda, o API Gateway e o Amazon S3.\n\n",
        "Bootcamp - Cloud para dados/Aula_10/README.md\n\n# Aula 10: Gestão de Custos na AWS\n\n**Objetivo**: Vocês já sentiram que seus projetos na AWS estão sempre te assustando no final do mês com faturas inesperadas? Com recursos espalhados por todos os lados sem nenhum controle? Os custos estão subindo e a organização parece um desafio insuperável? Hoje, na nossa Aula 10 - Gestão de Custos na AWS, isso vai mudar. Nesta aula, vamos refatorar nossa conta, organizar os recursos e implementar práticas eficientes de gestão de custos. Nosso objetivo é organizar a casa e assumir o controle financeiro dos nossos projetos.\n\nHoje vamos desvendar os segredos da gestão eficiente de custos na AWS. Vamos aprender a estabelecer uma **política de tags focada em governança de custos**, utilizar grupos de recursos, aplicar tags inteligentes e explorar ferramentas como o AWS Cost Explorer e AWS Budgets, que nos darão o controle total sobre nossos projetos. Imagine transformar dez projetos caóticos em um ambiente organizado e econômico. Está na hora de assumir o controle, otimizar recursos e impulsionar nossos resultados. Preparados para essa jornada rumo à eficiência e à economia? Então, vamos começar hoje às 12h!\n\n### **Política de Tags Focada em Governança de Custos**\n\nAntes de iniciarmos com as ferramentas, é crucial estabelecermos uma política de tags consistente e obrigatória para todos os recursos AWS. As tags são fundamentais para a governança de custos, pois permitem a categorização e atribuição de custos aos respectivos responsáveis e projetos.\n\n**5 Tags Obrigatórias Focadas em Governança de Custos:**\n\n1. **CostCenter (Centro de Custo)**:\n   - **Descrição**: Identifica o centro de custo ou departamento responsável pelas despesas associadas ao recurso.\n   - **Exemplo**: `CostCenter: CC1001`\n\n2. **Project (Projeto)**:\n   - **Descrição**: Nome do projeto ao qual o recurso pertence.\n   - **Exemplo**: `Project: ProjetoX`\n\n3. **Owner (Responsável)**:\n   - **Descrição**: Pessoa ou equipe responsável pelo recurso.\n   - **Exemplo**: `Owner: JoãoSilva`\n\n4. **Environment (Ambiente)**:\n   - **Descrição**: Indica o ambiente de utilização do recurso.\n   - **Exemplo**: `Environment: Production` ou `Environment: Development`\n\n5. **Application (Aplicação)**:\n   - **Descrição**: Nome da aplicação ou serviço que utiliza o recurso.\n   - **Exemplo**: `Application: AppMobile`\n\n**Importância das Tags na Governança de Custos:**\n\n- **Atribuição de Custos**: Permite que os custos sejam atribuídos corretamente aos centros de custo, projetos e responsáveis.\n- **Responsabilidade**: Identifica claramente quem é responsável por cada recurso.\n- **Otimização**: Facilita a identificação de recursos subutilizados ou desnecessários.\n- **Relatórios Personalizados**: Possibilita a geração de relatórios detalhados por tags no AWS Cost Explorer e outros serviços.\n\n### **Projetos da Aula 10**\n\n1. **Estabelecendo e Aplicando a Política de Tags**\n\n   **Objetivo**: Criar uma política de tags consistente e aplicá-la a todos os recursos AWS, garantindo que as 5 tags obrigatórias estejam presentes em cada recurso.\n\n   **Passo a Passo**:\n\n   1. **Definir a Política de Tags**:\n      - Documente as tags obrigatórias e suas respectivas chaves e valores esperados.\n      - Distribua a política para todas as equipes envolvidas.\n\n   2. **Acessar o Console AWS**:\n      - Faça login no AWS Management Console.\n\n   3. **Identificar Recursos Sem as Tags Obrigatórias**:\n      - Navegue até o **Resource Groups & Tag Editor**.\n      - Clique em **Tag Editor**.\n      - Selecione as regiões e tipos de recursos relevantes.\n      - Clique em **Find resources**.\n\n   4. **Filtrar Recursos Sem as Tags Obrigatórias**:\n      - Utilize filtros para identificar recursos que não possuem as tags obrigatórias.\n\n   5. **Adicionar as Tags aos Recursos**:\n      - Selecione os recursos sem as tags.\n      - Clique em **Add tags to selected resources**.\n      - Adicione as 5 tags obrigatórias com os valores apropriados.\n      - Clique em **Save changes**.\n\n   6. **Automatizar a Conformidade de Tags**:\n      - Utilize o **AWS Config** para criar regras que verificam a conformidade das tags.\n      - Configure alertas para recursos que não atendam à política de tags.\n\n   **Benefícios**:\n\n   - **Consistência**: Todos os recursos seguem o mesmo padrão de tagueamento.\n   - **Governança**: Melhora o controle e a gestão dos recursos.\n   - **Facilidade na Gestão de Custos**: As tags permitem atribuir custos corretamente.\n\n   ```mermaid\n   graph LR\n       Policy[Política de Tags] --> Teams[Equipes]\n       Teams --> ApplyTags[Aplicam Tags nos Recursos]\n       ApplyTags --> AWSResources[Recursos AWS]\n   ```\n\n2. **Organização de Recursos com Tags**\n\n   **Objetivo**: Utilizar as tags aplicadas para organizar e gerenciar os recursos de forma eficaz.\n\n   **Passo a Passo**:\n\n   1. **Verificar Tags nos Recursos**:\n      - Confirme que todos os recursos possuem as 5 tags obrigatórias.\n\n   2. **Navegar pelos Recursos Usando Tags**:\n      - Utilize o **Tag Editor** para pesquisar recursos por tags.\n      - Filtre recursos por `Project`, `CostCenter`, `Owner`, etc.\n\n   3. **Analisar Recursos por Responsável**:\n      - Filtre recursos pela tag `Owner` para ver quais recursos estão sob responsabilidade de cada pessoa ou equipe.\n\n   4. **Gerenciar Recursos por Ambiente**:\n      - Use a tag `Environment` para distinguir recursos de produção, desenvolvimento, etc.\n\n   **Benefícios**:\n\n   - **Eficiência**: Localização rápida de recursos específicos.\n   - **Responsabilização**: Facilita o acompanhamento de quem é responsável por cada recurso.\n   - **Otimização**: Identificação de recursos redundantes ou subutilizados.\n\n   ```mermaid\n   graph LR\n       Tags[Tags] --> Organize[Organização de Recursos]\n       Organize --> EfficientManagement[Gestão Eficiente]\n   ```\n\n3. **Criando Grupos de Recursos Baseados em Tags**\n\n   **Objetivo**: Criar grupos de recursos utilizando as tags para facilitar o gerenciamento coletivo.\n\n   **Passo a Passo**:\n\n   1. **Acessar o AWS Resource Groups**:\n      - No Console AWS, procure por **Resource Groups**.\n\n   2. **Criar um Novo Grupo de Recursos**:\n      - Clique em **Create a resource group**.\n      - Escolha **Tag-based group**.\n\n   3. **Definir Critérios do Grupo**:\n      - Configure o grupo usando as tags obrigatórias.\n      - Por exemplo, para agrupar recursos de um projeto:\n        - **Key**: `Project`\n        - **Value**: `ProjetoX`\n\n   4. **Nomear e Salvar o Grupo**:\n      - Dê um nome ao grupo, como `Grupo_ProjetoX`.\n      - Clique em **Create group**.\n\n   5. **Utilizar Grupos para Gestão**:\n      - Navegue pelos grupos para gerenciar recursos coletivamente.\n      - Realize operações em lote quando aplicável.\n\n   **Benefícios**:\n\n   - **Visibilidade**: Visualização consolidada dos recursos por projeto, centro de custo, etc.\n   - **Gestão Simplificada**: Ações coletivas nos recursos do grupo.\n   - **Facilita a Governança**: Monitoramento e controle aprimorados.\n\n   ```mermaid\n   graph LR\n       Tags[Tags] --> ResourceGroups[Grupos de Recursos]\n       ResourceGroups --> Management[Gestão Simplificada]\n   ```\n\n4. **Utilizando o AWS Cost Explorer com Tags**\n\n   **Objetivo**: Analisar os custos detalhadamente utilizando as tags aplicadas nos recursos.\n\n   **Passo a Passo**:\n\n   1. **Acessar o AWS Cost Management**:\n      - No Console AWS, vá para **Cost Management**.\n      - Selecione **Cost Explorer**.\n\n   2. **Ativar o Cost Explorer (se ainda não estiver ativo)**:\n      - Clique em **Enable Cost Explorer**.\n\n   3. **Configurar o Cost Explorer para Exibir Tags**:\n      - Vá para **Cost Explorer Settings**.\n      - Em **Cost Allocation Tags**, ative as tags que deseja utilizar nos relatórios (as 5 tags obrigatórias).\n\n   4. **Explorar os Custos por Tags**:\n      - Crie relatórios filtrando ou agrupando por tags como `CostCenter`, `Project`, `Owner`, etc.\n      - Visualize os custos associados a cada centro de custo, projeto ou responsável.\n\n   5. **Identificar Tendências e Oportunidades de Otimização**:\n      - Analise os gráficos e dados para identificar áreas de alto gasto.\n      - Use as informações para tomar decisões de otimização.\n\n   **Benefícios**:\n\n   - **Transparência**: Visibilidade clara de onde os custos estão sendo gerados.\n   - **Atribuição de Custos Precisa**: Alocação correta de despesas aos centros de custo e projetos.\n   - **Decisões Informadas**: Dados detalhados para suportar estratégias de otimização.\n\n   ```mermaid\n   graph LR\n       CostExplorer[Cost Explorer] --> Analysis[Análise de Custos por Tags]\n       Analysis --> Optimization[Otimização]\n   ```\n\n5. **Configurando AWS Budgets com Base nas Tags**\n\n   **Objetivo**: Estabelecer orçamentos para monitorar os gastos e receber alertas quando os custos excederem limites definidos, utilizando as tags para especificar o escopo.\n\n   **Passo a Passo**:\n\n   1. **Acessar o AWS Budgets**:\n      - No **Cost Management**, selecione **Budgets**.\n\n   2. **Criar um Novo Orçamento**:\n      - Clique em **Create budget**.\n\n   3. **Selecionar Tipo de Orçamento**:\n      - Escolha **Cost Budget**.\n\n   4. **Definir Detalhes do Orçamento**:\n      - Nome do orçamento: `Orçamento_CentroDeCusto_CC1001`.\n      - Período: Mensal.\n      - Valor do orçamento: Defina o valor máximo para o centro de custo, por exemplo, `$5,000`.\n\n   5. **Aplicar Filtros com Tags**:\n      - Em **Filters**, adicione a tag:\n        - **Key**: `CostCenter`\n        - **Value**: `CC1001`\n\n   6. **Configurar Alertas**:\n      - Defina quando deseja ser notificado:\n        - Quando atingir 80% do orçamento.\n        - Quando ultrapassar 100% do orçamento.\n      - Adicione os emails dos responsáveis (utilizando a tag `Owner` para referência).\n\n   7. **Revisar e Criar**:\n      - Revise as configurações e clique em **Create budget**.\n\n   8. **Repetir para Outros Centros de Custo ou Projetos**:\n      - Crie orçamentos para cada centro de custo ou projeto conforme necessário.\n\n   **Benefícios**:\n\n   - **Controle Financeiro**: Monitoramento proativo dos gastos por centro de custo ou projeto.\n   - **Alertas Personalizados**: Notificações para os responsáveis quando os limites são atingidos.\n   - **Planejamento Financeiro Eficaz**: Apoio na elaboração e cumprimento de orçamentos.\n\n   ```mermaid\n   graph LR\n       Budgets[AWS Budgets] --> Monitoring[Monitoramento de Custos por Tags]\n       Monitoring --> Alerts[Alertas Personalizados]\n   ```\n\n6. **Implementando AWS Cost Anomaly Detection com Tags**\n\n   **Objetivo**: Configurar o AWS Cost Anomaly Detection para identificar automaticamente anomalias nos gastos específicos a centros de custo, projetos ou responsáveis.\n\n   **Passo a Passo**:\n\n   1. **Acessar o Cost Anomaly Detection**:\n      - No **Cost Management**, selecione **Anomaly Detection**.\n\n   2. **Criar um Monitor de Anomalias**:\n      - Clique em **Create monitor**.\n\n   3. **Definir o Escopo do Monitor com Tags**:\n      - Escolha **Single Account** ou **Linked Accounts** se houver.\n      - Em **Monitor dimensions**, selecione **Tag**:\n        - **Key**: `CostCenter`\n        - **Value**: `CC1001`\n\n   4. **Configurar Alertas**:\n      - Defina o **Anomaly threshold** para determinar a sensibilidade.\n      - Adicione os emails dos responsáveis (`Owner`) para notificações.\n\n   5. **Revisar e Criar**:\n      - Revise as configurações e clique em **Create monitor**.\n\n   **Benefícios**:\n\n   - **Detecção Personalizada**: Análise de anomalias em áreas específicas.\n   - **Resposta Rápida**: Notificações imediatas para os responsáveis.\n   - **Proteção Financeira**: Prevenção de gastos inesperados.\n\n   ```mermaid\n   graph LR\n       AnomalyDetection[Anomaly Detection] --> IdentifyAnomalies[Identifica Anomalias por Tags]\n       IdentifyAnomalies --> Notifications[Notificações para Responsáveis]\n   ```\n\n### **Ferramentas Adicionais**\n\n- **AWS Config**:\n  - Configure regras para garantir que todos os recursos tenham as tags obrigatórias.\n  - Receba alertas ou visualize não conformidades quando recursos não atenderem à política.\n\n- **AWS Trusted Advisor**:\n  - Utilize as verificações de otimização de custos, segurança e desempenho.\n  - Acesse através do Console AWS em **Trusted Advisor**.\n\n- **AWS Cost and Usage Reports**:\n  - Obtenha relatórios detalhados que incluem as tags para análise avançada.\n  - Configure no **Cost Management** em **Cost and Usage Reports**.\n\n### **Conclusão da Aula 10**\n\nNesta aula, enfatizamos a importância de uma política de tags bem definida e consistente como base para a governança eficaz de custos na AWS. Aprendemos a aplicar as 5 tags obrigatórias focadas em governança de custos, permitindo uma gestão detalhada e precisa dos recursos. Utilizamos essas tags em conjunto com ferramentas poderosas como o AWS Cost Explorer, AWS Budgets e AWS Cost Anomaly Detection para obter visibilidade, controle e otimização dos gastos.\n\nAo estabelecer e aderir a uma política de tags robusta, capacitamos nossas equipes a serem responsáveis pelos recursos que gerenciam, promovendo uma cultura de responsabilidade e eficiência. Com essas práticas implementadas, estamos melhor posicionados para transformar um ambiente desorganizado em uma infraestrutura otimizada e financeiramente saudável, garantindo o sucesso contínuo dos nossos projetos.\n\n---\n\n**Próximos Passos**:\n\n- **Revisar a Política de Tags Regularmente**: Certifique-se de que a política continua relevante e atualizada com as necessidades da organização.\n- **Treinamento Contínuo**: Promova workshops e sessões de treinamento para as equipes sobre a importância da tagueação correta.\n- **Automatização**: Explore ferramentas e scripts para automatizar a aplicação de tags e a conformidade com a política estabelecida.\n\nCom essas ações, garantiremos uma gestão de custos eficaz e uma governança sólida, permitindo que nos concentremos no que realmente importa: impulsionar nossos projetos para o sucesso.\n\n### **Checklist para Evitar Surpresas no Final do Mês**\n\nPara garantir que os custos na AWS permaneçam sob controle e evitar despesas inesperadas, siga este checklist regularmente:\n\n1. **Revisar Orçamentos no AWS Budgets**:\n   - Verifique os orçamentos configurados para cada centro de custo, projeto e responsável.\n   - Certifique-se de que os alertas estão configurados corretamente e os contatos estão atualizados.\n\n2. **Monitorar Gastos Diariamente**:\n   - Utilize o **AWS Cost Explorer** para acompanhar os gastos diários ou semanais.\n   - Configure relatórios personalizados para serem enviados por e-mail regularmente.\n\n3. **Analisar Anomalias com o AWS Cost Anomaly Detection**:\n   - Revise notificações de anomalias de custo.\n   - Investigue imediatamente quaisquer gastos inesperados ou incomuns.\n\n4. **Garantir a Conformidade das Tags**:\n   - Use o **AWS Config** para verificar se todos os recursos possuem as **5 tags obrigatórias**.\n   - Corrija recursos que não estejam em conformidade com a política de tags.\n\n5. **Desligar Recursos Não Utilizados**:\n   - Identifique recursos ociosos ou subutilizados, como instâncias EC2 não utilizadas, volumes EBS não anexados, etc.\n   - Desligue ou exclua esses recursos para reduzir custos desnecessários.\n\n6. **Revisar Reservas e Savings Plans**:\n   - Avalie se **Reserved Instances** ou **Savings Plans** podem ser utilizados para recursos de uso contínuo.\n   - Ajuste ou renove conforme necessário para maximizar economias.\n\n7. **Comunicar-se com as Equipes Responsáveis**:\n   - Mantenha as equipes informadas sobre o status dos gastos e orçamentos.\n   - Promova uma cultura de responsabilidade financeira e uso consciente dos recursos.\n\n8. **Atualizar a Política de Tags e Governança**:\n   - Revise a política de tags periodicamente para garantir que ela atenda às necessidades atuais da organização.\n   - Treine novas equipes ou membros sobre a importância e a aplicação correta das tags.\n\n9. **Automatizar Alertas e Ações**:\n   - Considere a configuração de alertas adicionais via **Amazon CloudWatch** para monitorar métricas específicas.\n   - Use scripts ou **AWS Lambda** para automatizar ações corretivas quando certos limites forem atingidos.\n\n10. **Auditar Configurações de Segurança e Compliance**:\n    - Verifique se não há recursos expostos que possam gerar custos devido a uso indevido.\n    - Utilize o **AWS Trusted Advisor** e o **AWS Security Hub** para identificar vulnerabilidades.\n\n11. **Documentar e Revisar Processos**:\n    - Mantenha a documentação atualizada sobre procedimentos de gestão de custos e governança.\n    - Realize revisões regulares dos processos para identificar melhorias potenciais.\n\n12. **Planejar para Eventos Futuros**:\n    - Considere lançamentos de novos projetos, campanhas ou eventos sazonais que possam aumentar o uso de recursos.\n    - Ajuste orçamentos e recursos antecipadamente para acomodar aumentos previstos.\n\n13. **Utilizar Relatórios Detalhados**:\n    - Configure o **AWS Cost and Usage Report** para obter dados detalhados de uso e custo.\n    - Analise esses relatórios para insights aprofundados e para identificar áreas de otimização.\n\n14. **Verificar Limites de Serviço**:\n    - Monitore os limites de serviço (quotas) para evitar interrupções inesperadas ou custos adicionais.\n    - Solicite aumentos de limite antecipadamente, se necessário.\n\n15. **Implementar Políticas de Encerramento Automático**:\n    - Configure políticas para encerrar automaticamente recursos não utilizados após um período definido.\n    - Utilize tags como `ExpirationDate` ou `AutoDelete` para auxiliar nesse processo.\n\nSeguindo este checklist regularmente, você estará melhor preparado para evitar surpresas no final do mês e manter os custos da AWS sob controle. A gestão proativa e contínua é essencial para uma operação financeira saudável e para o sucesso dos seus projetos na nuvem.\n\n1) Verificar Frankfurt para ver quanto vai reduzir os custos! (removemos RDS e EC2)\n\n2) Paris deletamos TUDO, lambda, ec2, rds e vpc\n\n\n\n",
        "Bootcamp - Cloud para dados/Aula_11/README.md\n\n# Aula 11: Projetos Práticos com AWS Lambda\n\n**Objetivo**: Nesta aula, realizaremos uma série de projetos práticos utilizando AWS Lambda. Vamos explorar como configurar funções Lambda para serem acionadas por eventos temporais e específicos, realizar requests HTTP e integrar com o Amazon RDS e com o AmazonS3 para criar soluções serverless eficientes.\n\n## **Projetos da Aula 11**\n\n### 1. **Configuração de Timer de 10 em 10 Minutos com AWS Lambda**\n\n   **Objetivo**: Demonstrar como agendar uma função Lambda para ser executada a cada 10 minutos usando o Amazon CloudWatch Events (ou EventBridge).\n\n   **Passo a Passo**:\n   1. Acesse o AWS Management Console e selecione **Lambda**.\n   2. Crie uma nova função Lambda com o nome `TimerFunction`.\n   3. Vá para **CloudWatch Events** e crie uma nova regra com uma expressão cron `cron(0/10 * * * ? *)` para disparar a cada 10 minutos.\n   4. Vincule essa regra à função `TimerFunction`.\n   5. Teste para confirmar que a função está sendo acionada conforme esperado.\n\n   ```python\n   def lambda_handler(event, context):\n       print(\"Função executada a cada 10 minutos.\")\n       return {\n           'statusCode': 200,\n           'body': 'Execução bem-sucedida.'\n       }\n   ```\n\n   ```mermaid\n   graph LR\n       CW[CloudWatch Events] -->|Trigger a cada 10 minutos| Lambda1[AWS Lambda TimerFunction]\n       Lambda1 --> Process1[Executa a Função]\n   ```\n\n### 2. **Configuração de Funções Lambda para Horários Específicos**\n\n**Objetivo**: Ensinar como configurar a execução de uma função Lambda em horários específicos, como às 9h, 12h, e 18h diariamente.\n\n**Passo a Passo**:\n1. No AWS Management Console, crie uma função Lambda chamada `SpecificTimeFunction`.\n2. Acesse **EventBridge (antigo CloudWatch Events)**, que é o serviço da AWS responsável por gerenciar eventos e agendamentos.\n3. Crie uma regra com uma expressão cron, por exemplo, `cron(0 9,12,18 * * ? *)`, que dispara a função nos horários específicos: 9h, 12h e 18h.\n4. Vincule essa regra à função Lambda `SpecificTimeFunction` como destino.\n5. Teste a configuração simulando o disparo da função nos horários especificados.\n\n```python\ndef lambda_handler(event, context):\n    print(\"Função executada nos horários específicos: 9h, 12h e 18h.\")\n    return {\n        'statusCode': 200,\n        'body': 'Execução bem-sucedida nos horários específicos.'\n    }\n```\n\n```mermaid\ngraph LR\n    EB[Amazon EventBridge] -->|Trigger às 9h, 12h, 18h| Lambda2[AWS Lambda SpecificTimeFunction]\n    Lambda2 --> Process2[Executa a Função]\n```\n\n### **Amazon EventBridge (antigo CloudWatch Events)**\n\nAmazon EventBridge é um serviço que facilita a criação de arquiteturas baseadas em eventos, permitindo que os serviços da AWS e aplicativos personalizados respondam a eventos de forma rápida e escalável. EventBridge é amplamente utilizado para agendamentos e automatização de fluxos de trabalho, incluindo a execução de funções Lambda em horários específicos.\n\n#### **Possibilidades com EventBridge (CloudWatch Scheduler)**\n\nEventBridge oferece uma série de funcionalidades e flexibilidade na configuração de eventos e cronogramas:\n\n1. **Cronogramas (Schedules)**:\n   - Permite a criação de agendamentos para acionar funções Lambda, tarefas ECS, Step Functions, e outros destinos com base em uma expressão cron ou rate.\n   - Pode ser utilizado para tarefas repetitivas como limpeza de logs, sincronização de dados, e execução de processos diários.\n\n2. **Criar um Cronograma no EventBridge**:\n   - **Etapa 1: Especificar Detalhes do Cronograma**:\n     - Defina um nome para o cronograma e escolha entre uma expressão cron ou rate para determinar a frequência de execução.\n     - Por exemplo, a expressão `cron(0 9,12,18 * * ? *)` agenda a execução para às 9h, 12h e 18h todos os dias.\n\n   - **Etapa 2: Selecionar Destinos**:\n     - Escolha o destino da regra de evento, como AWS Lambda, Step Functions, SQS, SNS, entre outros.\n     - Nesse caso, o destino será a função `SpecificTimeFunction`.\n\n   - **Etapa 3: Configurações**:\n     - Configure permissões para que o EventBridge possa invocar o destino selecionado.\n     - Inclua outras configurações opcionais, como filtros de eventos e políticas de retry em caso de falhas.\n\n   - **Etapa 4: Revisar e Criar uma Programação**:\n     - Revise as configurações definidas e clique em “Create” para finalizar a criação da regra.\n\n3. **Exemplos de Uso do EventBridge**:\n   - **Automação de Processos**: Execução de scripts de manutenção de banco de dados, backup de arquivos, ou limpeza de logs em horários pré-definidos.\n   - **Alertas e Monitoramento**: Envio de alertas baseados em agendamentos ou eventos ocorridos em outros serviços AWS.\n   - **Integração de Sistemas**: Facilita a comunicação entre sistemas, disparando eventos com base em horários ou outras condições.\n\n4. **Benefícios do EventBridge**:\n   - **Escalabilidade e Confiabilidade**: Gerencia eventos de forma eficiente e escalável, garantindo que os agendamentos ocorram conforme especificado.\n   - **Redução de Complexidade**: Elimina a necessidade de gerenciar servidores para agendamentos, simplificando a arquitetura.\n   - **Facilidade de Integração**: Integra facilmente com múltiplos serviços AWS e aplicações personalizadas.\n\nAmazon EventBridge é uma ferramenta poderosa para agendamento e gerenciamento de eventos em sistemas distribuídos, proporcionando uma solução flexível para automatizar processos com Lambda e outros serviços na AWS. A utilização de cronogramas e regras facilita o desenvolvimento de soluções eficientes e escaláveis, reduzindo a carga operacional e permitindo que você foque em lógica de negócios ao invés de infraestrutura.\n\nSe precisar de mais detalhes sobre EventBridge ou outros aspectos dos exemplos, me avise!\n\n### 3. **Criação de Funções Lambda para Realizar Requests HTTP**\n\n   **Objetivo**: Demonstrar como usar AWS Lambda para fazer requests HTTP utilizando o módulo `urllib3`, nativo do Python.\n\n   **Passo a Passo**:\n   1. Crie uma nova função Lambda chamada `HTTPRequestFunction`.\n   2. Utilize o módulo `urllib3` para realizar um GET request para uma API pública e processar a resposta.\n   3. Teste a função e visualize os logs para garantir que o request foi realizado corretamente.\n\n   ```python\nimport urllib3\nimport json\nimport os\n\n# Carregar variáveis de ambiente\nurl  os.getenv('API_URL')  # URL da API obtida das variáveis de ambiente\napi_key  os.getenv('CMC_API_KEY')  # Chave da API obtida das variáveis de ambiente\n\n# Parâmetros da requisição para obter a cotação do Bitcoin\nparameters  {\n    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo\n    'convert': 'USD'  # Convertendo a cotação para USD\n}\n\n# Headers com a chave da API\nheaders  {\n    'Accept': 'application/json',\n    'X-CMC_PRO_API_KEY': api_key  # Obtendo a chave da API das variáveis de ambiente\n}\n\n# Criar um PoolManager para gerenciar conexões\nhttp  urllib3.PoolManager()\n\n# Função Lambda\ndef lambda_handler(event, context):\n    try:\n        # Converte os parâmetros para o formato de query string\n        query_string  '&'.join([f'{key}{value}' for key, value in parameters.items()])\n        full_url  f\"{url}?{query_string}\"\n        \n        # Fazendo o request GET para a API\n        response  http.request('GET', full_url, headersheaders)\n        data  json.loads(response.data.decode('utf-8'))\n        \n        # Verificar se os dados do Bitcoin estão presentes na resposta\n        if 'data' in data and 'BTC' in data['data']:\n            bitcoin_data  data['data']['BTC']\n            usd_quote  bitcoin_data['quote']['USD']\n            \n            # Log da resposta\n            print(f\"Cotação do Bitcoin obtida: {usd_quote}\")\n        else:\n            print(\"Erro ao obter a cotação do Bitcoin:\", data.get('status', {}).get('error_message', 'Erro desconhecido'))\n\n    except urllib3.exceptions.HTTPError as e:\n        print(f\"Erro na requisição: {e}\")\n\n   ```\n\n   ```mermaid\n   graph LR\n       Lambda3[AWS Lambda HTTPRequestFunction] -->|Faz Request HTTP| API[API Externa]\n       API --> Process3[Processa Resposta]\n   ```\n\nPara criar um cron job que execute uma AWS Lambda a cada minuto, você pode usar o Amazon CloudWatch Events (agora parte do Amazon EventBridge) para configurar um agendamento. A seguir, explico como criar um evento cron que dispara sua Lambda a cada minuto:\n\n### Passo a passo para configurar um cron job de 1 minuto para AWS Lambda:\n\n1. **Acesse o Amazon EventBridge:**\n   - Vá para o Console da AWS.\n   - Pesquise por \"EventBridge\" (ou \"CloudWatch Events\" dependendo da interface).\n\n2. **Crie uma nova regra:**\n   - Clique em \"Create rule\".\n\n3. **Configure a regra:**\n   - **Name:** Dê um nome para a regra, como `lambda-every-minute`.\n   - **Description:** Adicione uma descrição opcional para explicar o que a regra faz.\n   - **Define pattern:** Escolha \"Event Source\" como \"Event pattern\" ou \"Schedule\".\n   - Selecione **Schedule** e escolha **Cron expression**.\n\n4. **Defina a expressão cron:**\n   - Use a expressão cron para executar a cada minuto:\n     ```\n     cron(0/1 * * * ? *)\n     ```\n   - Explicação da expressão:\n     - `0/1`: Começa no segundo zero e repete a cada 1 minuto.\n     - `*`: Qualquer hora.\n     - `*`: Qualquer dia do mês.\n     - `*`: Qualquer mês.\n     - `?`: Qualquer dia da semana.\n\n5. **Defina o destino da regra:**\n   - Escolha \"Lambda function\".\n   - Selecione a função Lambda que deseja disparar a cada minuto.\n\n6. **Configurações adicionais:**\n   - Se precisar, adicione um \"Input Transformer\" para modificar os dados enviados para a Lambda, ou ajuste permissões conforme necessário.\n\n7. **Permissões:**\n   - O EventBridge precisará de permissões para invocar sua função Lambda. Se necessário, o AWS adicionará as permissões automaticamente quando você configurar o destino.\n\n8. **Criar a regra:**\n   - Clique em \"Create\" para finalizar a configuração.\n\n### Nota Importante:\n\n- **Limites de execução:** Executar uma Lambda a cada minuto pode impactar os limites de execução simultânea e a fatura da AWS. Monitore o uso para garantir que o cron não exceda os limites da sua conta.\n- **Lambda Timeout:** Certifique-se de que sua Lambda está configurada com um timeout adequado para o processamento, especialmente se a execução em 1 minuto for crítica.\n\nSe precisar de mais alguma coisa, me avise!\n\n### 4. **Criação de Funções Lambda para Realizar Requests HTTP com `requests`**\n\n   **Objetivo**: Mostrar como instalar o módulo `requests` e utilizá-lo para fazer um GET request para uma API.\n\n   **Passo a Passo**:\n   1. No AWS Lambda, crie uma função chamada `GetRequestFunction`.\n   2. Adicione a biblioteca `requests` ao ambiente Lambda (você pode empacotar o `requests` com o código ou usar um Lambda Layer).\n   3. Utilize `requests` para realizar um GET request a uma API pública.\n   4. Entre no WSL\n   5. cd ..\n   6. mkdir python\n   7. cd python\n   8. pip3 install requests -t .\n   9. cd ..\n   10. zip -r python.zip python\n\n```python\nimport requests\nimport json\nimport os\n\n# Carregar variáveis de ambiente\nurl  os.getenv('API_URL')  # URL da API obtida das variáveis de ambiente\napi_key  os.getenv('CMC_API_KEY')  # Chave da API obtida das variáveis de ambiente\n\n# Parâmetros da requisição para obter a cotação do Bitcoin\nparameters  {\n    'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo\n    'convert': 'USD'  # Convertendo a cotação para USD\n}\n\n# Headers com a chave da API\nheaders  {\n    'Accept': 'application/json',\n    'X-CMC_PRO_API_KEY': api_key  # Obtendo a chave da API das variáveis de ambiente\n}\n\n# Função Lambda\ndef lambda_handler(event, context):\n    try:\n        # Criar uma sessão para gerenciar as requisições\n        with requests.Session() as session:\n            # Configurar os headers da sessão\n            session.headers.update(headers)\n            \n            # Fazer o request GET para a API com parâmetros\n            response  session.get(url, paramsparameters)\n            response.raise_for_status()  # Levanta um erro se o status code for 4xx ou 5xx\n            \n            data  response.json()  # Carregar a resposta JSON\n            \n            # Verificar se os dados do Bitcoin estão presentes na resposta\n            if 'data' in data and 'BTC' in data['data']:\n                bitcoin_data  data['data']['BTC']\n                usd_quote  bitcoin_data['quote']['USD']\n                \n                # Log da resposta\n                print(f\"Cotação do Bitcoin obtida: {usd_quote}\")\n            else:\n                print(\"Erro ao obter a cotação do Bitcoin:\", data.get('status', {}).get('error_message', 'Erro desconhecido'))\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Erro na requisição: {e}\")\n\n```\n\n```mermaid\ngraph LR\n    Lambda4[AWS Lambda GetRequestFunction] -->|Faz Request GET| API[API Externa]\n    API --> Process4[Processa Resposta]\n```\n\n### **Conclusão da Aula 11**\n\nNesta aula prática, exploramos cinco cenários diferentes de uso do AWS Lambda, destacando sua flexibilidade e integração com outros serviços da AWS. As demonstrações forneceram uma base sólida sobre como configurar funções Lambda para operar com eventos temporais, realizar requisições HTTP e conectar-se a bancos de dados RDS, reforçando o papel do Lambda como uma ferramenta essencial para arquiteturas modernas e escaláveis.\n\nBootcamp - Cloud para dados/Aula_11/exemplo.py\n\ndef lambda_handler(event, context):\n    print(\"Função executada a cada 10 minutos.\")\n    return {\n        'statusCode': 200,\n        'body': 'Execução bem-sucedida.'\n    }\n\n\n\nBootcamp - Cloud para dados/Aula_11/exemplo_02.py\n\nimport urllib3\nimport json\nimport os\n\n# Carregar variáveis de ambiente\nurl  'https://pro-api.coinmarketcap.com/v1/cryptocurrency/quotes/latest'\napi_key  ''  # Chave da API obtida das variáveis de ambiente\n\n# Parâmetros da requisição para obter a cotação do Bitcoin\nparameters  {\n 'symbol': 'BTC',  # Identificando o Bitcoin pelo símbolo\n 'convert': 'USD'  # Convertendo a cotação para USD\n}\n\n# Headers com a chave da API\nheaders  {\n 'Accept': 'application/json',\n 'X-CMC_PRO_API_KEY': api_key  # Obtendo a chave da API das variáveis de ambiente\n}\n\n# Criar um PoolManager para gerenciar conexões\nhttp  urllib3.PoolManager()\n\n# Função Lambda\ndef lambda_handler(event, context):\n try:\n     # Converte os parâmetros para o formato de query string\n     query_string  '&'.join([f'{key}{value}' for key, value in parameters.items()])\n     full_url  f\"{url}?{query_string}\"\n     \n     # Fazendo o request GET para a API\n     response  http.request('GET', full_url, headersheaders)\n     data  json.loads(response.data.decode('utf-8'))\n     \n     # Verificar se os dados do Bitcoin estão presentes na resposta\n     if 'data' in data and 'BTC' in data['data']:\n         bitcoin_data  data['data']['BTC']\n         usd_quote  bitcoin_data['quote']['USD']\n         \n         # Log da resposta\n         print(f\"Cotação do Bitcoin obtida: {usd_quote}\")\n     else:\n         print(\"Erro ao obter a cotação do Bitcoin:\", data.get('status', {}).get('error_message', 'Erro desconhecido'))\n\n except urllib3.exceptions.HTTPError as e:\n     print(f\"Erro na requisição: {e}\")\n\n\n",
        "Bootcamp - Cloud para dados/Aula_12/README.md - Parte (1/2)\nBootcamp - Cloud para dados/Aula_12/README.md\n\n# Aula 12: Introdução ao AWS CLI\n\n**Objetivo**: Nesta aula, vamos aprender sobre o AWS Command Line Interface (CLI), explorando como instalar a ferramenta, configurá-la e utilizá-la para gerenciar serviços da AWS diretamente do terminal. Vamos também discutir o AWS CloudShell, uma alternativa prática que permite usar o AWS CLI sem precisar instalar nada. O objetivo é que você saia desta aula sabendo como interagir com a AWS de forma eficiente e automatizada.\n\n```mermaid\ngraph TD\n    A(AWS) -->|Autenticação| B(IAM)\n    B --> C[AWS Management Console]\n    B --> D[AWS CLI]\n    B --> E[AWS CloudShell]\n    B --> F[Terraform]\n    B --> G[Boto3]\n    C --> H(AWS APIs)\n    D --> H(AWS APIs)\n    E --> H(AWS APIs)\n    F --> H(AWS APIs)\n    G --> H(AWS APIs)\n    H --> A\n```\n\n## **Tópicos da Aula 12**\n\n### 1. **Introdução ao AWS CLI e AWS CloudShell**\n\n   **Objetivo**: Entender o que é o AWS CLI e conhecer o AWS CloudShell como uma alternativa para começar a utilizar o CLI sem instalação local.\n\n   **O que é AWS CLI**:\n   - O AWS Command Line Interface (CLI) é uma ferramenta unificada para gerenciar serviços da AWS diretamente do terminal. Ele permite a criação, gerenciamento e automação de recursos na AWS de forma programática.\n   - Com o AWS CLI, é possível interagir com todos os serviços da AWS, como S3, EC2, RDS, Lambda, entre outros, por meio de comandos simples.\n\n   **Vantagens do AWS CLI**:\n   - **Automação**: Facilita a automação de tarefas como criação de recursos, backups e monitoramento.\n   - **Integração**: Trabalha bem com ferramentas de DevOps, pipelines de CI/CD, e scripts shell.\n   - **Eficiência**: Reduz a necessidade de navegar pelo console da AWS para tarefas repetitivas.\n\n   **O que é AWS CloudShell**:\n   - O AWS CloudShell é uma ferramenta que permite usar o AWS CLI diretamente no navegador, sem precisar instalar nada no seu computador.\n   - Ele oferece um ambiente shell baseado em navegador com o AWS CLI pré-instalado e configurado com suas credenciais de usuário.\n   - Isso é particularmente útil quando você precisa executar comandos rápidos ou testar scripts em diferentes máquinas sem se preocupar com configurações locais.\n\n   **Como usar o AWS CloudShell**:\n   1. Acesse o **AWS Management Console** e procure por \"CloudShell\".\n   2. Clique em **CloudShell** no menu superior. Isso abrirá um terminal diretamente no navegador, já autenticado com suas credenciais AWS.\n   3. Use o AWS CLI no CloudShell como se estivesse no seu terminal local:\n   ```bash\n   aws s3 ls\n   ```\n\n   **Vantagens do CloudShell**:\n   - Acesso rápido ao AWS CLI sem instalação.\n   - Ambiente seguro e pré-configurado.\n   - Capacidade de armazenar até 1 GB de dados no ambiente CloudShell.\n   - Ideal para quem está começando ou para quem trabalha com múltiplos dispositivos.\n\n### 2. **Instalação do AWS CLI no Windows**\n\n   **Objetivo**: Demonstrar o passo a passo para instalar o AWS CLI em máquinas com o sistema operacional Windows.\n\n   **Passo a Passo**:\n   1. **Baixar o AWS CLI**:\n      - Acesse a [página oficial de download do AWS CLI](https://aws.amazon.com/cli/) e selecione a versão para Windows.\n   \n   2. **Instalar o AWS CLI**:\n      - Após o download, execute o arquivo `.msi` e siga as instruções do instalador.\n   \n   3. **Verificar a instalação**:\n      - Abra o Prompt de Comando (cmd) ou PowerShell e digite o seguinte comando para verificar se a instalação foi concluída com sucesso:\n      ```bash\n      aws --version\n      ```\n      - A saída deve ser algo como `aws-cli/2.x.x Python/3.x.x`.\n\n   4. **Configuração do AWS CLI**:\n      - Para configurar o AWS CLI com suas credenciais, você pode usar o comando `aws configure`. Isso cria um perfil padrão que será usado para autenticação:\n      ```bash\n      aws configure\n      ```\n      - Insira as seguintes informações:\n        - **AWS Access Key ID**: Sua chave de acesso.\n        - **AWS Secret Access Key**: Sua chave secreta.\n        - **Default region name**: A região padrão, como `us-east-1`.\n        - **Default output format**: O formato de saída, como `json`.\n\n### 3. **Configuração de Perfis com AWS CLI**\n\n   **Objetivo**: Aprender a configurar múltiplos perfis para gerenciar diferentes contas ou ambientes de trabalho na AWS.\n\n   **O que são perfis no AWS CLI?**\n   - Perfis permitem que você configure diferentes credenciais e configurações regionais para várias contas ou projetos AWS, sem sobrescrever suas credenciais padrão.\n   - Você pode alternar facilmente entre perfis para acessar diferentes ambientes (produção, desenvolvimento, etc.).\n\n   **Como configurar perfis no AWS CLI**:\n   1. Use o comando `aws configure --profile` para criar um novo perfil:\n   ```bash\n   aws configure --profile nome-do-perfil\n   ```\n   2. Você será solicitado a inserir suas credenciais da mesma forma que no comando `aws configure`:\n      - **AWS Access Key ID**: Insira a chave de acesso para esse perfil específico.\n      - **AWS Secret Access Key**: Insira a chave secreta associada.\n      - **Default region name**: A região para este perfil.\n      - **Default output format**: O formato de saída desejado.\n\n   **Alternar entre perfis**:\n   - Para usar um perfil específico em um comando, você pode adicionar o parâmetro `--profile`:\n   ```bash\n   aws s3 ls --profile nome-do-perfil\n   ```\n   - Para definir um perfil como o padrão para a sessão, defina a variável de ambiente `AWS_PROFILE`:\n   ```bash\n   export AWS_PROFILEnome-do-perfil\n   ```\n\n   **Exemplo de uso de múltiplos perfis**:\n   - Pode ser útil se você estiver trabalhando com diferentes ambientes, como `desenvolvimento`, `staging` e `produção`. Cada perfil pode ter suas próprias credenciais e configuração regional:\n   ```bash\n   aws ec2 describe-instances --profile producao\n   aws ec2 describe-instances --profile desenvolvimento\n   ```\n\n### 3.1 Exercício\n\nAqui está o passo a passo detalhado para criar dois perfis no AWS CLI e alternar entre eles:\n\n### 1. **Configuração de Perfis com AWS CLI**\n\n   **Objetivo**: Criar dois perfis de CLI para diferentes contas ou ambientes (por exemplo, desenvolvimento e produção) e alternar entre eles.\n\n#### **Passo 1: Criar o primeiro perfil (Desenvolvimento)**\n\n1. Abra o terminal ou o Prompt de Comando (cmd) no Windows.\n2. Execute o comando `aws configure --profile` para configurar o primeiro perfil, que será para **desenvolvimento**:\n   ```bash\n   aws configure --profile desenvolvimento\n   ```\n3. O AWS CLI solicitará as seguintes informações:\n   - **AWS Access Key ID**: Insira a chave de acesso da conta de desenvolvimento.\n   - **AWS Secret Access Key**: Insira a chave secreta correspondente.\n   - **Default region name**: Especifique a região, como `us-east-1`.\n   - **Default output format**: Escolha o formato de saída desejado, como `json` ou `text`.\n\n#### **Passo 2: Criar o segundo perfil (Produção)**\n\n1. Novamente, no terminal ou Prompt de Comando, execute o comando para configurar o segundo perfil, que será para **produção**:\n   ```bash\n   aws configure --profile producao\n   ```\n2. O AWS CLI solicitará as mesmas informações:\n   - **AWS Access Key ID**: Insira a chave de acesso da conta de produção.\n   - **AWS Secret Access Key**: Insira a chave secreta correspondente.\n   - **Default region name**: Especifique a região, como `us-west-2`.\n   - **Default output format**: Escolha o formato de saída, como `json`.\n\n### 2. **Verificar a Configuração dos Perfis**\n\nPara verificar se os perfis foram configurados corretamente, você pode listar os perfis que estão salvos no arquivo de configuração:\n\n```bash\ncat ~/.aws/credentials\n```\n\nOu no Windows:\n```bash\ntype %USERPROFILE%\\.aws\\credentials\n```\n\nO arquivo `credentials` deve mostrar algo assim:\n\n```plaintext\n[desenvolvimento]\naws_access_key_id  YOUR_ACCESS_KEY_ID_DEV\naws_secret_access_key  YOUR_SECRET_ACCESS_KEY_DEV\n\n[producao]\naws_access_key_id  YOUR_ACCESS_KEY_ID_PROD\naws_secret_access_key  YOUR_SECRET_ACCESS_KEY_PROD\n```\n\n### 3. **Como Alternar Entre Perfis**\n\n#### **Método 1: Usar o parâmetro `--profile` em cada comando**\n\nSempre que quiser executar um comando usando um perfil específico, adicione o parâmetro `--profile`:\n\n- Para listar buckets S3 no ambiente de **desenvolvimento**:\n  ```bash\n  aws s3 ls --profile desenvolvimento\n  ```\n\n- Para listar buckets S3 no ambiente de **produção**:\n  ```bash\n  aws s3 ls --profile producao\n  ```\n\n#### **Método 2: Definir o perfil padrão para a sessão usando a variável de ambiente `AWS_PROFILE`**\n\nVocê pode configurar o perfil padrão para a sessão atual definindo a variável de ambiente `AWS_PROFILE`.\n\n- Para definir o perfil **desenvolvimento** como padrão:\n  - No Linux/macOS:\n    ```bash\n    export AWS_PROFILEdesenvolvimento\n    ```\n  - No Windows (PowerShell):\n    ```bash\n    $env:AWS_PROFILE\"desenvolvimento\"\n    ```\n  - No Windows (CMD):\n    ```bash\n    set AWS_PROFILEdesenvolvimento\n    ```\n\n- Para definir o perfil **produção** como padrão:\n  - No Linux/macOS:\n    ```bash\n    export AWS_PROFILEproducao\n    ```\n  - No Windows (PowerShell):\n    ```bash\n    $env:AWS_PROFILE\"producao\"\n    ```\n  - No Windows (CMD):\n    ```bash\n    set AWS_PROFILEproducao\n    ```\n\nCom o perfil definido como variável de ambiente, todos os comandos subsequentes usarão esse perfil por padrão. Por exemplo:\n\n```bash\naws s3 ls\n```\n\n### 4. **Alternando Perfis em Scripts**\n\nSe estiver escrevendo scripts, você pode incluir o parâmetro `--profile` diretamente nos comandos, ou definir o perfil desejado no início do script:\n\n```bash\n#!/bin/bash\n\n# Definindo o perfil de produção\nexport AWS_PROFILEproducao\n\n# Listar instâncias EC2 no ambiente de produção\naws ec2 describe-instances\n```\n\n### 5. **Conclusão**\n\nAgora você sabe como configurar múltiplos perfis no AWS CLI e alternar entre eles. Isso facilita muito o gerenciamento de diferentes ambientes ou contas AWS, permitindo que você mantenha as credenciais organizadas e use o perfil correto para cada tarefa.\n\n### 4. **Comandos Básicos no AWS CLI**\n\n   **Objetivo**: Introduzir os principais comandos utilizados no AWS CLI e explicar sua sintaxe básica.\n\n   **Comandos Comuns**:\n   - **Listar buckets no S3**:\n     ```bash\n     aws s3 ls\n     ```\n   - **Criar um bucket S3**:\n     ```bash\n     aws s3 mb s3://nome-do-seu-bucket\n     ```\n   - **Descrever instâncias EC2**:\n     ```bash\n     aws ec2 describe-instances\n     ```\n\n   **Estrutura dos comandos do AWS CLI**:\n   ```bash\n   aws [serviço] [operação] [opções]\n   ```\n\nAqui está o passo a passo para criar um arquivo JSON usando o comando `touch`, e em seguida, exportá-lo para o bucket S3 que você criou:\n\n### Passo a Passo\n\n#### 1. **Criar o bucket S3**\nSe você ainda não criou o bucket no S3, utilize o comando abaixo para criá-lo:\n\n```bash\naws s3 mb s3://nome-do-se",
        "Bootcamp - Cloud para dados/Aula_12/README.md - Parte (2/2)\nu-bucket\n```\n\n#### 2. **Criar o arquivo JSON**\nAgora vamos criar um arquivo JSON localmente.\n\n1. No terminal, use o comando `touch` para criar um arquivo vazio chamado `dados.json`:\n   ```bash\n   touch dados.json\n   ```\n\n2. Em seguida, abra o arquivo com um editor de texto (como `nano`, `vim`, ou outro de sua escolha) e adicione alguns dados JSON de exemplo:\n\n   ```bash\n   nano dados.json\n   ```\n\n   Adicione o seguinte conteúdo:\n\n   ```json\n   {\n     \"nome\": \"Luciano\",\n     \"idade\": 34,\n     \"profissao\": \"Engenheiro de Dados\"\n   }\n   ```\n\n   Salve o arquivo e saia do editor.\n\n#### 3. **Enviar o arquivo JSON para o bucket S3**\n\nAgora, use o comando `aws s3 cp` para copiar o arquivo JSON que você criou para o bucket S3:\n\n```bash\naws s3 cp dados.json s3://nome-do-seu-bucket/dados.json\n```\n\nEsse comando copiará o arquivo `dados.json` para o bucket `nome-do-seu-bucket`.\n\n#### 4. **Verificar o upload no S3**\nVocê pode listar o conteúdo do bucket para verificar se o arquivo foi enviado corretamente:\n\n```bash\naws s3 ls s3://nome-do-seu-bucket/\n```\n\nSe tudo estiver certo, você verá o arquivo `dados.json` listado no bucket.\n\n#### 3.2 Principais comandos do S3 no AWS CLI\n\nAqui está uma lista completa dos comandos do AWS CLI para interagir com o **Amazon S3**:\n\n### Comandos Principais do S3 no AWS CLI\n\n1. **`mb`** (Make Bucket): Cria um novo bucket no Amazon S3.\n   ```bash\n   aws s3 mb s3://nome-do-seu-bucket\n   ```\n\n2. **`rb`** (Remove Bucket): Remove (deleta) um bucket vazio no S3.\n   ```bash\n   aws s3 rb s3://nome-do-seu-bucket\n   ```\n\n3. **`cp`** (Copy): Copia arquivos ou objetos entre o sistema de arquivos local e o S3, ou entre dois buckets S3.\n   ```bash\n   aws s3 cp origem destino\n   ```\n\n4. **`mv`** (Move): Move arquivos ou objetos entre o sistema de arquivos local e o S3, ou entre dois buckets S3.\n   ```bash\n   aws s3 mv origem destino\n   ```\n\n5. **`ls`** (List): Lista todos os buckets ou lista os arquivos em um bucket S3.\n   - Para listar todos os buckets:\n     ```bash\n     aws s3 ls\n     ```\n   - Para listar os arquivos em um bucket:\n     ```bash\n     aws s3 ls s3://nome-do-seu-bucket/\n     ```\n\n6. **`rm`** (Remove): Remove arquivos ou objetos do bucket no S3.\n   ```bash\n   aws s3 rm s3://nome-do-seu-bucket/arquivo.txt\n   ```\n\n7. **`sync`** (Synchronize): Sincroniza o conteúdo entre um diretório local e um bucket S3, ou entre dois buckets S3.\n   - Sincronizar um diretório local com um bucket S3:\n     ```bash\n     aws s3 sync ./diretorio-local s3://nome-do-seu-bucket\n     ```\n   - Sincronizar um bucket S3 com outro:\n     ```bash\n     aws s3 sync s3://bucket-origem s3://bucket-destino\n     ```\n\n8. **`website`**: Configura um bucket S3 para hospedar um site estático.\n   - Para definir um bucket como um site:\n     ```bash\n     aws s3 website s3://nome-do-seu-bucket/ --index-document index.html --error-document error.html\n     ```\n\n9. **`presign`**: Gera uma URL pré-assinada para um objeto no S3, permitindo que o objeto seja acessado temporariamente.\n   ```bash\n   aws s3 presign s3://nome-do-seu-bucket/arquivo.txt\n   ```\n\n### Comandos Avançados\n\n1. **`control-list` (ACL)**: Define ou recupera permissões de acesso ao bucket.\n   - Para obter a lista de permissões (ACL) de um bucket:\n     ```bash\n     aws s3api get-bucket-acl --bucket nome-do-seu-bucket\n     ```\n\n2. **`head-object`**: Recupera metadados de um objeto sem precisar baixá-lo.\n   ```bash\n   aws s3api head-object --bucket nome-do-seu-bucket --key caminho-do-arquivo\n   ```\n\n3. **`put-object`**: Carrega um arquivo para um bucket S3 (similar ao `cp`).\n   ```bash\n   aws s3api put-object --bucket nome-do-seu-bucket --key caminho-do-arquivo --body arquivo.txt\n   ```\n\n4. **`get-object`**: Baixa um arquivo de um bucket S3 (similar ao `cp`).\n   ```bash\n   aws s3api get-object --bucket nome-do-seu-bucket --key caminho-do-arquivo arquivo.txt\n   ```\n\n5. **`list-objects`**: Lista todos os objetos dentro de um bucket S3 (similar ao `ls`, mas com mais detalhes).\n   ```bash\n   aws s3api list-objects --bucket nome-do-seu-bucket\n   ```\n\n### Outros Comandos Úteis\n\n1. **`put-bucket-encryption`**: Ativa a criptografia no lado do servidor (Server-Side Encryption) para um bucket.\n   ```bash\n   aws s3api put-bucket-encryption --bucket nome-do-seu-bucket --server-side-encryption-configuration file://encryption.json\n   ```\n\n2. **`put-bucket-policy`**: Define uma política de acesso para o bucket.\n   ```bash\n   aws s3api put-bucket-policy --bucket nome-do-seu-bucket --policy file://policy.json\n   ```\n\n3. **`delete-bucket-policy`**: Remove uma política de acesso de um bucket.\n   ```bash\n   aws s3api delete-bucket-policy --bucket nome-do-seu-bucket\n   ```\n\n4. **`put-bucket-lifecycle-configuration`**: Define as regras de ciclo de vida para o bucket (ex.: mover para o Glacier, expirar objetos antigos).\n   ```bash\n   aws s3api put-bucket-lifecycle-configuration --bucket nome-do-seu-bucket --lifecycle-configuration file://lifecycle.json\n   ```\n\n---\n\n### Resumo dos Comandos:\n\n- **`mb`**: Criar bucket.\n- **`rb`**: Remover bucket.\n- **`cp`**: Copiar arquivos.\n- **`mv`**: Mover arquivos.\n- **`ls`**: Listar buckets ou arquivos.\n- **`rm`**: Remover arquivos.\n- **`sync`**: Sincronizar diretórios/buckets.\n- **`website`**: Configurar bucket para hospedagem de sites.\n- **`presign`**: Gerar URL pré-assinada.\n- **`put-object`** e **`get-object`**: Enviar/baixar arquivos para/do bucket.\n\nEsses são os comandos principais e avançados do AWS CLI para o serviço Amazon S3, abrangendo desde a criação e remoção de buckets até a configuração de permissões e políticas de acesso.\n\nAqui está um fluxo completo que inclui os comandos para **criar**, **verificar** e **deletar** uma instância RDS no AWS, com detalhes e formatação amigável.\n\n### 1. **Criar a Instância RDS**\n\nUse o seguinte comando para criar uma instância MySQL com a classe `db.t4g.micro` e a versão `8.0.35`:\n\n```bash\naws rds create-db-instance \\\n--db-instance-identifier mydbinstance \\\n--db-instance-class db.t4g.micro \\\n--engine mysql \\\n--engine-version 8.0.35 \\\n--master-username admin \\\n--master-user-password password123 \\\n--allocated-storage 20\n```\n\n### 2. **Verificar o Status da Instância RDS**\n\nApós iniciar o processo de criação, você pode verificar o status da instância com o comando abaixo:\n\n```bash\naws rds describe-db-instances --db-instance-identifier mydbinstance\n```\n\nPara uma saída mais organizada e fácil de entender, você pode formatar a resposta em forma de tabela, exibindo o **identificador**, o **status** e o **endpoint** da instância:\n\n```bash\naws rds describe-db-instances \\\n--db-instance-identifier mydbinstance \\\n--query 'DBInstances[*].[DBInstanceIdentifier,DBInstanceStatus,Endpoint.Address]' \\\n--output table\n```\n\nA saída será semelhante a esta quando a instância estiver disponível:\n\n```plaintext\n-------------------------------\n| DBInstanceIdentifier | DBInstanceStatus | Endpoint.Address      |\n-------------------------------\n| mydbinstance          | available        | mydbinstance.xxxxxx.us-east-1.rds.amazonaws.com |\n-------------------------------\n```\n\n### 3. **Script para Monitorar o Status da Instância RDS**\n\nVocê pode monitorar continuamente o status da instância com este script, que verifica o status a cada 30 segundos e exibe uma mensagem quando a instância estiver disponível:\n\n```bash\nwhile true; do\n    STATUS$(aws rds describe-db-instances --db-instance-identifier mydbinstance --query 'DBInstances[0].DBInstanceStatus' --output text)\n    echo \"Status: $STATUS\"\n    if [ \"$STATUS\"  \"available\" ]; then\n        echo \"A instância está disponível!\"\n        break\n    fi\n    sleep 30\ndone\n```\n\n### 4. **Deletar a Instância RDS**\n\nDepois que você terminar de usar a instância, é importante deletá-la para evitar custos adicionais. O comando abaixo exclui a instância RDS:\n\n```bash\naws rds delete-db-instance \\\n--db-instance-identifier mydbinstance \\\n--skip-final-snapshot\n```\n\n- **`--skip-final-snapshot`**: O parâmetro `--skip-final-snapshot` instrui o AWS a **não criar** um snapshot final antes de excluir a instância. Se você quiser criar um snapshot para backup antes de deletar, remova essa opção e adicione `--final-db-snapshot-identifier` seguido do nome do snapshot.\n\n### 5. **Verificar o Status de Deleção**\n\nVocê pode verificar se a instância foi marcada para deleção ou já foi excluída usando o seguinte comando:\n\n```bash\naws rds describe-db-instances --db-instance-identifier mydbinstance\n```\n\nSe a instância estiver em processo de deleção, o status será algo como `deleting`. Uma vez excluída, o comando retornará um erro informando que a instância não existe mais.\n\n### Resumo Completo dos Comandos:\n\n1. **Criar a Instância**:\n   ```bash\n   aws rds create-db-instance \\\n   --db-instance-identifier mydbinstance \\\n   --db-instance-class db.t4g.micro \\\n   --engine mysql \\\n   --engine-version 8.0.35 \\\n   --master-username admin \\\n   --master-user-password password123 \\\n   --allocated-storage 20\n   ```\n\n2. **Verificar o Status**:\n   ```bash\n   aws rds describe-db-instances \\\n   --db-instance-identifier mydbinstance \\\n   --query 'DBInstances[*].[DBInstanceIdentifier,DBInstanceStatus,Endpoint.Address]' \\\n   --output table\n   ```\n\n3. **Deletar a Instância**:\n   ```bash\n   aws rds delete-db-instance \\\n   --db-instance-identifier mydbinstance \\\n   --skip-final-snapshot\n   ```\n\nEsse fluxo completo permite que você crie, verifique o status e depois delete a instância RDS conforme necessário.\n\n### 6. **Automatizando Tarefas com AWS CLI**\n\n   **Objetivo**: Demonstrar como usar scripts para automatizar processos na AWS com o AWS CLI.\n\n   **Exemplo**: Crie um arquivo Bash (.sh) contendo o script que você quer executar. Suponha que o arquivo se chame backup_rds.sh.\n\n   ```bash\n   #!/bin/bash\nDB_INSTANCE_IDENTIFIERmydbinstance\nBACKUP_NAMEdb-backup-$(date +%F)\n\nif aws rds create-db-snapshot --db-instance-identifier $DB_INSTANCE_IDENTIFIER --db-snapshot-identifier $BACKUP_NAME --profile producao; then\n    echo \"Backup criado com sucesso: $BACKUP_NAME\"\nelse\n    echo \"Erro ao criar o backup\"\nfi\n   ```\n\n\n### 7. **Conclusão da Aula 12**\n\n   **Resumo**:\n   - Exploramos o AWS CLI e o AWS CloudShell como duas formas de interagir com a AWS sem depender do console.\n   - Aprendemos a instalar e configurar o AWS CLI no Windows, e vimos como utilizar perfis para gerenciar múltiplas contas.\n   - Demonstramos como criar um banco de dados RDS e automatizar tarefas com scripts.\n\n   **Tarefa de Casa**:\n   - Experimente configurar múltiplos perfis no AWS CLI e realizar operações em diferentes contas ou ambientes.\n   - Crie um script para automatizar a criação e exclusão de buckets S3.\n\n---\n\n**Material de Apoio**:\n- [Documentação oficial do AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html)\n\n",
        "Bootcamp - Cloud para dados/Aula_13/README.md\n\n# Aula 13: Introdução ao Amazon SQS para Engenharia de Dados\n\n**Objetivo**: Nesta aula, vamos explorar o Amazon Simple Queue Service (SQS), sua aplicação na engenharia de dados e como integrá-lo com outros serviços AWS para garantir escalabilidade e desacoplamento de processos. Ao final da aula, você será capaz de criar filas SQS, enviar e receber mensagens de forma programática, e entender como utilizar SQS em pipelines de dados.\n\n![imagem](./pics/foto1.png)\n\nMais informções no site\n\n## **Tópicos da Aula 13**\n\n### 1. **O que é o Amazon SQS?**\n\n   **Objetivo**: Compreender o funcionamento e os conceitos básicos do Amazon SQS.\n\n   **O que é Amazon SQS?**\n   - O Amazon Simple Queue Service (SQS) é um serviço de fila de mensagens completamente gerenciado pela AWS, projetado para ajudar no desacoplamento de componentes em aplicações distribuídas e pipelines de dados.\n   - Com o SQS, você pode enviar, armazenar e receber mensagens entre diferentes partes de uma aplicação de maneira confiável e assíncrona.\n\n   **Vantagens do Amazon SQS**:\n   - **Escalabilidade**: Suporta grandes volumes de mensagens sem se preocupar com a infraestrutura subjacente.\n   - **Desacoplamento**: Permite a separação dos componentes de uma aplicação, o que melhora a modularidade e manutenção.\n   - **Durabilidade**: As mensagens são armazenadas de forma durável até que sejam processadas, com backups automáticos.\n\n### 2. **Tipos de Filas no SQS**\n\n   **Objetivo**: Entender os dois principais tipos de filas disponíveis no SQS: Standard e FIFO.\n\n   - **Standard Queue**: \n     - Oferece entrega em \"melhor esforço\", garantindo alta taxa de transferência, mas sem garantia de ordem exata na entrega das mensagens.\n     - **Caso de uso**: Processos que podem tolerar duplicação ocasional de mensagens e não precisam que as mensagens sejam processadas exatamente na ordem de envio.\n\n   - **FIFO Queue**:\n     - Oferece entrega exatamente uma vez e preserva a ordem das mensagens. Ideal para aplicações em que a ordem exata é crítica.\n     - **Caso de uso**: Processos onde a ordem e entrega única das mensagens são fundamentais, como processamento de transações.\n\n### 3. **Criando uma Fila SQS**\n\n   **Objetivo**: Demonstrar o processo de criação de uma fila SQS no AWS Management Console e via AWS CLI.\n\n   **No Console AWS**:\n   1. Acesse o **AWS Management Console** e procure por **SQS**.\n   2. Clique em **Criar Fila**.\n   3. Escolha o tipo de fila (Standard ou FIFO) e defina as configurações, como tempo de retenção de mensagens e permissões de acesso.\n\nAqui está uma descrição detalhada das opções disponíveis ao criar uma fila Amazon SQS no AWS Management Console:\n\n#### **1. Tipo de Fila (Type)**\n\n   - **Standard**:\n     - **Descrição**: Garante a entrega de cada mensagem ao menos uma vez, mas a ordem exata de entrega não é garantida.\n     - **Recursos**:\n       - **Entrega pelo menos uma vez**: A mensagem pode ser entregue mais de uma vez.\n       - **Melhor esforço na ordenação**: A ordem de entrega das mensagens pode não ser preservada.\n     - **Caso de Uso**: Ideal para sistemas onde a ordem exata de entrega não é crítica, como processamento em massa e comunicação entre microserviços.\n\n   - **FIFO (First-in, First-out)**:\n     - **Descrição**: Garante a entrega das mensagens na ordem em que foram enviadas e exatamente uma vez.\n     - **Recursos**:\n       - **Entrega exatamente uma vez**: As mensagens não serão duplicadas.\n       - **Ordenação garantida**: As mensagens são processadas na ordem de envio.\n     - **Caso de Uso**: Adequado para sistemas onde a ordem das mensagens é crucial, como processamento de transações financeiras.\n\n#### **2. Nome da Fila (Name)**\n\n   - O nome da fila é sensível a maiúsculas e minúsculas e pode ter até 80 caracteres.\n   - Permitidos: Caracteres alfanuméricos, hífens (`-`), e underscores (`_`).\n   - Fila FIFO deve terminar com `.fifo` no nome.\n\n#### **3. Configurações da Fila (Configuration)**\n\n   - **Visibility Timeout** (Timeout de Visibilidade):\n     - **Descrição**: O tempo em segundos durante o qual uma mensagem ficará invisível para outros consumidores após ser lida.\n     - **Padrão**: 30 segundos.\n     - **Intervalo**: Entre 0 segundos e 12 horas.\n     - **Caso de Uso**: Configurar para evitar que mensagens em processamento sejam visíveis para outros consumidores antes que o processo seja concluído.\n\n   - **Message Retention Period** (Período de Retenção de Mensagens):\n     - **Descrição**: O tempo durante o qual uma mensagem será armazenada na fila antes de ser excluída.\n     - **Padrão**: 4 dias.\n     - **Intervalo**: Entre 1 minuto e 14 dias.\n     - **Caso de Uso**: Útil para gerenciar o tempo de vida das mensagens e evitar sobrecarga de armazenamento.\n\n   - **Delivery Delay** (Atraso de Entrega):\n     - **Descrição**: Define um atraso em segundos antes que uma mensagem seja entregue aos consumidores.\n     - **Padrão**: 0 segundos.\n     - **Intervalo**: Entre 0 segundos e 15 minutos.\n     - **Caso de Uso**: Usado para controlar o tempo em que as mensagens ficam disponíveis para consumo.\n\n   - **Maximum Message Size** (Tamanho Máximo da Mensagem):\n     - **Descrição**: O tamanho máximo permitido para uma mensagem.\n     - **Padrão**: 256 KB.\n     - **Intervalo**: Entre 1 KB e 256 KB.\n     - **Caso de Uso**: Definir o limite de tamanho para mensagens grandes ou pequenas, conforme a necessidade do sistema.\n\n   - **Receive Message Wait Time** (Tempo de Espera para Receber Mensagens):\n     - **Descrição**: O tempo máximo que o Amazon SQS aguardará por novas mensagens antes de retornar uma resposta vazia a um consumidor.\n     - **Padrão**: 0 segundos.\n     - **Intervalo**: Entre 0 e 20 segundos.\n     - **Caso de Uso**: Usado para implementar o **long polling**, que reduz o custo de requisições e melhora a eficiência da fila.\n\n#### **4. Criptografia (Encryption)**\n\n   - **Criptografia Server-side (SSE)**:\n     - **Opções**:\n       - **Desativada**: A fila não usará criptografia em repouso.\n       - **Ativada**: A fila usará criptografia para mensagens armazenadas.\n     - **Tipos de Chave**:\n       - **SSE-SQS (Chave SQS)**: Uma chave de criptografia gerenciada pela Amazon SQS.\n       - **SSE-KMS (Chave AWS KMS)**: Uma chave gerenciada pelo AWS Key Management Service (KMS), que oferece mais controle sobre o gerenciamento da chave.\n     - **Caso de Uso**: Para aumentar a segurança de dados em repouso, como em filas que processam informações sensíveis.\n\n#### **5. Políticas de Acesso (Access Policy)**\n\n   - **Método de Configuração**:\n     - **Básico**: Use critérios simples para definir uma política de acesso.\n     - **Avançado**: Use um objeto JSON para definir uma política de acesso mais detalhada.\n\n   - **Quem pode enviar mensagens para a fila**:\n     - **Apenas o proprietário**: Apenas o proprietário da fila pode enviar mensagens.\n     - **AWS Accounts, IAM users and roles especificados**: Especifique IDs de contas da AWS, usuários e roles do IAM que podem enviar mensagens.\n\n   - **Quem pode receber mensagens da fila**:\n     - **Apenas o proprietário**: Apenas o proprietário da fila pode receber mensagens.\n     - **AWS Accounts, IAM users and roles especificados**: Especifique IDs de contas da AWS, usuários e roles do IAM que podem receber mensagens.\n\n   - **JSON (Política de Acesso)**:\n     - É possível definir diretamente uma política em formato JSON, fornecendo um controle mais granular sobre quem pode realizar ações específicas na fila.\n\n#### **6. Política de Redrive Allow (Redrive Allow Policy)**\n\n   - **Descrição**: Determina quais filas de origem podem usar essa fila como fila de \"dead-letter\" (mensagens não entregues).\n   - **Opções**:\n     - **Desabilitada**: A fila não será usada como uma fila \"dead-letter\".\n     - **Habilitada**: Permite que essa fila seja usada como fila \"dead-letter\" por outras filas de origem.\n\n#### **7. Fila Dead-letter (Dead-letter Queue)**\n\n   - **Descrição**: Configura essa fila para receber mensagens não entregáveis de outras filas. Se uma mensagem falhar repetidamente, ela é redirecionada para uma fila \"dead-letter\" para análise posterior.\n   - **Opções**:\n     - **Desabilitada**: A fila não atuará como uma fila \"dead-letter\".\n     - **Habilitada**: A fila pode ser usada para receber mensagens de filas que falharam em processar mensagens após várias tentativas.\n\n#### **8. Tags (Etiquetas - Tags)**\n\n   - **Descrição**: Etiquetas são rótulos associados a um recurso AWS para ajudar a organizar, filtrar e rastrear os custos de recursos.\n   - **Configuração**:\n     - Adicionar até 50 tags no formato chave-valor.\n     - **Exemplo**:\n       - **Key**: `Projeto`\n       - **Value**: `ProcessamentoDeDados`\n\n### 4. **Envio e Recebimento de Mensagens com o CLI**\n\nVerifique suas listas ativas\n\n```bash\naws sqs list-queues\n```\n\nPara evitar a necessidade de adicionar o `--profile jornadadedados` em cada comando, você pode definir o perfil padrão da sessão atual utilizando o comando `export`. Esse comando irá configurar o perfil `AWS_PROFILE`, e todos os comandos subsequentes utilizarão automaticamente esse perfil.\n\n### Configurando o perfil padrão com `export`:\n\n```bash\nexport AWS_PROFILEjornadadedados\n```\n\nAgora, o `AWS CLI` usará o perfil `jornadadedados` por padrão para todos os comandos, sem precisar especificá-lo novamente. Após configurar o perfil com o comando `export`, você pode executar seus comandos como normalmente, sem o parâmetro `--profile`.\n\nVamos explicar em detalhes o passo a passo dos comandos utilizados e a saída recebida, além de configurar o ambiente para não precisar especificar o perfil a cada comando.\n\n#### Como verificar se o perfil foi configurado corretamente:\n\nVocê pode testar a configuração do perfil padrão verificando se o AWS CLI usa o perfil configurado ao executar qualquer comando:\n\n```bash\naws sqs list-queues\n```\n\nSe o comando listar as filas sem exigir a região ou o perfil, significa que a configuração foi aplicada corretamente.\n\n### Criando uma Fila SQS\n\n**Objetivo**: Demonstrar como criar uma fila SQS no AWS Management Console e via AWS CLI.\n\n#### No Console AWS:\n\n1. Acesse o **AWS Management Console** e, na barra de busca, procure por **SQS**.\n2. Clique em **Criar Fila**.\n3. Escolha o tipo de fila que melhor atende às suas necessidades:\n   - **Standard**: Oferece alta taxa de transferência e não garante a ordem exata de entrega das mensagens.\n   - **FIFO**: Garante a entrega em ordem e a entrega única de cada mensagem.\n4. Defina as configurações da fila, como:\n   - Tempo de retenção de mensagens\n   - Visibilidade das mensagens para consumidores\n   - Tamanho máximo das mensagens\n   - Permissões de acesso (quem pode enviar e receber mensagens da fila)\n5. Clique em **Criar Fila** para finalizar a criação.\n\n#### Usando AWS CLI:\n\n- Para criar uma fila **Standard**, execute o seguinte comando no terminal:\n   ```bash\n   aws sqs create-queue --queue-name minha-fila-standard\n   ```\n\n- Para criar uma fila **FIFO**, adicione o atributo `FifoQueuetrue` ao comando:\n   ```bash\n   aws sqs create-queue --queue-name minha-fila.fifo --attributes FifoQueuetrue\n   ```\n\n---\n\n### Envio e Recebimento de Mensagens\n\n**Objetivo**: Demonstrar como enviar, receber e remover mensagens de uma fila SQS de forma programática usando AWS CLI.\n\n#### Enviando Mensagens:\n\nPara enviar uma mensagem para a fila, utilize o comando abaixo, especificando a URL da fila e o corpo da mensagem:\n\n```bash\naws sqs send-message --queue-url URL_DA_FILA --message-body \"Mensagem para a fila\"\n```\n\n#### Recebendo Mensagens:\n\nPara receber mensagens de uma fila SQS, use o comando:\n\n```bash\naws sqs receive-message --queue-url URL_DA_FILA\n```\n\nEsse comando retornará as mensagens que estão na fila, junto com informações como o `ReceiptHandle`, necessário para deletar a mensagem após o processamento.\n\n#### Explicação da Saída:\n\n- **MessageId**: O identificador único da mensagem que foi recebida (mesmo ID da mensagem enviada anteriormente).\n- **ReceiptHandle**: Um identificador temporário que será usado para remover a mensagem da fila após o processamento. Esse handle deve ser utilizado na operação de exclusão de mensagem.\n- **MD5OfBody**: Um hash MD5 da mensagem recebida para garantir a integridade dos dados.\n- **Body**: O conteúdo da mensagem, que é \"Mensagem para a fila\".\n\n#### Removendo Mensagens:\n\nApós processar a mensagem, ela deve ser removida da fila. Para isso, use o `ReceiptHandle` da mensagem recebida:\n\n```bash\naws sqs delete-message --queue-url URL_DA_FILA --receipt-handle HANDLE_DA_MENSAGEM\n```\n\nEsses passos cobrem o ciclo completo de envio, recebimento e remoção de mensagens em uma fila SQS usando o AWS CLI.\n\n### 5. **Integrando o SQS com código Python**\n\n### 6. **Controle de Concorrência e Limite de Taxa**\n\n   **Objetivo**: Explicar como gerenciar a concorrência no processamento de mensagens e configurar limites para evitar sobrecarga.\n\n   - **Controle de Concorrência**: Defina o número máximo de mensagens que um worker pode processar simultaneamente.\n   - **Visibilidade de Mensagens**: Use o parâmetro `Visibility Timeout` para garantir que uma mensagem seja invisível para outros consumidores enquanto está sendo processada.\n\n   - **Exemplo**:\n     ```bash\n     aws sqs change-message-visibility --queue-url URL_DA_FILA --receipt-handle HANDLE_DA_MENSAGEM --visibility-timeout 30\n     ```\n\n### 7. **Boas Práticas com SQS na Engenharia de Dados**\n\n   **Objetivo**: Discutir as melhores práticas ao usar o SQS em projetos de engenharia de dados.\n\n   - **Monitoramento**: Utilize o Amazon CloudWatch para monitorar o número de mensagens não processadas e ajustar a escala dos consumidores.\n   - **Erros e Retries**: Implemente Dead Letter Queues (DLQs) para capturar mensagens que falharam repetidamente no processamento.\n   - **Segurança**: Configure permissões no IAM para restringir quem pode enviar e receber mensagens da fila.\n\n### 8. **Exercício**\n\n   **Objetivo**: Colocar em prática o que foi aprendido nesta aula.\n\n   - **Tarefa**: Crie uma fila SQS, envie e receba mensagens utilizando a AWS CLI. Em seguida, integre o SQS com uma função AWS Lambda para processar as mensagens automaticamente.\n\n### 9. **Conclusão da Aula 13**\n\n   **Resumo**:\n   - Exploramos o Amazon SQS e como ele pode ser utilizado para construir pipelines de dados escaláveis e desacoplados.\n   - Aprendemos a criar filas, enviar e receber mensagens, e integrar o SQS com outros serviços AWS.\n   - Discutimos as melhores práticas para implementar o SQS em projetos de engenharia de dados.\n\n   **Tarefa de Casa**:\n   - Configure uma Dead Letter Queue para gerenciar mensagens com falhas e monitore o uso do SQS com CloudWatch.\n\n--- \n\n**Material de Apoio**:\n- [Documentação oficial do Amazon SQS](https://docs.aws.amazon.com/sqs/)\n\nBootcamp - Cloud para dados/Aula_13/enviando.py\n\nimport boto3\n\n# Criando o cliente SQS\nsqs  boto3.client('sqs')\n\n# URL da fila SQS\nqueue_url  'https://sqs.us-east-1.amazonaws.com/148761673709/minha-fila-standard'\n\n# Enviando uma mensagem para a fila\nresponse  sqs.send_message(\n    QueueUrlqueue_url,\n    MessageBody'Mensagem de exemplo para minha-fila-standard'\n)\n\n# Exibindo o ID da mensagem enviada\nprint(f\"Mensagem enviada com sucesso! ID da mensagem: {response['MessageId']}\")\n\n\nBootcamp - Cloud para dados/Aula_13/recebendo.py\n\nimport boto3\n\n# Criando o cliente SQS\nsqs  boto3.client('sqs')\n\n# URL da fila SQS\nqueue_url  'https://sqs.us-east-1.amazonaws.com/148761673709/minha-fila-standard'\n\n# Recebendo mensagens da fila\nresponse  sqs.receive_message(\n    QueueUrlqueue_url,\n    MaxNumberOfMessages10,  # Número máximo de mensagens a receber\n    WaitTimeSeconds10       # Tempo máximo de espera (em segundos)\n)\n\n# Verificando se há mensagens recebidas\nif 'Messages' in response:\n    for message in response['Messages']:\n        print(f\"Mensagem recebida: {message['Body']}\")\n        \n        # Excluindo a mensagem da fila após o processamento\n        sqs.delete_message(\n            QueueUrlqueue_url,\n            ReceiptHandlemessage['ReceiptHandle']\n        )\n        print(f\"Mensagem excluída: ID {message['MessageId']}\")\nelse:\n    print(\"Nenhuma mensagem disponível na fila.\")\n\n\n",
        "Bootcamp - Cloud para dados/Aula_14/README.md\n\n# Aula 14: Amazon SNS (Simple Notification Service)\n\n## **Objetivo da Aula**\nO foco desta aula é mostrar como engenheiros e analistas de dados podem utilizar o **Amazon SNS** para implementar sistemas de notificação e publicação/assinatura (Pub/Sub) em suas pipelines de dados e outras aplicações distribuídas. Vamos explorar os tópicos, as assinaturas, a configuração de tópicos no console da AWS, e as diferenças principais entre **SNS e SQS**.\n\n## **Conteúdo**\n1. O que é o Amazon SNS?\n2. Diferença entre SNS e SQS.\n3. Demonstração: Criando um tópico no Console AWS.\n4. Como assinar um tópico (subscribers).\n5. Pub/Sub (Publicação/Assinatura) e casos de uso.\n6. Boas práticas e pontos importantes.\n\n## **O que é o Amazon SNS?**\nO **Amazon SNS** (Simple Notification Service) é um serviço de mensageria gerenciado que facilita a criação de sistemas baseados no modelo **Pub/Sub** (publicação/assinatura). Ele é utilizado para enviar notificações ou mensagens de eventos importantes para diferentes serviços ou usuários.\n\n**Por que engenheiros e analistas de dados utilizariam o SNS?**\n- **Notificações em tempo real**: Alertas sobre eventos importantes em pipelines de dados.\n- **Escalabilidade**: SNS pode lidar com grandes volumes de mensagens e entrega quase em tempo real.\n- **Integração entre serviços**: SNS permite conectar diversos serviços AWS como Lambda, SQS, e até endpoints HTTP para automatizar fluxos de trabalho de dados.\n\n### **Tópicos e Assinantes**\n- **Tópico**: Um canal onde as mensagens são publicadas. Os assinantes vinculados a esse tópico recebem as mensagens.\n- **Assinantes (Subscribers)**: Os destinos que recebem as mensagens publicadas. Isso pode incluir serviços como Lambda, SQS, HTTP/S, e-mail, SMS, entre outros.\n\n---\n\n## **Comparação SNS vs SQS**\n\n| **Características**              | **Amazon SNS**                                    | **Amazon SQS**                           |\n|-----------------------------------|---------------------------------------------------|------------------------------------------|\n| **Modelo**                        | Pub/Sub                                           | Fila                                     |\n| **Entrega de mensagens**          | Entrega para múltiplos assinantes simultaneamente    | Entrega individual para consumidores   |\n| **Tipo de comunicação**           | Múltiplos assinantes (fan-out)                    | Ponto-a-ponto                            |\n| **Latência**                      | Baixa latência (entrega quase em tempo real)      | Processamento assíncrono, maior latência |\n| **Método de entrega**             | HTTP/S, Lambda, SQS, SMS, e-mail                  | Apenas consumidores de filas             |\n| **Uso típico**                    | Notificações em tempo real                        | Processamento em segundo plano           |\n| **Mensagens persistidas?**        | Não                                               | Sim, até 14 dias                         |\n| **Garantia de ordem**             | Não no modelo padrão (apenas em FIFO)             | FIFO (quando configurado)                |\n\n### **Mermaid Diagram para SNS e SQS**:\nAqui está uma representação visual para você entender melhor como SNS e SQS funcionam juntos em um sistema Pub/Sub:\n\n```mermaid\ngraph TD\n    A[Publicador] -->|Publica| B[Tópico SNS]\n    B --> C[Lambda]\n    B --> D[SQS]\n    B --> E[Email]\n    B --> F[SMS]\n    B --> G[HTTP/S]\n    C --> H[Processa evento]\n    D --> I[Processa via fila]\n    E --> J[Notificação por Email]\n    F --> K[Notificação por SMS]\n    G --> L[Notificação HTTP/S]\n```\n\n### **Modelo Pub/Sub e Casos de Uso**\nNo modelo **Pub/Sub**, o SNS permite que múltiplos assinantes sejam notificados ao mesmo tempo quando uma mensagem é publicada em um tópico. Esse padrão é especialmente útil para criar notificações em larga escala e orquestrar serviços automatizados.\n\n**Exemplos de uso de SNS:**\n- **Monitoramento de pipelines de dados**: Notificar equipes quando um job de ETL falhar ou for concluído.\n- **Alertas em tempo real**: Notificações para usuários de sistemas quando determinados eventos ocorrerem, como grandes volumes de dados sendo processados ou dados chegando em um banco de dados.\n- **Integração de sistemas**: Usar SNS para conectar diferentes serviços em uma arquitetura de microserviços.\n\n---\n### **Demonstração: Criando um Tópico no Console AWS**\n\n### **Fluxo da Arquitetura (Mermaid)**\n\n```mermaid\nsequenceDiagram\n    participant Publisher\n    participant SNS_Topic\n    participant Lambda_Python\n    participant Lambda_JavaScript\n\n    Publisher->>SNS_Topic: Envia Mensagem {\"nome\": \"Luciano\", \"aula\": \"aula13\"}\n    SNS_Topic->>Lambda_Python: Aciona Lambda Python\n    SNS_Topic->>Lambda_JavaScript: Aciona Lambda JavaScript\n    Lambda_Python->>SNS_Topic: Processa Mensagem\n    Lambda_JavaScript->>SNS_Topic: Processa Mensagem\n```\n\nNeste diagrama, uma mensagem é enviada para o tópico SNS, que então aciona duas funções Lambda: uma escrita em Python e outra em JavaScript, para processar a mesma mensagem de diferentes formas.\n\n---\n\n### **1. Criar um Tópico SNS**\nNo **AWS Management Console**:\n1. Acesse o **Amazon SNS** e clique em **Create Topic**.\n2. Escolha o tipo de tópico:\n   - **FIFO (First-In-First-Out)**: Garante a ordem das mensagens.\n   - **Standard**: Permite alta taxa de mensagens, mas a ordem de entrega não é garantida.\n   \n   Para este exemplo, vamos usar o tipo **Standard**.\n   \n3. Defina o nome do tópico, por exemplo: `MeuTopicoJornadadeDados`.\n4. Complete as demais configurações (como criptografia, políticas de acesso, etc.) e clique em **Create Topic**.\n\n---\n\n### **2. Criar as Funções Lambda**\n\n#### **Função Lambda em Python**\n\n1. Acesse o **AWS Lambda** e clique em **Create Function**.\n2. Escolha **Author from Scratch**.\n3. Dê um nome à sua função, por exemplo: `ProcessaMensagemSNSPython`.\n4. Selecione **Python 3.9** como a linguagem de runtime.\n5. Role até as permissões e configure permissões básicas que permitam à função Lambda ser acionada pelo SNS.\n6. Clique em **Create Function**.\n\n#### **Função Lambda em JavaScript**\n\n1. Repita o processo acima, mas desta vez nomeie a função como `ProcessaMensagemSNSJavaScript`.\n2. Selecione **Node.js 14.x** como a linguagem de runtime.\n3. Configure as permissões como no exemplo anterior.\n\n---\n\n### **3. Escrevendo o Código Lambda**\n\n#### **Código Python**\n\nEste código extrai a mensagem enviada pelo SNS, faz o parsing para JSON e imprime os dados recebidos.\n\n```python\nimport json\n\ndef lambda_handler(event, context):\n    # Extrair a mensagem do evento SNS\n    message  event['Records'][0]['Sns']['Message']\n    \n    # Parsear a mensagem para JSON\n    parsed_message  json.loads(message)\n    \n    # Extrair informações da mensagem\n    nome  parsed_message.get('nome')\n    aula  parsed_message.get('aula')\n    \n    # Imprimir as informações\n    print(f\"Nome: {nome}, Aula: {aula}\")\n    \n    # Retornar a resposta\n    return {\n        'statusCode': 200,\n        'body': json.dumps({'nome': nome, 'aula': aula})\n    }\n```\n\n#### **Código JavaScript**\n\nA função Lambda em JavaScript também extrai e processa a mensagem SNS recebida.\n\n```javascript\nexport const handler  async (event) > {\n    // Extrair a mensagem do evento SNS\n    const message  event.Records[0].Sns.Message;\n    \n    // Parsear a mensagem para JSON\n    let parsedMessage;\n    try {\n        parsedMessage  JSON.parse(message);\n    } catch (error) {\n        console.error(\"Erro ao parsear a mensagem:\", error);\n        return {\n            statusCode: 500,\n            body: JSON.stringify({ error: \"Erro ao processar a mensagem\" })\n        };\n    }\n    \n    // Extrair informações da mensagem\n    const nome  parsedMessage.nome;\n    const aula  parsedMessage.aula;\n    \n    // Imprimir as informações\n    console.log(`Nome: ${nome}, Aula: ${aula}`);\n    \n    // Retornar a resposta\n    return {\n        statusCode: 200,\n        body: JSON.stringify({ nome: nome, aula: aula })\n    };\n};\n```\n\n---\n\n### **4. Configurando o SNS como Trigger para os Lambdas**\n\n1. Volte para o **Console SNS** e selecione o tópico que você criou.\n2. Clique em **Create Subscription**.\n3. Escolha **AWS Lambda** como o protocolo.\n4. Selecione a função Lambda Python (`ProcessaMensagemSNSPython`) e crie a assinatura.\n5. Repita o processo para a função Lambda JavaScript (`ProcessaMensagemSNSJavaScript`).\n\nAgora, as duas funções Lambda serão acionadas automaticamente sempre que uma mensagem for publicada no tópico SNS.\n\n---\n\n### **5. Publicando Mensagens no SNS**\n\n#### **Exemplo de Mensagem SNS Válida**\nA mensagem enviada para o SNS deve estar formatada como JSON:\n\n```json\n{\n  \"nome\": \"Luciano\",\n  \"aula\": \"aula13\"\n}\n```\n\n#### **Publicação Usando Python (com boto3)**\n\nAqui está um exemplo de código Python para publicar uma mensagem no SNS:\n\n```python\nimport boto3\nimport json\n\n# Criar o cliente SNS\nsns_client  boto3.client('sns', region_name'us-east-2')\n\n# Criar a mensagem JSON\nmessage  {\n    \"nome\": \"Luciano\",\n    \"aula\": \"aula13\"\n}\n\n# Publicar a mensagem no tópico SNS\nresponse  sns_client.publish(\n    TopicArn'arn:aws:sns:us-east-2:148761673709:MeuTopico',\n    Messagejson.dumps(message)\n)\n\nprint(response)\n```\n\n#### **Publicação Usando AWS CLI**\nSe preferir usar a CLI da AWS, publique uma mensagem no SNS com o seguinte comando:\n\n```bash\naws sns publish \\\n    --topic-arn arn:aws:sns:us-east-2:148761673709:MeuTopico \\\n    --message '{\"nome\": \"Luciano\", \"aula\": \"aua13\"}' \\\n    --region us-east-2\n```\n\n#### **Publicação via Console AWS**\n1. No Console AWS, navegue até o tópico SNS que você criou.\n2. Clique em **Publish Message**.\n3. No campo **Message Body**, insira a mensagem JSON como mostrado acima.\n4. Clique em **Publish** para enviar a mensagem.\n\n---\n\n### **6. Verificando a Mensagem no Lambda**\n\n1. Acesse o **AWS CloudWatch**.\n2. Vá até os logs da função Lambda.\n3. Verifique o log da execução e veja se as informações da mensagem foram processadas corretamente.\n   \nPara cada mensagem enviada ao tópico SNS, ambas as funções Lambda (Python e JavaScript) serão acionadas, processando os dados e registrando-os nos logs.\n\n### **Conclusão**\n\nEste exemplo demonstra como integrar o **SNS** com **Lambda** para processar mensagens em tempo real. Essa arquitetura é poderosa para pipelines de dados, alertas e orquestração de serviços, fornecendo escalabilidade e desacoplamento dos componentes.\n\nAqui está um exemplo de uma ETL simples que transforma um dicionário em um arquivo CSV e, no final, envia uma mensagem para o Amazon SNS indicando que a ETL foi bem-sucedida:\n\n### **Código Python para ETL com notificação SNS**\n\n```python\nimport csv\nimport boto3\nimport json\n\n# Função para converter o dicionário em CSV\ndef dicionario_para_csv(dicionario, nome_arquivo):\n    # Abrir arquivo CSV para escrita\n    with open(nome_arquivo, mode'w', newline'') as file:\n        writer  csv.DictWriter(file, fieldnamesdicionario[0].keys())\n        writer.writeheader()\n        writer.writerows(dicionario)\n    print(f\"Arquivo CSV '{nome_arquivo}' gerado com sucesso!\")\n\n# Função para enviar notificação SNS\ndef enviar_notificacao_sns(topico_arn, mensagem):\n    sns_client  boto3.client('sns', region_name'us-east-2')\n    response  sns_client.publish(\n        TopicArntopico_arn,\n        Messagejson.dumps(mensagem)\n    )\n    print(\"Notificação enviada:\", response)\n\n# ETL principal\ndef executar_etl():\n    # Dicionário de exemplo\n    dados  [\n        {\"nome\": \"Luciano\", \"idade\": 34, \"profissao\": \"Engenheiro de Dados\"},\n        {\"nome\": \"Maria\", \"idade\": 29, \"profissao\": \"Cientista de Dados\"},\n        {\"nome\": \"José\", \"idade\": 45, \"profissao\": \"Analista de Dados\"}\n    ]\n    \n    # Nome do arquivo CSV a ser gerado\n    nome_arquivo_csv  'dados.csv'\n    \n    # Executar transformação (dicionário para CSV)\n    dicionario_para_csv(dados, nome_arquivo_csv)\n    \n    # Enviar notificação para o SNS\n    mensagem_sns  {\n        \"status\": \"Sucesso\",\n        \"descricao\": \"A ETL foi executada com sucesso e o arquivo CSV foi gerado.\"\n    }\n    \n    # ARN do tópico SNS (substitua pelo seu ARN de SNS)\n    topico_arn  'arn:aws:sns:us-east-2:148761673709:MeuTopico'\n    \n    # Enviar a notificação\n    enviar_notificacao_sns(topico_arn, mensagem_sns)\n\n# Executar a ETL\nexecutar_etl()\n```\n\n### **Explicação**:\n1. **Dicionário para CSV**: A função `dicionario_para_csv` pega o dicionário de dados e o transforma em um arquivo CSV. \n   - Neste exemplo, os dados consistem em uma lista de dicionários, onde cada dicionário representa uma linha no arquivo CSV.\n2. **Notificação SNS**: A função `enviar_notificacao_sns` usa o cliente boto3 para enviar uma mensagem JSON para um tópico SNS, notificando que a ETL foi bem-sucedida.\n3. **ETL Principal**: A função `executar_etl` executa a lógica de transformação dos dados em CSV e, em seguida, envia uma notificação ao SNS.\n\n### **Mensagem de Notificação SNS**:\nA mensagem SNS enviada ao final indica que a ETL foi concluída com sucesso:\n\n```json\n{\n    \"status\": \"Sucesso\",\n    \"descricao\": \"A ETL foi executada com sucesso e o arquivo CSV foi gerado.\"\n}\n```\n\n### **Output esperado**:\n- Um arquivo CSV chamado `dados.csv` será gerado.\n- A notificação será enviada ao SNS informando que a ETL foi concluída com sucesso.\n\nEste exemplo pode ser usado para automações de ETL em pipelines de dados e pode ser integrado com outros serviços na AWS para notificações e monitoramento.\n\n",
        "Bootcamp - Cloud para dados/Aula_15/README.md\n\n# Aula 15: Construção de uma Página de Sorteio com AWS\n\n## **Objetivo da Aula**\n\nNesta aula, vamos construir uma página de sorteio automática usando AWS. O usuário poderá inserir o nome do sorteio, o número mínimo e o número máximo, e nossa aplicação, utilizando AWS Lambda, retornará um número aleatório dentro desse intervalo. Vamos dividir o projeto em duas etapas principais, utilizando o AWS Amplify para hospedar a página e AWS Lambda para a lógica de geração do número.\n\n## **Conteúdo**\n1. Estruturação da aplicação.\n2. Criando a página web com Amplify (Etapa 1 e Etapa 2).\n3. Configuração da lógica de sorteio com Lambda.\n4. Integração com API Gateway.\n5. Persistindo dados com DynamoDB (opcional).\n6. Controle de permissões com IAM.\n7. Conclusão e boas práticas.\n\n---\n\n## **1. Estruturação da Aplicação**\n\nVamos criar uma aplicação simples, onde os usuários poderão realizar sorteios online. A página web será criada em duas etapas: a primeira será bem básica, e na segunda, vamos melhorar a interação e adicionar a funcionalidade de sorteio.\n\nOs dados que o usuário fornecerá serão:\n- Nome do sorteio\n- Número mínimo\n- Número máximo\n\nCom essas informações, uma função Lambda será acionada para gerar um número aleatório entre os números fornecidos.\n\n---\n\n## **2. Criando a Página Web com Amplify**\n\n### **O que é o Amplify?**\nO AWS Amplify é uma ferramenta que facilita a criação e hospedagem de páginas web. Vamos utilizá-lo para hospedar nossa página de sorteio.\n\n### **Etapa 1: Página Simples**\n\nNesta primeira etapa, vamos criar uma página estática simples com HTML, para garantir que a estrutura está funcionando corretamente.\n\n#### **Passo a Passo: Criando a página HTML simples**\n\n1. Crie um arquivo `index.html` com o seguinte conteúdo:\n\n   ```html\n   <!DOCTYPE html>\n   <html>\n   <head>\n       <title>Sorteio Automático</title>\n   </head>\n   <body>\n       <h1>Essa é nossa página de sorteio</h1>\n       <p>Agora os nossos sorteios vão ficar automáticos!</p>\n   </body>\n   </html>\n   ```\n\n2. **Empacotamento e Upload para o Amplify:**\n   - Comprima o arquivo `index.html` em um arquivo `.zip`.\n   - No console da AWS, vá até **AWS Amplify** e crie um novo aplicativo.\n   - Selecione a opção de **\"Host a web app\"**.\n   - Faça o upload do arquivo `.zip` e implante o aplicativo.\n\nAgora você tem uma página web simples com a estrutura inicial do seu projeto de sorteio.\n\n---\n\n## **3. Configurando a Lógica de Sorteio com Lambda**\n\n### **O que é o Lambda?**\nO AWS Lambda permite executar código em resposta a eventos. Vamos usá-lo para gerar um número aleatório entre o número mínimo e o máximo fornecido pelo usuário.\n\n#### **Passo a Passo: Criando a Função Lambda**\n\n1. No console da AWS, acesse **AWS Lambda** e crie uma nova função chamada `SorteioFunction`.\n2. Selecione **Python 3.9** como a linguagem de execução.\n3. Substitua o código padrão pelo seguinte:\n\n```python\nimport json\nimport random\n\ndef lambda_handler(event, context):\n    nome_sorteio  event['nomeSorteio']\n    num_min  int(event['numMin'])\n    num_max  int(event['numMax'])\n    numero_sorteado  random.randint(num_min, num_max)\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps({\n            'nomeSorteio': nome_sorteio,\n            'numeroSorteado': numero_sorteado\n        })\n    }\n```\n\nAqui está um body em JSON para testar essa função Lambda:\n\n```json\n{\n    \"nomeSorteio\": \"Sorteio Aula 15\",\n    \"numMin\": 1,\n    \"numMax\": 15\n}\n```\n\nEste body passará o nome do sorteio como \"Sorteio Aula 15\" e sorteará um número aleatório entre 1 e 15.\n\n4. **Salve** e **implante** a função.\n\nEssa função receberá os números mínimo e máximo e retornará um número aleatório entre eles.\n\n---\n\n## **4. Integração com API Gateway**\n\n### **O que é o API Gateway?**\nO AWS API Gateway nos permite criar uma API REST para invocar a função Lambda a partir da página web.\n\n#### **Passo a Passo: Configurando o API Gateway**\n\n1. Acesse o **API Gateway** no console da AWS e crie uma nova API do tipo **REST API**.\n2. Crie um método **POST** vinculado à função Lambda (`SorteioFunction`).\n3. Copie a URL de invocação da API, que será usada no próximo passo para conectar o frontend ao backend.\n\n---\n\n## **5. Persistindo Dados no DynamoDB**\n\nAgora que já temos a página de sorteio e a função Lambda configurada para realizar o sorteio, o próximo passo é armazenar os dados do sorteio no DynamoDB. Vamos criar uma tabela DynamoDB onde vamos salvar as seguintes informações:\n- Nome do sorteio\n- Número mínimo\n- Número máximo\n- Número sorteado\n\n### **Passo a Passo: Criando a Tabela DynamoDB**\n\n1. No console da AWS, vá até o serviço **DynamoDB** e clique em **Create Table**.\n2. Defina o nome da tabela como `Sorteios`.\n3. A chave de partição será `SorteioID` (tipo String).\n4. Conclua a criação da tabela.\n\nAgora, a tabela `Sorteios` está pronta para receber os dados dos sorteios.\n\n---\n\n## **6. Atualizando a Função Lambda para Persistir os Dados no DynamoDB**\n\nAlém de realizar o sorteio, agora a função Lambda vai salvar no DynamoDB o nome do sorteio, o intervalo de números (mínimo e máximo) e o número sorteado. Vamos atualizar a função Lambda para incluir essa lógica.\n\n### **Passo a Passo: Atualizando a Função Lambda**\n\nAtualize o código da sua função Lambda para persistir os dados no DynamoDB:\n\n```python\nimport json\nimport random\nimport boto3\nfrom time import gmtime, strftime\n\n# Criar o objeto DynamoDB usando o SDK boto3\ndynamodb  boto3.resource('dynamodb')\n# Selecionar a tabela que criamos\ntable  dynamodb.Table('Sorteios')\n\n# Função Lambda que realiza o sorteio e persiste os dados\ndef lambda_handler(event, context):\n    nome_sorteio  event['nomeSorteio']\n    num_min  int(event['numMin'])\n    num_max  int(event['numMax'])\n    numero_sorteado  random.randint(num_min, num_max)\n    \n    # Salvar os dados no DynamoDB\n    response  table.put_item(\n        Item{\n            'SorteioID': strftime(\"%Y%m%d%H%M%S\", gmtime()),  # Gerar um ID único baseado no horário\n            'NomeSorteio': nome_sorteio,\n            'NumeroMin': num_min,\n            'NumeroMax': num_max,\n            'NumeroSorteado': numero_sorteado,\n            'DataSorteio': strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())  # Data e hora do sorteio\n        }\n    )\n    \n    # Retornar o resultado do sorteio\n    return {\n        'statusCode': 200,\n        'body': json.dumps({\n            'nomeSorteio': nome_sorteio,\n            'numeroSorteado': numero_sorteado\n        })\n    }\n```\n\n### **Explicação do Código:**\n- **boto3**: Usamos o boto3 para interagir com o DynamoDB.\n- **strftime**: Geramos um ID único para cada sorteio baseado na data e hora em que ele foi realizado.\n- **put_item**: Salvamos os dados do sorteio (nome, números mínimo e máximo, número sorteado, e a data do sorteio) na tabela DynamoDB.\n\n### **Atualizando o Frontend (Formulário HTML)**\n\n### Aula Completa: Criando um Sistema de Sorteio com AWS e Frontend Integrado\n\nNesta aula, vamos implementar uma aplicação completa de sorteio, integrando frontend, AWS Lambda, DynamoDB e API Gateway. O usuário poderá inserir o nome do sorteio, o número mínimo e o número máximo. A função Lambda, acionada via API Gateway, retornará um número aleatório dentro desse intervalo e salvará os detalhes no DynamoDB.\n\n---\n\n### **7. Frontend: Página de Sorteio**\n\nVamos começar criando o frontend da nossa aplicação. Ele terá um formulário que captura os dados do sorteio e chama a API para realizar o sorteio.\n\n#### **Código HTML do Frontend:**\n\nAqui está o código HTML para a página de sorteio. Vamos aplicar um design simples com tons de azul.\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset\"UTF-8\">\n    <title>Sorteio Automático</title>\n    <!-- Estilização da página -->\n    <style>\n    h1 {\n        color: #FFFFFF;\n        font-family: system-ui;\n        margin-left: 20px;\n    }\n    body {\n        background-color: #003366;\n    }\n    label {\n        color: #66CCFF;\n        font-family: system-ui;\n        font-size: 20px;\n        margin-left: 20px;\n        margin-top: 20px;\n    }\n    button {\n        background-color: #66CCFF;\n        border-color: #66CCFF;\n        color: #FFFFFF;\n        font-family: system-ui;\n        font-size: 20px;\n        font-weight: bold;\n        margin-left: 30px;\n        margin-top: 20px;\n        width: 140px;\n    }\n    input {\n        color: #003366;\n        font-family: system-ui;\n        font-size: 20px;\n        margin-left: 10px;\n        margin-top: 20px;\n        width: 100px;\n    }\n    </style>\n    <script>\n        // Função para chamar a API\n        var realizarSorteio  (nomeSorteio, numMin, numMax) > {\n            var myHeaders  new Headers();\n            myHeaders.append(\"Content-Type\", \"application/json\");\n\n            var raw  JSON.stringify({\n                \"nomeSorteio\": nomeSorteio,\n                \"numMin\": numMin,\n                \"numMax\": numMax\n            });\n\n            var requestOptions  {\n                method: 'POST',\n                headers: myHeaders,\n                body: raw,\n                redirect: 'follow'\n            };\n\n            fetch(\"YOUR_API_GATEWAY_ENDPOINT\", requestOptions)\n            .then(response > response.text())\n            .then(result > alert(\"Número sorteado: \" + JSON.parse(result).numeroSorteado))\n            .catch(error > console.log('error', error));\n        }\n    </script>\n</head>\n<body>\n    <h1>Bem-vindo ao Sorteio Automático!</h1>\n    <form>\n        <label>Nome do sorteio:</label>\n        <input type\"text\" id\"nomeSorteio\"><br>\n        <label>Número Mínimo:</label>\n        <input type\"number\" id\"numMin\"><br>\n        <label>Número Máximo:</label>\n        <input type\"number\" id\"numMax\"><br>\n        <!-- Botão que chama a função realizarSorteio -->\n        <button type\"button\" onclick\"realizarSorteio(\n            document.getElementById('nomeSorteio').value,\n            document.getElementById('numMin').value,\n            document.getElementById('numMax').value\n        )\">Realizar Sorteio</button>\n    </form>\n</body>\n</html>\n```\n\nBootcamp - Cloud para dados/Aula_15/Policy.txt\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Sid\": \"VisualEditor0\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"dynamodb:PutItem\",\n            \"dynamodb:DeleteItem\",\n            \"dynamodb:GetItem\",\n            \"dynamodb:Scan\",\n            \"dynamodb:Query\",\n            \"dynamodb:UpdateItem\"\n        ],\n        \"Resource\": \"arn:aws:dynamodb:us-east-1:148761673709:table/SorteioJornada\"\n    }\n    ]\n}\n\nBootcamp - Cloud para dados/Aula_15/index.html\n\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset\"UTF-8\">\n    <title>Sorteio Automático</title>\n    <!-- Estilização da página -->\n    <style>\n    h1 {\n        color: #FFFFFF;\n        font-family: system-ui;\n        margin-left: 20px;\n    }\n    body {\n        background-color: #003366;\n    }\n    label {\n        color: #66CCFF;\n        font-family: system-ui;\n        font-size: 20px;\n        margin-left: 20px;\n        margin-top: 20px;\n    }\n    button {\n        background-color: #66CCFF;\n        border-color: #66CCFF;\n        color: #FFFFFF;\n        font-family: system-ui;\n        font-size: 20px;\n        font-weight: bold;\n        margin-left: 30px;\n        margin-top: 20px;\n        width: 140px;\n    }\n    input {\n        color: #003366;\n        font-family: system-ui;\n        font-size: 20px;\n        margin-left: 10px;\n        margin-top: 20px;\n        width: 100px;\n    }\n    </style>\n    <script>\n        // Função para chamar a API\n        var realizarSorteio  (nomeSorteio, numMin, numMax) > {\n            var myHeaders  new Headers();\n            myHeaders.append(\"Content-Type\", \"application/json\");\n\n            var raw  JSON.stringify({\n                \"nomeSorteio\": nomeSorteio,\n                \"numMin\": numMin,\n                \"numMax\": numMax\n            });\n\n            var requestOptions  {\n                method: 'POST',\n                headers: myHeaders,\n                body: raw,\n                redirect: 'follow'\n            };\n\n            fetch(\"https://5bciix4nk2.execute-api.us-east-2.amazonaws.com/devc\", requestOptions)\n            .then(response > response.text())\n            .then(result > alert(\"Número sorteado: \" + JSON.parse(result).numeroSorteado))\n            .catch(error > console.log('error', error));\n        }\n    </script>\n</head>\n<body>\n    <h1>Bem-vindo ao Sorteio Automático!</h1>\n    <form>\n        <label>Nome do sorteio:</label>\n        <input type\"text\" id\"nomeSorteio\"><br>\n        <label>Número Mínimo:</label>\n        <input type\"number\" id\"numMin\"><br>\n        <label>Número Máximo:</label>\n        <input type\"number\" id\"numMax\"><br>\n        <!-- Botão que chama a função realizarSorteio -->\n        <button type\"button\" onclick\"realizarSorteio(\n            document.getElementById('nomeSorteio').value,\n            document.getElementById('numMin').value,\n            document.getElementById('numMax').value\n        )\">Realizar Sorteio</button>\n    </form>\n</body>\n</html>\n\n\n",
        "Bootcamp - Cloud para dados/Aula_16_17/README.md - Parte (1/2)\nBootcamp - Cloud para dados/Aula_16_17/README.md\n\n# **Bootcamp Cloud - Aula 16: Introdução ao Azure para Dados**  \n\n---\n\n### **Objetivo:**  \nExplorar o ambiente do Azure, abordando a criação de conta, uso do **Azure Blob Storage** (equivalente ao S3), **máquinas virtuais (VMs)** para processamento de dados e o **Azure Active Directory (AAD)**, que substitui o IAM para gerenciamento de identidade e controle de acesso.\n\n---\n\n## Criação de Conta no Azure  \n1. Acesse [portal.azure.com](https://portal.azure.com) e selecione **Create a Free Account**.  \n2. Preencha as informações básicas: nome completo, telefone, e-mail e senha.  \n3. Adicione as informações de **cartão de crédito** (somente para verificação; não haverá cobranças iniciais).  \n4. Escolha o plano gratuito:  \n   - 750 horas de uma VM B1S por mês (Ubuntu ou Windows).  \n   - 5 GB gratuitos no Blob Storage.  \n   - Banco de dados SQL com 250 GB gratuitos.  \n5. No **Cost Management**, configure alertas para controlar custos e crie um orçamento mensal de teste.\n\n---\n\n## Assinaturas no Azure  \nUma **Assinatura do Azure** é uma unidade lógica para gerenciar o acesso, controle de custos e recursos. Permite organizar e isolar recursos como máquinas virtuais, bancos de dados e serviços.  \n\n### Funções e Finalidades  \n- **Gestão de Recursos e Acesso:** Define permissões específicas para equipes e projetos.  \n- **Faturamento e Controle de Custos:** Cada assinatura tem seu próprio relatório de faturamento.  \n- **Isolamento e Limites:** Permite separar ambientes de produção, desenvolvimento e teste.  \n- **Governança:** As assinaturas aplicam políticas e limites via Grupos de Gerenciamento e AAD.\n\n### Tipos de Assinaturas  \n- **Plano Gratuito:** Oferece crédito inicial e limites gratuitos por 12 meses.  \n- **Pay-As-You-Go:** Paga-se apenas pelo uso.  \n- **Enterprise Agreement (EA):** Contrato com desconto por volume para grandes empresas.  \n- **Cloud Solution Provider (CSP):** Parceiros que revendem serviços Azure.\n\n---\n\n## Equivalente na AWS  \nNa AWS, o conceito mais próximo de uma assinatura do Azure é uma **Conta AWS**.  \n\n- **AWS Organizations:** Permite consolidar e gerenciar várias contas AWS sob uma organização.  \n- **Controle de Custos:** Cada conta tem seus próprios alertas e relatórios de custo.  \n\n### Comparação entre Assinatura do Azure e Conta AWS  \n| Azure (Assinatura)             | AWS (Conta AWS)                       |\n|---------------------------------|----------------------------------------|\n| Separa recursos e define limites | Isola ambientes e projetos            |\n| Gerenciada por AAD              | Gerenciada por IAM e AWS Organizations |\n| Faturamento por assinatura      | Faturamento por conta                 |\n\n---\n\n## Projeto 1. Acessando variáveis de ambiente no Azure\n\n```mermaid\nsequenceDiagram\n    participant User as Usuário (Local)\n    participant AzureAD as Azure AD\n    participant AppReg as App Registration\n    participant KeyVault as Azure Key Vault\n    participant IAMUser as IAM do Usuário\n    participant IAMService as IAM do Serviço\n\n    User->>AzureAD: 1. Solicita Token de Acesso\n    AzureAD-->>User: 2. Retorna Token\n\n    User->>AppReg: 3. Envia Credenciais e Token\n    AppReg-->>User: 4. Validação e Permissão\n\n    User->>KeyVault: 5. Solicita Segredo (com Token)\n    KeyVault-->>IAMUser: 6. Verifica Permissões do Usuário\n    KeyVault-->>IAMService: 7. Verifica Permissões do Serviço\n\n    KeyVault-->>User: 8. Retorna Valor do Segredo\n\n    User->>User: 9. Exibe Segredo no Terminal\n```\n\n---\n\n### **Azure Key Vault: O que é e para que serve?**\n\n### **Objetivo:**\nO **Azure Key Vault** é um serviço de nuvem da Microsoft projetado para gerenciar segredos, chaves de criptografia e certificados de maneira segura. Seu principal objetivo é fornecer uma forma centralizada e protegida de armazenar e acessar informações sensíveis, como senhas, tokens e chaves criptográficas, minimizando riscos de segurança.\n\n### **Funcionalidades do Azure Key Vault:**\n\n1. **Armazenamento Seguro de Segredos:**\n   - Gerencia senhas, strings de conexão, tokens de API e outras informações confidenciais.\n   - Fornece acesso seguro aos segredos por meio de autenticação robusta e controle de permissões via **Azure AD**.\n\n2. **Gerenciamento de Chaves de Criptografia:**\n   - Armazena e protege chaves usadas para criptografia de dados em repouso e em trânsito.\n   - Oferece suporte para criptografia assimétrica e simétrica, podendo ser integrado a outros serviços do Azure, como SQL Database.\n\n3. **Gestão de Certificados:**\n   - Automatiza o processo de renovação e gerenciamento de certificados SSL/TLS.\n   - Permite criar e manter certificados seguros que podem ser utilizados por serviços na nuvem ou locais.\n\n4. **Controle de Acesso Granular:**\n   - Usa **IAM (Identity and Access Management)** para controlar quais aplicações e usuários podem acessar os segredos e chaves.\n   - Mantém logs de auditoria de todas as interações com o Key Vault.\n\n### **Benefícios:**\n- **Centralização:** Consolida a gestão de segredos e chaves, evitando que sejam armazenados em diferentes locais de forma não segura.\n- **Automação:** Facilita a renovação automática de certificados e permite que aplicações acessem segredos sem intervenção manual.\n- **Conformidade:** Ajuda a cumprir regulamentações de segurança e privacidade, mantendo as informações críticas em um ambiente protegido.\n- **Alta Disponibilidade:** O serviço é distribuído por regiões do Azure, garantindo que segredos e chaves estejam sempre acessíveis.\n\n---\n\n### **Comparação com AWS Secrets Manager:**\n\n| **Azure Key Vault**                    | **AWS Secrets Manager**                |\n|----------------------------------------|----------------------------------------|\n| Gerencia segredos, chaves e certificados | Gerencia segredos e credenciais |\n| Integra-se com serviços Azure, como SQL Database | Integra-se com serviços AWS, como RDS |\n| Oferece criptografia usando HSM (Hardware Security Modules) | Oferece criptografia e rotação automática de segredos |\n| Controle de acesso via Azure AD        | Controle de acesso via IAM |\n| Preços por transação e armazenamento   | Preços por segredos armazenados |\n\n---\n\n### **Nosso primeiro Erro**\n\nImportância do IAM\nO uso do IAM (Identity and Access Management) é essencial para criar e gerenciar um Key Vault. Ele permite configurar controle de acesso baseado em papéis (RBAC), garantindo que apenas usuários ou aplicações autorizadas possam acessar ou modificar segredos e chaves.\n\nIAM no Azure é o serviço responsável por gerenciar identidades e permissões, assegurando que cada recurso ou aplicação tenha apenas os acessos necessários. Ele garante uma abordagem de segurança baseada em princípio de menor privilégio.\n\n## **View Exemple**\n\n### **\"View Example\" no Portal Azure Key Vault**\n\nA opção **\"View Example\"** (ou \"Ver Exemplo\") na interface do **Key Vault** facilita o entendimento e uso do serviço. Esta funcionalidade:\n\n1. **Demonstração de Exemplos Práticos:**  \n   - Mostra exemplos de como armazenar e acessar chaves e segredos.\n   - Fornece código de amostra, muitas vezes em **Python, C#, ou PowerShell**, para acessar os segredos via SDK.\n\n2. **Ajuda na Automação:**  \n   - Exemplos incluem snippets para automação com **Azure CLI** ou scripts para aplicações.\n   - Acelera o processo de integração com outras aplicações e pipelines de dados.\n\n3. **Facilita a Configuração Inicial:**  \n   - Apresenta instruções claras de como criar e acessar segredos, chaves e certificados.\n   - Demonstra como configurar corretamente o acesso via **IAM** ou atribuir permissões específicas.\n\nA funcionalidade **\"View Example\"** é uma poderosa ferramenta educacional e prática no portal Azure. Ela facilita a adoção do Azure Key Vault ao fornecer exemplos de código claros, economizando tempo e simplificando o processo de integração. Com essa abordagem orientada a exemplos, é possível configurar o acesso seguro e garantir a conformidade com as boas práticas de segurança na nuvem.\n\n---\n\n## **Azure Active Directory (Azure AD)**\n\n### **Visão Geral**\n\nO **Azure Active Directory (Azure AD)** é o serviço de gerenciamento de identidade e acesso baseado na nuvem da Microsoft. Ele permite que as organizações administrem identidades e controlem o acesso a recursos em nuvem e on-premises, além de aplicativos SaaS. \n\n---\n\n### **Objetivos do Azure AD**\n\n1. **Autenticação e Autorização**  \n   - Verifica identidades para permitir ou negar o acesso a recursos e aplicativos.\n   - Suporta métodos como senha, MFA (autenticação multi-fator) e biometria.\n\n2. **Gerenciamento de Usuários e Grupos**  \n   - Permite criar usuários e organizar grupos para definir permissões e acessos.\n   - Facilita a ativação e desativação de contas de usuários.\n\n3. **Integração com Aplicativos SaaS**  \n   - Oferece autenticação única (SSO) para diversos aplicativos, como Salesforce e Google Workspace.\n   - Melhora a experiência do usuário eliminando a necessidade de múltiplas senhas.\n\n4. **Políticas de Acesso e Segurança**  \n   - Controla o acesso com base na localização e tipo de dispositivo.\n   - Oferece políticas de acesso condicional para aumentar a segurança.\n\n5. **Autenticação para APIs e Aplicativos Customizados**  \n   - Desenvolvedores podem integrar o Azure AD para autenticação segura em suas APIs e aplicações.\n\n---\n\n### **Passo a Passo: Configuração no Azure AD**\n\n#### **1. Acessar o Azure AD no Portal**\n- Vá para o [portal do Azure](https://portal.azure.com).\n- No menu, clique em **Azure Active Directory**.\n\n#### **2. Registrar um Aplicativo no Azure AD**\n- Clique em **App Registrations** > **New Registration**.\n- Dê um nome ao aplicativo e escolha a conta ou organização que terá acesso.\n- Clique em **Register** para finalizar.\n\n#### **3. Criar um Client Secret**\n- Na aplicação registrada, vá para **Certificates & Secrets**.\n- Clique em **New Client Secret**.\n- Adicione uma descrição (opcional) e defina a validade do segredo (6, 12 ou 24 meses).\n- Ao clicar em **Add**, o segredo será gerado. **Copie o valor agora** – ele não será exibido novamente.\n\n#### **4. Configurar IAM para o Serviço**\n- Volte para o **Azure Key Vault** ou outro serviço relevante.\n- Vá para **Access Control (IAM)** > **Role Assignments**.\n- Adicione um novo papel, como **Key Vault Secrets User** ou **Reader**.\n- Em **Select Members**, selecione o **App Registration** que você criou.\n\n---\n\n### **Relação entre Azure AD e AWS IAM**\n\n| **Azure AD**                                | **AWS IAM**                                  |\n|---------------------------------------------|----------------------------------------------|\n| Gerencia identidade de usuários e autenticação. | Gerencia permissões de recursos na AWS. |\n| Oferece SSO e políticas de acesso condicional. | Configura políticas detalhadas para acesso a recursos. |\n| Integrado com Office 365 e aplicativos SaaS. | Integrado com serviços AWS, como EC2 e S3. |\n| Suporta políticas baseadas em usuário e dispositivo. | Suporta permissões detalhadas por função e políticas. |\n\n---\n\n### **Conclusão**\n\nO **Azure AD** é essencial para a autenticação e controle de identidades em ambientes Microsoft e aplicativos SaaS. Ele complementa o **IAM da AWS**, que é mais voltado para gerenciar permissões de recursos. Integrar o Azure AD com a AWS pode proporcionar uma experiência de autenticação unificada e simplificar a gestão de identidades e acessos.\n\n## Projeto 2. Consumindo arquivos no Blob Storage\n\n```mermaid\nsequenceDiagram\n    participant User as Usuário (Local)\n    participant AzureAD as Azure AD\n    participant AppReg as App Registration\n    participant BlobStorage as Azure Blob Storage\n    participant IAMUser as IAM do Usuário\n    participant IAMService as IAM do Serviço\n\n    User->>AzureAD: 1. Solicita Token de Acesso\n    AzureAD-->>User: 2. Retorna Token\n\n    User->>AppReg: 3. Envia Credenciais e Token\n    AppReg-->>User: 4. Validação e Permissão\n\n    User->>BlobStorage: 5. Solicita Listagem de Arquivos (com Token)\n    BlobStorage-->>IAMUser: 6. Verifica Permissões do Usuário\n    BlobStorage-->>IAMService: 7. Verifica Permissões do Serviço\n\n    BlobStorage-->>User: 8. Retorna Lista de Arquivos\n\n    User->>BlobStorage: 9. Baixa Arquivo Específico\n    BlobStorage-->>User: 10. Envia Arquivo\n\n    User->>User: 11. Exibe ou Processa Arquivo Localmente\n```\n\n### **Projeto: Acessar e Fazer Upload de Arquivos no Azure Blob Storage com Python SDK**\n\nEste passo a passo mostrará como criar uma **Storage Account no Azure**, configurar permissões usando **IAM**, e listar arquivos dentro de um **container Blob** utilizando o **Python SDK**.\n\n---\n\n## **Passo 1: Criar uma Storage Account no Azure**\n\n1. Acesse o [Azure Portal](https://portal.azure.com/).\n2. No menu de navegação, clique em **Storage accounts** e depois em **Create**.\n3. **Configuração básica**:\n   - **Subscription**: Escolha a assinatura correta.\n   - **Resource group**: Selecione o grupo criado anteriormente.\n   - **Storage account name**: Escolha um nome único (por exemplo, `armazenamentoexemplo`).\n   - **Region**: Escolha a mesma região onde seus recursos estão hospedados (ex.: East US).\n   - **Performance**: Standard.\n   - **Replication**: LRS (Locally Redundant Storage) para este exemplo.\n4. Clique em **Review + Create** e, em seguida, **Create**.\n\n---\n\n## **Passo 2: Criar um Container no Blob Storage**\n\n1. Dentro da **Storage Account** recém-criada, vá para **Containers** no menu lateral.\n2. Clique em **+ Container**.\n3. **Nome do Container**: Por exemplo, `meus-arquivos`.\n4. **Tipo de acesso público**: Deixe como **Private** (somente acesso autenticado).\n5. Clique em **Create**.\n\n---\n\n## **Passo 3: Obter a URL de Conexão do Storage**\n\n1. Na **Storage Account**, vá para **Access keys** no menu lateral.\n2. Copie o **Connection string**. Ele será usado no código para conectar-se ao Blob Storage.\n\n---\n\n## **Passo 4: Dar Acesso no IAM ao Aplicativo Criado Anteriormente**\n\n1. Acesse o **Azure Portal** e vá para **Storage accounts** > **sua Storage Account**.\n2. No menu lateral, clique em **Access control (IAM)**.\n3. Clique em **Add role assignment**.\n4. **Função**: Selecione **Blob Data Contributor**.\n5. **Membro**: Selecione o **App Registration** criado anteriormente no Azure Active Directory.\n6. Clique em **Review + Assign**.\n\n---\n\n## **Passo 5: Código Python para Listar Arquivos no Blob Storage**\n\nCrie um arquivo Python chamado `list_blob_files.py` e adicione o seguinte código:\n\n### **Código Python**\n\n```python\nfrom azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom dotenv import load_dotenv\nimport os\n\n# Carrega as variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Variáveis de ambiente necessárias\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nstorage_account_url  os.environ[\"AZURE_STORAGE_URL\"]  # Ex: https://<nome_da_storage>.blob.core.windows.net/\ncontainer_name  \"meus-arquivos\"  # Nome do container criado\n\n# Configura as credenciais usando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id,\n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Conectar ao Blob Storage\nblob_service_client  BlobServiceClient(\n    account_urlstorage_account_url,\n    credentialcredentials\n)\n\n# Acessa o container\ncontainer_client  blob_service_client.get_container_client(container_name)\n\n# Lista todos os arquivos dentro do container\nprint(f\"Listando arquivos no container '{container_name}':\")\nfor blob in container_client.list_blobs():\n    print(f\" - {blob.name}\")\n```\n\n---\n\n## **Passo 6: Configurar o Arquivo `.env`**\n\nCrie um arquivo chamado `.env` na mesma pasta do código e adicione as variáveis necessárias:\n\n```env\nAZURE_CLIENT_IDseu_client_id\nAZURE_TENANT_IDseu_tenant_id\nAZURE_CLIENT_SECRETseu_client_secret\nAZURE_STORAGE_URLhttps://<nome_da_storage>.blob.core.windows.net/\n```\n\n---\n\n## **Passo 7: Instalar as Dependências**\n\nNo terminal, execute o seguinte comando para instalar as bibliotecas necessárias:\n\n```bash\npip install azure-identity azure-storage-blob python-dotenv\n```\n\n---\n\n## **Passo 8: Executar o Código**\n\nNo terminal, execute o script:\n\n```bash\npython list_blob_files.py\n```\n\n---\n\n## **Resultado Esperado**\n\nAo executar o código, você verá a lista de todos os arquivos presentes no container:\n\n```\nListando arquivos no container 'meus-arquivos':\n - exemplo1.txt\n - relatorio2024.csv\n - imagem.png\n```\n\n---\n\n## **Conclusão**\n\nCom este projeto, você aprendeu a:\n\n- **Criar uma Storage Account** e **container Blob** no Azure.\n- **Configurar permissões IAM** para o aplicativo.\n- **Listar arquivos** armazenados no container Blob usando **Python SDK**.\n\nEste processo é fundamental para manipular dados na nuvem com segurança e eficiência, garantindo o acesso controlado por meio de credenciais e políticas de IAM.\n\n## Projeto 3. Streamlit para inserir dados no Blob Storage\n\n### **Código com Streamlit: Inserir Arquivos no Blob Storage**\n\nEste e",
        "Bootcamp - Cloud para dados/Aula_16_17/README.md - Parte (2/2)\nxemplo utiliza o **Streamlit** para criar uma interface gráfica que permite ao usuário fazer upload de arquivos para o **Azure Blob Storage**.\n\n---\n\n#### **Instalar as Dependências**\n\nAntes de começar, certifique-se de instalar as bibliotecas necessárias:\n\n```bash\npip install streamlit azure-identity azure-storage-blob python-dotenv\n```\n\n---\n\n#### **Código Python (app.py)**\n\n```python\nimport streamlit as st\nfrom azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom dotenv import load_dotenv\nimport os\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Variáveis de ambiente necessárias\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nstorage_account_url  os.environ[\"AZURE_STORAGE_URL\"]\ncontainer_name  \"meucontainer\"\n\n# Configura credenciais usando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id,\n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Conectar ao Blob Storage\nblob_service_client  BlobServiceClient(\n    account_urlstorage_account_url,\n    credentialcredentials\n)\n\n# Acessa o container\ncontainer_client  blob_service_client.get_container_client(container_name)\n\n# Função para upload de arquivo\ndef upload_file(file):\n    try:\n        blob_client  container_client.get_blob_client(file.name)\n        blob_client.upload_blob(file, overwriteTrue)\n        st.success(f\"Arquivo '{file.name}' enviado com sucesso!\")\n    except Exception as e:\n        st.error(f\"Erro ao enviar arquivo: {str(e)}\")\n\n# Função para listar arquivos no container\ndef listar_arquivos():\n    try:\n        blobs  container_client.list_blobs()\n        return [blob.name for blob in blobs]\n    except Exception as e:\n        st.error(f\"Erro ao listar arquivos: {str(e)}\")\n        return []\n\n# Interface do Streamlit\nst.title(\"Upload para Azure Blob Storage\")\n\nuploaded_file  st.file_uploader(\"Escolha um arquivo para enviar\", type[\"csv\", \"txt\", \"png\", \"jpg\", \"pdf\"])\n\nif uploaded_file is not None:\n    if st.button(\"Enviar\"):\n        upload_file(uploaded_file)\n\nst.subheader(\"Arquivos no Container\")\narquivos  listar_arquivos()\nif arquivos:\n    for arquivo in arquivos:\n        st.write(f\"- {arquivo}\")\nelse:\n    st.write(\"Nenhum arquivo encontrado.\")\n```\n\n---\n\n#### **Configuração do Arquivo `.env`**\n\nCrie um arquivo chamado `.env` e adicione as variáveis de ambiente:\n\n```env\nAZURE_CLIENT_IDseu_client_id\nAZURE_TENANT_IDseu_tenant_id\nAZURE_CLIENT_SECRETseu_client_secret\nAZURE_STORAGE_URLhttps://<nome_da_storage>.blob.core.windows.net/\n```\n\n---\n\n#### **Como Executar o Projeto**\n\n1. **Inicie o Streamlit** com o seguinte comando:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n2. **Acesse a interface** no navegador através do link fornecido no terminal (por exemplo, `http://localhost:8501`).\n\n---\n\n#### **O que este Código Faz?**\n\n- **Upload de Arquivos**: O usuário pode selecionar um arquivo e enviá-lo para o **Blob Storage** clicando em \"Enviar\".\n- **Listagem de Arquivos**: Todos os arquivos presentes no container são listados abaixo da interface.\n- **Tratamento de Erros**: Mensagens de erro e sucesso são exibidas para garantir uma melhor experiência do usuário.\n\n---\n\n#### **Conclusão**\n\nCom este projeto, você pode enviar e gerenciar arquivos diretamente no **Azure Blob Storage** através de uma interface simples e intuitiva criada com **Streamlit**.\n\n---\n## Projeto 4. Máquinas Virtuais (VMs) no Azure  \nVMs no Azure permitem criar máquinas para processamento de dados e desenvolvimento de aplicações.  \n\n### Configuração de uma VM  \n1. No Azure Portal, vá em **Virtual Machine** > **Create**.  \n2. Escolha a região e imagem (ex.: Ubuntu Server 20.04 LTS).  \n3. Configure o tamanho (ex.: B1S).  \n4. Escolha entre SSH ou senha como método de login.\n\n### Configuração de Rede e Segurança  \n1. Crie uma VNet e uma sub-rede para isolar a comunicação.  \n2. Configure o NSG para liberar a porta 22 (SSH).  \n3. Defina o IP como estático.\n\n---\n\n## Deploy da Aplicação no Azure VM  \n\nPara rodar uma aplicação Streamlit em uma **VM do Azure** utilizando HTTP (porta 80), siga os passos abaixo.\n\n---\n\n### Configuração da VM no Azure  \n\n1. No **Azure Portal**, vá em **Virtual Machines** > **Create** > **Azure Virtual Machine**.  \n2. Preencha os detalhes básicos:  \n   - Nome: `streamlit-vm`  \n   - Região: **East US**  \n   - Imagem: **Ubuntu Server 20.04 LTS**  \n   - Tamanho: **B1S** (ou outro disponível no plano gratuito).  \n3. Em **Opções de Autenticação**, selecione **Chave SSH** e gere um novo par de chaves ou utilize um existente.  \n4. Configure a **Rede**:  \n   - Crie uma nova **Virtual Network (VNet)** e uma **sub-rede**.  \n   - Configure um **Network Security Group (NSG)** e adicione uma **regra de entrada** para liberar a **porta 80 (HTTP)**.  \n   - Defina o IP público como **estático**.  \n5. Clique em **Review + Create** e, após a validação, selecione **Create**.  \n\n---\n\n### Acesso à VM via SSH  \n\nApós a criação, conecte-se à VM utilizando o terminal:  \n\n```bash\nssh -i /caminho/para/chave.pem azureuser@<ip-publico-da-vm>\n```\n\n---\n\n### Instalação do Docker e Git na VM  \n\n1. Atualize os pacotes do sistema:  \n   ```bash\n   sudo apt update && sudo apt upgrade -y\n   ```  \n2. Instale o Docker:  \n   ```bash\n   sudo apt install docker.io -y\n   sudo systemctl start docker\n   sudo systemctl enable docker\n   ```  \n3. Adicione o usuário ao grupo Docker para evitar o uso do `sudo`:  \n   ```bash\n   sudo usermod -aG docker $USER\n   ```  \n4. Desconecte-se e reconecte-se para aplicar as permissões:  \n   ```bash\n   exit\n   ssh -i /caminho/para/chave.pem azureuser@<ip-publico-da-vm>\n   ```\n\n---\n\n### Configuração da Aplicação e Dockerfile  \n\n1. Clone o repositório com o código da aplicação:  \n   ```bash\n   git clone https://github.com/lvgalvao/hello-world-streamlit.git\n   cd hello-world-streamlit\n   ```  \n\n2. Crie o **Dockerfile** se ainda não existir:  \n   ```dockerfile\n   FROM python:3.9-slim\n   WORKDIR /app\n   COPY requirements.txt .\n   COPY app.py .\n   RUN pip install --no-cache-dir -r requirements.txt\n   EXPOSE 80\n   CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port80\", \"--server.address0.0.0.0\"]\n   ```\n\n---\n\n### Construção e Execução da Imagem Docker  \n\n1. **Construa a imagem Docker:**  \n   ```bash\n   docker build -t streamlit-app .\n   ```  \n2. **Rode o container na porta 80:**  \n   ```bash\n   docker run -d -p 80:80 streamlit-app\n   ```  \n\n---\n\n### Acessando a Aplicação  \n\nAbra o navegador e acesse a aplicação através do IP público da sua VM:  \n\n```\nhttp://<ip-publico-da-vm>\n```\n\n---\n\n### Solução de Problemas  \n\n1. **Erro de permissão com Docker:**  \n   Se receber um erro de permissão, tente:  \n   ```bash\n   sudo docker run -d -p 80:80 streamlit-app\n   ```  \n\n2. **Problemas de conexão via HTTP:**  \n   - Verifique se a porta 80 está aberta no **NSG**.  \n   - Confirme que o container está rodando:  \n     ```bash\n     docker ps\n     ```\n\n---\n\n### Parar o Container (Opcional)  \n\n1. Liste os containers em execução:  \n   ```bash\n   docker ps\n   ```  \n2. Pare o container usando o ID:  \n   ```bash\n   docker stop <container_id>\n   ```\n\n---\n\nAgora você tem sua aplicação **Streamlit \"Hello World\"** rodando em uma **VM do Azure** utilizando HTTP na porta 80, pronta para ser acessada e utilizada.\n\n---\n\n## Azure Active Directory (AAD)  \nO AAD é o sistema de identidade e controle de acesso do Azure, semelhante ao IAM da AWS.\n\n### Criando um Usuário e Atribuindo Funções  \n1. No portal, vá em **Azure Active Directory** > **Usuários** > **Novo usuário**.  \n2. Configure nome, função (Contributor) e senha inicial.  \n3. No Access Control (IAM), adicione o usuário como Contributor na VM.\n\n---\n\n## Automatizando Uploads para Azure Blob Storage  \nExemplo de código em Python:\n```python\nfrom azure.storage.blob import BlobServiceClient\n\nconnect_str  \"sua-connection-string\"\nservice_client  BlobServiceClient.from_connection_string(connect_str)\ncontainer_client  service_client.get_container_client(\"dados-clientes\")\ncontainer_client.create_container()\n\nwith open(\"exemplo.csv\", \"rb\") as data:\n    container_client.upload_blob(data)\n\nprint(\"Upload concluído!\")\n```\n\n---\n\n## Comparação: Azure vs AWS  \n| Serviço                 | Azure                      | AWS                   |\n|-------------------------|----------------------------|-----------------------|\n| Armazenamento           | Azure Blob Storage         | Amazon S3             |\n| Máquinas Virtuais       | Azure Virtual Machines     | EC2                   |\n| Controle de Acesso      | Azure Active Directory     | IAM                   |\n| Banco de Dados          | Azure SQL Database         | Amazon RDS            |\n| Rede Virtual            | Virtual Network (VNet)     | VPC                   |\n| Automação               | Azure Functions            | AWS Lambda            |\n\n---\n\n## Boas Práticas no Azure  \n- **Automação com CLI:** Use Azure CLI para tarefas repetitivas.  \n- **Tags:** Aplique tags para organizar e controlar custos.  \n- **Backup:** Use replicação GRS para resiliência.  \n- **Orçamento:** Configure alertas no Cost Management.\n\n---\n\n## Conclusão  \nEsta aula apresentou conceitos fundamentais do Azure e sua comparação com a AWS, mostrando como configurar contas, VMs, Storage Accounts e identidade. Na próxima aula, exploraremos pipelines de dados no Azure com Data Factory e SQL Database.\n\nSim, a **VNet (Virtual Network)** no Azure é equivalente à **VPC (Virtual Private Cloud)** na AWS, embora cada uma tenha suas particularidades. Ambas são usadas para criar redes isoladas onde você pode executar seus recursos, como máquinas virtuais e bancos de dados, garantindo segurança e controle de tráfego.\n\n### **Comparação entre VNet e VPC**\n\n| **Aspecto**                     | **VNet (Azure)**                           | **VPC (AWS)**                          |\n|----------------------------------|--------------------------------------------|---------------------------------------|\n| **Propósito**                   | Rede virtual para isolar recursos no Azure | Rede virtual para isolar recursos na AWS |\n| **Isolamento**                   | Totalmente isolada de outras VNets          | Totalmente isolada de outras VPCs    |\n| **Sub-redes**                    | Suporta múltiplas sub-redes dentro da VNet | Suporta múltiplas sub-redes na VPC   |\n| **Controle de Tráfego**          | NSG (Network Security Group) para regras de segurança | Security Groups e NACLs (Network ACLs) |\n| **Conectividade entre Redes**   | VNet Peering                               | VPC Peering                          |\n| **Gateway VPN**                  | VPN Gateway para conectar on-premises ou outras VNets | VPN Gateway para conectar on-premises ou outras VPCs |\n| **CIDR Blocks**                  | Define o intervalo IP privado com CIDR     | Define o intervalo IP privado com CIDR |\n| **DNS**                         | Azure-provided DNS ou customizado          | AWS-provided DNS ou customizado      |\n| **Gateway NAT**                  | NAT Gateway para acesso à internet         | NAT Gateway ou NAT Instance          |\n| **Firewall**                    | Azure Firewall                            | AWS Network Firewall                 |\n\n### **Principais Semelhanças**\n- **Isolamento:** Ambas permitem criar redes isoladas para manter a segurança e controle dos recursos.\n- **Sub-redes:** Em ambas, você pode definir várias sub-redes para segmentar diferentes tipos de recursos.\n- **Conectividade:** Tanto a VNet quanto a VPC suportam **peering**, permitindo comunicação entre redes.\n\n### **Diferenças Notáveis**\n1. **Controle de Tráfego:** \n   - Azure usa **NSGs (Network Security Groups)** para definir regras de tráfego para sub-redes e VMs.\n   - AWS utiliza **Security Groups** e **Network ACLs**.\n\n2. **Peering Global:**\n   - O Azure permite **VNet Peering** global entre regiões.\n   - A AWS também suporta **VPC Peering** entre contas e regiões, mas com algumas limitações dependendo do tipo de configuração.\n\n3. **Firewall:**\n   - O Azure oferece o **Azure Firewall** como um serviço integrado para proteção avançada.\n   - A AWS possui o **AWS Network Firewall** e outras ferramentas como **WAF** para proteção.\n\n### **Exemplo de Uso em Engenharia de Dados**\n- No Azure, você pode criar uma **VNet** e definir sub-redes específicas para hospedar uma **máquina virtual** que processa dados.\n- Essa VM pode acessar dados armazenados em uma **Storage Account** (como Blob Storage) pela VNet, garantindo que o tráfego não passe pela internet pública.\n\nEssa arquitetura é ideal para pipelines de dados seguros e de alta performance, como processamento de arquivos CSV para Parquet, conforme o projeto que você está desenvolvendo.\n\n### **Projeto 5. Criação de Banco de Dados SQL e Inserção de Dados com Streamlit**\n\nNecessário baixar o driver de sqlserver\n\nhttps://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?redirectedfromMSDN&viewsql-server-ver16\n\n#### **1. Criando o Banco de Dados SQL no Azure**\n\n1. Acesse o portal Azure ([portal.azure.com](https://portal.azure.com)).\n2. Navegue para **SQL Databases** e clique em **Create**.\n3. **Configurações Básicas**:\n   - Nome do Banco: `meubancodedados`.\n   - Servidor: Crie um novo servidor ou selecione um existente.\n   - Grupo de Recursos: Selecione ou crie um novo grupo, por exemplo, `myResourceGroup`.\n   - Região: Selecione **Brazil South** ou outra região próxima.\n   - Elastic Pool: Escolha **Não** (se não for usar um pool).\n4. **Compute + Storage**:\n   - Escolha **General Purpose** com 2 vCores.\n   - Defina 32 GB de armazenamento e redundância local ou geo-redundante.\n5. Clique em **Review + Create** e depois em **Create**.\n\n#### **2. Configurando o Acesso (IAM)**\n\n1. Vá até o banco de dados criado e clique em **Access Control (IAM)**.\n2. Adicione uma função **Contributor** ou **Data Reader** ao serviço usado no **App Registration** que foi criado anteriormente.\n3. Copie a **string de conexão** do banco para utilizá-la no código.\n\n#### **3. Configurando o Projeto em Python com Streamlit**\n\n```sql\nCREATE TABLE pessoas (\n    id INT IDENTITY(1,1) PRIMARY KEY,  -- Coluna com auto incremento\n    nome VARCHAR(100) NOT NULL,         -- Nome com limite de 100 caracteres\n    idade INT NOT NULL                  -- Idade como número inteiro\n);\n```\n\n1. Instale as dependências no terminal:\n   ```bash\n   pip install streamlit pyodbc python-dotenv\n   ```\n2. Crie um arquivo **`.env`** com as variáveis:\n   ```\n   AZURE_DB_SERVER<seu-servidor>.database.windows.net\n   AZURE_DB_NAMEmeubancodedados\n   AZURE_DB_USER<seu-usuario>\n   AZURE_DB_PASSWORD<sua-senha>\n   ```\n3. Crie o seguinte **código Python** no arquivo `app.py`:\n\n   ```python\n   import streamlit as st\n   import pyodbc\n   from dotenv import load_dotenv\n   import os\n\n   # Carregar variáveis de ambiente\n   load_dotenv()\n\n   # Conectar ao banco de dados\n   server  os.getenv(\"AZURE_DB_SERVER\")\n   database  os.getenv(\"AZURE_DB_NAME\")\n   username  os.getenv(\"AZURE_DB_USER\")\n   password  os.getenv(\"AZURE_DB_PASSWORD\")\n   \n   connection_string  f'DRIVER{{ODBC Driver 17 for SQL Server}};SERVER{server};DATABASE{database};UID{username};PWD{password}'\n   conn  pyodbc.connect(connection_string)\n   cursor  conn.cursor()\n\n   # Interface Streamlit\n   st.title(\"Inserção de Dados no Banco SQL\")\n\n   nome  st.text_input(\"Nome\")\n   idade  st.number_input(\"Idade\", min_value0, max_value120)\n\n   if st.button(\"Inserir Dados\"):\n       cursor.execute(f\"INSERT INTO pessoas (nome, idade) VALUES (?, ?)\", (nome, idade))\n       conn.commit()\n       st.success(\"Dados inseridos com sucesso!\")\n\n   cursor.close()\n   conn.close()\n   ```\n\n4. Execute o **Streamlit**:\n   ```bash\n   streamlit run app.py\n   ```\n\n---\n\n### **Mermaid: Inserção de Dados com Streamlit e Azure SQL Database**\n\n```mermaid\nsequenceDiagram\n    participant User as Usuário (Interface Streamlit)\n    participant Streamlit as Streamlit App\n    participant Env as .env (Variáveis de Ambiente)\n    participant SQLDB as Banco de Dados SQL (Azure)\n    participant IAM as IAM (Controle de Acesso)\n\n    User->>Streamlit: 1. Preenche nome e idade na UI\n    User->>Streamlit: 2. Clica no botão \"Inserir Dados\"\n    \n    Streamlit->>Env: 3. Carrega variáveis de ambiente (.env)\n    Env-->>Streamlit: 4. Retorna credenciais do banco de dados\n    \n    Streamlit->>IAM: 5. Solicita acesso ao banco SQL\n    IAM-->>Streamlit: 6. Concede acesso (verificação do IAM)\n    \n    Streamlit->>SQLDB: 7. Conecta ao banco de dados\n    SQLDB-->>Streamlit: 8. Conexão estabelecida\n    \n    Streamlit->>SQLDB: 9. Insere dados na tabela 'pessoas'\n    SQLDB-->>Streamlit: 10. Confirmação de inserção\n\n    Streamlit->>User: 11. Exibe mensagem de sucesso\n```\n\n---\n\n### **Conclusão**\n\nCom este projeto, você agora possui uma aplicação em **Streamlit** conectada a um banco de dados **SQL no Azure**. A interface permite a inserção de dados e a configuração utiliza boas práticas, como o uso de **variáveis de ambiente** e **controle de acesso IAM**.\n\n",
        "Bootcamp - Cloud para dados/Aula_16_17/Dockerfile\n\n# Imagem base oficial do Python\nFROM python:3.9-slim\n\n# Define o diretório de trabalho dentro do container\nWORKDIR /app\n\n# Copia o arquivo de requisitos e o código para o container\nCOPY requirements.txt .\nCOPY app.py .\n\n# Instala o Streamlit e outras dependências\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Define a porta padrão que o Streamlit usará\nEXPOSE 80\n\n# Comando para rodar a aplicação Streamlit\nCMD [\"streamlit\", \"run\", \"app.py\", \"--server.port80\", \"--server.enableCORSfalse\"]\n\n\nBootcamp - Cloud para dados/Aula_16_17/README-resourcegroup.md\n\n### **Resource Group no Azure**\n\nO **Resource Group** no Azure é uma unidade lógica para **organizar, gerenciar e agrupar recursos** relacionados, como VMs, bancos de dados, contas de armazenamento e redes virtuais. Ele facilita o gerenciamento de infraestrutura em nuvem, garantindo que todos os recursos de um projeto ou serviço específico estejam concentrados em um único local.\n\n---\n\n### **Características Principais do Resource Group**\n\n1. **Organização e Gestão Centralizada:**\n   - Todos os recursos de uma aplicação ou projeto ficam organizados em um único grupo.\n   - Exemplo: Uma aplicação pode ter VMs, bancos de dados e uma VNet no mesmo Resource Group.\n\n2. **Controle de Acesso e Permissões:**\n   - Permissões podem ser atribuídas ao Resource Group inteiro usando **Azure Role-Based Access Control (RBAC)**.\n   - Assim, equipes específicas podem ter acesso apenas aos recursos dentro do grupo correspondente.\n\n3. **Monitoramento e Gestão de Custos:**\n   - Cada Resource Group pode ter seu custo monitorado separadamente.\n   - Ajuda a visualizar e controlar o orçamento de projetos específicos.\n\n4. **Aplicação de Tags:**\n   - Tags podem ser aplicadas para facilitar a **organização e busca** de recursos com base em critérios como projeto, cliente ou departamento.\n\n---\n\n### **Casos de Uso do Resource Group**\n\n- **Isolamento de Ambientes:**  \n  Organize ambientes de **produção, desenvolvimento e teste** em Resource Groups separados.\n\n- **Projetos Multi-Equipe:**  \n  Múltiplos times trabalhando em diferentes serviços podem criar grupos específicos para gerenciar seus recursos sem interferir entre si.\n\n- **Automação:**  \n  Scripts de **Infraestrutura como Código (IaC)**, como ARM Templates e Terraform, utilizam Resource Groups para provisionar e gerenciar recursos.\n\n---\n\n### **Benefícios de Usar Resource Groups**\n\n- **Escalabilidade e Flexibilidade:**  \n  Recursos podem ser adicionados e removidos rapidamente dentro de um grupo.\n\n- **Governança Simplificada:**  \n  Atribuir políticas de segurança e controle de acesso a grupos inteiros facilita a governança.\n\n- **Gerenciamento Facilitado:**  \n  Operações como **backup**, **exportação** ou **deleção** podem ser feitas no nível do Resource Group, afetando todos os recursos contidos nele.\n\n---\n\n### **Como Criar um Resource Group no Azure**\n\n1. **Acessar o Portal do Azure:**  \n   - [https://portal.azure.com](https://portal.azure.com)\n\n2. **Ir para Resource Groups:**  \n   - No menu lateral, selecione **Resource Groups** e clique em **Create**.\n\n3. **Preencher Informações:**\n   - **Nome:** Defina um nome significativo (ex.: `RG-ProjetoX`).\n   - **Região:** Escolha a região onde os recursos serão provisionados (ex.: **East US**).\n   - **Tags:** (Opcional) Aplique tags para facilitar o controle de custos e organização.\n\n4. **Criar o Resource Group:**  \n   - Clique em **Review + Create** e em seguida **Create**.\n\n---\n\n### **Conclusão**\nO **Resource Group** é essencial para o gerenciamento eficaz dos recursos no Azure. Ele não apenas organiza recursos, mas também simplifica a **administração**, **segurança** e **monitoramento** de projetos e serviços em nuvem.\n\n### **Resource Group no Azure e o Equivalente na AWS**\n\nNa **AWS**, o equivalente direto ao **Resource Group** do Azure é o **AWS Resource Groups**. Ambos servem para organizar e gerenciar recursos relacionados, mas existem algumas diferenças na implementação e uso.\n\n---\n\n### **Comparação: Resource Group (Azure) vs. AWS Resource Groups**\n\n| **Aspecto**               | **Resource Group (Azure)**                     | **Resource Groups (AWS)**                    |\n|---------------------------|-------------------------------------------------|---------------------------------------------|\n| **Propósito**              | Organizar recursos relacionados por projeto ou serviço. | Agrupar e gerenciar recursos por projeto, ambiente ou finalidade. |\n| **Controle de Acesso**     | Gerenciado via **Azure Role-Based Access Control (RBAC)** aplicado ao grupo inteiro. | Controle de acesso configurado por **IAM Policies** e **Tags**. |\n| **Monitoramento de Custos**| Monitoramento de custos do grupo no **Cost Management**. | Visualização de custos no **AWS Cost Explorer** por tags ou grupos. |\n| **Organização**            | Agrupa múltiplos recursos em um único container lógico. | Agrupa recursos por meio de **tags** aplicadas. |\n| **Tags**                   | Tags são opcionais e ajudam na categorização. | Tags são fundamentais e podem ser usadas para definir Resource Groups dinamicamente. |\n\n---\n\n### **Funcionamento do AWS Resource Groups**\n\n1. **Organização e Tags:**\n   - Os recursos na AWS não são \"alocados fisicamente\" dentro de um grupo, mas organizados e filtrados usando **tags**.\n   - Exemplo: Todos os recursos (EC2, RDS, S3) de um projeto podem ter a tag `Projeto: X`.\n\n2. **Gerenciamento Centralizado:**\n   - O AWS Resource Groups permite visualizar, gerenciar e executar operações em massa para todos os recursos associados a um projeto ou serviço específico.\n\n3. **Controle de Acesso:**  \n   - O controle é feito via **IAM Policies**, que podem conceder permissões com base em tags aplicadas aos recursos.\n\n4. **Monitoramento e Custos:**\n   - Utilizando o **AWS Cost Explorer** e **AWS Budgets**, é possível monitorar o custo de recursos organizados por tags ou grupos.\n\n---\n\n### **Quando Usar AWS Resource Groups?**\n\n- **Ambientes Multi-Projetos:**  \n  Organize recursos por projetos usando tags como `Projeto: Marketing` ou `Projeto: Financeiro`.\n\n- **Isolamento de Ambientes:**  \n  Defina ambientes de **produção**, **desenvolvimento** e **teste** utilizando tags para cada ambiente.\n\n- **Automação:**  \n  Scripts como Terraform e CloudFormation usam tags para organizar e automatizar a criação e gerenciamento de recursos.\n\n---\n\n### **Como Criar um Resource Group na AWS**\n\n1. **Acessar o Console AWS:**  \n   - [https://aws.amazon.com/console/](https://aws.amazon.com/console/)\n\n2. **Ir para Resource Groups:**  \n   - No menu superior, clique em **Resource Groups** > **Create Group**.\n\n3. **Definir Critérios do Grupo:**\n   - Nomeie o grupo e defina **filtros de tags**. Por exemplo: `Projeto: X`.\n\n4. **Revisar e Criar:**  \n   - Revise as configurações e clique em **Create Group**.\n\n---\n\n### **Conclusão**\nEmbora **Azure Resource Group** e **AWS Resource Groups** compartilhem a função de organizar e gerenciar recursos relacionados, a AWS depende fortemente de **tags** para criar grupos dinâmicos, enquanto o Azure usa uma abordagem mais direta com grupos lógicos. Ambos simplificam o gerenciamento e são essenciais para controle de custos e governança na nuvem.\n\nBootcamp - Cloud para dados/Aula_16_17/app.py\n\nimport streamlit as st\n\n# Título do aplicativo\nst.title(\"Hello, World! com Streamlit\")\n\n# Exibindo uma mensagem de boas-vindas\nst.write(\"Este é um exemplo simples de uma aplicação Streamlit em Docker.\")\n\n# Adicionando um botão de interação\nif st.button(\"Clique aqui\"):\n    st.success(\"Você clicou no botão!\")\n\n\nBootcamp - Cloud para dados/Aula_16_17/projeto_01.py\n\nfrom azure.identity import ClientSecretCredential  # Importa a classe para autenticação usando Client Secret\nfrom azure.keyvault.secrets import SecretClient  # Importa a classe para manipular segredos no Key Vault\nfrom dotenv import load_dotenv  # Importa a função para carregar variáveis de ambiente de um arquivo .env\nimport os  # Importa o módulo para acessar variáveis de ambiente\n\n# Carrega as variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Obtém as variáveis de ambiente necessárias para a autenticação\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nvault_url  os.environ[\"AZURE_VAULT_URL\"]\n\n# Nome do segredo a ser acessado no Key Vault\nsecret_name  \"ExemploKey\"\n\n# Cria uma credencial para autenticação no Azure utilizando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id, \n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Cria o cliente do Key Vault para acessar os segredos\nsecret_client  SecretClient(vault_urlvault_url, credentialcredentials)\n\n# Recupera o valor do segredo a partir do Key Vault\nsecret  secret_client.get_secret(secret_name)\n\n# Exibe o valor do segredo no terminal\nprint(\"O valor do segredo é: \" + secret.value)\n\n\nBootcamp - Cloud para dados/Aula_16_17/projeto_02.py\n\nfrom azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom dotenv import load_dotenv\nimport os\n\n# Carrega as variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Variáveis de ambiente necessárias\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nstorage_account_url  os.environ[\"AZURE_STORAGE_URL\"]  # Ex: https://<nome_da_storage>.blob.core.windows.net/\ncontainer_name  \"meucontainer\"  # Nome do container criado\n\n# Configura as credenciais usando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id,\n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Conectar ao Blob Storage\nblob_service_client  BlobServiceClient(\n    account_urlstorage_account_url,\n    credentialcredentials\n)\n\n# Acessa o container\ncontainer_client  blob_service_client.get_container_client(container_name)\n\n# Lista todos os arquivos dentro do container\nprint(f\"Listando arquivos no container '{container_name}':\")\nfor blob in container_client.list_blobs():\n    print(f\" - {blob.name}\")\n\n\nBootcamp - Cloud para dados/Aula_16_17/projeto_03.py\n\nimport streamlit as st\nfrom azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom dotenv import load_dotenv\nimport os\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Variáveis de ambiente necessárias\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nstorage_account_url  os.environ[\"AZURE_STORAGE_URL\"]\ncontainer_name  \"meucontainer\"\n\n# Configura credenciais usando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id,\n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Conectar ao Blob Storage\nblob_service_client  BlobServiceClient(\n    account_urlstorage_account_url,\n    credentialcredentials\n)\n\n# Acessa o container\ncontainer_client  blob_service_client.get_container_client(container_name)\n\n# Função para upload de arquivo\ndef upload_file(file):\n    try:\n        blob_client  container_client.get_blob_client(file.name)\n        blob_client.upload_blob(file, overwriteTrue)\n        st.success(f\"Arquivo '{file.name}' enviado com sucesso!\")\n    except Exception as e:\n        st.error(f\"Erro ao enviar arquivo: {str(e)}\")\n\n# Função para listar arquivos no container\ndef listar_arquivos():\n    try:\n        blobs  container_client.list_blobs()\n        return [blob.name for blob in blobs]\n    except Exception as e:\n        st.error(f\"Erro ao listar arquivos: {str(e)}\")\n        return []\n\n# Interface do Streamlit\nst.title(\"Upload para Azure Blob Storage\")\n\nuploaded_file  st.file_uploader(\"Escolha um arquivo para enviar\", type[\"csv\", \"txt\", \"png\", \"jpg\", \"pdf\"])\n\nif uploaded_file is not None:\n    if st.button(\"Enviar\"):\n        upload_file(uploaded_file)\n\nst.subheader(\"Arquivos no Container\")\narquivos  listar_arquivos()\nif arquivos:\n    for arquivo in arquivos:\n        st.write(f\"- {arquivo}\")\nelse:\n    st.write(\"Nenhum arquivo encontrado.\")\n\n\nBootcamp - Cloud para dados/Aula_16_17/projeto_05.py\n\nimport streamlit as st\nimport pyodbc\nimport os\nfrom dotenv import load_dotenv\n\n# Carregar variáveis de ambiente\nload_dotenv()\n\n# Configuração do banco de dados\nserver  os.getenv(\"DB_SERVER\")\ndatabase  os.getenv(\"DB_NAME\")\nusername  os.getenv(\"DB_USERNAME\")\npassword  os.getenv(\"DB_PASSWORD\")\nconnection_string  f'DRIVER{{ODBC Driver 17 for SQL Server}};SERVER{server};DATABASE{database};UID{username};PWD{password}'\n\n# Função para conectar ao banco\ndef connect_to_database():\n    try:\n        conn  pyodbc.connect(connection_string)\n        return conn\n    except Exception as e:\n        st.error(f\"Erro ao conectar ao banco de dados: {e}\")\n        return None\n\n# Função para inserir dados\ndef insert_data(nome, idade):\n    conn  connect_to_database()\n    if conn:\n        cursor  conn.cursor()\n        try:\n            cursor.execute(\"INSERT INTO pessoas (nome, idade) VALUES (?, ?)\", (nome, idade))\n            conn.commit()\n            st.success(f\"Dados inseridos: {nome}, {idade} anos\")\n        except Exception as e:\n            st.error(f\"Erro ao inserir dados: {e}\")\n        finally:\n            conn.close()\n\n# Interface Streamlit\nst.title(\"Inserção de Dados no Banco de Dados SQL\")\n\nnome  st.text_input(\"Nome:\")\nidade  st.number_input(\"Idade:\", min_value0, step1)\n\nif st.button(\"Inserir Dados\"):\n    if nome and idade:\n        insert_data(nome, idade)\n    else:\n        st.warning(\"Preencha todos os campos.\")\n\n\nBootcamp - Cloud para dados/Aula_16_17/requirements.txt\n\nstreamlit\npython-dotenv\nazurealtair5.4.1\nattrs24.2.0\nazure-core1.31.0\nazure-identity1.19.0\nazure-keyvault4.2.0\nazure-keyvault-certificates4.9.0\nazure-keyvault-keys4.10.0\nazure-keyvault-secrets4.9.0\nazure-storage-blob12.23.1\nblinker1.8.2\ncachetools5.5.0\ncertifi2024.8.30\ncffi1.17.1\ncharset-normalizer3.4.0\nclick8.1.7\ncolorama0.4.6\ncryptography43.0.3\ngitdb4.0.11\nGitPython3.1.43\nidna3.10\nisodate0.7.2\nJinja23.1.4\njsonschema4.23.0\njsonschema-specifications2024.10.1\nmarkdown-it-py3.0.0\nMarkupSafe3.0.2\nmdurl0.1.2\nmsal1.31.0\nmsal-extensions1.2.0\nnarwhals1.11.1\nnumpy2.1.2\npackaging24.1\npandas2.2.3\npillow10.4.0\nportalocker2.10.1\nprotobuf5.28.3\npyarrow18.0.0\npycparser2.22\npydeck0.9.1\nPygments2.18.0\nPyJWT2.9.0\npyodbc5.2.0\npython-dateutil2.9.0.post0\npython-dotenv1.0.1\npytz2024.2\npywin32308\nreferencing0.35.1\nrequests2.32.3\nrich13.9.3\nrpds-py0.20.0\nsix1.16.0\nsmmap5.0.1\nstreamlit1.39.0\ntenacity9.0.0\ntoml0.10.2\ntornado6.4.1\ntyping_extensions4.12.2\ntzdata2024.2\nurllib32.2.3\nwatchdog5.0.3\n\n\n",
        "Bootcamp - Cloud para dados/Aula_18_19/README.md - Parte (1/2)\nBootcamp - Cloud para dados/Aula_18_19/README.md\n\n# **Bootcamp Cloud - Aula 16: Introdução ao Azure para Dados**  \n\n---\n\n### **Objetivo:**  \nExplorar o ambiente do Azure, abordando a criação de conta, uso do **Azure Blob Storage** (equivalente ao S3), **máquinas virtuais (VMs)** para processamento de dados e o **Azure Active Directory (AAD)**, que substitui o IAM para gerenciamento de identidade e controle de acesso.\n\n---\n\n## Criação de Conta no Azure  \n1. Acesse [portal.azure.com](https://portal.azure.com) e selecione **Create a Free Account**.  \n2. Preencha as informações básicas: nome completo, telefone, e-mail e senha.  \n3. Adicione as informações de **cartão de crédito** (somente para verificação; não haverá cobranças iniciais).  \n4. Escolha o plano gratuito:  \n   - 750 horas de uma VM B1S por mês (Ubuntu ou Windows).  \n   - 5 GB gratuitos no Blob Storage.  \n   - Banco de dados SQL com 250 GB gratuitos.  \n5. No **Cost Management**, configure alertas para controlar custos e crie um orçamento mensal de teste.\n\n---\n\n## Assinaturas no Azure  \nUma **Assinatura do Azure** é uma unidade lógica para gerenciar o acesso, controle de custos e recursos. Permite organizar e isolar recursos como máquinas virtuais, bancos de dados e serviços.  \n\n### Funções e Finalidades  \n- **Gestão de Recursos e Acesso:** Define permissões específicas para equipes e projetos.  \n- **Faturamento e Controle de Custos:** Cada assinatura tem seu próprio relatório de faturamento.  \n- **Isolamento e Limites:** Permite separar ambientes de produção, desenvolvimento e teste.  \n- **Governança:** As assinaturas aplicam políticas e limites via Grupos de Gerenciamento e AAD.\n\n### Tipos de Assinaturas  \n- **Plano Gratuito:** Oferece crédito inicial e limites gratuitos por 12 meses.  \n- **Pay-As-You-Go:** Paga-se apenas pelo uso.  \n- **Enterprise Agreement (EA):** Contrato com desconto por volume para grandes empresas.  \n- **Cloud Solution Provider (CSP):** Parceiros que revendem serviços Azure.\n\n---\n\n## Equivalente na AWS  \nNa AWS, o conceito mais próximo de uma assinatura do Azure é uma **Conta AWS**.  \n\n- **AWS Organizations:** Permite consolidar e gerenciar várias contas AWS sob uma organização.  \n- **Controle de Custos:** Cada conta tem seus próprios alertas e relatórios de custo.  \n\n### Comparação entre Assinatura do Azure e Conta AWS  \n| Azure (Assinatura)             | AWS (Conta AWS)                       |\n|---------------------------------|----------------------------------------|\n| Separa recursos e define limites | Isola ambientes e projetos            |\n| Gerenciada por AAD              | Gerenciada por IAM e AWS Organizations |\n| Faturamento por assinatura      | Faturamento por conta                 |\n\n---\n\n## Projeto 1. Acessando variáveis de ambiente no Azure\n\n```mermaid\nsequenceDiagram\n    participant User as Usuário (Local)\n    participant AzureAD as Azure AD\n    participant AppReg as App Registration\n    participant KeyVault as Azure Key Vault\n    participant IAMUser as IAM do Usuário\n    participant IAMService as IAM do Serviço\n\n    User->>AzureAD: 1. Solicita Token de Acesso\n    AzureAD-->>User: 2. Retorna Token\n\n    User->>AppReg: 3. Envia Credenciais e Token\n    AppReg-->>User: 4. Validação e Permissão\n\n    User->>KeyVault: 5. Solicita Segredo (com Token)\n    KeyVault-->>IAMUser: 6. Verifica Permissões do Usuário\n    KeyVault-->>IAMService: 7. Verifica Permissões do Serviço\n\n    KeyVault-->>User: 8. Retorna Valor do Segredo\n\n    User->>User: 9. Exibe Segredo no Terminal\n```\n\n---\n\n### **Azure Key Vault: O que é e para que serve?**\n\n### **Objetivo:**\nO **Azure Key Vault** é um serviço de nuvem da Microsoft projetado para gerenciar segredos, chaves de criptografia e certificados de maneira segura. Seu principal objetivo é fornecer uma forma centralizada e protegida de armazenar e acessar informações sensíveis, como senhas, tokens e chaves criptográficas, minimizando riscos de segurança.\n\n### **Funcionalidades do Azure Key Vault:**\n\n1. **Armazenamento Seguro de Segredos:**\n   - Gerencia senhas, strings de conexão, tokens de API e outras informações confidenciais.\n   - Fornece acesso seguro aos segredos por meio de autenticação robusta e controle de permissões via **Azure AD**.\n\n2. **Gerenciamento de Chaves de Criptografia:**\n   - Armazena e protege chaves usadas para criptografia de dados em repouso e em trânsito.\n   - Oferece suporte para criptografia assimétrica e simétrica, podendo ser integrado a outros serviços do Azure, como SQL Database.\n\n3. **Gestão de Certificados:**\n   - Automatiza o processo de renovação e gerenciamento de certificados SSL/TLS.\n   - Permite criar e manter certificados seguros que podem ser utilizados por serviços na nuvem ou locais.\n\n4. **Controle de Acesso Granular:**\n   - Usa **IAM (Identity and Access Management)** para controlar quais aplicações e usuários podem acessar os segredos e chaves.\n   - Mantém logs de auditoria de todas as interações com o Key Vault.\n\n### **Benefícios:**\n- **Centralização:** Consolida a gestão de segredos e chaves, evitando que sejam armazenados em diferentes locais de forma não segura.\n- **Automação:** Facilita a renovação automática de certificados e permite que aplicações acessem segredos sem intervenção manual.\n- **Conformidade:** Ajuda a cumprir regulamentações de segurança e privacidade, mantendo as informações críticas em um ambiente protegido.\n- **Alta Disponibilidade:** O serviço é distribuído por regiões do Azure, garantindo que segredos e chaves estejam sempre acessíveis.\n\n---\n\n### **Comparação com AWS Secrets Manager:**\n\n| **Azure Key Vault**                    | **AWS Secrets Manager**                |\n|----------------------------------------|----------------------------------------|\n| Gerencia segredos, chaves e certificados | Gerencia segredos e credenciais |\n| Integra-se com serviços Azure, como SQL Database | Integra-se com serviços AWS, como RDS |\n| Oferece criptografia usando HSM (Hardware Security Modules) | Oferece criptografia e rotação automática de segredos |\n| Controle de acesso via Azure AD        | Controle de acesso via IAM |\n| Preços por transação e armazenamento   | Preços por segredos armazenados |\n\n---\n\n### **Nosso primeiro Erro**\n\nImportância do IAM\nO uso do IAM (Identity and Access Management) é essencial para criar e gerenciar um Key Vault. Ele permite configurar controle de acesso baseado em papéis (RBAC), garantindo que apenas usuários ou aplicações autorizadas possam acessar ou modificar segredos e chaves.\n\nIAM no Azure é o serviço responsável por gerenciar identidades e permissões, assegurando que cada recurso ou aplicação tenha apenas os acessos necessários. Ele garante uma abordagem de segurança baseada em princípio de menor privilégio.\n\n## **View Exemple**\n\n### **\"View Example\" no Portal Azure Key Vault**\n\nA opção **\"View Example\"** (ou \"Ver Exemplo\") na interface do **Key Vault** facilita o entendimento e uso do serviço. Esta funcionalidade:\n\n1. **Demonstração de Exemplos Práticos:**  \n   - Mostra exemplos de como armazenar e acessar chaves e segredos.\n   - Fornece código de amostra, muitas vezes em **Python, C#, ou PowerShell**, para acessar os segredos via SDK.\n\n2. **Ajuda na Automação:**  \n   - Exemplos incluem snippets para automação com **Azure CLI** ou scripts para aplicações.\n   - Acelera o processo de integração com outras aplicações e pipelines de dados.\n\n3. **Facilita a Configuração Inicial:**  \n   - Apresenta instruções claras de como criar e acessar segredos, chaves e certificados.\n   - Demonstra como configurar corretamente o acesso via **IAM** ou atribuir permissões específicas.\n\nA funcionalidade **\"View Example\"** é uma poderosa ferramenta educacional e prática no portal Azure. Ela facilita a adoção do Azure Key Vault ao fornecer exemplos de código claros, economizando tempo e simplificando o processo de integração. Com essa abordagem orientada a exemplos, é possível configurar o acesso seguro e garantir a conformidade com as boas práticas de segurança na nuvem.\n\n---\n\n## **Azure Active Directory (Azure AD)**\n\n### **Visão Geral**\n\nO **Azure Active Directory (Azure AD)** é o serviço de gerenciamento de identidade e acesso baseado na nuvem da Microsoft. Ele permite que as organizações administrem identidades e controlem o acesso a recursos em nuvem e on-premises, além de aplicativos SaaS. \n\n---\n\n### **Objetivos do Azure AD**\n\n1. **Autenticação e Autorização**  \n   - Verifica identidades para permitir ou negar o acesso a recursos e aplicativos.\n   - Suporta métodos como senha, MFA (autenticação multi-fator) e biometria.\n\n2. **Gerenciamento de Usuários e Grupos**  \n   - Permite criar usuários e organizar grupos para definir permissões e acessos.\n   - Facilita a ativação e desativação de contas de usuários.\n\n3. **Integração com Aplicativos SaaS**  \n   - Oferece autenticação única (SSO) para diversos aplicativos, como Salesforce e Google Workspace.\n   - Melhora a experiência do usuário eliminando a necessidade de múltiplas senhas.\n\n4. **Políticas de Acesso e Segurança**  \n   - Controla o acesso com base na localização e tipo de dispositivo.\n   - Oferece políticas de acesso condicional para aumentar a segurança.\n\n5. **Autenticação para APIs e Aplicativos Customizados**  \n   - Desenvolvedores podem integrar o Azure AD para autenticação segura em suas APIs e aplicações.\n\n---\n\n### **Passo a Passo: Configuração no Azure AD**\n\n#### **1. Acessar o Azure AD no Portal**\n- Vá para o [portal do Azure](https://portal.azure.com).\n- No menu, clique em **Azure Active Directory**.\n\n#### **2. Registrar um Aplicativo no Azure AD**\n- Clique em **App Registrations** > **New Registration**.\n- Dê um nome ao aplicativo e escolha a conta ou organização que terá acesso.\n- Clique em **Register** para finalizar.\n\n#### **3. Criar um Client Secret**\n- Na aplicação registrada, vá para **Certificates & Secrets**.\n- Clique em **New Client Secret**.\n- Adicione uma descrição (opcional) e defina a validade do segredo (6, 12 ou 24 meses).\n- Ao clicar em **Add**, o segredo será gerado. **Copie o valor agora** – ele não será exibido novamente.\n\n#### **4. Configurar IAM para o Serviço**\n- Volte para o **Azure Key Vault** ou outro serviço relevante.\n- Vá para **Access Control (IAM)** > **Role Assignments**.\n- Adicione um novo papel, como **Key Vault Secrets User** ou **Reader**.\n- Em **Select Members**, selecione o **App Registration** que você criou.\n\n---\n\n### **Relação entre Azure AD e AWS IAM**\n\n| **Azure AD**                                | **AWS IAM**                                  |\n|---------------------------------------------|----------------------------------------------|\n| Gerencia identidade de usuários e autenticação. | Gerencia permissões de recursos na AWS. |\n| Oferece SSO e políticas de acesso condicional. | Configura políticas detalhadas para acesso a recursos. |\n| Integrado com Office 365 e aplicativos SaaS. | Integrado com serviços AWS, como EC2 e S3. |\n| Suporta políticas baseadas em usuário e dispositivo. | Suporta permissões detalhadas por função e políticas. |\n\n---\n\n### **Conclusão**\n\nO **Azure AD** é essencial para a autenticação e controle de identidades em ambientes Microsoft e aplicativos SaaS. Ele complementa o **IAM da AWS**, que é mais voltado para gerenciar permissões de recursos. Integrar o Azure AD com a AWS pode proporcionar uma experiência de autenticação unificada e simplificar a gestão de identidades e acessos.\n\n## Projeto 2. Consumindo arquivos no Blob Storage\n\n```mermaid\nsequenceDiagram\n    participant User as Usuário (Local)\n    participant AzureAD as Azure AD\n    participant AppReg as App Registration\n    participant BlobStorage as Azure Blob Storage\n    participant IAMUser as IAM do Usuário\n    participant IAMService as IAM do Serviço\n\n    User->>AzureAD: 1. Solicita Token de Acesso\n    AzureAD-->>User: 2. Retorna Token\n\n    User->>AppReg: 3. Envia Credenciais e Token\n    AppReg-->>User: 4. Validação e Permissão\n\n    User->>BlobStorage: 5. Solicita Listagem de Arquivos (com Token)\n    BlobStorage-->>IAMUser: 6. Verifica Permissões do Usuário\n    BlobStorage-->>IAMService: 7. Verifica Permissões do Serviço\n\n    BlobStorage-->>User: 8. Retorna Lista de Arquivos\n\n    User->>BlobStorage: 9. Baixa Arquivo Específico\n    BlobStorage-->>User: 10. Envia Arquivo\n\n    User->>User: 11. Exibe ou Processa Arquivo Localmente\n```\n\n### **Projeto: Acessar e Fazer Upload de Arquivos no Azure Blob Storage com Python SDK**\n\nEste passo a passo mostrará como criar uma **Storage Account no Azure**, configurar permissões usando **IAM**, e listar arquivos dentro de um **container Blob** utilizando o **Python SDK**.\n\n---\n\n## **Passo 1: Criar uma Storage Account no Azure**\n\n1. Acesse o [Azure Portal](https://portal.azure.com/).\n2. No menu de navegação, clique em **Storage accounts** e depois em **Create**.\n3. **Configuração básica**:\n   - **Subscription**: Escolha a assinatura correta.\n   - **Resource group**: Selecione o grupo criado anteriormente.\n   - **Storage account name**: Escolha um nome único (por exemplo, `armazenamentoexemplo`).\n   - **Region**: Escolha a mesma região onde seus recursos estão hospedados (ex.: East US).\n   - **Performance**: Standard.\n   - **Replication**: LRS (Locally Redundant Storage) para este exemplo.\n4. Clique em **Review + Create** e, em seguida, **Create**.\n\n---\n\n## **Passo 2: Criar um Container no Blob Storage**\n\n1. Dentro da **Storage Account** recém-criada, vá para **Containers** no menu lateral.\n2. Clique em **+ Container**.\n3. **Nome do Container**: Por exemplo, `meus-arquivos`.\n4. **Tipo de acesso público**: Deixe como **Private** (somente acesso autenticado).\n5. Clique em **Create**.\n\n---\n\n## **Passo 3: Obter a URL de Conexão do Storage**\n\n1. Na **Storage Account**, vá para **Access keys** no menu lateral.\n2. Copie o **Connection string**. Ele será usado no código para conectar-se ao Blob Storage.\n\n---\n\n## **Passo 4: Dar Acesso no IAM ao Aplicativo Criado Anteriormente**\n\n1. Acesse o **Azure Portal** e vá para **Storage accounts** > **sua Storage Account**.\n2. No menu lateral, clique em **Access control (IAM)**.\n3. Clique em **Add role assignment**.\n4. **Função**: Selecione **Blob Data Contributor**.\n5. **Membro**: Selecione o **App Registration** criado anteriormente no Azure Active Directory.\n6. Clique em **Review + Assign**.\n\n---\n\n## **Passo 5: Código Python para Listar Arquivos no Blob Storage**\n\nCrie um arquivo Python chamado `list_blob_files.py` e adicione o seguinte código:\n\n### **Código Python**\n\n```python\nfrom azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom dotenv import load_dotenv\nimport os\n\n# Carrega as variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Variáveis de ambiente necessárias\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nstorage_account_url  os.environ[\"AZURE_STORAGE_URL\"]  # Ex: https://<nome_da_storage>.blob.core.windows.net/\ncontainer_name  \"meus-arquivos\"  # Nome do container criado\n\n# Configura as credenciais usando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id,\n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Conectar ao Blob Storage\nblob_service_client  BlobServiceClient(\n    account_urlstorage_account_url,\n    credentialcredentials\n)\n\n# Acessa o container\ncontainer_client  blob_service_client.get_container_client(container_name)\n\n# Lista todos os arquivos dentro do container\nprint(f\"Listando arquivos no container '{container_name}':\")\nfor blob in container_client.list_blobs():\n    print(f\" - {blob.name}\")\n```\n\n---\n\n## **Passo 6: Configurar o Arquivo `.env`**\n\nCrie um arquivo chamado `.env` na mesma pasta do código e adicione as variáveis necessárias:\n\n```env\nAZURE_CLIENT_IDseu_client_id\nAZURE_TENANT_IDseu_tenant_id\nAZURE_CLIENT_SECRETseu_client_secret\nAZURE_STORAGE_URLhttps://<nome_da_storage>.blob.core.windows.net/\n```\n\n---\n\n## **Passo 7: Instalar as Dependências**\n\nNo terminal, execute o seguinte comando para instalar as bibliotecas necessárias:\n\n```bash\npip install azure-identity azure-storage-blob python-dotenv\n```\n\n---\n\n## **Passo 8: Executar o Código**\n\nNo terminal, execute o script:\n\n```bash\npython list_blob_files.py\n```\n\n---\n\n## **Resultado Esperado**\n\nAo executar o código, você verá a lista de todos os arquivos presentes no container:\n\n```\nListando arquivos no container 'meus-arquivos':\n - exemplo1.txt\n - relatorio2024.csv\n - imagem.png\n```\n\n---\n\n## **Conclusão**\n\nCom este projeto, você aprendeu a:\n\n- **Criar uma Storage Account** e **container Blob** no Azure.\n- **Configurar permissões IAM** para o aplicativo.\n- **Listar arquivos** armazenados no container Blob usando **Python SDK**.\n\nEste processo é fundamental para manipular dados na nuvem com segurança e eficiência, garantindo o acesso controlado por meio de credenciais e políticas de IAM.\n\n## Projeto 3. Streamlit para inserir dados no Blob Storage\n\n### **Código com Streamlit: Inserir Arquivos no Blob Storage**\n\nEste e",
        "Bootcamp - Cloud para dados/Aula_18_19/README.md - Parte (2/2)\nxemplo utiliza o **Streamlit** para criar uma interface gráfica que permite ao usuário fazer upload de arquivos para o **Azure Blob Storage**.\n\n---\n\n#### **Instalar as Dependências**\n\nAntes de começar, certifique-se de instalar as bibliotecas necessárias:\n\n```bash\npip install streamlit azure-identity azure-storage-blob python-dotenv\n```\n\n---\n\n#### **Código Python (app.py)**\n\n```python\nimport streamlit as st\nfrom azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom dotenv import load_dotenv\nimport os\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Variáveis de ambiente necessárias\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nstorage_account_url  os.environ[\"AZURE_STORAGE_URL\"]\ncontainer_name  \"meucontainer\"\n\n# Configura credenciais usando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id,\n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Conectar ao Blob Storage\nblob_service_client  BlobServiceClient(\n    account_urlstorage_account_url,\n    credentialcredentials\n)\n\n# Acessa o container\ncontainer_client  blob_service_client.get_container_client(container_name)\n\n# Função para upload de arquivo\ndef upload_file(file):\n    try:\n        blob_client  container_client.get_blob_client(file.name)\n        blob_client.upload_blob(file, overwriteTrue)\n        st.success(f\"Arquivo '{file.name}' enviado com sucesso!\")\n    except Exception as e:\n        st.error(f\"Erro ao enviar arquivo: {str(e)}\")\n\n# Função para listar arquivos no container\ndef listar_arquivos():\n    try:\n        blobs  container_client.list_blobs()\n        return [blob.name for blob in blobs]\n    except Exception as e:\n        st.error(f\"Erro ao listar arquivos: {str(e)}\")\n        return []\n\n# Interface do Streamlit\nst.title(\"Upload para Azure Blob Storage\")\n\nuploaded_file  st.file_uploader(\"Escolha um arquivo para enviar\", type[\"csv\", \"txt\", \"png\", \"jpg\", \"pdf\"])\n\nif uploaded_file is not None:\n    if st.button(\"Enviar\"):\n        upload_file(uploaded_file)\n\nst.subheader(\"Arquivos no Container\")\narquivos  listar_arquivos()\nif arquivos:\n    for arquivo in arquivos:\n        st.write(f\"- {arquivo}\")\nelse:\n    st.write(\"Nenhum arquivo encontrado.\")\n```\n\n---\n\n#### **Configuração do Arquivo `.env`**\n\nCrie um arquivo chamado `.env` e adicione as variáveis de ambiente:\n\n```env\nAZURE_CLIENT_IDseu_client_id\nAZURE_TENANT_IDseu_tenant_id\nAZURE_CLIENT_SECRETseu_client_secret\nAZURE_STORAGE_URLhttps://<nome_da_storage>.blob.core.windows.net/\n```\n\n---\n\n#### **Como Executar o Projeto**\n\n1. **Inicie o Streamlit** com o seguinte comando:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n2. **Acesse a interface** no navegador através do link fornecido no terminal (por exemplo, `http://localhost:8501`).\n\n---\n\n#### **O que este Código Faz?**\n\n- **Upload de Arquivos**: O usuário pode selecionar um arquivo e enviá-lo para o **Blob Storage** clicando em \"Enviar\".\n- **Listagem de Arquivos**: Todos os arquivos presentes no container são listados abaixo da interface.\n- **Tratamento de Erros**: Mensagens de erro e sucesso são exibidas para garantir uma melhor experiência do usuário.\n\n---\n\n#### **Conclusão**\n\nCom este projeto, você pode enviar e gerenciar arquivos diretamente no **Azure Blob Storage** através de uma interface simples e intuitiva criada com **Streamlit**.\n\n---\n## Projeto 4. Máquinas Virtuais (VMs) no Azure  \nVMs no Azure permitem criar máquinas para processamento de dados e desenvolvimento de aplicações.  \n\n### Configuração de uma VM  \n1. No Azure Portal, vá em **Virtual Machine** > **Create**.  \n2. Escolha a região e imagem (ex.: Ubuntu Server 20.04 LTS).  \n3. Configure o tamanho (ex.: B1S).  \n4. Escolha entre SSH ou senha como método de login.\n\n### Configuração de Rede e Segurança  \n1. Crie uma VNet e uma sub-rede para isolar a comunicação.  \n2. Configure o NSG para liberar a porta 22 (SSH).  \n3. Defina o IP como estático.\n\n---\n\n## Deploy da Aplicação no Azure VM  \n\nPara rodar uma aplicação Streamlit em uma **VM do Azure** utilizando HTTP (porta 80), siga os passos abaixo.\n\n---\n\n### Configuração da VM no Azure  \n\n1. No **Azure Portal**, vá em **Virtual Machines** > **Create** > **Azure Virtual Machine**.  \n2. Preencha os detalhes básicos:  \n   - Nome: `streamlit-vm`  \n   - Região: **East US**  \n   - Imagem: **Ubuntu Server 20.04 LTS**  \n   - Tamanho: **B1S** (ou outro disponível no plano gratuito).  \n3. Em **Opções de Autenticação**, selecione **Chave SSH** e gere um novo par de chaves ou utilize um existente.  \n4. Configure a **Rede**:  \n   - Crie uma nova **Virtual Network (VNet)** e uma **sub-rede**.  \n   - Configure um **Network Security Group (NSG)** e adicione uma **regra de entrada** para liberar a **porta 80 (HTTP)**.  \n   - Defina o IP público como **estático**.  \n5. Clique em **Review + Create** e, após a validação, selecione **Create**.  \n\n---\n\n### Acesso à VM via SSH  \n\nApós a criação, conecte-se à VM utilizando o terminal:  \n\n```bash\nssh -i /caminho/para/chave.pem azureuser@<ip-publico-da-vm>\n```\n\n---\n\n### Instalação do Docker e Git na VM  \n\n1. Atualize os pacotes do sistema:  \n   ```bash\n   sudo apt update && sudo apt upgrade -y\n   ```  \n2. Instale o Docker:  \n   ```bash\n   sudo apt install docker.io -y\n   sudo systemctl start docker\n   sudo systemctl enable docker\n   ```  \n3. Adicione o usuário ao grupo Docker para evitar o uso do `sudo`:  \n   ```bash\n   sudo usermod -aG docker $USER\n   ```  \n4. Desconecte-se e reconecte-se para aplicar as permissões:  \n   ```bash\n   exit\n   ssh -i /caminho/para/chave.pem azureuser@<ip-publico-da-vm>\n   ```\n\n---\n\n### Configuração da Aplicação e Dockerfile  \n\n1. Clone o repositório com o código da aplicação:  \n   ```bash\n   git clone https://github.com/lvgalvao/hello-world-streamlit.git\n   cd hello-world-streamlit\n   ```  \n\n2. Crie o **Dockerfile** se ainda não existir:  \n   ```dockerfile\n   FROM python:3.9-slim\n   WORKDIR /app\n   COPY requirements.txt .\n   COPY app.py .\n   RUN pip install --no-cache-dir -r requirements.txt\n   EXPOSE 80\n   CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port80\", \"--server.address0.0.0.0\"]\n   ```\n\n---\n\n### Construção e Execução da Imagem Docker  \n\n1. **Construa a imagem Docker:**  \n   ```bash\n   docker build -t streamlit-app .\n   ```  \n2. **Rode o container na porta 80:**  \n   ```bash\n   docker run -d -p 80:80 streamlit-app\n   ```  \n\n---\n\n### Acessando a Aplicação  \n\nAbra o navegador e acesse a aplicação através do IP público da sua VM:  \n\n```\nhttp://<ip-publico-da-vm>\n```\n\n---\n\n### Solução de Problemas  \n\n1. **Erro de permissão com Docker:**  \n   Se receber um erro de permissão, tente:  \n   ```bash\n   sudo docker run -d -p 80:80 streamlit-app\n   ```  \n\n2. **Problemas de conexão via HTTP:**  \n   - Verifique se a porta 80 está aberta no **NSG**.  \n   - Confirme que o container está rodando:  \n     ```bash\n     docker ps\n     ```\n\n---\n\n### Parar o Container (Opcional)  \n\n1. Liste os containers em execução:  \n   ```bash\n   docker ps\n   ```  \n2. Pare o container usando o ID:  \n   ```bash\n   docker stop <container_id>\n   ```\n\n---\n\nAgora você tem sua aplicação **Streamlit \"Hello World\"** rodando em uma **VM do Azure** utilizando HTTP na porta 80, pronta para ser acessada e utilizada.\n\n---\n\n## Azure Active Directory (AAD)  \nO AAD é o sistema de identidade e controle de acesso do Azure, semelhante ao IAM da AWS.\n\n### Criando um Usuário e Atribuindo Funções  \n1. No portal, vá em **Azure Active Directory** > **Usuários** > **Novo usuário**.  \n2. Configure nome, função (Contributor) e senha inicial.  \n3. No Access Control (IAM), adicione o usuário como Contributor na VM.\n\n---\n\n## Automatizando Uploads para Azure Blob Storage  \nExemplo de código em Python:\n```python\nfrom azure.storage.blob import BlobServiceClient\n\nconnect_str  \"sua-connection-string\"\nservice_client  BlobServiceClient.from_connection_string(connect_str)\ncontainer_client  service_client.get_container_client(\"dados-clientes\")\ncontainer_client.create_container()\n\nwith open(\"exemplo.csv\", \"rb\") as data:\n    container_client.upload_blob(data)\n\nprint(\"Upload concluído!\")\n```\n\n---\n\n## Comparação: Azure vs AWS  \n| Serviço                 | Azure                      | AWS                   |\n|-------------------------|----------------------------|-----------------------|\n| Armazenamento           | Azure Blob Storage         | Amazon S3             |\n| Máquinas Virtuais       | Azure Virtual Machines     | EC2                   |\n| Controle de Acesso      | Azure Active Directory     | IAM                   |\n| Banco de Dados          | Azure SQL Database         | Amazon RDS            |\n| Rede Virtual            | Virtual Network (VNet)     | VPC                   |\n| Automação               | Azure Functions            | AWS Lambda            |\n\n---\n\n## Boas Práticas no Azure  \n- **Automação com CLI:** Use Azure CLI para tarefas repetitivas.  \n- **Tags:** Aplique tags para organizar e controlar custos.  \n- **Backup:** Use replicação GRS para resiliência.  \n- **Orçamento:** Configure alertas no Cost Management.\n\n---\n\n## Conclusão  \nEsta aula apresentou conceitos fundamentais do Azure e sua comparação com a AWS, mostrando como configurar contas, VMs, Storage Accounts e identidade. Na próxima aula, exploraremos pipelines de dados no Azure com Data Factory e SQL Database.\n\nSim, a **VNet (Virtual Network)** no Azure é equivalente à **VPC (Virtual Private Cloud)** na AWS, embora cada uma tenha suas particularidades. Ambas são usadas para criar redes isoladas onde você pode executar seus recursos, como máquinas virtuais e bancos de dados, garantindo segurança e controle de tráfego.\n\n### **Comparação entre VNet e VPC**\n\n| **Aspecto**                     | **VNet (Azure)**                           | **VPC (AWS)**                          |\n|----------------------------------|--------------------------------------------|---------------------------------------|\n| **Propósito**                   | Rede virtual para isolar recursos no Azure | Rede virtual para isolar recursos na AWS |\n| **Isolamento**                   | Totalmente isolada de outras VNets          | Totalmente isolada de outras VPCs    |\n| **Sub-redes**                    | Suporta múltiplas sub-redes dentro da VNet | Suporta múltiplas sub-redes na VPC   |\n| **Controle de Tráfego**          | NSG (Network Security Group) para regras de segurança | Security Groups e NACLs (Network ACLs) |\n| **Conectividade entre Redes**   | VNet Peering                               | VPC Peering                          |\n| **Gateway VPN**                  | VPN Gateway para conectar on-premises ou outras VNets | VPN Gateway para conectar on-premises ou outras VPCs |\n| **CIDR Blocks**                  | Define o intervalo IP privado com CIDR     | Define o intervalo IP privado com CIDR |\n| **DNS**                         | Azure-provided DNS ou customizado          | AWS-provided DNS ou customizado      |\n| **Gateway NAT**                  | NAT Gateway para acesso à internet         | NAT Gateway ou NAT Instance          |\n| **Firewall**                    | Azure Firewall                            | AWS Network Firewall                 |\n\n### **Principais Semelhanças**\n- **Isolamento:** Ambas permitem criar redes isoladas para manter a segurança e controle dos recursos.\n- **Sub-redes:** Em ambas, você pode definir várias sub-redes para segmentar diferentes tipos de recursos.\n- **Conectividade:** Tanto a VNet quanto a VPC suportam **peering**, permitindo comunicação entre redes.\n\n### **Diferenças Notáveis**\n1. **Controle de Tráfego:** \n   - Azure usa **NSGs (Network Security Groups)** para definir regras de tráfego para sub-redes e VMs.\n   - AWS utiliza **Security Groups** e **Network ACLs**.\n\n2. **Peering Global:**\n   - O Azure permite **VNet Peering** global entre regiões.\n   - A AWS também suporta **VPC Peering** entre contas e regiões, mas com algumas limitações dependendo do tipo de configuração.\n\n3. **Firewall:**\n   - O Azure oferece o **Azure Firewall** como um serviço integrado para proteção avançada.\n   - A AWS possui o **AWS Network Firewall** e outras ferramentas como **WAF** para proteção.\n\n### **Exemplo de Uso em Engenharia de Dados**\n- No Azure, você pode criar uma **VNet** e definir sub-redes específicas para hospedar uma **máquina virtual** que processa dados.\n- Essa VM pode acessar dados armazenados em uma **Storage Account** (como Blob Storage) pela VNet, garantindo que o tráfego não passe pela internet pública.\n\nEssa arquitetura é ideal para pipelines de dados seguros e de alta performance, como processamento de arquivos CSV para Parquet, conforme o projeto que você está desenvolvendo.\n\n### **Projeto 5. Criação de Banco de Dados SQL e Inserção de Dados com Streamlit**\n\nNecessário baixar o driver de sqlserver\n\nhttps://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server?redirectedfromMSDN&viewsql-server-ver16\n\n#### **1. Criando o Banco de Dados SQL no Azure**\n\n1. Acesse o portal Azure ([portal.azure.com](https://portal.azure.com)).\n2. Navegue para **SQL Databases** e clique em **Create**.\n3. **Configurações Básicas**:\n   - Nome do Banco: `meubancodedados`.\n   - Servidor: Crie um novo servidor ou selecione um existente.\n   - Grupo de Recursos: Selecione ou crie um novo grupo, por exemplo, `myResourceGroup`.\n   - Região: Selecione **Brazil South** ou outra região próxima.\n   - Elastic Pool: Escolha **Não** (se não for usar um pool).\n4. **Compute + Storage**:\n   - Escolha **General Purpose** com 2 vCores.\n   - Defina 32 GB de armazenamento e redundância local ou geo-redundante.\n5. Clique em **Review + Create** e depois em **Create**.\n\n#### **2. Configurando o Acesso (IAM)**\n\n1. Vá até o banco de dados criado e clique em **Access Control (IAM)**.\n2. Adicione uma função **Contributor** ou **Data Reader** ao serviço usado no **App Registration** que foi criado anteriormente.\n3. Copie a **string de conexão** do banco para utilizá-la no código.\n\n#### **3. Configurando o Projeto em Python com Streamlit**\n\n```sql\nCREATE TABLE pessoas (\n    id INT IDENTITY(1,1) PRIMARY KEY,  -- Coluna com auto incremento\n    nome VARCHAR(100) NOT NULL,         -- Nome com limite de 100 caracteres\n    idade INT NOT NULL                  -- Idade como número inteiro\n);\n```\n\n1. Instale as dependências no terminal:\n   ```bash\n   pip install streamlit pyodbc python-dotenv\n   ```\n2. Crie um arquivo **`.env`** com as variáveis:\n   ```\n   AZURE_DB_SERVER<seu-servidor>.database.windows.net\n   AZURE_DB_NAMEmeubancodedados\n   AZURE_DB_USER<seu-usuario>\n   AZURE_DB_PASSWORD<sua-senha>\n   ```\n3. Crie o seguinte **código Python** no arquivo `app.py`:\n\n   ```python\n   import streamlit as st\n   import pyodbc\n   from dotenv import load_dotenv\n   import os\n\n   # Carregar variáveis de ambiente\n   load_dotenv()\n\n   # Conectar ao banco de dados\n   server  os.getenv(\"AZURE_DB_SERVER\")\n   database  os.getenv(\"AZURE_DB_NAME\")\n   username  os.getenv(\"AZURE_DB_USER\")\n   password  os.getenv(\"AZURE_DB_PASSWORD\")\n   \n   connection_string  f'DRIVER{{ODBC Driver 17 for SQL Server}};SERVER{server};DATABASE{database};UID{username};PWD{password}'\n   conn  pyodbc.connect(connection_string)\n   cursor  conn.cursor()\n\n   # Interface Streamlit\n   st.title(\"Inserção de Dados no Banco SQL\")\n\n   nome  st.text_input(\"Nome\")\n   idade  st.number_input(\"Idade\", min_value0, max_value120)\n\n   if st.button(\"Inserir Dados\"):\n       cursor.execute(f\"INSERT INTO pessoas (nome, idade) VALUES (?, ?)\", (nome, idade))\n       conn.commit()\n       st.success(\"Dados inseridos com sucesso!\")\n\n   cursor.close()\n   conn.close()\n   ```\n\n4. Execute o **Streamlit**:\n   ```bash\n   streamlit run app.py\n   ```\n\n---\n\n### **Mermaid: Inserção de Dados com Streamlit e Azure SQL Database**\n\n```mermaid\nsequenceDiagram\n    participant User as Usuário (Interface Streamlit)\n    participant Streamlit as Streamlit App\n    participant Env as .env (Variáveis de Ambiente)\n    participant SQLDB as Banco de Dados SQL (Azure)\n    participant IAM as IAM (Controle de Acesso)\n\n    User->>Streamlit: 1. Preenche nome e idade na UI\n    User->>Streamlit: 2. Clica no botão \"Inserir Dados\"\n    \n    Streamlit->>Env: 3. Carrega variáveis de ambiente (.env)\n    Env-->>Streamlit: 4. Retorna credenciais do banco de dados\n    \n    Streamlit->>IAM: 5. Solicita acesso ao banco SQL\n    IAM-->>Streamlit: 6. Concede acesso (verificação do IAM)\n    \n    Streamlit->>SQLDB: 7. Conecta ao banco de dados\n    SQLDB-->>Streamlit: 8. Conexão estabelecida\n    \n    Streamlit->>SQLDB: 9. Insere dados na tabela 'pessoas'\n    SQLDB-->>Streamlit: 10. Confirmação de inserção\n\n    Streamlit->>User: 11. Exibe mensagem de sucesso\n```\n\n---\n\n### **Conclusão**\n\nCom este projeto, você agora possui uma aplicação em **Streamlit** conectada a um banco de dados **SQL no Azure**. A interface permite a inserção de dados e a configuração utiliza boas práticas, como o uso de **variáveis de ambiente** e **controle de acesso IAM**.\n\n",
        "Bootcamp - Cloud para dados/Aula_18_19/Dockerfile\n\n# Imagem base oficial do Python\nFROM python:3.9-slim\n\n# Define o diretório de trabalho dentro do container\nWORKDIR /app\n\n# Copia o arquivo de requisitos e o código para o container\nCOPY requirements.txt .\nCOPY app.py .\n\n# Instala o Streamlit e outras dependências\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Define a porta padrão que o Streamlit usará\nEXPOSE 80\n\n# Comando para rodar a aplicação Streamlit\nCMD [\"streamlit\", \"run\", \"app.py\", \"--server.port80\", \"--server.enableCORSfalse\"]\n\n\nBootcamp - Cloud para dados/Aula_18_19/README-resourcegroup.md\n\n### **Resource Group no Azure**\n\nO **Resource Group** no Azure é uma unidade lógica para **organizar, gerenciar e agrupar recursos** relacionados, como VMs, bancos de dados, contas de armazenamento e redes virtuais. Ele facilita o gerenciamento de infraestrutura em nuvem, garantindo que todos os recursos de um projeto ou serviço específico estejam concentrados em um único local.\n\n---\n\n### **Características Principais do Resource Group**\n\n1. **Organização e Gestão Centralizada:**\n   - Todos os recursos de uma aplicação ou projeto ficam organizados em um único grupo.\n   - Exemplo: Uma aplicação pode ter VMs, bancos de dados e uma VNet no mesmo Resource Group.\n\n2. **Controle de Acesso e Permissões:**\n   - Permissões podem ser atribuídas ao Resource Group inteiro usando **Azure Role-Based Access Control (RBAC)**.\n   - Assim, equipes específicas podem ter acesso apenas aos recursos dentro do grupo correspondente.\n\n3. **Monitoramento e Gestão de Custos:**\n   - Cada Resource Group pode ter seu custo monitorado separadamente.\n   - Ajuda a visualizar e controlar o orçamento de projetos específicos.\n\n4. **Aplicação de Tags:**\n   - Tags podem ser aplicadas para facilitar a **organização e busca** de recursos com base em critérios como projeto, cliente ou departamento.\n\n---\n\n### **Casos de Uso do Resource Group**\n\n- **Isolamento de Ambientes:**  \n  Organize ambientes de **produção, desenvolvimento e teste** em Resource Groups separados.\n\n- **Projetos Multi-Equipe:**  \n  Múltiplos times trabalhando em diferentes serviços podem criar grupos específicos para gerenciar seus recursos sem interferir entre si.\n\n- **Automação:**  \n  Scripts de **Infraestrutura como Código (IaC)**, como ARM Templates e Terraform, utilizam Resource Groups para provisionar e gerenciar recursos.\n\n---\n\n### **Benefícios de Usar Resource Groups**\n\n- **Escalabilidade e Flexibilidade:**  \n  Recursos podem ser adicionados e removidos rapidamente dentro de um grupo.\n\n- **Governança Simplificada:**  \n  Atribuir políticas de segurança e controle de acesso a grupos inteiros facilita a governança.\n\n- **Gerenciamento Facilitado:**  \n  Operações como **backup**, **exportação** ou **deleção** podem ser feitas no nível do Resource Group, afetando todos os recursos contidos nele.\n\n---\n\n### **Como Criar um Resource Group no Azure**\n\n1. **Acessar o Portal do Azure:**  \n   - [https://portal.azure.com](https://portal.azure.com)\n\n2. **Ir para Resource Groups:**  \n   - No menu lateral, selecione **Resource Groups** e clique em **Create**.\n\n3. **Preencher Informações:**\n   - **Nome:** Defina um nome significativo (ex.: `RG-ProjetoX`).\n   - **Região:** Escolha a região onde os recursos serão provisionados (ex.: **East US**).\n   - **Tags:** (Opcional) Aplique tags para facilitar o controle de custos e organização.\n\n4. **Criar o Resource Group:**  \n   - Clique em **Review + Create** e em seguida **Create**.\n\n---\n\n### **Conclusão**\nO **Resource Group** é essencial para o gerenciamento eficaz dos recursos no Azure. Ele não apenas organiza recursos, mas também simplifica a **administração**, **segurança** e **monitoramento** de projetos e serviços em nuvem.\n\n### **Resource Group no Azure e o Equivalente na AWS**\n\nNa **AWS**, o equivalente direto ao **Resource Group** do Azure é o **AWS Resource Groups**. Ambos servem para organizar e gerenciar recursos relacionados, mas existem algumas diferenças na implementação e uso.\n\n---\n\n### **Comparação: Resource Group (Azure) vs. AWS Resource Groups**\n\n| **Aspecto**               | **Resource Group (Azure)**                     | **Resource Groups (AWS)**                    |\n|---------------------------|-------------------------------------------------|---------------------------------------------|\n| **Propósito**              | Organizar recursos relacionados por projeto ou serviço. | Agrupar e gerenciar recursos por projeto, ambiente ou finalidade. |\n| **Controle de Acesso**     | Gerenciado via **Azure Role-Based Access Control (RBAC)** aplicado ao grupo inteiro. | Controle de acesso configurado por **IAM Policies** e **Tags**. |\n| **Monitoramento de Custos**| Monitoramento de custos do grupo no **Cost Management**. | Visualização de custos no **AWS Cost Explorer** por tags ou grupos. |\n| **Organização**            | Agrupa múltiplos recursos em um único container lógico. | Agrupa recursos por meio de **tags** aplicadas. |\n| **Tags**                   | Tags são opcionais e ajudam na categorização. | Tags são fundamentais e podem ser usadas para definir Resource Groups dinamicamente. |\n\n---\n\n### **Funcionamento do AWS Resource Groups**\n\n1. **Organização e Tags:**\n   - Os recursos na AWS não são \"alocados fisicamente\" dentro de um grupo, mas organizados e filtrados usando **tags**.\n   - Exemplo: Todos os recursos (EC2, RDS, S3) de um projeto podem ter a tag `Projeto: X`.\n\n2. **Gerenciamento Centralizado:**\n   - O AWS Resource Groups permite visualizar, gerenciar e executar operações em massa para todos os recursos associados a um projeto ou serviço específico.\n\n3. **Controle de Acesso:**  \n   - O controle é feito via **IAM Policies**, que podem conceder permissões com base em tags aplicadas aos recursos.\n\n4. **Monitoramento e Custos:**\n   - Utilizando o **AWS Cost Explorer** e **AWS Budgets**, é possível monitorar o custo de recursos organizados por tags ou grupos.\n\n---\n\n### **Quando Usar AWS Resource Groups?**\n\n- **Ambientes Multi-Projetos:**  \n  Organize recursos por projetos usando tags como `Projeto: Marketing` ou `Projeto: Financeiro`.\n\n- **Isolamento de Ambientes:**  \n  Defina ambientes de **produção**, **desenvolvimento** e **teste** utilizando tags para cada ambiente.\n\n- **Automação:**  \n  Scripts como Terraform e CloudFormation usam tags para organizar e automatizar a criação e gerenciamento de recursos.\n\n---\n\n### **Como Criar um Resource Group na AWS**\n\n1. **Acessar o Console AWS:**  \n   - [https://aws.amazon.com/console/](https://aws.amazon.com/console/)\n\n2. **Ir para Resource Groups:**  \n   - No menu superior, clique em **Resource Groups** > **Create Group**.\n\n3. **Definir Critérios do Grupo:**\n   - Nomeie o grupo e defina **filtros de tags**. Por exemplo: `Projeto: X`.\n\n4. **Revisar e Criar:**  \n   - Revise as configurações e clique em **Create Group**.\n\n---\n\n### **Conclusão**\nEmbora **Azure Resource Group** e **AWS Resource Groups** compartilhem a função de organizar e gerenciar recursos relacionados, a AWS depende fortemente de **tags** para criar grupos dinâmicos, enquanto o Azure usa uma abordagem mais direta com grupos lógicos. Ambos simplificam o gerenciamento e são essenciais para controle de custos e governança na nuvem.\n\nBootcamp - Cloud para dados/Aula_18_19/app.py\n\nimport streamlit as st\n\n# Título do aplicativo\nst.title(\"Hello, World! com Streamlit\")\n\n# Exibindo uma mensagem de boas-vindas\nst.write(\"Este é um exemplo simples de uma aplicação Streamlit em Docker.\")\n\n# Adicionando um botão de interação\nif st.button(\"Clique aqui\"):\n    st.success(\"Você clicou no botão!\")\n\n\nBootcamp - Cloud para dados/Aula_18_19/projeto_01.py\n\nfrom azure.identity import ClientSecretCredential  # Importa a classe para autenticação usando Client Secret\nfrom azure.keyvault.secrets import SecretClient  # Importa a classe para manipular segredos no Key Vault\nfrom dotenv import load_dotenv  # Importa a função para carregar variáveis de ambiente de um arquivo .env\nimport os  # Importa o módulo para acessar variáveis de ambiente\n\n# Carrega as variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Obtém as variáveis de ambiente necessárias para a autenticação\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nvault_url  os.environ[\"AZURE_VAULT_URL\"]\n\n# Nome do segredo a ser acessado no Key Vault\nsecret_name  \"ExemploKey\"\n\n# Cria uma credencial para autenticação no Azure utilizando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id, \n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Cria o cliente do Key Vault para acessar os segredos\nsecret_client  SecretClient(vault_urlvault_url, credentialcredentials)\n\n# Recupera o valor do segredo a partir do Key Vault\nsecret  secret_client.get_secret(secret_name)\n\n# Exibe o valor do segredo no terminal\nprint(\"O valor do segredo é: \" + secret.value)\n\n\nBootcamp - Cloud para dados/Aula_18_19/projeto_02.py\n\nfrom azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom dotenv import load_dotenv\nimport os\n\n# Carrega as variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Variáveis de ambiente necessárias\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nstorage_account_url  os.environ[\"AZURE_STORAGE_URL\"]  # Ex: https://<nome_da_storage>.blob.core.windows.net/\ncontainer_name  \"meucontainer\"  # Nome do container criado\n\n# Configura as credenciais usando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id,\n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Conectar ao Blob Storage\nblob_service_client  BlobServiceClient(\n    account_urlstorage_account_url,\n    credentialcredentials\n)\n\n# Acessa o container\ncontainer_client  blob_service_client.get_container_client(container_name)\n\n# Lista todos os arquivos dentro do container\nprint(f\"Listando arquivos no container '{container_name}':\")\nfor blob in container_client.list_blobs():\n    print(f\" - {blob.name}\")\n\n\nBootcamp - Cloud para dados/Aula_18_19/projeto_03.py\n\nimport streamlit as st\nfrom azure.identity import ClientSecretCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom dotenv import load_dotenv\nimport os\n\n# Carregar variáveis de ambiente do arquivo .env\nload_dotenv()\n\n# Variáveis de ambiente necessárias\nclient_id  os.environ['AZURE_CLIENT_ID']\ntenant_id  os.environ['AZURE_TENANT_ID']\nclient_secret  os.environ['AZURE_CLIENT_SECRET']\nstorage_account_url  os.environ[\"AZURE_STORAGE_URL\"]\ncontainer_name  \"meucontainer\"\n\n# Configura credenciais usando Client Secret\ncredentials  ClientSecretCredential(\n    client_idclient_id,\n    client_secretclient_secret,\n    tenant_idtenant_id\n)\n\n# Conectar ao Blob Storage\nblob_service_client  BlobServiceClient(\n    account_urlstorage_account_url,\n    credentialcredentials\n)\n\n# Acessa o container\ncontainer_client  blob_service_client.get_container_client(container_name)\n\n# Função para upload de arquivo\ndef upload_file(file):\n    try:\n        blob_client  container_client.get_blob_client(file.name)\n        blob_client.upload_blob(file, overwriteTrue)\n        st.success(f\"Arquivo '{file.name}' enviado com sucesso!\")\n    except Exception as e:\n        st.error(f\"Erro ao enviar arquivo: {str(e)}\")\n\n# Função para listar arquivos no container\ndef listar_arquivos():\n    try:\n        blobs  container_client.list_blobs()\n        return [blob.name for blob in blobs]\n    except Exception as e:\n        st.error(f\"Erro ao listar arquivos: {str(e)}\")\n        return []\n\n# Interface do Streamlit\nst.title(\"Upload para Azure Blob Storage\")\n\nuploaded_file  st.file_uploader(\"Escolha um arquivo para enviar\", type[\"csv\", \"txt\", \"png\", \"jpg\", \"pdf\"])\n\nif uploaded_file is not None:\n    if st.button(\"Enviar\"):\n        upload_file(uploaded_file)\n\nst.subheader(\"Arquivos no Container\")\narquivos  listar_arquivos()\nif arquivos:\n    for arquivo in arquivos:\n        st.write(f\"- {arquivo}\")\nelse:\n    st.write(\"Nenhum arquivo encontrado.\")\n\n\nBootcamp - Cloud para dados/Aula_18_19/projeto_05.py\n\nimport streamlit as st\nimport pyodbc\nimport os\nfrom dotenv import load_dotenv\n\n# Carregar variáveis de ambiente\nload_dotenv()\n\n# Configuração do banco de dados\nserver  os.getenv(\"DB_SERVER\")\ndatabase  os.getenv(\"DB_NAME\")\nusername  os.getenv(\"DB_USERNAME\")\npassword  os.getenv(\"DB_PASSWORD\")\nconnection_string  f'DRIVER{{ODBC Driver 17 for SQL Server}};SERVER{server};DATABASE{database};UID{username};PWD{password}'\n\n# Função para conectar ao banco\ndef connect_to_database():\n    try:\n        conn  pyodbc.connect(connection_string)\n        return conn\n    except Exception as e:\n        st.error(f\"Erro ao conectar ao banco de dados: {e}\")\n        return None\n\n# Função para inserir dados\ndef insert_data(nome, idade):\n    conn  connect_to_database()\n    if conn:\n        cursor  conn.cursor()\n        try:\n            cursor.execute(\"INSERT INTO pessoas (nome, idade) VALUES (?, ?)\", (nome, idade))\n            conn.commit()\n            st.success(f\"Dados inseridos: {nome}, {idade} anos\")\n        except Exception as e:\n            st.error(f\"Erro ao inserir dados: {e}\")\n        finally:\n            conn.close()\n\n# Interface Streamlit\nst.title(\"Inserção de Dados no Banco de Dados SQL\")\n\nnome  st.text_input(\"Nome:\")\nidade  st.number_input(\"Idade:\", min_value0, step1)\n\nif st.button(\"Inserir Dados\"):\n    if nome and idade:\n        insert_data(nome, idade)\n    else:\n        st.warning(\"Preencha todos os campos.\")\n\n\nBootcamp - Cloud para dados/Aula_18_19/requirements.txt\n\nstreamlit\npython-dotenv\nazurealtair5.4.1\nattrs24.2.0\nazure-core1.31.0\nazure-identity1.19.0\nazure-keyvault4.2.0\nazure-keyvault-certificates4.9.0\nazure-keyvault-keys4.10.0\nazure-keyvault-secrets4.9.0\nazure-storage-blob12.23.1\nblinker1.8.2\ncachetools5.5.0\ncertifi2024.8.30\ncffi1.17.1\ncharset-normalizer3.4.0\nclick8.1.7\ncolorama0.4.6\ncryptography43.0.3\ngitdb4.0.11\nGitPython3.1.43\nidna3.10\nisodate0.7.2\nJinja23.1.4\njsonschema4.23.0\njsonschema-specifications2024.10.1\nmarkdown-it-py3.0.0\nMarkupSafe3.0.2\nmdurl0.1.2\nmsal1.31.0\nmsal-extensions1.2.0\nnarwhals1.11.1\nnumpy2.1.2\npackaging24.1\npandas2.2.3\npillow10.4.0\nportalocker2.10.1\nprotobuf5.28.3\npyarrow18.0.0\npycparser2.22\npydeck0.9.1\nPygments2.18.0\nPyJWT2.9.0\npyodbc5.2.0\npython-dateutil2.9.0.post0\npython-dotenv1.0.1\npytz2024.2\npywin32308\nreferencing0.35.1\nrequests2.32.3\nrich13.9.3\nrpds-py0.20.0\nsix1.16.0\nsmmap5.0.1\nstreamlit1.39.0\ntenacity9.0.0\ntoml0.10.2\ntornado6.4.1\ntyping_extensions4.12.2\ntzdata2024.2\nurllib32.2.3\nwatchdog5.0.3\n\n\n"
    ],
    "04-workflow-orchestration-deploy-airflow": [
        "04-workflow-orchestration-deploy-airflow/README.md\n\n# Orquestração de ETLs\n\n## Eu realmente preciso do Airflow?\n\nhttps://link.excalidraw.com/l/8pvW6zbNUnD/4OrmI6gFNlj\n\nVamos pensar em uma pipeline inicial, onde possui 3 atividades e uma pipeline que encadeia ela.\n\n![Exemplo_00](./pic/exemplo_00.png)\n\nexemplo_00.py\n```python\nfrom time import sleep\n\ndef primeira_atividade():\n    print(\"Primeira atividade iniciada\")\n    sleep(1)\n    print(\"Primeira atividade finalizada\")\n\ndef segunda_atividade():\n    print(\"Segunda atividade iniciada\")\n    sleep(1)\n    print(\"Segunda atividade finalizada\")\n\ndef terceira_atividade():\n    print(\"Terceira atividade iniciada\")\n    sleep(1)\n    print(\"Terceira atividade finalizada\")\n\ndef pipeline():\n    primeira_atividade()\n    segunda_atividade()\n    terceira_atividade()\n    print(\"Pipeline finalizada\")\n\nif __name__  \"__main__\":\n    pipeline()\n```\n\nQual o problema de realizar uma ETL desse jeito?\n\n![Exemplo_01](./pic/exemplo_01.png)\n\nComo programar ela a cada 10 segundos?\n\nexemplo_01.py\n```python\nfrom time import sleep\n\ndef primeira_atividade():\n    print(\"Primeira atividade iniciada\")\n    sleep(1)\n    print(\"Primeira atividade finalizada\")\n\ndef segunda_atividade():\n    print(\"Segunda atividade iniciada\")\n    sleep(1)\n    print(\"Segunda atividade finalizada\")\n\ndef terceira_atividade():\n    print(\"Terceira atividade iniciada\")\n    sleep(1)\n    print(\"Terceira atividade finalizada\")\n\ndef pipeline():\n    primeira_atividade()\n    segunda_atividade()\n    terceira_atividade()\n    print(\"Pipeline finalizada\")\n\nif __name__  \"__main__\":\n    while True:\n        pipeline()\n        sleep(10)\n```\n\nQual o problema de realizar uma ETL desse jeito?\n\n![Exemplo_02](./pic/exemplo_02.png)\n\nComo ter o log a auditoria do que foi feito?\n\n```python\nfrom time import sleep\n\nfrom loguru import logger\n\nlogger.add(\"execution_logs.log\", format\"{time} - {message}\", level\"INFO\", rotation\"1 day\")\n\ndef primeira_atividade():\n    logger.info(\"Primeira atividade iniciada\")\n    sleep(1)\n    logger.info(\"Primeira atividade finalizada\")\n\ndef segunda_atividade():\n    logger.info(\"Segunda atividade iniciada\")\n    sleep(1)\n    logger.info(\"Segunda atividade finalizada\")\n\ndef terceira_atividade():\n    logger.info(\"Terceira atividade iniciada\")\n    sleep(1)\n    logger.info(\"Terceira atividade finalizada\")\n\ndef pipeline():\n    primeira_atividade()\n    segunda_atividade()\n    terceira_atividade()\n    logger.info(\"Pipeline finalizada\")\n\nif __name__  \"__main__\":\n    while True:\n        pipeline()\n        sleep(10)\n```\n\nQual o problema de realizar uma ETL desse jeito?\n\n![Exemplo_03](./pic/exemplo_03.png)\n\nComo ter um webserver (dashboard) para ver o que foi feito?\n\n(...)\n\n## Vocês entenderam onde eu quero chegar\n\nGastei 10% do tempo gerando valor e 90% do tempo reinventando a roda\n\n## Airflow Overview\n\n![Exemplo_03](./pic/airflow_overview.png)\n\nO Apache Airflow é uma plataforma projetada para criar, agendar e monitorar fluxos de trabalho de forma programática.\n\nQuando os fluxos de trabalho são definidos como código, eles se tornam mais fáceis de manter, versionar, testar e colaborar.\n\nUtilize o Airflow para compor fluxos de trabalho como grafos acíclicos dirigidos (DAGs) de tarefas. O agendador do Airflow executa suas tarefas em uma série de workers respeitando as dependências definidas. Ferramentas de linha de comando abrangentes facilitam a realização de operações complexas nos DAGs. A interface de usuário intuitiva permite visualizar facilmente os pipelines em execução, monitorar o progresso e resolver problemas quando necessário.\n\nO Airflow é especialmente útil em contextos de engenharia de dados e ciência de dados, pois permite a automação e a orquestração de processos complexos de tratamento de dados, treinamento de modelos de machine learning, execução de ETLs e muito mais. Tudo isso contribui para uma gestão mais eficiente do ciclo de vida dos dados e dos modelos preditivos.\n\n## User Interface\n\n- **DAGs**: Visão deral do seu ambiente.\n\n  ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/dags.png)\n\n- **Grid**: Visão de todas as execuções de um DAG.\n\n  ![Grid](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/grid.png)\n\n- **Graph**: Visão de todas as tarefas de um DAG e suas dependências.\n\n  ![Graph](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/graph.png)\n\n- **Task Duration**: Tempo de execução de cada tarefa de um DAG.\n\n  ![Task Duration](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/duration.png)\n\n- **Gantt**: Duração de cada execução de um DAG.\n\n  ![Gantt](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/gantt.png)\n\n- **Code**: Código de cada DAG.\n\n  ![Code](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/code.png)\n\n## Quick Start\n\nVamos fazer o nosso primeiro projeto com Airflow, vamos refatorar o nosso código acima.\n\nPorém, precisamos criar toda nossa infraestrutura do Airflow.\n\n[![Exemplo_04](./pic/airflow_tech.png)](https://docs.astronomer.io/learn/airflow-components)\n\nComo subir toda essa infra?\n\nPara isso vamos conhecer o Astro CLI\n\n## Criando um projeto Astro\n\nEm um diretório vazio, rode o seguinte comando:\n\n```bash\nastro dev init\n```\n\nEsse comando vai gerar os seguintes arquivos no seu diretório:\n\n![astro_init](./pic/astro_init.png)\n\n```graphql\n.\n├── .env                        # Variáveis de ambiente locais\n├── dags                        # Onde suas DAGs ficam\n│   ├── example-dag-basic.py    # DAG de exemplo que mostra uma simples pipeline de dados ETL\n│   └── example-dag-advanced.py # DAG de exemplo que mostra recursos mais avançados do Airflow, como a API TaskFlow\n├── Dockerfile                  # Para a imagem Docker do Astro Runtime, variáveis de ambiente e sobrescritas\n├── include                     # Para quaisquer outros arquivos que você gostaria de incluir\n├── plugins                     # Para quaisquer plugins personalizados ou da comunidade do Airflow\n│   └── example-plugin.py\n├── tests                       # Para quaisquer arquivos de teste de unidade das DAGs a serem executados com pytest\n│   └── test_dag_example.py     # Teste que verifica erros básicos em suas DAGs\n├── airflow_settings.yaml       # Para suas conexões do Airflow, variáveis e pools (localmente)\n├── packages.txt                # Para pacotes a nível do sistema operacional\n└── requirements.txt            # Para pacotes Python\n```\n\n### DAGs\n\n[Documentação](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)\n\n\nUma DAG (Directed Acyclic Graph ou Grafo Acíclico Direcionado) é o conceito central do Airflow, reunindo Tarefas, organizadas com dependências e relações para dizer como elas devem ser executadas.\n\n![dag](./pic/dag.png)\n\nEla define quatro Tarefas - A, B, C e D - e dita a ordem na qual devem ser executadas, e quais tarefas dependem de quais outras. Também determina com que frequência a DAG deve ser executada - talvez \"a cada 5 minutos a partir de amanhã\", ou \"todos os dias desde 1º de janeiro de 2020\".\n\nA própria DAG não se preocupa com o que está acontecendo dentro das tarefas; ela está meramente preocupada em como executá-las - a ordem de execução, quantas vezes tentar novamente em caso de falha, se elas têm tempos de espera, e assim por diante.\n\nDeclarando uma DAG Existem três maneiras de declarar uma DAG - você pode usar um gerenciador de contexto, que adicionará a DAG a qualquer coisa dentro dele implicitamente:\n\n```python\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\nwith DAG(\n    dag_id\"meu_nome_de_dag\",\n    start_datedatetime.datetime(2021, 1, 1),\n    schedule\"@daily\",\n    catchupFalse\n):\n    EmptyOperator(task_id\"tarefa\")\n```\n\nOu, você pode usar um construtor padrão, passando a DAG para qualquer operador que você usar:\n\n```python\nimport datetime\n\nfrom airflow import DAG\nfrom airflow.operators.empty import EmptyOperator\n\nminha_dag  DAG(\n    dag_id\"meu_nome_de_dag\",\n    start_datedatetime.datetime(2024, 3, 23),\n    schedule\"@daily\",\n    catchupFalse,  # Adiciona esta linha\n)\nEmptyOperator(task_id\"tarefa\", dagminha_dag)\n```\n\nOu, você pode usar o decorador `@dag` para transformar uma função em um gerador de DAG:\n\n```python\nimport datetime\n\nfrom airflow.decorators import dag\nfrom airflow.operators.empty import EmptyOperator\n\n@dag(start_datedatetime.datetime(2024, 3, 23), schedule\"@daily\", catchupFalse)\ndef gerar_dag():\n    EmptyOperator(task_id\"tarefa\")\n\ngerar_dag()\n```\n\nDAGs não são nada sem Tasks para rodar, e essas sempre vão vir em forma de Operadores, Sensores ou Taskflows.\n\n### Dependências de Tasks\n\nUma Task/Operador normalmente não vive sozinha, ela depende de outras tasks (aquelas que suas que são seu upstream), e outras tasks dependem dela (aquelas que são seu downstream). Declaram essas dependencias entre elas é o que cria uma estrutura de DAG (as edges/linhas dos directed acyclic graph)\n\nExistem duas maneiras principais de declarar suas dependências, os operadores `>>` e `<<`\n\n```python\nfirst_task >> [second_task, third_task]\nthird_task << fourth_task\n```\n\nVocê também pode declarar de uma maneira mais explicita com os métodos `set_upstream` e `set_downstream`\n\n```python\nfirst_task.set_downstream([second_task, third_task])\nthird_task.set_upstream(fourth_task)\n```\n\nTambém existem alguns atalhos para declarar DAGs mais compelxas. Se você quer que duas lista de duas TASKS dependam todas de uma da outra, você pode usar o `cross_downstream`:\n\n```python\nfrom airflow.models.baseoperator import cross_downstream\n\n# Replaces\n# [op1, op2] >> op3\n# [op1, op2] >> op4\ncross_downstream([op1, op2], [op3, op4])\n```\n\nE se você quer que uma série de tasks tenham depedências entre sí, você pode usar uma `chain`:\n\n```python\nfrom airflow.models.baseoperator import chain\n\n# Replaces op1 >> op2 >> op3 >> op4\nchain(op1, op2, op3, op4)\n\n# Replaces\n# op1 >> op2 >> op4 >> op6\n# op1 >> op3 >> op5 >> op6\nchain(op1, [op2, op3], [op4, op5], op6)\n```\n\n### Carregando DAGs\n\nAirflow carrega as DAGs dos seus arquivos Python, declarados dentro do seu `DAG_FOLDER`. Que irá pegar cada arquivo, executar, e carregar dentro de um objeto do tipo DAG.\n\n### Rodando as DAGs\n\nAs DAGs vão rodar sempre de duas maneiras:\n\n- Quando você são `triggered` seja de forma manual ou via uma API\n\n- Definida via `schedule`, que é parte integrante de uma DAG\n\nVocê pode declarar seu schedule via argumento, exatamente assim:\n\nO Airflow utiliza uma notação semelhante à dos cron jobs para definir os intervalos de agendamento das DAGs. Aqui estão alguns exemplos dos valores possíveis para o parâmetro schedule:\n\n@once - Executa a DAG apenas uma vez, no momento da sua criação.\n\"* * * * *\" - Executa a DAG a cada minuto\n@hourly - Executa a DAG a cada hora.\n@daily - Executa a DAG uma vez por dia.\n@weekly - Executa a DAG uma vez por semana.\n@monthly - Executa a DAG uma vez por mês.\n@yearly - Executa a DAG uma vez por ano.\nNone - Se você não quer que a DAG seja agendada, mas apenas acionada manualmente, você pode usar None.\nNotação cron (e.g., \"0 0 * * *\" para meia-noite diária, \"*/10 * * * *\" para a cada 10 minutos) - Permite especificar intervalos de agendamento personalizados utilizando a sintaxe cron.\n\n### Nossa primeira ETL com Python\n\n### Fazendo o Nosso Deploy\n\n### Refatorando um Projeto seguindo MVC\n\n\n\n04-workflow-orchestration-deploy-airflow/exemplo_00.py\n\nfrom time import sleep\n\ndef primeira_atividade():\n    print(\"Primeira atividade iniciada\")\n    sleep(1)\n    print(\"Primeira atividade finalizada\")\n\ndef segunda_atividade():\n    print(\"Segunda atividade iniciada\")\n    sleep(1)\n    print(\"Segunda atividade finalizada\")\n\ndef terceira_atividade():\n    print(\"Terceira atividade iniciada\")\n    sleep(1)\n    print(\"Terceira atividade finalizada\")\n\ndef pipeline():\n    primeira_atividade()\n    segunda_atividade()\n    terceira_atividade()\n    print(\"Pipeline finalizada\")\n\nif __name__  \"__main__\":\n    pipeline()\n\n04-workflow-orchestration-deploy-airflow/exemplo_01.py\n\nfrom time import sleep\n\ndef primeira_atividade():\n    print(\"Primeira atividade iniciada\")\n    sleep(1)\n    print(\"Primeira atividade finalizada\")\n\ndef segunda_atividade():\n    print(\"Segunda atividade iniciada\")\n    sleep(1)\n    print(\"Segunda atividade finalizada\")\n\ndef terceira_atividade():\n    print(\"Terceira atividade iniciada\")\n    sleep(1)\n    print(\"Terceira atividade finalizada\")\n\ndef pipeline():\n    primeira_atividade()\n    segunda_atividade()\n    terceira_atividade()\n    print(\"Pipeline finalizada\")\n\nif __name__  \"__main__\":\n    while True:\n        pipeline()\n        sleep(10)\n\n\n04-workflow-orchestration-deploy-airflow/exemplo_02/exemplo_02.py\n\nfrom time import sleep\n\nfrom loguru import logger\n\nlogger.add(\"execution_logs.log\", format\"{time} - {message}\", level\"INFO\", rotation\"1 day\")\n\ndef primeira_atividade():\n    logger.info(\"Primeira atividade iniciada\")\n    sleep(1)\n    logger.info(\"Primeira atividade finalizada\")\n\ndef segunda_atividade():\n    logger.info(\"Segunda atividade iniciada\")\n    sleep(1)\n    logger.info(\"Segunda atividade finalizada\")\n\ndef terceira_atividade():\n    logger.info(\"Terceira atividade iniciada\")\n    sleep(1)\n    logger.info(\"Terceira atividade finalizada\")\n\ndef pipeline():\n    primeira_atividade()\n    segunda_atividade()\n    terceira_atividade()\n    logger.info(\"Pipeline finalizada\")\n\nif __name__  \"__main__\":\n    while True:\n        pipeline()\n        sleep(10)\n\n04-workflow-orchestration-deploy-airflow/exemplo_02/requirements.txt\n\nloguru\n\n04-workflow-orchestration-deploy-airflow/exemplo_03/exemplo_03.py\n\n# poetry add streamlit pandas\n\nimport streamlit as st\nimport pandas as pd\nimport subprocess\n\n# Função para carregar os dados do arquivo CSV\ndef load_data():\n    df  pd.read_csv(\"execution_logs.log\")\n    return df\n\n# Função para executar o script Python\ndef run_python_script():\n    subprocess.run(\"poetry run python pipeline/pipeline.py\")\n\n\n# Layout do aplicativo Streamlit\ndef main():\n    st.title(\"Visualização de Logs e Execução de Scripts\")\n    st.image(\"pics/AirflowLogo.png\")\n\n    # Carregar os dados do arquivo CSV\n    df  load_data()\n\n    # Exibir os dados na interface do Streamlit\n    st.write(\"Logs de Execução:\", df)\n\n    # Botão para atualizar os dados\n    if st.button(\"Atualizar Dados\"):\n        df  load_data()\n        st.write(\"Dados Atualizados com Sucesso!\")\n\n    # Botão para executar o script Python\n    if st.button(\"Executar Script Python\"):\n        run_python_script()\n        st.write(\"Script Python executado com sucesso!\")\n\nif __name__  \"__main__\":\n    main()\n\n\n04-workflow-orchestration-deploy-airflow/exemplo_03/requirements.txt\n\nloguru\npandas\nstreamlit\n\n04-workflow-orchestration-deploy-airflow/exemplo_03/pipeline/pipeline.py\n\nfrom time import sleep\n\nfrom loguru import logger\n\nlogger.add(\"execution_logs.log\", format\"{time} - {message}\", level\"INFO\", rotation\"1 day\")\n\ndef primeira_atividade():\n    logger.info(\"Primeira atividade iniciada\")\n    sleep(1)\n    logger.info(\"Primeira atividade finalizada\")\n\ndef segunda_atividade():\n    logger.info(\"Segunda atividade iniciada\")\n    sleep(1)\n    logger.info(\"Segunda atividade finalizada\")\n\ndef terceira_atividade():\n    logger.info(\"Terceira atividade iniciada\")\n    sleep(1)\n    logger.info(\"Terceira atividade finalizada\")\n\ndef pipeline():\n    primeira_atividade()\n    segunda_atividade()\n    terceira_atividade()\n    logger.info(\"Pipeline finalizada\")\n\nif __name__  \"__main__\":\n    while True:\n        pipeline()\n        sleep(10)\n\n04-workflow-orchestration-deploy-airflow/exemplo_04/primeira_dag_com_python_operator.py\n\nfrom datetime import datetime\n\nfrom airflow.decorators import dag, task\n\nfrom time import sleep\n\n@dag(start_datedatetime(2024, 3, 23), \n     schedule\"@daily\", \n     catchupFalse)\ndef primeira_dag_com_python_operator():\n    \"\"\"\n    minha primeira Pipipeline\n    \"\"\"\n    @task\n    def primeira_atividade():\n        print(\"Primeira atividade iniciada\")\n        sleep(1)\n        print(\"Primeira atividade finalizada\")\n\n    @task\n    def segunda_atividade():\n        print(\"Segunda atividade iniciada\")\n        sleep(1)\n        print(\"Segunda atividade finalizada\")\n\n    @task\n    def terceira_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    t1  primeira_atividade()\n    t2  segunda_atividade()\n    t3  terceira_atividade()\n\n    t1 >> t2 >> t3\n\nprimeira_dag_com_python_operator()\n\n04-workflow-orchestration-deploy-airflow/exemplo_04/quarta_dag_com_python_operator.py\n\nfrom datetime import datetime\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import cross_downstream\n\nfrom time import sleep\n\n@dag(start_datedatetime(2024, 3, 23), \n     schedule\"@daily\", \n     catchupFalse)\ndef quarta_dag_com_python_operator():\n    \"\"\"\n    minha primeira Pipipeline\n    \"\"\"\n    @task\n    def primeira_atividade():\n        print(\"Primeira atividade iniciada\")\n        sleep(1)\n        print(\"Primeira atividade finalizada\")\n\n    @task\n    def segunda_atividade():\n        print(\"Segunda atividade iniciada\")\n        sleep(1)\n        print(\"Segunda atividade finalizada\")\n\n    @task\n    def terceira_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    @task\n    def quarta_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    t1  primeira_atividade()\n    t2  segunda_atividade()\n    t3  terceira_atividade()\n    t4  quarta_atividade()\n\n    cross_downstream([t1,t2],[t3,t4])\n\nquarta_dag_com_python_operator()\n\n04-workflow-orchestration-deploy-airflow/exemplo_04/quinta_dag_com_python_operator.py\n\nfrom datetime import datetime\n\nfrom airflow.decorators import dag, task\nfrom airflow.models.baseoperator import chain\n\nfrom time import sleep\n\n@dag(start_datedatetime(2024, 3, 23), \n     schedule\"@daily\", \n     catchupFalse)\ndef quinta_dag_com_python_operator():\n    \"\"\"\n    minha primeira Pipipeline\n    \"\"\"\n    @task(owner\"luciano\", retries3)\n    def primeira_atividade():\n        \"\"\"\n        essa e minha primeira atividade\n        \"\"\"\n        print(\"Primeira atividade iniciada\")\n        sleep(1)\n        print(\"Primeira atividade finalizada\")\n\n    @task\n    def segunda_atividade():\n        print(\"Segunda atividade iniciada\")\n        sleep(1)\n        print(\"Segunda atividade finalizada\")\n\n    @task\n    def terceira_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    @task\n    def quarta_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    @task\n    def quinta_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n    \n    @task\n    def sexta_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    t1  primeira_atividade()\n    t2  segunda_atividade()\n    t3  terceira_atividade()\n    t4  quarta_atividade()\n    t5  quinta_atividade()\n    t6  sexta_atividade()\n\n    chain(t1,[t2,t3],[t4,t5], t6)\n\nquinta_dag_com_python_operator()\n\n04-workflow-orchestration-deploy-airflow/exemplo_04/segunda_dag_com_python_operator.py\n\nfrom datetime import datetime\n\nfrom airflow.decorators import dag, task\n\nfrom time import sleep\n\n@dag(start_datedatetime(2024, 3, 23), \n     schedule\"@daily\", \n     catchupFalse)\ndef segunda_dag_com_python_operator():\n    \"\"\"\n    minha primeira Pipipeline\n    \"\"\"\n    @task\n    def primeira_atividade():\n        print(\"Primeira atividade iniciada\")\n        sleep(1)\n        print(\"Primeira atividade finalizada\")\n\n    @task\n    def segunda_atividade():\n        print(\"Segunda atividade iniciada\")\n        sleep(1)\n        print(\"Segunda atividade finalizada\")\n\n    @task\n    def terceira_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    @task\n    def quarta_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    t1  primeira_atividade()\n    t2  segunda_atividade()\n    t3  terceira_atividade()\n    t4  quarta_atividade()\n\n    t1 >> [t2,t3]\n    t3 << t4\n\nsegunda_dag_com_python_operator()\n\n04-workflow-orchestration-deploy-airflow/exemplo_04/terceira_dag_com_python_operator.py\n\nfrom datetime import datetime\n\nfrom airflow.decorators import dag, task\n\nfrom time import sleep\n\n@dag(start_datedatetime(2024, 3, 23), \n     schedule\"@daily\", \n     catchupFalse)\ndef terceira_dag_com_python_operator():\n    \"\"\"\n    minha primeira Pipipeline\n    \"\"\"\n    @task\n    def primeira_atividade():\n        print(\"Primeira atividade iniciada\")\n        sleep(1)\n        print(\"Primeira atividade finalizada\")\n\n    @task\n    def segunda_atividade():\n        print(\"Segunda atividade iniciada\")\n        sleep(1)\n        print(\"Segunda atividade finalizada\")\n\n    @task\n    def terceira_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    @task\n    def quarta_atividade():\n        print(\"Terceira atividade iniciada\")\n        sleep(1)\n        print(\"Terceira atividade finalizada\")\n\n    t1  primeira_atividade()\n    t2  segunda_atividade()\n    t3  terceira_atividade()\n    t4  quarta_atividade()\n\n    t1.set_downstream([t2,t3])\n    t3.set_upstream(t4)\n\nterceira_dag_com_python_operator()\n\n04-workflow-orchestration-deploy-airflow/exemplo_05/pegar_um_personagem.py\n\nimport requests\n\nAPI  \"https://www.boredapi.com/api/activity\"\n\ndef descobrir_atividade():\n\n    def pegar_atividade():\n        r  requests.get(API, timeout10)\n        return r.json()\n    \n    def salvar_atividade_em_um_arquivo(response):\n        filepath  \"./activity.txt\"\t\n        with open(filepath, \"w\") as f:\n            f.write(f\"Hoje eu vou: {response['activity']}\")\n        return filepath\n\n    def ler_atividade_do_arquivo(filepath):\n        with open(filepath, \"r\") as f:\n            print(f.read())\n\n    real_response  pegar_atividade()\n    real_caminho  salvar_atividade_em_um_arquivo(real_response)\n    ler_atividade_do_arquivo(real_caminho)\n\ndescobrir_atividade()\n\n\n",
        "04-workflow-orchestration-deploy-airflow/exemplo_05/pegar_um_personagem_airflow.py\n\nfrom airflow.decorators import dag, task\n\nfrom airflow.models import Variable\n\nfrom datetime import datetime\n\nimport requests\n\nAPI  \"https://www.boredapi.com/api/activity\"\n\n@dag(dag_id\"pegar_um_personagem_airflow\",\n     description\"pipeline que pega um personagem, salva em um arquivo temp e le\",\n     schedule\"* * * * *\",\n     start_datedatetime(2024,3,23),\n     catchupFalse,\n     tags[\"tutorial\"])\ndef descobrir_atividade():\n\n    @task\n    def pegar_atividade():\n        r  requests.get(API, timeout10)\n        return r.json()\n    \n    @task\n    def salvar_atividade_em_um_arquivo(response):\n        filepath  Variable.get(\"activity_file\")\n        with open(filepath, \"a\") as f:\n            f.write(f\"Hoje eu vou: {response['activity']}\")\n        return filepath\n\n    @task\n    def ler_atividade_do_arquivo(filepath):\n        with open(filepath, \"r\") as f:\n            print(f.read())\n\n    t1  pegar_atividade()\n    t2  salvar_atividade_em_um_arquivo(t1)\n    t3  ler_atividade_do_arquivo(t2)\n\n    t1 >> t2 >> t3\n\ndescobrir_atividade()\n\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/controller.py\n\nimport requests\nfrom db import SessionLocal, engine, Base\nfrom models import Pokemon\nfrom schema import PokemonSchema\nfrom random import randint\n\nBase.metadata.create_all(bindengine)\n\ndef gerar_numero_aleatorio():\n    return randint(1, 350)\n\ndef fetch_pokemon_data(pokemon_id: int):\n    response  requests.get(f\"https://pokeapi.co/api/v2/pokemon/{pokemon_id}\")\n    if response.status_code  200:\n        data  response.json()\n        types  ', '.join(type['type']['name'] for type in data['types'])\n        return PokemonSchema(namedata['name'], typetypes)\n    else:\n        return None\n\ndef add_pokemon_to_db(pokemon_schema: PokemonSchema) -> Pokemon:\n    with SessionLocal() as db:\n        db_pokemon  Pokemon(namepokemon_schema.name, typepokemon_schema.type)\n        db.add(db_pokemon)\n        db.commit()\n        db.refresh(db_pokemon)\n    return db_pokemon\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/db.py\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nSQLALCHEMY_DATABASE_URL  \"postgresql://dbuser:ldxrJfmAE68oYsMKqKhcQwSeXh4Kevgc@dpg-cnv1g4ljm4es73dpa26g-a.oregon-postgres.render.com/dbname_3s0u\"\nengine  create_engine(SQLALCHEMY_DATABASE_URL)\nSessionLocal  sessionmaker(autocommitFalse, autoflushFalse, bindengine)\nBase  declarative_base()\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/main.py\n\nimport time\nimport random\nfrom controller import fetch_pokemon_data, add_pokemon_to_db, gerar_numero_aleatorio\n\ndef main():\n    while True:\n        pokemon_id  gerar_numero_aleatorio()  # Gera um ID aleatório entre 1 e 350\n        pokemon_schema  fetch_pokemon_data(pokemon_id)\n        if pokemon_schema:\n            print(f\"Adicionando {pokemon_schema.name} ao banco de dados.\")\n            add_pokemon_to_db(pokemon_schema)\n        else:\n            print(f\"Não foi possível obter dados para o Pokémon com ID {pokemon_id}.\")\n        time.sleep(10)\n\nif __name__  \"__main__\":\n    main()\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/models.py\n\nfrom sqlalchemy import Column, Integer, String, DateTime\nfrom sqlalchemy.sql import func\nfrom db import Base\n\nclass Pokemon(Base):\n    __tablename__  'pokemons'\n    id  Column(Integer, primary_keyTrue, indexTrue)\n    name  Column(String)\n    type  Column(String)\n    created_at  Column(DateTime, defaultfunc.now())  # Campo adicionado\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/schema.py\n\nfrom pydantic import BaseModel\n\nclass PokemonSchema(BaseModel):\n    name: str\n    type: str\n\n    class Config:\n        from_attributes  True\n\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/final/capturar_pokemon.py\n\nfrom airflow.decorators import task, dag\n\nfrom include.controller import fetch_pokemon_data, add_pokemon_to_db, gerar_numero_aleatorio\n\nfrom datetime import datetime\n\n@dag(dag_id\"capturar_pokemon\",\n     description\"pipeline_para_capturar_pokemon\",\n     start_datedatetime(2024,3,23),\n     schedule\"* * * * *\",\n     catchupFalse)\ndef capturar_pokemon():\n\n    @task(task_id'gerar_numero_aleatorio')\n    def task_gerar_numero_aleatorio():\n        return gerar_numero_aleatorio()\n\n    @task(task_id'fetch_pokemon_data')\n    def task_fetch_pokemon_data(numero_aleatorio):\n        return fetch_pokemon_data(numero_aleatorio)\n    \n    @task(task_id'add_pokemon_to_db')\n    def task_add_pokemon_to_db(pokemon_data):\n        add_pokemon_to_db(pokemon_data)\n    \n    t1  task_gerar_numero_aleatorio()\n    t2  task_fetch_pokemon_data(t1)\n    t3  task_add_pokemon_to_db(t2)\n\n    t1 >> t2 >> t3\n\ncapturar_pokemon()\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/final/controller.py\n\nimport requests\nfrom .db import SessionLocal, engine, Base\nfrom .models import Pokemon\nfrom .schema import PokemonSchema\nfrom random import randint\n\nBase.metadata.create_all(bindengine)\n\ndef gerar_numero_aleatorio():\n    return randint(1, 350)\n\ndef fetch_pokemon_data(pokemon_id: int):\n    response  requests.get(f\"https://pokeapi.co/api/v2/pokemon/{pokemon_id}\")\n    if response.status_code  200:\n        data  response.json()\n        types  ', '.join(type['type']['name'] for type in data['types'])\n        return PokemonSchema(namedata['name'], typetypes)\n    else:\n        return None\n\ndef add_pokemon_to_db(pokemon_schema: PokemonSchema) -> Pokemon:\n    with SessionLocal() as db:\n        db_pokemon  Pokemon(namepokemon_schema.name, typepokemon_schema.type)\n        db.add(db_pokemon)\n        db.commit()\n        db.refresh(db_pokemon)\n    return db_pokemon\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/final/db.py\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nSQLALCHEMY_DATABASE_URL  \"postgresql://dbuser:ldxrJfmAE68oYsMKqKhcQwSeXh4Kevgc@dpg-cnv1g4ljm4es73dpa26g-a.oregon-postgres.render.com/dbname_3s0u\"\nengine  create_engine(SQLALCHEMY_DATABASE_URL)\nSessionLocal  sessionmaker(autocommitFalse, autoflushFalse, bindengine)\nBase  declarative_base()\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/final/models.py\n\nfrom sqlalchemy import Column, Integer, String, DateTime\nfrom sqlalchemy.sql import func\nfrom .db import Base\n\nclass Pokemon(Base):\n    __tablename__  'pokemons'\n    id  Column(Integer, primary_keyTrue, indexTrue)\n    name  Column(String)\n    type  Column(String)\n    created_at  Column(DateTime, defaultfunc.now())  # Campo adicionado\n\n04-workflow-orchestration-deploy-airflow/exemplo_06/final/schema.py\n\nfrom pydantic import BaseModel\n\nclass PokemonSchema(BaseModel):\n    name: str\n    type: str\n\n    class Config:\n        from_attributes  True\n\n\n04-workflow-orchestration-deploy-airflow/exemplo_07/README.md\n\nAo utilizar super().__init__(*args, **kwargs) dentro do método __init__ da sua classe derivada (neste caso, PostgresToDuckDBOperator), você está efetivamente chamando o construtor da classe pai (BaseOperator) e passando a ele quaisquer argumentos adicionais ou argumentos de palavra-chave que foram recebidos. Isso garante que a inicialização definida na classe pai seja executada, permitindo que sua classe derivada se beneficie de toda a lógica de inicialização e configuração já implementada na classe pai.\n\nEsse mecanismo é particularmente útil em frameworks e bibliotecas onde a herança é um meio comum de estender funcionalidades. No Airflow, por exemplo, ao criar um operador customizado, geralmente você quer que ele tenha todos os comportamentos de um operador padrão do Airflow, incluindo a capacidade de receber argumentos padrão como task_id, retries, dag, entre outros, que são definidos e tratados na classe BaseOperator. Ao chamar super().__init__(*args, **kwargs), você assegura que seu operador customizado não apenas tem seus próprios atributos e métodos, mas também herda e se comporta conforme esperado dentro do ecossistema Airflow.\n\nEssa prática não só apoia o princípio DRY (Don't Repeat Yourself) ao evitar a duplicação de código, como também facilita a manutenção e a atualização do código ao longo do tempo, já que as mudanças na lógica comum da classe pai automaticamente se aplicam a todas as suas subclasses, a menos que explicitamente sobrescritas.\n\n\n04-workflow-orchestration-deploy-airflow/exemplo_07/Dockerfile\n\nFROM --platformlinux/amd64 quay.io/astronomer/astro-runtime:8.6.0\n\n04-workflow-orchestration-deploy-airflow/exemplo_07/packages.txt\n\nbuild-essential\n\n04-workflow-orchestration-deploy-airflow/exemplo_07/requirements.txt\n\n# Astro Runtime includes the following pre-installed providers packages: https://docs.astronomer.io/astro/runtime-image-architecture#provider-packages\nduckdb0.9.2\nairflow-provider-duckdb0.2.0\nastro-sdk-python[duckdb]1.6.1\napache-airflow-providers-postgres\n\n04-workflow-orchestration-deploy-airflow/exemplo_07/.env_example\n\nMOTHERDUCK_TOKEN'your motherduck token'\n\n04-workflow-orchestration-deploy-airflow/exemplo_07/dags/duckdb_custom_operator_example.py\n\nfrom airflow.decorators import dag\nfrom pendulum import datetime\nfrom include.custom_operators.postgres_to_duckdb_operator import PostgresToDuckDBOperator\n\nCONNECTION_DUCKDB  \"my_motherduck_conn\"  # minha connection ID da MotherDuck connection\nCONNECTION_POSTGRESDB  \"my_postgresdb_conn\" # minha connection ID do PostgreSQL connection\n\n@dag(start_datedatetime(2024, 3, 23), schedule\"*/5 * * * *\", catchupFalse)\ndef pipeline_de_migracao_postgres_to_duckdb():\n    PostgresToDuckDBOperator(\n        task_id\"postgres_to_duckdb\",\n        postgres_schema\"public\",\n        postgres_table_name\"pokemons\",\n        duckdb_conn_idCONNECTION_DUCKDB,\n        postgres_conn_idCONNECTION_POSTGRESDB\n    )\n\npipeline_de_migracao_postgres_to_duckdb()\n\n\n04-workflow-orchestration-deploy-airflow/exemplo_07/include/custom_operators/postgres_to_duckdb_operator.py\n\nfrom airflow.models.baseoperator import BaseOperator\nfrom duckdb_provider.hooks.duckdb_hook import DuckDBHook\nfrom airflow.hooks.base_hook import BaseHook\nimport os\n\nclass PostgresToDuckDBOperator(BaseOperator):\n\n    \"\"\"\n    https://github.com/apache/airflow/blob/main/airflow/models/baseoperator.py\n\n    Essa classe define o operador customizado. Herda de BaseOperator, \n    o que significa que adota todas as funcionalidades de um operador \n    do Airflow, mas com lógica customizada definida no método execute.\n\n    Operador que carrega um Postgres dentro de uma tabela no Duckdb.\n\n    :param postgres_schema: Nome do schema de origem no PostgreSQL.\n    :param postgres_table_name: Nome da tabela de origem no PostgreSQL e destino no DuckDB.\n    :param duckdb_conn_id: ID da conexão Airflow para o DuckDB.\n    :param postgres_conn_id: ID da conexão Airflow para o PostgreSQL.\n    \"\"\"\n\n    def __init__(\n        self,\n        postgres_schema,\n        postgres_table_name,\n        duckdb_conn_id,\n        postgres_conn_id,\n        *args, **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.postgres_schema  postgres_schema\n        self.postgres_table_name  postgres_table_name\n        self.duckdb_conn_id  duckdb_conn_id\n        self.postgresdb_conn_id  postgres_conn_id\n\n    def execute(self, context):\n        \"\"\"\n        Este método contém a lógica principal do operador, \n        que é executada quando a task correspondente ao operador \n        é acionada em um DAG do Airflow. Aqui está o que acontece \n        neste método:\n        \"\"\"\n        ts  context[\"ts\"]\n        duckdb_hook  DuckDBHook(duckdb_conn_idself.duckdb_conn_id)\n        postgresql_conn  BaseHook.get_connection(self.postgresdb_conn_id)\n        duckdb_conn  duckdb_hook.get_conn()\n        duckdb_conn.execute(\"INSTALL postgres;\")\n        duckdb_conn.execute(\"LOAD postgres;\")\n        duckdb_conn.execute(f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.postgres_table_name} \n            AS SELECT * FROM postgres_scan('\n                host{postgresql_conn.host} \n                user{postgresql_conn.login} \n                port5432 \n                dbname{postgresql_conn.schema} \n                password{postgresql_conn.password}', \n                '{self.postgres_schema}', \n                '{self.postgres_table_name}');\"\"\")\n        query  f\"\"\"\n            INSERT INTO {self.postgres_table_name} \n            SELECT * FROM postgres_scan(\n                'host{postgresql_conn.host} user{postgresql_conn.login} port5432 dbname{postgresql_conn.schema} password{postgresql_conn.password}',\n                '{self.postgres_schema}',\n                '{self.postgres_table_name}')\n                WHERE created_at > (SELECT MAX(created_at) FROM {self.postgres_table_name});\n            \"\"\"\n        duckdb_conn.execute(query)\n        self.log.info(f\"Inserted new rows into {self.postgres_table_name}\")\n\n"
    ],
    "README.md": [
        "README.md\n\n# Roadmap de Engenharia de Dados \n\n<p align\"center\">\n  <a href\"https://suajornadadedados.com.br/\"><img src\"pics/logo.png\" alt\"Jornada de Dados\"></a>\n</p>\n<p align\"center\">\n    <em>Nossa missão é fornecer o melhor ensino em engenharia de dados</em>\n</p>\n\nBem-vindo ao **Roadmap de Engenharia de Dados** da **Jornada de Dados**! Este repositório foi construído colaborativamente e tem como objetivo capacitá-lo para a carreira de engenharia de dados, ajudando profissionais como você a atingir novos patamares em sua trajetória profissional.\n\nAqui você encontrará:\n\n- **Roadmap de Estudo:** Links, referências e materiais complementares para auxiliar no seu aprendizado. Se você quer ter um plano de estudo, encontrará uma série de projetos e recursos que o guiarão passo a passo no desenvolvimento das habilidades essenciais em engenharia de dados.\n- **Códigos das Aulas:** Todos os códigos e materiais utilizados durante as aulas estão disponíveis neste repositório para que você possa praticar e aprofundar seu conhecimento.\n- **Calendário dos Próximos Encontros:** Fique por dentro das datas e temas dos próximos workshops e bootcamps da Jornada de Dados.\n\nSe você gostou do conteúdo e quer se inscrever em nosso programa profissional, pode fazer isso aqui:\n\n---\n\n## Próximos Eventos\n\n| Nome/Github                                      | Onde     | Data   | Mês       |\n|-------------------------------------------|----------|--------|-----------|\n| [Pipeline de dados no Azure](https://github.com/lvgalvao/workshop-azure-jornadadedados)                | Jornada  | 21/12  | Hoje  |\n| ETL 10 bilhões de linhas           | Youtube  | Terça dia 14/01 ás 19h30  | Janeiro   |\n| Databricks e delta table (OLAP + OLTP)           | Jornada  | 22/01  | Janeiro   |\n| Bootcamp Multiengine Databricks, Spark e DuckDB           | Jornada  | 27/01 até 31/01  | Janeiro   |\n| Convidado surpresa           | Jornada  | 04/02  | Janeiro   |\n| Kubernetes e Airflow no Google Cloud                     | Jornada  | 22/02  | Fevereiro     |\n| Bootcamp de AI Agents                     | Jornada  | 27/03  | Março     |\n\n![roadmap](./pics/roadmap_2025.png)\n\nTem sugestões de Tópicos? [clique aqui](https://github.com/lvgalvao/data-engineering-roadmap/issues)\n\n![imagem](./pics/issue.png)\n\nAbra uma Issue aqui\n\n![imagem](./pics/issue.png)\n\nParticipe e interaja\n\n![imagem](./pics/issue.png)\n\n___\n\n## Youtube - Workshops ao vivo - Lives abertas\n\n| Nome                                      | Link                                                    | Duração  | Stack/Github |\n|-------------------------------------------|---------------------------------------------------------|----------|------ |\n| Extração de API do zero | [Assistir no YouTube](https://youtube.com/live/xvCwZ73muV8) | 2h  | [GitHub, Python, API, SQL, Azure, Streamlit](https://github.com/lvgalvao/ETLProjectAPIExtract) |\n| Pipeline ETL - Web Scraping com Requests e Beautiful Soup 4 | [Assistir no YouTube](https://youtube.com/live/z1EOlFV8g7g) | 2h  | GitHub, Python, Requests e Beautiful Soup 4 |\n| Pipeline de Dados com GA4 e Typeform | [Assistir no YouTube](https://youtube.com/live/kt72obCvo0k) | 6h  | Python, SQL e Cursor AI |\n| Pipeline Gen AI - ETL com API e CRM de vendas | [Assistir no YouTube](https://youtube.com/live/I-4noY9hGTQ) | 6h  | Python, SQL, OpenAI, Langchain e Git  |\n| WORKSHOP ABERTO #1 - DO ZERO AO DEPLOY COM LUCIANO | [Assistir no YouTube](https://www.youtube.com/watch?vHxY2UhHkFWA) | 2h  | GitHub, VirtualEnv, TDD, taskipy, pytest, Streamlit, Selenium, Pydantic, MkDocs |\n| Criando ETL Com Python e DUCKDB DO ZERO AO DEPLOY ft. [@mehd-io](https://github.com/mehd-io) | [Assistir no YouTube](https://www.youtube.com/watch?v4w6UQNn_6X0) | 1h47min  | DuckDB, SQL, S3, CSV, JSON, MotherDuck, Streamlit, Docker, Render |\n| CRIANDO ETL COM PYTHON E DUCKDB DO ZERO AO DEPLOY | [Assistir no YouTube](https://www.youtube.com/watch?veXXImkz-vMs) | 2h32min  | DuckDB, SQLAlchemy, Google Drive, SQL, Postgres, Python, CSV, Parquet, JSON, Streamlit, Docker, Render |\n| Modern Data Stack com SQL - Parte 1 de 3 ft. [@MarcLamberti](https://github.com/marclamberti) | [Assistir no YouTube](https://www.youtube.com/watch?vlhMIMrEj_4Q) | 2h  | AirFlow, Astro-cli, Docker, Airbyte Cloud , Render, Postgres|\n| Modern Data Stack com SQL - Parte 2 de 3 | [Assistir no YouTube](https://www.youtube.com/watch?vWG96Z7uGTHg) | 1h53min  | SQL, dbt, Render |\n| Modern Data Stack com SQL - Parte 3 de 3 | [Assistir no YouTube](https://www.youtube.com/watch?vbrfl7hdC060) | 1h53min  | Python, AirFlow, SQL, dbt, Render, Docker |\n___\n\n# Workshops - Quem sabe faz ao vivo\n\n| Nome                                      | Link                                                    | Profissional  | Stack |\n|-------------------------------------------|---------------------------------------------------------|----------|------ |\n| Construindo um Pipeline ETL em Tempo Real | [Assistir no YouTube](https://youtube.com/live/daUC8kMzeLw) | Caio Machado  | Kafka, PostgreSQL e Streamlit |\n| Como sair do ZERO com SQL na AWS | [Assistir no YouTube](https://youtube.com/live/ko3D76GP5d4) | Ghabriel Fiorotti  | ETL Parquet S3 Athena e Glue |\n| Plataforma com Big Query do Zero| [Assistir no YouTube](https://youtube.com/live/NP08fHker5U) | Alan Lanceloty | Python, Airflow, dbt, soda e docker |\n| Qualidade de dados e Contrato de Dados  | [Assistir no YouTube](https://youtube.com/live/IQtuWsNmB4o) | Renan Heckert  | Pandera e Pydantic |\n\n# Workshops - Especialistas\n\n| Nome                                      | Link                                                    | Profissional  | Stack |\n|-------------------------------------------|---------------------------------------------------------|----------|------ |\n| Como sair do ZERO em Observabilidade com Logfire| [Assistir no YouTube](https://youtube.com/live/bxtsTP0a0mU) | Marcelo Trylesinski  | Logfire |\n| dbt no Airflow - Como melhorar o desempenho do seu deploy de forma correta | [Assistir no YouTube](https://youtube.com/live/xvCwZ73muV8) | Tatiana Martins | Airflow e dbt-core |\n| Construa Data Apps Completos com Briefer | [Assistir no YouTube](https://youtube.com/live/6KyxRpX6oY4) | Lucas Costa  | Briefer, SQL e Python |\n| Como sair do ZERO no AIRBYTE| [Assistir no YouTube](https://youtube.com/live/4hQroajva0s) | Alan Lanceloty  | Airbyte |\n| Como criar do ZERO um Lakehouse | [Assistir no YouTube](https://youtube.com/live/O9q5owTOpMw) | Nilton Ueda  | Conceitual |\n\n___\n\n\n## Youtube - Vídeos tutoriais\n| Nome                                      | Link                                                    | Duração  | Stack |\n|-------------------------------------------|---------------------------------------------------------|----------|------ |\n| Top 5 Projetos de Engenharia de Dados Aprenda ETL, Python e SQL Gratuitamente! | [Assistir no YouTube](https://www.youtube.com/watch?vldjbV_0mqXI) | 14min | Python, DuckDB, Spark, GitHub, Docker |\n| O que é CLOUD? Explicação COMPLETA para DADOS (Deploy Python e SQL na AWS, Azure, GCP com Terraform) | [Assistir no YouTube](https://www.youtube.com/watch?vIff6Nr3sK4U) | 15min | Python, GitHub, AWS, Azur, GCP, SQL, Docker, Terraform |\n| Como fazer o Deploy de Airflow na EC2 AWS | [Assistir no YouTube](https://www.youtube.com/watch?vaYLmKbxXcls) | 14min | AWS, EC2, Airflow | \n| Amazon SQS e Rabbit MQ eu preciso mesmo disso? | [Assistir no YouTube](https://www.youtube.com/watch?vsSBFCffBSac) | 5min | SQS, Rabbit MQ | \n| Segredos para economizar com a AWS | [Assistir no YouTube](https://www.youtube.com/watch?vaKvCjSQHb_w) | 5min | AWS | \n| CRUD, qual a vantagem de usar um ORM? | [Assistir no YouTube](https://www.youtube.com/watch?vhl5YjfvqkB0) | 6min | SQL Alchemy, Python, ORM, SQL | \n| Como instalar Python em 2024 + Pyenv, PIP, VENV, PIPX e Poetry | [Assistir no YouTube](https://www.youtube.com/watch?v9LYqtLuD7z4) | 33min | Pyenv, PIP, VENV, PIPX, Poetry, Python | \n| Como instalar Python em 2024 + VSCode, Git e GitHub do Zero | [Assistir no YouTube](https://www.youtube.com/watch?v-M4pMd2yQOM) | 33min | Python, VSCode, Git, GitHub | \n| O que é o arquivo __init__.py em Python? Explicado com 4 exemplos | [Assistir no YouTube](https://www.youtube.com/watch?vH7rINLV6e0I) | 14min | Python | \n\n\n---\n\n## Especialização Jornada de dados\n\n| Nome do Treinamento                     | Link GitHub | Status       | Descrição                                                                              |\n|-----------------------------------------|-------------|--------------|---------------------------------------------------------------------------------------|\n| Python para Dados                       | [Link](#)   | Concluído    | Fundamentos de Python para engenharia de dados, com foco em bibliotecas como Pandas.  |\n| SQL para Analytics Engineer             | [Link](#)   | Concluído    | Domine SQL avançado para análise de dados e otimização de consultas complexas.        |\n| dbt-core                                | [Link](#)   | Concluído    | Utilize dbt-core para transformação de dados no data warehouse com práticas modernas. |\n| Web Scraping Avançado                   | [Link](#)   | Concluído    | Técnicas avançadas de extração de dados de sites e manipulação de APIs.               |\n| Cloud para Dados                        | [Link](#)   | Em andamento | Serviços de nuvem aplicados à engenharia de dados com AWS, Azure e GCP.               |\n\n---\n\nCaso queira sugerir temas para próximos workshops, [abra uma issue](https://GitHub.com/lvgalvao/data-engineering-roadmap/issues).\n\n"
    ],
    "06-restAPI-fastAPI-deploy": [
        "06-restAPI-fastAPI-deploy/primeiro-dia/README.md\n\n## Objetivo\n\n## README em revisão\n\nExcalidraw: https://link.excalidraw.com/l/8pvW6zbNUnD/7APGEyy8COJ\n\nEste workshop visa introduzir os conceitos fundamentais (Redes, protocolo, HTTP, comumicação entre processos etc) e as práticas necessárias para criar uma API usando a framework FastAPI em Python.\n\nOs participantes aprenderão desde a configuração do ambiente de desenvolvimento até a criação e documentação de APIs.\n\n## Curl (Cliente por linha de comando)\n\n### Introdução ao Curl\n\n**Curl** (Client URL) é uma ferramenta de software e uma biblioteca de linha de comando usada para transferir dados com URLs. Ela suporta uma diversidade de protocolos, como HTTP, HTTPS, FTP, FTPS, SCP, SFTP, entre outros. Amplamente utilizada por desenvolvedores e administradores de sistema, a ferramenta permite a interação com servidores web e outros tipos de servidores de internet para baixar ou enviar dados.\n\n### Por Que Usar Curl?\n\n**1. Versatilidade:** Curl pode ser usado para testar conectividade de API, desenvolver e depurar serviços web, automatizar tarefas de upload e download de arquivos e muito mais.\n\n**2. Suporte Amplo de Protocolos:** Suportando uma grande variedade de protocolos de comunicação de dados, curl é extremamente flexível para qualquer necessidade de rede.\n\n**3. Automação:** Curl é ideal para scripts automatizados devido à sua natureza de linha de comando. Ele pode ser integrado em scripts bash ou shell para automação de tarefas de rede.\n\n**4. Disponibilidade:** Disponível em quase todas as plataformas Unix-like, incluindo Linux e macOS, e também em Windows, curl é uma ferramenta universal.\n\n**5. Comunidade e Suporte:** Sendo um projeto de código aberto, curl é bem documentado e tem uma comunidade ativa que contribui para sua melhoria contínua.\n\n### O Que é o Protocolo HTTP?\n\n**HTTP (HyperText Transfer Protocol)** é o protocolo de comunicação utilizado para transmitir informações na web. É a base para qualquer troca de dados na internet, permitindo a comunicação entre clientes web (navegadores) e servidores web. O protocolo define métodos de requisição que indicam a ação desejada para um determinado recurso, como `GET` para solicitar dados de um recurso, ou `POST` para submeter dados para serem processados a um recurso.\n\n### Características do HTTP:\n\n**1. Simples e Extensível:** Projetado para ser simples e fácil de implementar, enquanto permite extensões para aumentar sua funcionalidade.\n\n**2. Stateless:** HTTP é um protocolo sem estado, o que significa que cada requisição é independente das outras e não deve afetar o comportamento das outras requisições. Contudo, sessões e cookies podem ser usados para adicionar estado em comunicações HTTP.\n\n**3. Flexível:** HTTP permite a transferência de qualquer tipo de dados, desde que ambas as partes (cliente e servidor) possam interpretar esses dados.\n\nAqui estão os oito exercícios propostos para praticar o uso do `curl`, acompanhados das respostas esperadas para cada um:\n\n### Exercício 1: Fazendo Requisições Básicas\n\n**Objetivo**: Familiarizar-se com requisições GET e a saída do `curl`.\n\n* **Comando**:\n    \n    ```bash\n    curl -v http://httpbin.org/get\n    ```\n    \n* **Resposta Esperada**: Você verá detalhes completos da requisição e da resposta, incluindo cabeçalhos HTTP enviados e recebidos. Isso inclui informações sobre o método usado (GET), o host acessado, e cabeçalhos como `User-Agent` e `Accept`.\n\n### Exercício 2: Trabalhando com Parâmetros de Query\n\n**Objetivo**: Aprender a enviar parâmetros de query em URLs.\n\n* **Comando**:\n    \n    ```bash\n    curl http://httpbin.org/get?nameJohn&age30\n    ```\n    \n* **Resposta Esperada**: A resposta de httpbin.org refletirá os parâmetros que você enviou. No JSON de resposta, você verá um objeto \"args\" contendo `\"name\": \"John\", \"age\": \"30\"`.\n\n### Exercício 3: Postando Dados JSON\n\n**Objetivo**: Praticar o envio de dados JSON em uma requisição POST.\n\n* **Comando**:\n    \n    ```bash\n    curl -X POST http://httpbin.org/post -H \"Content-Type: application/json\" -d '{\"username\":\"john\", \"password\":\"12345\"}'\n    ```\n\nImportância do -X:\nO uso do -X é particularmente importante quando você precisa realizar operações específicas que requerem diferentes tipos de métodos HTTP. Por exemplo:\n\nGET: Usado para solicitar dados de um recurso específico.\nPOST: Usado para enviar dados para serem processados para um recurso específico. Normalmente resulta em uma mudança de estado ou efeitos colaterais no servidor.\nPUT: Usado para enviar dados para atualizar um recurso existente.\nDELETE: Usado para deletar um recurso específico.\n    \n* **Resposta Esperada**: Httpbin irá ecoar de volta os dados que você enviou em um objeto JSON. A seção `json` da resposta incluirá os dados `{\"username\": \"john\", \"password\": \"12345\"}`.\n\n### Exercício 4: Usando Diferentes Métodos HTTP\n\n**Objetivo**: Experimentar com diferentes métodos HTTP, como POST, DELETE, PUT.\n\n* **Comando PUT**:\n    \n    ```bash\n    curl -X PUT http://httpbin.org/put -d \"dataexample\"\n    ```\n    \n* **Comando DELETE**:\n    \n    ```bash\n    curl -X DELETE http://httpbin.org/delete\n    ```\n    \n* **Resposta Esperada**: Para PUT, httpbin mostrará os dados que você enviou no corpo da requisição, enquanto para DELETE, você receberá uma confirmação de que a requisição DELETE foi recebida, geralmente sem corpo de dados.\n\n### Exercício 5: Manipulando Headers\n\n**Objetivo**: Aprender a enviar cabeçalhos customizados.\n\n* **Comando**:\n    \n    ```bash\n    curl http://httpbin.org/headers -H \"X-My-Custom-Header: 12345\"\n    ```\n    \n* **Resposta Esperada**: A resposta incluirá um objeto `headers` que mostra todos os cabeçalhos recebidos, incluindo seu cabeçalho personalizado `X-My-Custom-Header` com o valor `12345`.\n\n### Exercício 6: Trabalhando com Cookies\n\n**Objetivo**: Entender como enviar e receber cookies.\n\n* **Comando**:\n    \n    ```bash\n    curl http://httpbin.org/cookies/set?namevalue\n    curl http://httpbin.org/cookies\n    ```\n    \n* **Resposta Esperada**: Após definir o cookie, a segunda requisição mostrará um objeto `cookies` com o par `{\"name\": \"value\"}`.\n\n### Exercício 7: Baixando e Salvando Arquivos\n\n**Objetivo**: Praticar o download de arquivos usando `curl`.\n\n* **Comando**:\n    \n    ```bash\n    curl https://via.placeholder.com/150 -o example.jpg\n    ```\n    \n* **Resposta Esperada**: O arquivo de imagem será baixado e salvo localmente com o nome `example.jpg`. Você não verá saída no terminal, exceto mensagens relacionadas ao progresso do download.\n\n### Exercício 8: Explorando APIs Restritas\n\n**Objetivo**: Aprender a lidar com autenticação.\n\n* **Comando**:\n    \n    ```bash\n    curl -u user:passwd https://httpbin.org/basic-auth/user/passwd\n    ```\n    \n* **Resposta Esperada**: Se a autenticação for bem-sucedida, httpbin retornará um status de sucesso e confirmará que você foi autenticado. \n\n## Programação\n\n### 1. Introdução a APIs e Web Servers\n\n* **O que é uma API?**\n    * Explicação de API (Interface de Programação de Aplicações) como um conjunto de regras e especificações que softwares podem seguir para se comunicar.\n\n* **Diferenças entre API e Web Server**\n    * Clarificação de que um Web Server lida com requisições HTTP para servir conteúdo web, enquanto uma API fornece uma interface para realizar operações específicas através de um servidor.\n\n### 2. Introdução ao FastAPI\n\n* **Visão Geral do FastAPI**\n    * Discussão sobre as características do FastAPI, como alta performance, fácil aprendizado e recursos como a geração automática de documentação.\n* **Comparação com Outras Frameworks**\n    * Breve comparação do FastAPI com outras frameworks populares como Flask e Django, destacando a facilidade de uso e eficiência em operações assíncronas.\n\n### 3. Configuração do Ambiente de Desenvolvimento\n\n* **Instalação do Python e Setup do Ambiente**\n    * Passo a passo para configurar o ambiente Python, incluindo a instalação do FastAPI e do servidor Uvicorn usando pip.\n\n### 4. Criando sua Primeira API com FastAPI\n\n* **Hello World API**\n    * Tutorial para criar um endpoint básico que retorna uma mensagem de \"Hello, World!\" usando FastAPI.\n\n```python\nfrom fastapi import FastAPI\n\napp  FastAPI()\n\n# Decorator -> É aqui que faz a mágica de transformar nossa função.\n@app.get(\"/\")\n# Function é função padrão do Python\ndef root(): # O nome não importa\n    return {\"message\": \"Hello world!\"} # Essa será a data que vamos retornar ao usuário\n```\n\n```bash\nuvicorn main:app\n```\n\n```bash\nuvicorn main:app --reload\n```\n\nHTTP metodos\n\n#### Curl\n\nPara usar `curl` para fazer uma requisição à sua API que está rodando com FastAPI, você pode enviar dados como JSON através de uma requisição POST. Suponhamos que você tenha um endpoint em sua API que espera receber os dados de uma casa e então retorna uma previsão de preço baseada nessas informações. Aqui está como você pode fazer isso com `curl`.\n\n### Exemplo de Requisição com Curl\n\nSuponha que seu endpoint para prever o preço da casa esteja configurado como `http://127.0.0.1:8000/prever/` e aceite um JSON com dois campos: `tamanho` e `quartos`. Aqui está como você pode enviar uma requisição:\n\n```bash\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/prever/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"tamanho\": 120,\n  \"quartos\": 3\n}'\n```\n\n### Explicação do Comando Curl\n\n* `-X 'POST'`: Especifica o método HTTP para a requisição, que é POST neste caso.\n* `http://127.0.0.1:8000/prever/`: URL do endpoint da API.\n* `-H 'accept: application/json'`: Define o cabeçalho HTTP para indicar que a resposta esperada deve ser em JSON.\n* `-H 'Content-Type: application/json'`: Define o cabeçalho HTTP para indicar que o corpo da requisição está em formato JSON.\n* `-d '{...}'`: Os dados sendo enviados à API. Substitua os valores de `tamanho` e `quartos` conforme necessário para os dados específicos da casa que você quer avaliar.\n\n### Testando a Requisição\n\n1. Certifique-se de que sua API FastAPI esteja rodando e acessível em `http://127.0.0.1:8000`.\n2. Abra um terminal e execute o comando `curl` fornecido.\n3. Observe a resposta da API, que deve incluir a previsão do preço da casa baseada nos dados fornecidos.\n\nUsar os cabeçalhos Accept e Content-Type nas suas requisições HTTP é uma forma de comunicar claramente ao servidor tanto o formato dos dados que você está enviando quanto o formato que você espera receber em resposta:\n\nContent-Type: application/json: Este cabeçalho informa ao servidor que o corpo da requisição que você está enviando está em formato JSON. É uma maneira de dizer, \"Ei, os dados que estou enviando estão em JSON; por favor, interprete-os dessa forma.\"\nAccept: application/json: Este cabeçalho diz ao servidor que você deseja que a resposta seja em JSON. Isso é particularmente útil em APIs que podem retornar dados em diferentes formatos. Ao especificar application/json, você está solicitando que a API responda com dados nesse formato específico.\n\n#### Postman\n\nTambém temos a opção usar o Postman (uma aplicação)\n\n\n### 5. Trabalhando com Dados\n\n* **Uso de Modelos Pydantic**\n    * Introdução aos modelos Pydantic para validação de dados e como integrá-los com FastAPI.\n* **Endpoints GET e POST**\n    * Criação de exemplos práticos de endpoints que lidam com métodos GET e POST para enviar e receber dados.\n\n### 6. Uvicorn: O Servidor ASGI\n\n* **Por que Uvicorn?**\n    * Explicação sobre o papel do Uvicorn como um servidor ASGI, necessário para executar aplicações FastAPI de forma eficiente e assíncrona.\n\n### 7. Documentação Automática\n\n* **Swagger UI**\n    * Demonstração de como acessar e utilizar a documentação automática gerada pelo FastAPI.\n\n### 8. Exercícios Práticos\n\n* **Hands-on Coding**\n    * Série de exercícios práticos para aplicar o conhecimento adquirido na criação de APIs mais complexas.\n* **Desafio com Banco de Dados**\n    * Desafio para criar uma API que interage com um banco de dados usando operações básicas de CRUD.\n\n### 9. Sessão de Perguntas e Respostas (Q&A)\n\n* **Discussão Aberta**\n    * Espaço para perguntas, troca de ideias e esclarecimento de dúvidas.\n\n### 10. Encerramento\n\n* **Recursos para Aprendizado Contínuo**\n    * Compartilhamento de recursos adicionais, como documentação online, tutoriais e fóruns para aprofundamento nos temas abordados.\n\n## Materiais Necessários\n\n* Slides das apresentações.\n* Códigos de exemplo e templates para exercícios.\n* Acesso à Internet para documentação e pesquisa.\n\n## Pré-requisitos\n\n* Conhecimento básico de programação em Python.\n* Ambiente de desenvolvimento Python configurado com acesso à internet.\n\n## Recursos Adicionais\n\n* Documentação Oficial do FastAPI\n* Pydantic Documentation\n* [Uvicorn Documentation](https://www.uvicorn.org/)\n\n\n## Ideias\n\n06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_00.py\n\nimport random\nimport time\n\ndef numero_aleatorio():\n    \"\"\" Gera um numero aleatorio. \"\"\"\n    return random.randint(1, 10)\n\ndef dobra_um_numero(num):\n    \"\"\" Calcula o quadrado do dobro de um número. \"\"\"\n    return num ** 2\n\nif __name__  \"__main__\":\n    while True:\n        num  random.randint(1, 10)  # Gera um número aleatório entre 1 e 10\n        resultado  dobra_um_numero(num)\n        print(f\"O quadrado do dobro de {num} é {resultado}\")\n        time.sleep(1)  # Pausa a execução por 1 segundo\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_01_a.py\n\nimport random\nimport time\n\ndef gerar_numero_aleatorio():\n    num  random.randint(1, 10)\n    print(num)\n    with open('numeros.txt', 'a') as file:\n        file.write(f\"{num}\\n\")\n\nif __name__  \"__main__\":\n    while True:\n        gerar_numero_aleatorio()\n        time.sleep(1)\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_01_b.py\n\nimport time\ndef dobrar_numero(num):\n    return num * 2\n\ndef ler_ultimo_numero():\n    try:\n        with open('numeros.txt', 'r') as file:\n            lines  file.readlines()\n            if lines:\n                return int(lines[-1].strip())\n            else:\n                print(\"Arquivo está vazio.\")\n                return None\n    except FileNotFoundError:\n        print(\"Arquivo não encontrado.\")\n        return None\n\nif __name__  \"__main__\":\n    while True:\n        num  ler_ultimo_numero()\n        if num is not None:\n            resultado  dobrar_numero(num)\n            print(f\"O quadrado do dobro do último número ({num}) é {resultado}\")\n            time.sleep(1)\n\n06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_02_pid_e_porta.py\n\n# aplicacao simples so para mostrar o que é PID vs PORTA\n\nimport streamlit as st\nimport numpy as np\nimport pandas as pd\n\ndataframe  pd.DataFrame(\n    np.random.randn(10, 20),\n    columns('col %d' % i for i in range(20)))\nst.table(dataframe)\n\n06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_03_a_usando_api.py\n\nfrom fastapi import FastAPI\n\nimport random\nimport time\n\napp  FastAPI()\n\n@app.get(\"/gerar_numero_aleatorio\")\ndef gerar_numero_aleatorio():\n    num  random.randint(1, 10)\n    return {\"data\": num} # num\n\n06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_03_b_usando_cliente.py\n\nimport time\nimport requests\n\ndef dobrar_numero(num):\n    return num * 2\n\ndef ler_ultimo_numero():\n    url  \"http://127.0.0.1:8000/gerar_numero_aleatorio\"\n    response  requests.get(url)\n    data  response.json() # response.test\n    return data[\"data\"] # data\n\n\nif __name__  \"__main__\":\n    while True:\n        num  ler_ultimo_numero()\n        if num is not None:\n            resultado  dobrar_numero(num)\n            print(f\"O quadrado do dobro do último número {num} é {resultado}\")\n            time.sleep(1)\n\n06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_04_decorador_a.py\n\nimport time\nfrom functools import wraps\n\ndef alou_decorator(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        while True:\n            print(\"alou\")\n            time.sleep(1)\n            result  func(*args, **kwargs)\n            return result\n    return wrapper\n\n06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_04_decorador_b.py\n\nimport random\nimport time\nfrom programa_04_decorador_exemplo import alou_decorator\n\n\n@alou_decorator\ndef gerar_numero_aleatorio():\n    num  random.randint(1, 10)\n    print(num)\n    with open('numeros.txt', 'a') as file:\n        file.write(f\"{num}\\n\")\n\nif __name__  \"__main__\":\n    while True:\n        gerar_numero_aleatorio()\n        time.sleep(1)\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/exemplo_05_diferentes_tipos_de_respostas.py\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse, JSONResponse, HTMLResponse\n\nimport csv\nfrom io import StringIO\n\napp  FastAPI()\n\n@app.get(\"/listar-csv\")\ndef listar_csv():\n    data  [[\"Name\", \"Age\"], [\"Alice\", 30], [\"Bob\", 25]] # Dados\n    stream  StringIO() # Transforma um objeto em um arquivo\n    csv.writer(stream).writerows(data) # Dessa forma escrevemos os \"data\" no formarto csv no stream\n    return stream.getvalue()\n\n@app.get(\"/download-csv/\")\ndef download_csv():\n    data  [[\"Name\", \"Age\"], [\"Alice\", 30], [\"Bob\", 25]] # Dados\n    stream  StringIO() # Transforma um objeto em um arquivo\n    csv.writer(stream).writerows(data) # Dessa forma escrevemos os \"data\" no formarto csv no stream\n    return StreamingResponse(iter([stream.read()]), media_type\"text/csv\", headers{\"Content-Disposition\": \"attachment; filenamereport.csv\"})\n\n@app.get(\"/portal\")\nasync def get_portal(teleport: bool  False):\n    if teleport:\n        return HTMLResponse(content\"\"\"\n                            <html>\n                                <head>\n                                    <title>Some HTML in here</title>\n                                </head>\n                                <body>\n                                    <h1>Look ma! HTML!</h1>\n                                </body>\n                            </html>\"\"\")\n    return JSONResponse(content{\"message\": \"Here's your interdimensional portal.\"})\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/exemplo_00_func.py\n\nimport joblib\n\nmodelo  joblib.load(\"modelo_casas.pkl\")\n\ndef prever_preco(tamanho: int,\n                 quartos: int,\n                 n_vagas: int):\n    \n    dados_entrada  [[tamanho, quartos, n_vagas]]\n    preco_estimado  modelo.predict(dados_entrada)[0]\n    return round(preco_estimado,2)\n\nprint(prever_preco(200,3,3))\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/exemplo_01_api.py\n\nfrom fastapi import FastAPI\nimport joblib\nfrom pydantic import BaseModel\n\napp  FastAPI()\n\nmodelo  joblib.load(\"modelo_casas.pkl\")\n\nclass EspecificacoesCasa(BaseModel):\n    tamanho: int\n    quartos: int\n    n_vagas: int\n\n@app.post(\"/prever/\")\ndef prever_preco(especificacoes_cada: EspecificacoesCasa):\n    \n    dados_entrada  [[especificacoes_cada.tamanho, especificacoes_cada.quartos, especificacoes_cada.n_vagas]]\n    preco_estimado  modelo.predict(dados_entrada)[0]\n    return {\n        \"preco_estimado\": round(preco_estimado,2)\n    }\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/exemplo_02_api_request_response.py\n\nfrom fastapi import FastAPI\nimport joblib\nfrom pydantic import BaseModel, EmailStr, PositiveInt, PositiveFloat\nfrom typing import Optional\n\napp  FastAPI()\n\nmodelo  joblib.load(\"modelo_casas.pkl\")\n\nclass EspecificacoesCasa(BaseModel):\n    tamanho: PositiveInt\n    quartos: PositiveInt\n    n_vagas: PositiveInt\n\nclass EspecificacoesCasaRequest(EspecificacoesCasa):\n    email: Optional[EmailStr]  None\n\nclass EspecificacoesCasaResponse(BaseModel):\n    preco_estimado: PositiveFloat\n    dados: EspecificacoesCasa\n\n@app.post(\"/prever/\", response_modelEspecificacoesCasaResponse)\ndef prever_preco(especificacoes_cada: EspecificacoesCasa):\n    \n    dados_entrada  [[especificacoes_cada.tamanho, especificacoes_cada.quartos, especificacoes_cada.n_vagas]]\n    preco_estimado  modelo.predict(dados_entrada)[0]\n    return EspecificacoesCasaResponse(preco_estimadopreco_estimado, dadosespecificacoes_cada)\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/exemplo_03_banco.py\n\nfrom fastapi import FastAPI\nimport joblib\nfrom pydantic import BaseModel, EmailStr, PositiveInt, PositiveFloat\nfrom typing import Optional\n\napp  FastAPI()\n\nmodelo  joblib.load(\"modelo_casas.pkl\")\n\nclass EspecificacoesCasa(BaseModel):\n    tamanho: PositiveInt\n    quartos: PositiveInt\n    n_vagas: PositiveInt\n\nclass EspecificacoesCasaRequest(EspecificacoesCasa):\n    email: Optional[EmailStr]  None\n\nclass EspecificacoesCasaResponse(BaseModel):\n    preco_estimado: PositiveFloat\n    dados: EspecificacoesCasa\n\nCRM  []\n\n@app.post(\"/prever/\", response_modelEspecificacoesCasaResponse)\ndef prever_preco(especificacoes_cada: EspecificacoesCasa):\n    \n    dados_entrada  [[especificacoes_cada.tamanho, especificacoes_cada.quartos, especificacoes_cada.n_vagas]]\n    preco_estimado  modelo.predict(dados_entrada)[0]\n    response  EspecificacoesCasaResponse(preco_estimadopreco_estimado, dadosespecificacoes_cada)\n    CRM.append(response.model_dump())\n    return response\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/modelo.py\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport joblib\n\n# Exemplo de dados: [tamanho, quartos, vagas], preço\nX  np.array([\n    [100, 3, 2],\n    [150, 4, 3],\n    [120, 2, 1],\n    [300, 5, 4],\n    [200, 3, 2],\n    [250, 4, 3],\n    [180, 3, 2],\n    [140, 2, 1],\n    [320, 5, 4],\n    [210, 3, 0]  # Exemplo de uma casa sem vaga de garagem\n])\ny  np.array([200000, 300000, 180000, 500000, 400000, 450000, 360000, 220000, 520000, 410000])\n\n# Dividindo os dados em conjuntos de treino e teste\nX_train, X_test, y_train, y_test  train_test_split(X, y, test_size0.2, random_state42)\n\n# Treinando o modelo de regressão linear\nmodelo  LinearRegression()\nmodelo.fit(X_train, y_train)\n\n# Avaliando o modelo com o conjunto de teste\ny_pred  modelo.predict(X_test)\nmse  mean_squared_error(y_test, y_pred)\nprint(f\"MSE: {mse}\")\n\n# Salvando o modelo treinado\njoblib.dump(modelo, 'modelo_casas.pkl')\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/requirements.txt\n\nscikit-learn\npydantic[email]\n\n",
        "06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/Dockerfile-backend\n\nFROM python:3.11\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the requirements file and install dependencies\nCOPY ./requirements-backend.txt .\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir -r requirements-backend.txt\n\n# Copy the application code\nCOPY ./src .\n\n# Command to run the application\nCMD [\"uvicorn\", \"src.app:app\", \"--reload\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/Dockerfile-frontend\n\nFROM python:3.11\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the requirements file and install dependencies\nCOPY ./requirements-frontend.txt .\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir -r requirements-frontend.txt\n\n# Copy the application code\nCOPY ./src .\n\n# Command to run the application\nCMD [\"streamlit\", \"run\", \"src/frontend.py\", \"--server.port5000\"]\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/docker-compose.yml\n\nversion: \"3.9\"\n\nservices:\n  backend:\n    build:\n      context: .\n      dockerfile: Dockerfile-backend\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - .:/app\n    networks:\n      - app-network\n\n  frontend:\n    build:\n      context: .\n      dockerfile: Dockerfile-frontend\n    ports:\n      - \"5000:5000\"  # Assuming your frontend runs on port 5000\n    volumes:\n      - .:/app\n    networks:\n      - app-network\n\n  mongodb:\n    image: mongo:latest\n    hostname: mongodb\n    volumes:\n      - mongodb_data:/data/db\n    ports:\n      - \"27017:27017\"\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: root\n      MONGO_INITDB_ROOT_PASSWORD: example\n    networks:\n      - app-network\n\n  mongo-express:\n    image: mongo-express:latest\n    hostname: mongo-express\n    ports:\n      - \"8081:8081\"\n    environment:\n      ME_CONFIG_MONGODB_ADMINUSERNAME: root\n      ME_CONFIG_MONGODB_ADMINPASSWORD: example\n      ME_CONFIG_MONGODB_SERVER: mongodb\n    depends_on:\n      - mongodb\n    networks:\n      - app-network\n    \n\nvolumes:\n  mongodb_data:\nnetworks:\n  app-network:\n    driver: bridge\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/requirements-backend.txt\n\nemail_validator\nfastapi\npydantic\npymongo\npython-dotenv\nuvicorn\njoblib\nscikit-learn\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/requirements-frontend.txt\n\nstreamlit\nrequests\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/src/app.py\n\nfrom pymongo import MongoClient\nfrom fastapi import FastAPI\nimport joblib\nfrom pydantic import BaseModel, EmailStr, PositiveInt, PositiveFloat\nfrom typing import Optional, List\n\napp  FastAPI()\n\nmodelo  joblib.load(\"./src/modelo_casas.pkl\")\n\nclass EspecificacoesCasa(BaseModel):\n    tamanho: PositiveInt\n    quartos: PositiveInt\n    n_vagas: PositiveInt\n\nclass EspecificacoesCasaRequest(EspecificacoesCasa):\n    email: EmailStr  None\n\nclass EspecificacoesCasaResponse(BaseModel):\n    preco_estimado: PositiveFloat\n    dados: EspecificacoesCasaRequest\n\n# Configuração do MongoDB\nclient  MongoClient('mongodb://root:example@mongodb:27017')\ndb  client['dbmongo']\ncollection  db['dbscrm']\n\n@app.post(\"/quanto-cobrar-de-casa/\", response_modelEspecificacoesCasaResponse)\ndef prever_preco(especificacoes_cada: EspecificacoesCasaRequest):\n    \n    dados_entrada  [[especificacoes_cada.tamanho, especificacoes_cada.quartos, especificacoes_cada.n_vagas]]\n    preco_estimado  modelo.predict(dados_entrada)[0]\n    response  EspecificacoesCasaResponse(preco_estimadopreco_estimado, dadosespecificacoes_cada)\n    collection.insert_one(response.model_dump())\n    return response\n\n@app.get(\"/mkt/\", response_modelList[EspecificacoesCasaResponse])\ndef read_all_leads():\n    casas  list(collection.find({}))\n    return [EspecificacoesCasaResponse(preco_estimadocasa['preco_estimado'], dadoscasa['dados']) for casa in casas]\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/src/frontend.py\n\nimport streamlit as st\nimport requests\n\n# Função para enviar dados para a API e receber a previsão\ndef get_previsao(tamanho, quartos, n_vagas, email):\n    url  'http://backend:8000/quanto-cobrar-de-casa/'  # Endereço da API FastAPI\n    data  {\n        \"tamanho\": tamanho,\n        \"quartos\": quartos,\n        \"n_vagas\": n_vagas,\n        \"email\": email if email else None  # Inclui o email na requisição apenas se for fornecido\n    }\n    response  requests.post(url, jsondata)\n    print(\"Status Code:\", response.status_code)\n    print(\"Response Body:\", response.text)  # Print or log the raw response\n    try:\n        return response.json()  # Attempt to parse JSON\n    except ValueError:\n        # Handle the case where parsing JSON fails\n        print(\"Failed to decode JSON from response:\")\n        print(response.text)\n        return None\n\n# Interface do usuário no Streamlit\nst.title('Quanto vale o seu imóvel')\n\n# Entrada de dados pelo usuário\ntamanho  st.number_input('Insira o tamanho da casa (em m²):', min_value10.0, step0.1, format\"%.1f\")\nquartos  st.number_input('Insira o número de quartos:', min_value1, step1)\nn_vagas  st.number_input('Insira o número de vagas de garagem:', min_value0, max_value4, step1)\nemail  st.text_input('Insira seu email (opcional):', '')\n\n# Botão para fazer a previsão\nif st.button('Prever preço'):\n    resposta  get_previsao(tamanho, quartos, n_vagas, email)\n    if 'preco_estimado' in resposta:\n        st.success(f'O preço estimado da casa é: R${resposta[\"preco_estimado\"]:.2f}')\n    else:\n        st.error('Erro ao obter previsão.')\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_modelo_ml/src/src/modelo.py\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport joblib\n\n# Exemplo de dados: [tamanho, quartos, vagas], preço\nX  np.array([\n    [100, 3, 2],\n    [150, 4, 3],\n    [120, 2, 1],\n    [300, 5, 4],\n    [200, 3, 2],\n    [250, 4, 3],\n    [180, 3, 2],\n    [140, 2, 1],\n    [320, 5, 4],\n    [210, 3, 0]  # Exemplo de uma casa sem vaga de garagem\n])\ny  np.array([200000, 300000, 180000, 500000, 400000, 450000, 360000, 220000, 520000, 410000])\n\n# Dividindo os dados em conjuntos de treino e teste\nX_train, X_test, y_train, y_test  train_test_split(X, y, test_size0.2, random_state42)\n\n# Treinando o modelo de regressão linear\nmodelo  LinearRegression()\nmodelo.fit(X_train, y_train)\n\n# Avaliando o modelo com o conjunto de teste\ny_pred  modelo.predict(X_test)\nmse  mean_squared_error(y_test, y_pred)\nprint(f\"MSE: {mse}\")\n\n# Salvando o modelo treinado\njoblib.dump(modelo, 'modelo_casas.pkl')\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/README.md\n\n```bash\ncurl \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-IZAyac7K1DAkImDFavXsGhdj/user-ZydpZOHfvGETDmONdXQjgDCG/img-Qo5w2dx6pp8EJe7XYsbFnAYu.png?st2024-04-25T03%3A45%3A19Z&se2024-04-25T05%3A45%3A19Z&spr&sv2021-08-06&srb&rscdinline&rsctimage/png&skoid6aaadede-4fb3-4698-a8f6-684d7786b067&sktida48cca56-e6da-484e-a814-9c849652bcb3&skt2024-04-24T19%3A33%3A09Z&ske2024-04-25T19%3A33%3A09Z&sksb&skv2021-08-06&sig69MjVktvwm2F3J7SwdqsoPHI7NnL3qZRHSLG39BUM3U%3D\" -o saved_image.png\n```\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/exemplo_01.py\n\nimport requests\nimport json\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nOPENAI_API_KEY  os.getenv(\"OPENAI_API_KEY\")\n# URL da API\nurl  \"https://api.openai.com/v1/images/generations\"\n\n\n# Cabeçalhos da requisição\nheaders  {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"  # Substitua $OPENAI_API_KEY pelo seu token real\n}\n\n# Dados a serem enviados como corpo da requisição\nbody  {\n    \"model\": \"dall-e-3\",\n    \"prompt\": \"a white siamese cat\",\n    \"n\": 1,\n    \"size\": \"1024x1024\"\n}\n\n# Realizando a requisição POST\nresponse  requests.post(\n    url, \n    headersheaders, \n    datajson.dumps(body))\n\nprint(response.status_code)\nprint(response.json())\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/exemplo_02.py\n\nimport requests\nimport json\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nOPENAI_API_KEY  os.getenv(\"OPENAI_API_KEY\")\n# URL da API\nurl  \"https://api.openai.com/v1/images/generations\"\n\n\n# Cabeçalhos da requisição\nheaders  {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"  # Substitua $OPENAI_API_KEY pelo seu token real\n}\n\n# Dados a serem enviados como corpo da requisição\nbody  {\n    \"model\": \"dall-e-3\",\n    \"prompt\": \"a white siamese cat\",\n    \"n\": 1,\n    \"size\": \"1024x1024\"\n}\n\n# Realizando a requisição POST\nresponse  requests.post(\n    url, \n    headersheaders, \n    datajson.dumps(body))\n\nprint(response.status_code)\nprint(response.json())\n\nURL_IMAGE  response.json()[\"data\"][0][\"url\"]\n\nimagem_path  \"download_image.jpg\"\n\nprint(requests.get(URL_IMAGE).content)\n\nwith open(imagem_path, \"wb\") as file:\n    file.write(requests.get(URL_IMAGE).content)\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/exemplo_03.py\n\nimport streamlit as st\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\n\n# Carrega variáveis de ambiente\nload_dotenv()\n\n# Configuração inicial da chave da API\nOPENAI_API_KEY  os.getenv(\"OPENAI_API_KEY\")\n# URL da API\nurl  \"https://api.openai.com/v1/images/generations\"\n\n# Cabeçalhos da requisição\nheaders  {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n}\n\n# Interface do Streamlit\nst.title('Gerador de Imagens')\nprompt  st.text_area('Digite o prompt para gerar uma imagem:', value'Descrição da imagem aqui')\n\nif st.button('Gerar Imagem'):\n    # Corpo da requisição\n    body  {\n        \"model\": \"dall-e-3\",\n        \"prompt\": prompt,\n        \"n\": 1,\n        \"size\": \"1024x1024\"\n    }\n\n    # Realiza a requisição POST\n    response  requests.post(url, headersheaders, datajson.dumps(body))\n\n    if response.status_code  200:\n        # Extrai a URL da imagem\n        url_image  response.json()[\"data\"][0][\"url\"]\n        # Exibe a imagem na interface do Streamlit\n        st.image(url_image)\n    else:\n        st.error(\"Erro ao gerar a imagem. Por favor, tente novamente.\")\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/api_openapi/exemplo_04.py\n\nimport streamlit as st\nimport requests\nimport json\nimport os\nfrom dotenv import load_dotenv\n\n# Carrega variáveis de ambiente\nload_dotenv()\n\n# Configuração inicial da chave da API\nOPENAI_API_KEY  os.getenv(\"OPENAI_API_KEY\")\n# URL da API para completar texto\nurl  \"https://api.openai.com/v1/completions\"\n\n# Cabeçalhos da requisição\nheaders  {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n}\n\n# Interface do Streamlit\nst.title('Conversor de SQL: SQL Server para PostgreSQL')\nquery_sql_server  st.text_area('Digite a consulta SQL Server aqui:')\nprint(query_sql_server)\n\nif st.button('Converter para PostgreSQL'):\n    # Prompt de conversão\n    prompt  f\"Convert this SQL Server query to a PostgreSQL query: {query_sql_server}\"\n    \n    # Corpo da requisição\n    body  {\n        \"model\": \"text-davinci-003\",\n        \"prompt\": prompt,\n        \"max_tokens\": 1000\n    }\n\n    # Realiza a requisição POST\n    response  requests.post(url, headersheaders, datajson.dumps(body))\n\n    if response.status_code  200:\n        # Extrai a resposta e apresenta a consulta convertida\n        postgres_query  response.json()[\"choices\"][0][\"text\"].strip()\n        st.text_area('Consulta PostgreSQL:', valuepostgres_query, height700)\n    else:\n        st.error(\"Erro na conversão. Por favor, tente novamente.\")\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/async/main_asy.py\n\nimport asyncio\nimport aiohttp\n\nasync def fetch_url(url):\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            return await response.text()\n\nasync def main():\n    urls  [\"https://example.com\"] * 10  # Lista de URLs para fetch\n    tasks  [fetch_url(url) for url in urls]\n    responses  await asyncio.gather(*tasks)\n    for response in responses:\n        print(response[:100])  # Printa os primeiros 100 caracteres de cada resposta\n\nasyncio.run(main())\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/async/main_sync.py\n\nimport requests\n\ndef fetch_url(url):\n    response  requests.get(url)\n    return response.text\n\ndef main():\n    urls  [\"https://example.com\"] * 10  # Lista de URLs para fetch\n    responses  [fetch_url(url) for url in urls]\n    for response in responses:\n        print(response[:100])  # Printa os primeiros 100 caracteres de cada resposta\n\nmain()\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/response_json_html_xml/readme.md\n\nhttps://medium.com/@denniskoko/implementing-content-negotiation-in-fastapi-371d03c59c02\n\nAceitar diferentes tipos de comunicação\n\n### Exemplo com a API da NASA:\n\nPara acessar a API da NASA, você precisa passar sua chave de API como um parâmetro na solicitação. Aqui está um exemplo usando cURL:\n\n```bash\ncurl -X GET 'https://api.nasa.gov/planetary/apod?api_keyaJ87xjxneJaiBejYAfFrSSPJXR4Ce2RdMnlrYP3x' \\\n  -H 'Accept: application/json'\n```\n\nNeste exemplo, a chave da API (`api_keyaJ87xjxneJaiBejYAfFrSSPJXR4Ce2RdMnlrYP3x`) é passada como um parâmetro na URL.\n\n### Exemplo com a API da Wikipedia:\n\nA API da Wikipedia usa um método de autenticação diferente, onde você não precisa de uma chave de API, mas pode especificar o formato da resposta usando uma flag na solicitação. Aqui está um exemplo usando cURL:\n\n```bash\ncurl -X GET 'https://en.wikipedia.org/w/api.php?actionquery&formatxml&titlesAlbert%20Einstein'\n```\n\nNeste exemplo, estamos fazendo uma solicitação para obter informações sobre \"Albert Einstein\" da Wikipedia. A flag `formatxml` é usada para especificar que queremos a resposta no formato XML.\n\n### Diferença entre as duas APIs:\n\n* A API da NASA requer uma chave de API para autenticação e utiliza o cabeçalho `Accept` para especificar o tipo de conteúdo desejado na resposta (JSON neste caso).\n* A API da Wikipedia não requer uma chave de API, mas permite que você especifique o formato da resposta usando uma flag na própria URL da solicitação.\n\n\n### API da NASA - Solicitação JSON:\n\n```bash\n# JSON\ncurl -X GET 'https://api.nasa.gov/planetary/apod?api_keyaJ87xjxneJaiBejYAfFrSSPJXR4Ce2RdMnlrYP3x' \\\n  -H 'Accept: application/json'\n```\n\n### API da NASA - Solicitação XML:\n\ncurl -X GET 'https://api.nasa.gov/planetary/apod' \\                                                 \n  -H 'Accept: application/xml'\n<?xml version\"1.0\" encoding\"UTF-8\"?>\n<response>\n  <error>\n    <code>API_KEY_MISSING</code>\n    <message>No api_key was supplied. Get one at https://api.nasa.gov:443</message>\n  </error>\n</response>%  \n\n\n```bash\n# XML\ncurl -X GET 'https://api.nasa.gov/planetary/apod?api_keyaJ87xjxneJaiBejYAfFrSSPJXR4Ce2RdMnlrYP3x' -H 'Accept: application/xml'\n```\n\n### API da Wikipedia - Solicitação JSON:\n\n```bash\n# JSON\ncurl -X GET 'https://en.wikipedia.org/w/api.php?actionquery&formatjson&titlesAlbert%20Einstein'\n```\n\n### API da Wikipedia - Solicitação XML:\n\n```bash\n# XML\ncurl -X GET 'https://en.wikipedia.org/w/api.php?actionquery&formatxml&titlesAlbert%20Einstein'\n```\n\nEsses exemplos demonstram como fazer solicitações para ambas as APIs especificando o formato da resposta desejada (JSON ou XML).\n\n\nAqui está um exemplo de como você pode usar o cURL para fazer solicitações para o endpoint `get_user_by_extension` com diferentes tipos de formato:\n\n```bash\n# Solicitar dados no formato JSON\ncurl -X GET 'http://localhost:8000/users?formatjson'\n\n# Solicitar dados no formato HTML\ncurl -X GET 'http://localhost:8000/users?formathtml'\n\n# Solicitar dados no formato XML\ncurl -X GET 'http://localhost:8000/users?formatxml'\n```\n\n06-restAPI-fastAPI-deploy/primeiro-dia/response_json_html_xml/multiple.py\n\nfrom fastapi import FastAPI, Request, Response, Query, HTTPException\nfrom fastapi.responses import JSONResponse, HTMLResponse\nfrom typing import Optional\n\napp  FastAPI()\n\n# Sample data\ndata  {\n    \"name\": \"John Doe\",\n    \"age\": 30,\n    \"email\": \"john.doe@example.com\"\n}\n\n# Define function to generate HTML content from data\ndef generate_html(data):\n    html_content  \"\"\"\n    <!DOCTYPE html>\n    <html lang\"en\">\n    <head>\n        <meta charset\"UTF-8\">\n        <meta name\"viewport\" content\"widthdevice-width, initial-scale1.0\">\n        <title>User Info</title>\n    </head>\n    <body>\n        <h1>User Info</h1>\n        <ul>\n            <li>Name: {name}</li>\n            <li>Age: {age}</li>\n            <li>Email: {email}</li>\n        </ul>\n    </body>\n    </html>\n    \"\"\".format(**data)\n    return html_content\n\n# Define function to generate XML content from data\ndef generate_xml(data):\n    xml_content  f\"<user>\\n\" # noqa\n    for key, value in data.items():\n        xml_content + f\"    <{key}>{value}</{key}>\\n\"\n    xml_content + \"</user>\"\n    return xml_content\n\n# Define endpoint to handle content negotiation for Accept and query\n# Define endpoint to handle content negotiation for Accept and query\n@app.get(\"/user\", responses{\n    200: {\"description\": \"User information\", \"content\": {\n        \"application/json\": {},\n        \"text/html\": {},\n        \"application/xml\": {}\n    }}\n})\nasync def get_user(request: Request, format: Optional[str]  Query(None, pattern\"^(json|html|xml)$\")):\n    # Check if query parameter is present\n    if format:\n        if format  \"json\":\n            return JSONResponse(contentdata)\n        elif format  \"html\":\n            return HTMLResponse(contentgenerate_html(data))\n        elif format  \"xml\":\n            return Response(contentgenerate_xml(data), media_type\"application/xml\")\n\n    # Check client's preferred content type from Accept header\n    accept_header  request.headers.get(\"Accept\")\n    \n    # Default to JSON if Accept header is not provided\n    if not accept_header:\n        return JSONResponse(contentdata)\n    \n    # Parse Accept header to determine client's preferred content type\n    if \"application/json\" in accept_header:\n        return JSONResponse(contentdata)\n    elif \"text/html\" in accept_header:\n        return HTMLResponse(contentgenerate_html(data))\n    elif \"application/xml\" in accept_header:\n        return Response(contentgenerate_xml(data), media_type\"application/xml\")\n    else:\n        # If no matching content type is found, return a 406 Not Acceptable response\n        return Response(status_code406, content\"406 Not Acceptable: Unsupported Media Type\")\n\n# Define endpoint to handle content negotiation for file extension\n@app.get(\"/user.{format}\")\nasync def get_user_by_extension(format: str):\n    \"\"\"\n    Retorna informações do usuário no formato especificado.\n\n    Parâmetros:\n    - format: Formato desejado da resposta. Opções disponíveis: json, html, xml.\n    \"\"\"\n    if format  \"json\":\n        return JSONResponse(contentdata)\n    elif format  \"html\":\n        return HTMLResponse(contentgenerate_html(data))\n    elif format  \"xml\":\n        return Response(contentgenerate_xml(data), media_type\"application/xml\")\n    else:\n        raise HTTPException(status_code406, detail\"Unsupported Media Type\")\n    \n@app.get(\"/users\")\nasync def get_user_by_extension_final(format: str  None):\n    if format  \"json\":\n        return JSONResponse(contentdata)\n    elif format  \"html\":\n        return HTMLResponse(contentgenerate_html(data))\n    elif format  \"xml\":\n        return Response(contentgenerate_xml(data), media_type\"application/xml\")\n    else:\n        raise HTTPException(status_code406, detail\"Unsupported Media Type\")\n\n# Run the application\nif __name__  \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host\"0.0.0.0\", port8000)\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/README.md\n\n## Repositório Rinha\n\nhttps://github.com/zanfranceschi/rinha-de-backend-2024-q1\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/Dockerfile\n\nFROM python:3.12\n\nWORKDIR /app\n\nCOPY ./requirements.txt /app/requirements.txt\n\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir -r requirements.txt\n\nCOPY ./app /app\n\nCMD [\"uvicorn\", \"app.main:app\", \"--reload\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/docker-compose.yml\n\nversion: \"3.9\"\n\nservices:\n  api-1:\n    hostname: api-1\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - ./:/app\n    ports:\n      - \"8001:80\"\n\n  api-2:\n    hostname: api-2\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - ./:/app\n    ports:\n      - \"8002:80\"\n\n  nginx:\n    image: nginx:latest\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - api-1\n      - api-2\n\n  db:\n    image: postgres:latest\n    hostname: db\n    environment:\n      - POSTGRES_PASSWORD123\n      - POSTGRES_USERadmin\n      - POSTGRES_DBrinha\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ./script.sql:/docker-entrypoint-initdb.d/script.sql\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/nginx.conf\n\nevents {}\n\nhttp {\n    upstream api_servers {\n        server api-1:80;\n        server api-2:80;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://api_servers;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n        }\n    }\n}\n\n\n",
        "06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/pyproject.toml\n\n[tool.poetry]\nname  \"app\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"Your Name <you@example.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.11\"\naiohttp  \"^3.9.5\"\nrequests  \"^2.31.0\"\nfastapi  \"^0.110.2\"\nuvicorn  {extras  [\"standard\"], version  \"^0.29.0\"}\nsqlalchemy  \"^2.0.29\"\npsycopg2  \"^2.9.9\"\nasyncpg  \"^0.29.0\"\nscikit-learn  \"^1.4.2\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/requirements.txt\n\naiohttp3.9.5\naiosignal1.3.1\nannotated-types0.6.0\nanyio4.3.0\nasyncpg0.29.0\nattrs23.2.0\ncertifi2024.2.2\ncharset-normalizer3.3.2\nclick8.1.7\ncolorama0.4.6\nfastapi0.110.2\nfrozenlist1.4.1\ngreenlet3.0.3\nh110.14.0\nhttptools0.6.1\nidna3.7\nmultidict6.0.5\npsycopg22.9.9\npydantic2.7.1\npydantic_core2.18.2\npython-dotenv1.0.1\nPyYAML6.0.1\nrequests2.31.0\nsniffio1.3.1\nSQLAlchemy2.0.29\nstarlette0.37.2\ntyping_extensions4.11.0\nurllib32.2.1\nuvicorn0.29.0\nwatchfiles0.21.0\nwebsockets12.0\nyarl1.9.4\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/script.sql\n\nCREATE UNLOGGED TABLE IF NOT EXISTS clients (\n    id SERIAL PRIMARY KEY NOT NULL,\n    limite INTEGER NOT NULL,\n    saldo INTEGER NOT NULL\n);\n\nCREATE UNLOGGED TABLE IF NOT EXISTS transactions (\n    id SERIAL PRIMARY KEY NOT NULL,\n    tipo CHAR(1) NOT NULL,\n    descricao VARCHAR(10) NOT NULL,\n    valor INTEGER NOT NULL,\n    cliente_id INTEGER NOT NULL,\n    realizada_em TIMESTAMP NOT NULL DEFAULT NOW()\n);\n\nCREATE INDEX idx_cliente_id\nON transactions(cliente_id);\n\nINSERT INTO clients (limite, saldo)\nVALUES\n    (100000, 0),\n    (80000, 0),\n    (1000000, 0),\n    (10000000, 0),\n    (500000, 0);\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/.python-version\n\n3.12.1\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/app/database.py\n\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.orm import declarative_base\n\n# URL de conexão assíncrona para PostgreSQL, ajuste conforme necessário\nSQLALCHEMY_DATABASE_URL  \"postgresql+asyncpg://admin:123@db/rinha\"\n\n# Criar um engine assíncrono\nengine  create_async_engine(SQLALCHEMY_DATABASE_URL)\n\n# Configurar sessionmaker para criar sessões assíncronas\nasync_session_local  sessionmaker(\n    autocommitFalse,\n    autoflushFalse,\n    bindengine,\n    class_AsyncSession  # Isso especifica que a sessão deve ser assíncrona\n)\n\nBase  declarative_base()\n\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/app/main.py\n\nimport time\n\nfrom fastapi import FastAPI, HTTPException, Response\nfrom fastapi.params import Depends\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom . import schemas\nfrom .database import async_session_local\nfrom .models import Cliente, Transacao\n\napp  FastAPI()\n\nasync def get_session():\n    async with async_session_local() as session:\n        yield session\n\n\n@app.post(\"/clientes/{cliente_id}/transacoes\", response_modelschemas.ClienteResponse)\nasync def post_transacao(cliente_id: int, \n                         transacao: schemas.TransactionCreateRequest, \n                         session: AsyncSession  Depends(get_session)):\n    \n    result  await session.execute(select(Cliente).filter_by(idcliente_id))\n    cliente  result.scalars().one_or_none()\n\n    if not cliente:\n        raise HTTPException(status_code404, detail\"Cliente não encontrado\")\n\n    if transacao.tipo  \"d\":\n        if cliente.saldo - transacao.valor < -cliente.limite:\n            raise HTTPException(status_code422, detail\"Saldo insuficiente\")\n        cliente.saldo - transacao.valor\n    else:  # ou seja do tipo  \"c\"\n        cliente.saldo + transacao.valor\n\n    nova_transacao  Transacao(**transacao.model_dump(), cliente_idcliente_id)\n    session.add(nova_transacao)\n    await session.commit()\n    await session.refresh(cliente)\n\n    return cliente\n\n\n@app.get(\"/clientes/{cliente_id}/extrato\")\nasync def get_extrato(cliente_id: int, session: AsyncSession  Depends(get_session)):\n    # Obtém o cliente de forma assíncrona usando select e scalars\n    result  await session.execute(select(Cliente).filter_by(idcliente_id))\n    client  result.scalars().one_or_none()\n\n    if not client:\n        raise HTTPException(status_code404, detail\"Cliente não encontrado\")\n\n    # Executa a consulta para obter as últimas transações de forma assíncrona\n    transactions_result  await session.execute(\n        select(Transacao)\n        .where(Transacao.cliente_id  cliente_id)\n        .order_by(Transacao.id.desc())\n        .limit(10)\n    )\n    transactions  transactions_result.scalars().all()\n\n    return {\n        \"saldo\": {\n            \"total\": client.saldo,\n            \"data_extrato\": time.strftime(\"%Y-%m-%dT%H:%M:%S\", time.gmtime()),\n            \"limite\": client.limite\n        },\n        \"ultimas_transacoes\": [\n            t for t in transactions\n        ]\n    }\n\n@app.delete(\"/clientes/{cliente_id}\", status_code204)\nasync def delete_cliente(cliente_id: int, session: AsyncSession  Depends(get_session)):\n    result  await session.execute(select(Cliente).filter_by(idcliente_id))\n    cliente  result.scalars().one_or_none()\n\n    if not cliente:\n        raise HTTPException(status_code404, detail\"Cliente não encontrado\")\n\n    await session.delete(cliente)\n    await session.commit()\n    return Response(status_code204)\n\n@app.put(\"/clientes/{cliente_id}\", response_modelschemas.ClienteResponse)\nasync def update_cliente(cliente_id: int, update_data: schemas.ClientCreateRequest, session: AsyncSession  Depends(get_session)):\n    result  await session.execute(select(Cliente).filter_by(idcliente_id))\n    cliente  result.scalars().one_or_none()\n\n    if not cliente:\n        raise HTTPException(status_code404, detail\"Cliente não encontrado\")\n\n    for var, value in update_data.model_dump(exclude_unsetTrue).items():\n        setattr(cliente, var, value)\n\n    await session.commit()\n    await session.refresh(cliente)\n    return cliente\n\n@app.post(\"/clientes/\", response_modelschemas.ClienteResponse, status_code201)\nasync def create_cliente(cliente_data: schemas.ClientCreateRequest, \n                         session: AsyncSession  Depends(get_session)):\n    \n    novo_cliente  Cliente(\n        saldocliente_data.saldo,\n        limitecliente_data.limite\n    )\n    session.add(novo_cliente)\n    await session.commit()\n    await session.refresh(novo_cliente)\n    return novo_cliente\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/app/models.py\n\nfrom datetime import datetime\n\nfrom sqlalchemy import Boolean, Column, ForeignKey, Integer, String, DateTime\nfrom sqlalchemy.orm import relationship\nfrom .database import Base\n\n# Models represent the tables in the database\n\nclass Cliente(Base):\n    __tablename__  \"clients\"\n\n    id  Column(Integer, primary_keyTrue)\n    limite  Column(Integer)\n    saldo  Column(Integer)\n\n    transacoes  relationship(\"Transacao\", back_populates\"cliente\")\n\nclass Transacao(Base):\n    __tablename__  \"transactions\"\n\n    id  Column(Integer, primary_keyTrue)\n    cliente_id  Column(Integer, ForeignKey(\"clients.id\"))\n    valor  Column(Integer)\n    tipo  Column(String)\n    descricao  Column(String)\n    realizada_em  Column(DateTime, defaultdatetime.now())\n\n    cliente  relationship(\"Cliente\", back_populates\"transacoes\")\n\n06-restAPI-fastAPI-deploy/primeiro-dia/rinha_backend/app/schemas.py\n\nfrom enum import Enum\nfrom typing import Optional\n\nfrom pydantic import BaseModel,Field\n\nclass TransactionTypeEnum(str, Enum):\n    credito  'c'\n    debito  'd'\n\nclass TransactionBase(BaseModel):\n    valor: int\n    tipo: TransactionTypeEnum\n    descricao: str  Field(max_length10, min_length1)\n\nclass TransactionCreateRequest(TransactionBase):\n    pass\n\nclass ClienteBase(BaseModel):\n    limite: int\n    saldo: int\n\nclass ClienteResponse(ClienteBase):\n    pass\n\nclass ClientRequest(ClienteBase):\n    pass\n\nclass ClientCreateRequest(ClienteBase):\n    id: int\n\nclass ClienteUpdateRequest(BaseModel):\n    saldo: Optional[float]  None\n    limite: Optional[float]  None\n\n\n06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/README.md\n\n# api\n\n\n06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/docker-compose.yml\n\nversion: '3.8'\n\nservices:\n  db:\n    image: postgres:latest\n    restart: always\n    environment:\n      POSTGRES_DB: mydatabase\n      POSTGRES_USER: myuser\n      POSTGRES_PASSWORD: mypassword\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\nvolumes:\n  pgdata:\n\n\n06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/pyproject.toml\n\n[tool.poetry]\nname  \"workshop-api\"\nversion  \"0.1.0\"\ndescription  \"\"\nauthors  [\"Fabio Cantarim <fabio.cantarim@gmail.com>\"]\nreadme  \"README.md\"\n\n[tool.poetry.dependencies]\npython  \"^3.12\"\nfastapi  \"^0.111.0\"\nuvicorn  \"^0.29.0\"\npsycopg2-binary  \"^2.9.9\"\n\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n\n06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/src/controller/user.py\n\n\nfrom src.db.connection import PostgreSQLConnection\nfrom src.models.user import User\n\n\ndb  PostgreSQLConnection(dbname'postgres', user'myuser', password'mypassword')\n\n\nasync def c_get_user(user_id:int):\n    db.connect()\n    user  db.select_user(f\"SELECT * FROM users WHERE id  {user_id}\")\n    db.close()\n    return user\n\nasync def c_create_user(user: User):\n    db.connect()\n    db.insert_user(user.id, user.name, user.area, user.job_description, user.role, user.salary, user.is_active, user.last_evaluation)\n    db.close()\n    return user\n\n\nasync def c_delete_user(user_id: int):\n    db.connect()\n    user  db.delete_user(user_id)\n    db.close()\n    return user\n\nasync def c_update_user(user: User):\n    db.connect()\n    user  db.update_user(user.id, user.name, user.area, user.job_description, user.role, user.salary, user.is_active, user.last_evaluation)\n    db.close()\n    return user\n\n06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/src/db/connection.py\n\nimport psycopg2\n\nclass PostgreSQLConnection:\n    def __init__(self, dbname, user, password, host'localhost', port'5432'):\n        self.dbname  dbname\n        self.user  user\n        self.password  password\n        self.host  host\n        self.port  port\n        self.conn  None\n\n    def connect(self):\n        try:\n            self.conn   psycopg2.connect(dbname  self.dbname, userself.user, passwordself.password, hostself.host, portself.port)\n            print(\"Conexão com sucesso!\")\n        except psycopg2.Error as e:\n            print(\"Erro ao conectar ao Postgre:\", e)\n\n\n    def select_user(self, query):\n        if not self.conn:\n            print(\"Você não esta conectado\")\n            return None\n        try:\n            cursor  self.conn.cursor()\n            cursor.execute(query)\n            rows  cursor.fetchall()\n            cursor.close()\n            return rows\n        except psycopg2 as e:\n            print(\"Erro ao executar a consulta\", e)\n            return None\n\n    def insert_user(self, id, name, area, job_description, role, salary, is_active, last_evaluation):\n        if not self.conn:\n            print(\"Você nao esta conectado\")\n            return None\n        try:\n            cursor  self.conn.cursor()\n            cursor.execute(\n                \"INSERT INTO users (id, name, area, job_description, role, salary, is_active, last_evaluation) VALUES (%s, %s, %s,%s, %s, %s,%s,%s)\",\n                (id, name, area, job_description, role, salary, is_active, last_evaluation)\n            )\n            self.conn.commit()\n            cursor.close()\n            print(\"Usário registrado!\")\n        except psycopg2.Error as e:\n            print(\"Não foi possível registrar! - \",e)\n\n    def delete_user(self, user_id: int):\n        if not self.conn:\n            print(\"Você não esta conectado\")\n            return None\n        try:\n            cursor  self.conn.cursor()\n            cursor.execute(\n                \"DELETE FROM users WHERE id  %s\",\n                (user_id,)\n            )\n            self.conn.commit()\n            cursor.close()\n            print(\"Registro Deletado\")\n        except psycopg2.Error as e:\n            print(\"Não foi possível deletar! - \", e)\n\n    def update_user(self, id, nameNone, areaNone, job_descriptionNone, roleNone, salaryNone, is_activeNone, last_evaluationNone):\n        if not self.conn:\n            print(\"Você não esta conectado\")\n            return None\n\n        try:\n            cursor  self.conn.cursor()\n            update_query  \"UPDATE users SET\"\n            update_values  []\n\n            if name is not None:\n                update_query + \" name  %s,\"\n                update_values.append(name)\n            \n            if area is not None:\n                update_query + \" area  %s,\"\n                update_values.append(area)\n\n            if job_description is not None:\n                update_query + \" job_description  %s,\"\n                update_values.append(job_description)\n\n            if role is not None:\n                update_query + \" role  %s,\"\n                update_values.append(role)\n            \n            if salary is not None:\n                update_query + \" salary  %s,\"\n                update_values.append(salary)\n            \n            if is_active is not None:\n                update_query + \" is_active  %s,\"\n                update_values.append(is_active)\n\n            if last_evaluation is not None:\n                update_query + \" last_evaluation  %s,\"\n                update_values.append(last_evaluation)\n\n            update_query  update_query.rstrip(',') + \" WHERE id  %s\"\n            update_values.append(id)\n\n            cursor.execute(update_query, tuple(update_values))\n            self.conn.commit()\n            cursor.close()\n            print(\"Usuário Atualizado\")\n        except psycopg2.Error as e:\n            print(\"Não foi possível atualizar o user: \", e)\n\n    def close(self):\n        if self.conn:\n            self.conn.close()\n            print(\"Conexão Encerrada!\")\n    \n\n06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/src/models/user.py\n\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass  User(BaseModel):\n    id: int\n    name: Optional[str]\n    area: Optional[str]\n    job_description: Optional[str]\n    role: Optional[int] \n    salary: Optional[float]\n    is_active: Optional[bool]\n    last_evaluation: Optional[str]\n\n06-restAPI-fastAPI-deploy/segundo/fabio-api-crud/src/routes/user.py\n\n\nfrom fastapi import FastAPI, HTTPException\n\nfrom src.controller.user import c_create_user, c_delete_user, c_get_user, c_update_user\nfrom src.models.user import User\n\napp  FastAPI()\n\n@app.get(\"/users/{user_id}\")\nasync def get_user(user_id: int):\n    db_user  await c_get_user(user_id)\n    if db_user is None:\n        raise HTTPException(status_code404, detail\"Usuário Não Encontrado\")\n    return db_user\n\n@app.post(\"/users\")\nasync def create_user(user: User):\n    db_user  await c_get_user(user.id)\n    if db_user:\n        raise HTTPException(status_code400, detail\"O usuário já existe\")\n    user  await c_create_user(user)\n    return user\n\n@app.delete(\"/users/{user_id}\")\nasync def delete_user(user_id: int):\n    db_user  await c_get_user(user_id)\n    if db_user is None:\n        raise HTTPException(status_code404, detail\"O usuário não existe\")\n    await c_delete_user(user_id)\n    return {\"message\": \"Usuário foi deletado\"}\n\n@app.put(\"/users/{user_id}\")\nasync def update_user(user_id: int, user: User):\n    db_user  await c_get_user(user_id)\n    if db_user is None:\n        raise HTTPException(status_code404, detail\"O usuário não existe\")\n    await c_update_user(user)\n    return {\"message\": \"Usuário foi alterado\"}\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/README.md\n\n# FastAPI and Docker Workshop by Sebastián Ramírez (Tiangolo)\n\nJoin Docker Captain Sebastián Ramírez as he uses memes to teach you how to create an API ready for production in very little time using FastAPI.\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/Dockerfile\n\nFROM python:3.12 as builder\n\nENV PIP_YES1 \\\n    POETRY_VIRTUALENVS_IN_PROJECT1 \\\n    POETRY_VIRTUALENVS_CREATE1 \\\n    POETRY_CACHE_DIR/tmp/poetry_cache\n\nWORKDIR /app\n\nCOPY pyproject.toml poetry.lock /app/\nRUN touch README.md\n\nRUN --mounttypecache,target$POETRY_CACHE_DIR \\\n    pip install poetry1.8.2 && \\\n    poetry install --only main --no-root\n\nFROM python:3.12-slim as runtime\n\nENV VIRTUAL_ENV/app/.venv \\\n    PATH\"/app/.venv/bin:$PATH\"\n\nWORKDIR /app\n\nCOPY --frombuilder $VIRTUAL_ENV $VIRTUAL_ENV\nCOPY src ./src\nEXPOSE 8000\n#ENTRYPOINT [ \"gunicorn\", \"src.main:app\", \"--workers\", \"4\", \"--worker-class\", \"uvicorn.workers.UvicornWorker\", \"--bind\", \"0.0.0.0:8000\" ]\nENTRYPOINT [ \"uvicorn\", \"src.main:app\", \"--host\", \"127.0.0.1\", \"--port\", \"8000\" ]\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/Makefile\n\n.PHONY: dev-run\n\ndev-run:\n\t@poetry run python -m uvicorn src.main:app --reload --host localhost --port 8000\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/docker-compose.yml\n\nservices:\n  web:\n    build: .\n    environment:\n      - UVICORN_RELOADTrue\n    ports:\n      - 5000:8000\n    volumes:\n      - ./src:/app/src\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/mkdocs.yml\n\nsite_name: My Docs\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/pyproject.toml\n\n[tool.poetry]\nname  \"fastapi-docker\"\nversion  \"0.1.0\"\ndescription  \"Join Docker Captain Sebastián Ramírez as he uses memes to teach you how to create an API ready for production in very little time using FastAPI.\"\nauthors  [\"Kaio Silva <kaiohp.silva1@gmail.com>\"]\nlicense  \"MIT\"\nreadme  \"README.md\"\npackages  [{include  \"src\"}]\n\n\n[tool.poetry.dependencies]\npython  \"^3.12\"\nfastapi  {extras  [\"all\"], version  \"^0.110.1\"}\ngunicorn  \"^22.0.0\"\npython-jose  {extras  [\"cryptography\"], version  \"^3.3.0\"}\npasslib  {extras  [\"bcrypt\"], version  \"^1.7.4\"}\nbcrypt  \"4.0.1\"\nsqlalchemy  \"^2.0.29\"\n\n\n[tool.poetry.group.docs.dependencies]\nmkdocs  \"^1.5.3\"\n\n\n[tool.poetry.group.test.dependencies]\npytest  \"^8.1.1\"\n\n\n[tool.poetry.group.dev.dependencies]\nblack  \"^24.4.0\"\nisort  \"^5.13.2\"\nflake8  \"^7.0.0\"\npre-commit  \"^3.7.0\"\ncommitizen  \"^3.22.0\"\nmypy  \"^1.10.0\"\n\n[build-system]\nrequires  [\"poetry-core\"]\nbuild-backend  \"poetry.core.masonry.api\"\n\n[tool.black]\nline-length  79\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/.env.example\n\nSECRET_KEY\"<your hex 32>\"\nALGORITHM\"<algorithm>\"\nACCESS_TOKEN_EXPIRE_MINUTES\"<time>\"\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/.flake8\n\n[flake8]\nmax-line-length  79\nextend-ignore  E203,E701\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/.pre-commit-config.yaml\n\n# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n    - repo: https://github.com/pre-commit/pre-commit-hooks\n      rev: v4.6.0\n      hooks:\n          - id: trailing-whitespace\n          - id: end-of-file-fixer\n          - id: check-yaml\n          - id: check-added-large-files\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/.python-version\n\n3.12.2\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/docs/index.md\n\n# Welcome to MkDocs\n\nFor full documentation visit [mkdocs.org](https://www.mkdocs.org).\n\n## Commands\n\n* `mkdocs new [dir-name]` - Create a new project.\n* `mkdocs serve` - Start the live-reloading docs server.\n* `mkdocs build` - Build the documentation site.\n* `mkdocs -h` - Print help message and exit.\n\n## Project layout\n\n    mkdocs.yml    # The configuration file.\n    docs/\n        index.md  # The documentation homepage.\n        ...       # Other markdown pages, images and other files.\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/main.py\n\nimport time\nfrom datetime import timedelta\nfrom typing import Annotated\n\nfrom fastapi import Depends, FastAPI, HTTPException, Request, status\nfrom fastapi.security import OAuth2PasswordRequestForm\n\nfrom src.core import crud, models, schemas, security\nfrom src.core.config import settings\nfrom src.core.db import engine\nfrom src.core.deps import CurrentUser, SessionDep\nfrom src.core.schemas import Token\n\nmodels.Base.metadata.create_all(bindengine)\n\napp  FastAPI()\n\n\n@app.post(\"/login/access-token/\")\nasync def login(\n    session: SessionDep,\n    form_data: Annotated[OAuth2PasswordRequestForm, Depends()],\n) -> Token:\n\n    user  crud.authenticate(\n        sessionsession, emailform_data.username, passwordform_data.password\n    )\n    if not user:\n        raise HTTPException(\n            status_codestatus.HTTP_401_UNAUTHORIZED,\n            detail\"Incorrect username or password\",\n            headers{\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token_expires  timedelta(\n        minutessettings.ACCESS_TOKEN_EXPIRE_MINUTES\n    )\n    return Token(\n        access_tokensecurity.create_access_token(\n            user.id, expires_deltaaccess_token_expires\n        )\n    )\n\n\n@app.post(\"/singup\", response_modelschemas.User)\nasync def create_user(session: SessionDep, user: schemas.UserCreate):\n    db_user  crud.get_user(session, emailuser.email)\n    if db_user:\n        raise HTTPException(status_code400, detail\"Email already registered\")\n    return crud.create_user(session, user)\n\n\n@app.get(\"/users/me/\", response_modelschemas.User)\nasync def read_users_me(current_user: CurrentUser, session: SessionDep):\n    db_user  crud.get_user(session, user_idcurrent_user.id)\n    return db_user\n\n\n@app.get(\"/users/me/items/\", response_modellist[schemas.Item])\nasync def read_own_items(\n    session: SessionDep,\n    current_user: CurrentUser,\n    skip: int  0,\n    limit: int  100,\n):\n    db_items  crud.get_items(\n        session, user_idcurrent_user.id, skipskip, limitlimit\n    )\n\n    return db_items\n\n\n@app.middleware(\"http\")\nasync def add_process_time_header_middleware(request: Request, call_next):\n    start_time  time.time()\n    response  await call_next(request)\n    process_time  time.time() - start_time\n    response.headers[\"X-Process-Time\"]  str(process_time)\n    return response\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/app_log.py\n\nimport logging\nimport os\n\nfrom fastapi.logger import logger as fastapi_logger\n\nif \"gunicorn\" in os.environ.get(\"SERVER_SOFTWARE\", \"\"):\n    \"\"\"\n    When running with gunicorn the log handlers get suppressed instead of\n    passed along to the container manager. This forces the gunicorn handlers\n    to be used throughout the project.\n    \"\"\"\n\n    gunicorn_logger  logging.getLogger(\"gunicorn\")\n    log_level  gunicorn_logger.level\n\n    root_logger  logging.getLogger()\n    gunicorn_error_logger  logging.getLogger(\"gunicorn.error\")\n    uvicorn_access_logger  logging.getLogger(\"uvicorn.access\")\n\n    # Use gunicorn error handlers for root, uvicorn, and fastapi loggers\n    root_logger.handlers  gunicorn_error_logger.handlers\n    uvicorn_access_logger.handlers  gunicorn_error_logger.handlers\n    fastapi_logger.handlers  gunicorn_error_logger.handlers\n\n    # Pass on logging levels for root, uvicorn, and fastapi loggers\n    root_logger.setLevel(log_level)\n    uvicorn_access_logger.setLevel(log_level)\n    fastapi_logger.setLevel(log_level)\n\n\n",
        "06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/config.py\n\nfrom typing import Literal\n\nfrom pydantic import PostgresDsn, computed_field\nfrom pydantic_core import MultiHostUrl\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    model_config  SettingsConfigDict(\n        env_file\".env\",\n        env_file_encoding\"utf-8\",\n        env_ignore_emptyTrue,\n        extra\"ignore\",\n    )\n\n    SECRET_KEY: str\n    ALGORITHM: str\n    ACCESS_TOKEN_EXPIRE_MINUTES: int  60 * 24 * 7  # 7 days in minutes\n    DOMAIN: str  \"localhost\"\n    ENVIROMENT: Literal[\"dev\", \"prod\"]  \"dev\"\n\n    @computed_field  # type: ignore\n    @property\n    def server_host(self) -> str:\n        if self.ENVIROMENT  \"dev\":\n            return f\"http://{self.DOMAIN}\"\n        return f\"https://{self.DOMAIN}\"\n\n    if ENVIROMENT  \"prod\":\n        POSTGRES_SCHEME: str  \"postgresql+psycopg\"\n        POSTGRES_USER: str\n        POSTGRES_PASSWORD: str\n        POSTGRES_SERVER: str\n        POSTGRES_PORT: int  5432\n        POSTGRES_DB: str\n\n    @computed_field  # type: ignore\n    @property\n    def SQLALCHEMY_DATABASE_URI(self) -> str | PostgresDsn:\n        if self.ENVIROMENT  \"dev\":\n            return \"sqlite:///database/app.db\"\n        return MultiHostUrl.build(\n            schemeself.POSTGRES_SCHEME,\n            usernameself.POSTGRES_USER,\n            passwordself.POSTGRES_PASSWORD,\n            hostself.POSTGRES_SERVER,\n            portself.POSTGRES_PORT,\n            pathself.POSTGRES_DB,\n        )\n\n\nsettings  Settings()  # type: ignore\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/crud.py\n\nfrom sqlalchemy.exc import ArgumentError\nfrom sqlalchemy.orm import Session\n\nfrom src.core import models, schemas\nfrom src.core.security import get_password_hash, verify_password\n\n\ndef get_user(\n    db: Session,\n    user_id: int | None  None,\n    email: str | None  None,\n):\n    if not any([user_id, email]):\n        raise ArgumentError(\"Must provide user_id or email.\")\n\n    query  db.query(models.User)\n    if user_id:\n        query  query.filter(models.User.id  user_id)\n    if email:\n        query  query.filter(models.User.email  email)\n\n    return query.first()\n\n\ndef get_users(db: Session, skip: int  0, limit: int  100):\n    return db.query(models.User).offset(skip).limit(limit).all()\n\n\ndef create_user(db: Session, user: schemas.UserCreate):\n    db_user  models.User(\n        emailuser.email,\n        hashed_passwordget_password_hash(user.password),\n    )\n    db.add(db_user)\n    db.commit()\n    db.refresh(db_user)\n    return db_user\n\n\ndef get_items(db: Session, user_id: int, skip: int  0, limit: int  100):\n    return (\n        db.query(models.Item)\n        .where(models.Item.owner_id  user_id)\n        .offset(skip)\n        .limit(limit)\n        .all()\n    )\n\n\ndef create_user_item(db: Session, item: schemas.ItemCreate, user_id: int):\n    db_item  models.Item(**item.model_dump(), owner_iduser_id)\n    db.add(db_item)\n    db.commit()\n    db.refresh(db_item)\n    return db_item\n\n\ndef authenticate(session: Session, email: str, password: str):\n    db_user  get_user(dbsession, emailemail)\n    if not db_user or not verify_password(password, db_user.hashed_password):\n        return None\n    return db_user\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/db.py\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nfrom src.core.config import settings\n\nSQLALCHEMY_DATABASE_URL  settings.SQLALCHEMY_DATABASE_URI\n\nengine  create_engine(\n    str(SQLALCHEMY_DATABASE_URL), connect_args{\"check_same_thread\": False}\n)\n\nSessionLocal  sessionmaker(autocommitFalse, autoflushFalse, bindengine)\n\nBase  declarative_base()\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/deps.py\n\nfrom typing import Annotated\n\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import JWTError, jwt\nfrom pydantic import ValidationError\nfrom sqlalchemy.orm import Session\n\nfrom src.core.config import settings\nfrom src.core.db import SessionLocal\nfrom src.core.models import User\nfrom src.core.schemas import TokenPayload\n\noauth2  OAuth2PasswordBearer(tokenUrl\"login/access-token\")\n\n\ndef get_db():\n    db  SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n\nSessionDep  Annotated[Session, Depends(get_db)]\nTokenDep  Annotated[str, Depends(oauth2)]\n\n\nasync def get_current_user(session: SessionDep, token: TokenDep) -> User:\n    try:\n        payload  jwt.decode(\n            token, settings.SECRET_KEY, algorithms[settings.ALGORITHM]\n        )\n        token_data  TokenPayload(**payload)\n    except (JWTError, ValidationError):\n        raise HTTPException(\n            status_codestatus.HTTP_403_FORBIDDEN,\n            detail\"Could not validate credentials\",\n        )\n\n    user  session.get(User, token_data.sub)\n    if user is None:\n        raise HTTPException(status_code404, detail\"User not found\")\n    if not user.is_active:  # type: ignore\n        raise HTTPException(status_code400, detail\"Inactive user\")\n    return user\n\n\nCurrentUser  Annotated[User, Depends(get_current_user)]\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/models.py\n\nfrom sqlalchemy import Boolean, Column, ForeignKey, Integer, String\nfrom sqlalchemy.orm import relationship\n\nfrom src.core.db import Base\n\n\nclass User(Base):\n    __tablename__  \"users\"\n\n    id  Column(Integer, primary_keyTrue, indexTrue)\n    email  Column(String, uniqueTrue, indexTrue)\n    hashed_password  Column(String)\n    is_active  Column(Boolean, defaultTrue)\n\n    items  relationship(\"Item\", back_populates\"owner\")\n\n\nclass Item(Base):\n    __tablename__  \"items\"\n\n    id  Column(Integer, primary_keyTrue, indexTrue)\n    title  Column(String, indexTrue)\n    description  Column(String, indexTrue)\n    owner_id  Column(Integer, ForeignKey(\"users.id\"))\n\n    owner  relationship(\"User\", back_populates\"items\")\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/schemas.py\n\nfrom pydantic import BaseModel, EmailStr\n\n\nclass ItemBase(BaseModel):\n    title: str\n    description: str | None  None\n\n\nclass ItemCreate(ItemBase):\n    pass\n\n\nclass Item(ItemBase):\n    id: int\n    owner_id: int\n\n    class Config:\n        from_attributes  True\n\n\nclass UserBase(BaseModel):\n    email: EmailStr\n\n\nclass UserCreate(UserBase):\n    password: str\n\n\nclass User(UserBase):\n    id: int\n    is_active: bool\n    items: list[Item]  []\n\n    class Config:\n        from_attributes  True\n\n\nclass Token(BaseModel):\n    access_token: str\n    token_type: str  \"bearer\"\n\n\nclass TokenPayload(BaseModel):\n    sub: int | None  None\n\n\n06-restAPI-fastAPI-deploy/segundo/kaio-auth-fastapi-docker/src/core/security.py\n\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Any\n\nfrom jose import jwt\nfrom passlib.context import CryptContext\n\nfrom src.core.config import settings\n\npwd_context  CryptContext(schemes[\"bcrypt\"], deprecated\"auto\")\n\n\ndef create_access_token(subject: str | Any, expires_delta: timedelta):\n    expire  datetime.now(timezone.utc) + expires_delta\n    to_encode  {\"exp\": expire, \"sub\": str(subject)}\n    encoded_jwt  jwt.encode(\n        to_encode, settings.SECRET_KEY, algorithmsettings.ALGORITHM\n    )\n    return encoded_jwt\n\n\ndef verify_password(plain_password, hashed_password):\n    return pwd_context.verify(plain_password, hashed_password)\n\n\ndef get_password_hash(password):\n    return pwd_context.hash(password)\n\n\n06-restAPI-fastAPI-deploy/segundo/luciano-starter/README.md\n\n# api_aovivo\n\n\n06-restAPI-fastAPI-deploy/segundo/luciano-starter/main.py\n\nfrom fastapi import FastAPI\nimport random\n\nservidor  FastAPI()\n\n@servidor.get(\"/recursos\")\ndef numero_aleatorio():\n    num  random.randint(1,95)\n    print(num)\n    return num\n\n06-restAPI-fastAPI-deploy/segundo/luciano-starter/old.py\n\nimport random\n\ndef numero_aleatorio():\n    num  random.randint(1,95)\n    print(num)\n    return num\n\ndef dobra_o_numero(numero: int):\n    return numero * 2\n\ndef main():\n    num  numero_aleatorio()\n    return dobra_o_numero(num)\n\nif __name__  \"__main__\":\n    numero_dobrado  main()\n    print(numero_dobrado)\n\n06-restAPI-fastAPI-deploy/segundo/luciano-starter/programa_a.py\n\nimport random\nimport time\n\ndef numero_aleatorio():\n    num  random.randint(1,95)\n    print(num)\n    with open(\"recursos/arquivo.txt\", \"a\") as file:\n        file.write(f\"{num}\\n\")\n\n\nif __name__  \"__main__\":\n    while True:\n        numero_aleatorio()\n        time.sleep(1)\n\n06-restAPI-fastAPI-deploy/segundo/luciano-starter/programa_b.py\n\nimport time\n\ndef dobra_o_numero(numero: int):\n    return numero * 2\n\ndef le_o_ultimo_numero_do_arquivo():\n    with open(\"recursos/arquivo.txt\", \"r\") as file:\n        ultimo_numero  int(file.readlines()[-1])\n        return ultimo_numero\n\nif __name__  \"__main__\":\n    while True: \n        ultimo_numero  le_o_ultimo_numero_do_arquivo()\n        print(ultimo_numero)\n        numero_dobrado  dobra_o_numero(ultimo_numero)\n        print(numero_dobrado)\n        time.sleep(1)\n\n06-restAPI-fastAPI-deploy/segundo/luciano-starter/programa_c.py\n\nimport time\n\ndef dobra_o_numero(numero: int):\n    return numero * 2\n\ndef le_o_ultimo_numero_do_arquivo():\n    with open(\"recursos/arquivo.txt\", \"r\") as file:\n        ultimo_numero  int(file.readlines()[-1])\n        return ultimo_numero\n\nif __name__  \"__main__\":\n    while True: \n        ultimo_numero  le_o_ultimo_numero_do_arquivo()\n        print(ultimo_numero)\n        numero_dobrado  dobra_o_numero(ultimo_numero)\n        print(numero_dobrado)\n        time.sleep(1)\n\n06-restAPI-fastAPI-deploy/segundo/luciano-starter/programa_d.py\n\nimport random\nimport time\n\ndef numero_aleatorio():\n    num  random.randint(1,95)\n    print(num)\n    with open(\"recursos/arquivo.txt\", \"a\") as file:\n        file.write(f\"{num}\\n\")\n\n\nif __name__  \"__main__\":\n    while True:\n        numero_aleatorio()\n        time.sleep(1)\n\n06-restAPI-fastAPI-deploy/segundo/luciano-starter/requirements.txt\n\nannotated-types0.6.0\nanyio4.3.0\ncertifi2024.2.2\nclick8.1.7\ncolorama0.4.6\ndnspython2.6.1\nemail_validator2.1.1\nfastapi0.111.0\nfastapi-cli0.0.3\nh110.14.0\nhttpcore1.0.5\nhttptools0.6.1\nhttpx0.27.0\nidna3.7\nJinja23.1.4\nmarkdown-it-py3.0.0\nMarkupSafe2.1.5\nmdurl0.1.2\norjson3.10.3\npydantic2.7.1\npydantic_core2.18.2\nPygments2.18.0\npython-dotenv1.0.1\npython-multipart0.0.9\nPyYAML6.0.1\nrich13.7.1\nshellingham1.5.4\nsniffio1.3.1\nstarlette0.37.2\ntyper0.12.3\ntyping_extensions4.11.0\nujson5.9.0\nuvicorn0.29.0\nwatchfiles0.21.0\nwebsockets12.0\n\n\n06-restAPI-fastAPI-deploy/segundo/luciano-starter/recursos/arquivo.txt\n\n67\n41\n67\n15\n25\n94\n43\n93\n12\n21\n4\n26\n50\n91\n13\n64\n47\n69\n58\n83\n88\n58\n45\n1\n48\n72\n50\n53\n40\n75\n90\n18\n43\n89\n32\n29\n57\n66\n51\n64\n79\n61\n60\n32\n34\n65\n53\n12\n17\n45\n10\n44\n20\n94\n20\n76\n10\n46\n53\n19\n58\n17\n52\n30\n24\n94\n65\n34\n46\n74\n92\n9\n10\n13\n77\n77\n24\n79\n92\n40\n94\n52\n24\n73\n70\n72\n14\n77\n20\n85\n83\n63\n57\n13\n11\n58\n47\n4\n11\n79\n48\n36\n1\n31\n31\n78\n85\n29\n18\n76\n84\n63\n83\n2\n31\n67\n25\n45\n34\n80\n31\n46\n41\n25\n93\n17\n40\n85\n77\n86\n63\n61\n29\n39\n40\n34\n80\n15\n71\n3\n80\n63\n7\n12\n41\n59\n24\n39\n58\n68\n65\n86\n94\n53\n76\n51\n16\n17\n78\n34\n56\n20\n50\n38\n17\n83\n57\n62\n2\n43\n40\n6\n18\n67\n11\n10\n18\n24\n70\n69\n59\n40\n40\n85\n19\n92\n80\n91\n30\n54\n63\n36\n85\n58\n38\n44\n45\n34\n81\n38\n13\n65\n6\n20\n57\n61\n23\n63\n52\n58\n36\n64\n87\n86\n5\n88\n64\n63\n32\n34\n89\n43\n6\n64\n51\n83\n55\n81\n54\n49\n94\n3\n73\n75\n34\n34\n35\n32\n25\n87\n95\n13\n14\n49\n26\n86\n89\n21\n88\n92\n45\n37\n32\n68\n2\n30\n82\n86\n56\n23\n72\n85\n20\n44\n87\n21\n33\n83\n85\n44\n91\n64\n20\n72\n83\n42\n13\n54\n4\n72\n87\n29\n19\n74\n64\n23\n59\n25\n51\n45\n4\n46\n43\n63\n84\n52\n49\n51\n93\n20\n17\n68\n7\n74\n23\n79\n51\n22\n83\n88\n22\n30\n78\n34\n39\n43\n50\n39\n81\n10\n72\n81\n45\n81\n32\n51\n19\n45\n36\n70\n75\n17\n39\n89\n69\n53\n14\n86\n37\n41\n42\n50\n43\n51\n58\n6\n77\n13\n21\n80\n87\n53\n30\n24\n50\n49\n11\n69\n54\n63\n50\n78\n3\n11\n51\n9\n74\n21\n64\n29\n85\n55\n46\n8\n11\n93\n40\n33\n69\n25\n71\n13\n54\n32\n94\n91\n47\n47\n23\n33\n9\n41\n51\n26\n57\n32\n2\n58\n60\n68\n93\n37\n44\n51\n33\n81\n2\n56\n94\n84\n73\n68\n63\n2\n59\n30\n46\n81\n19\n40\n88\n89\n65\n1\n6\n25\n43\n41\n\n\n"
    ]
}